{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- This is the SCRIPT for testing QUANTIZED MOBILENET after RETRAINING -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include\n",
    "import argparse\n",
    "import os, imp, sys\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import copy, math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my script\n",
    "sys.path.append('../')\n",
    "from data import get_dataset, get_num_classes\n",
    "from preprocess import get_transform\n",
    "from utils import *\n",
    "import models\n",
    "import quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Network #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = '/home/fariselli/Work/training-mixed-precision-quantized-networks/results-nips/mobilenet_224_1.0_quant_auto_fix/checkpoint.pth.tar'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pretrained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'models.imagenet.mobilenet.mobilenet_quant_devel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'models.linear_quantized_modules.ScaledClippedLinearQuantizationChannel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.AvgPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(checkpoint_file):\n",
    "#            results.load(os.path.join(checkpoint_file, 'results.csv'))\n",
    "    checkpoint_file = os.path.join(\n",
    "        checkpoint_file, 'model_best.pth.tar')\n",
    "if os.path.isfile(checkpoint_file):\n",
    "    checkpoint_loaded = torch.load(checkpoint_file)\n",
    "    #checkpoint = checkpoint_loaded['state_dict']\n",
    "    #model.load_state_dict(checkpoint, strict=False)\n",
    "    print('Model pretrained' )\n",
    "else:\n",
    "    print('here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['config', 'regime', 'fold_type', 'state_dict', 'quantizer', 'model', 'epoch', 'add_config', 'best_prec1'])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_loaded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network model is: mobilenet\n"
     ]
    }
   ],
   "source": [
    "model_name = checkpoint_loaded['model']\n",
    "print(\"The network model is: {0}\" .format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PACT\n"
     ]
    }
   ],
   "source": [
    "model_config = checkpoint_loaded['config']\n",
    "activ_bits = model_config['activ_bits']\n",
    "activ_type = model_config['activ_type']\n",
    "dataset = model_config['dataset']\n",
    "input_dim = model_config['input_dim']\n",
    "input_size = input_dim\n",
    "num_classes = model_config['num_classes']\n",
    "type_quant = model_config['type_quant']\n",
    "weight_bits = model_config['weight_bits']\n",
    "width_mult = model_config['width_mult']\n",
    "additional_config = ''\n",
    "quant_add_config = checkpoint_loaded['add_config']\n",
    "fold_type = checkpoint_loaded['fold_type']\n",
    "\n",
    "print(type_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.354"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_loaded['best_prec1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n",
    "model = models.__dict__[model_name]\n",
    "\n",
    "nClasses = get_num_classes(dataset)\n",
    "model_config = {'input_size': input_size, 'dataset': dataset, 'num_classes': nClasses, \\\n",
    "                'type_quant': type_quant, 'weight_bits': weight_bits, 'activ_bits': activ_bits,\\\n",
    "                'activ_type': activ_type, 'width_mult': width_mult, 'input_dim': input_size }\n",
    "\n",
    "\n",
    "\n",
    "if additional_config is not '':\n",
    "    model_config = dict(model_config, **literal_eval(additional_config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activ_bits': 8,\n",
       " 'activ_type': 'learned',\n",
       " 'dataset': 'imagenet',\n",
       " 'input_dim': 224.0,\n",
       " 'input_size': 224.0,\n",
       " 'num_classes': 1000,\n",
       " 'type_quant': 'PACT',\n",
       " 'weight_bits': 8,\n",
       " 'width_mult': 1.0}"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = checkpoint_loaded['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer_load = checkpoint_loaded['quantizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a_bits': 4,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 0,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 4,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 1,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 2,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 2,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 3,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 4,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 4,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 4,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 5,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 4,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 6,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 7,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 8,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 9,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 10,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 11,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 12,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 13,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 14,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 4},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 15,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 16,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 4},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 17,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 18,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 4},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 19,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 20,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 4},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 21,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 22,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 23,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 24,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 4},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 25,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'fixed_batch',\n",
       "  'layer': 26,\n",
       "  'quant_type': 'PACT_CHANNEL',\n",
       "  'w_bits': 2},\n",
       " {'layer': 27, 'quant_type': 'PACT_CHANNEL', 'w_bits': 2}]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_add_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax folding_weights\n"
     ]
    }
   ],
   "source": [
    "# update naming\n",
    "def change_quant_type(t):\n",
    "    if t == 'PACT':\n",
    "        return 'PerLayerAsymPACT'\n",
    "    elif t == 'PACT_CHANNEL':\n",
    "        return 'PerChannelsAsymMinMax'\n",
    "    return t\n",
    "    \n",
    "def change_fold_type(t):\n",
    "    if t == 'fixed_batch':\n",
    "        return 'ICN'\n",
    "    elif t == 'folding_weights':\n",
    "        return 'folding_weights'\n",
    "    return t\n",
    "\n",
    "for item in quantizer_load.param_to_quantize:\n",
    "    item['quant_type'] = change_quant_type(item['quant_type'])\n",
    "    item['fold_type'] = change_fold_type(item['fold_type'])\n",
    "    print(item['quant_type'], item['fold_type'])\n",
    "\n",
    "model_config['type_quant'] = change_quant_type(model_config['type_quant'])\n",
    "\n",
    "\n",
    "type_quant = model_config['type_quant']\n",
    "fold_type = checkpoint_loaded['fold_type']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n"
     ]
    }
   ],
   "source": [
    "for item in quant_add_config:\n",
    "    if 'quant_type' in item : \n",
    "        item['quant_type'] = change_quant_type(item['quant_type'])\n",
    "    if 'fold_type' in item:\n",
    "        item['fold_type'] = change_fold_type(item['fold_type'])\n",
    "    if 'quant_type' in item and 'fold_type' in item: \n",
    "        print(item['quant_type'], item['fold_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax ICN\n",
      "PerChannelsAsymMinMax folding_weights\n"
     ]
    }
   ],
   "source": [
    "param_list = copy.deepcopy(quantizer_load.param_to_quantize)\n",
    "for item in param_list:\n",
    "    print(item['quant_type'], item['fold_type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  ) In Act bits - Out Act bits - Weights bits - Quantization - Batch norm\n",
      "Layer 1) 8 - 4 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 2) 4 - 4 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 3) 4 - 2 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 4) 2 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 5) 8 - 4 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 6) 4 - 4 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 7) 4 - 4 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 8) 4 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 9) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 10) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 11) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 12) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 13) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 14) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 15) 8 - 8 - 4 - PerChannelsAsymMinMax - ICN\n",
      "Layer 16) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 17) 8 - 8 - 4 - PerChannelsAsymMinMax - ICN\n",
      "Layer 18) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 19) 8 - 8 - 4 - PerChannelsAsymMinMax - ICN\n",
      "Layer 20) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 21) 8 - 8 - 4 - PerChannelsAsymMinMax - ICN\n",
      "Layer 22) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 23) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 24) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 25) 8 - 8 - 4 - PerChannelsAsymMinMax - ICN\n",
      "Layer 26) 8 - 8 - 8 - PerChannelsAsymMinMax - ICN\n",
      "Layer 27) 8 - 8 - 2 - PerChannelsAsymMinMax - ICN\n"
     ]
    }
   ],
   "source": [
    "inp = 8\n",
    "out = 8\n",
    "wt = 8\n",
    "nlayer = len(param_list)-1\n",
    "lay=1\n",
    "print('Layer  ) In Act bits - Out Act bits - Weights bits - Quantization - Batch norm')\n",
    "for item in param_list:\n",
    "    if lay <= nlayer:\n",
    "        out = item['act_o_bits']\n",
    "        wt = item['w_bits']\n",
    "        print('Layer '+str(lay)+')',inp, '-', out, '-', wt, '-', item['quant_type'], '-', item['fold_type'])\n",
    "        inp = out\n",
    "        lay+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'act', 'fold_type', 'act_o_bits', 'min_clip', 'w_min_thr', 'w_clip', 'max_clip', 'bias_bits', 'quant_conv', 'w_min_mat', 'quant_batch', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'name', 'batch_norm'])\n",
      "dict_keys(['w_max_thr', 'w_bits', 'fold_type', 'w_min_thr', 'bias_bits', 'act', 'w_clip', 'quant_conv', 'w_min_mat', 'name', 'w_max_mat', 'conv', 'weight', 'bias', 'quant_act', 'quant_type', 'batch_norm'])\n"
     ]
    }
   ],
   "source": [
    "for item in quantizer_load.param_to_quantize:\n",
    "    print(item.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'act': LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       " tensor([6.5764], device='cuda:0', requires_grad=True), inplace),\n",
       " 'act_o_bits': 4,\n",
       " 'batch_norm': BatchNorm2d(32, eps=0.001, momentum=0.003, affine=True, track_running_stats=True),\n",
       " 'bias': False,\n",
       " 'bias_bits': 32,\n",
       " 'conv': Conv2d_SAME(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
       " 'fold_type': 'ICN',\n",
       " 'max_clip': 1.0,\n",
       " 'min_clip': -1.0,\n",
       " 'name': '0.0',\n",
       " 'quant_act': ScaledClippedLinearQuantizationChannel(clip_val=15, inplace),\n",
       " 'quant_batch': None,\n",
       " 'quant_conv': Conv2d_SAME(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
       " 'quant_type': 'PerChannelsAsymMinMax',\n",
       " 'w_bits': 8,\n",
       " 'w_clip': None,\n",
       " 'w_max_mat': tensor([[[[ 5.1723e-05,  5.1723e-05,  5.1723e-05],\n",
       "           [ 5.1723e-05,  5.1723e-05,  5.1723e-05],\n",
       "           [ 5.1723e-05,  5.1723e-05,  5.1723e-05]],\n",
       " \n",
       "          [[ 5.1723e-05,  5.1723e-05,  5.1723e-05],\n",
       "           [ 5.1723e-05,  5.1723e-05,  5.1723e-05],\n",
       "           [ 5.1723e-05,  5.1723e-05,  5.1723e-05]],\n",
       " \n",
       "          [[ 5.1723e-05,  5.1723e-05,  5.1723e-05],\n",
       "           [ 5.1723e-05,  5.1723e-05,  5.1723e-05],\n",
       "           [ 5.1723e-05,  5.1723e-05,  5.1723e-05]]],\n",
       " \n",
       " \n",
       "         [[[ 8.6608e-02,  8.6608e-02,  8.6608e-02],\n",
       "           [ 8.6608e-02,  8.6608e-02,  8.6608e-02],\n",
       "           [ 8.6608e-02,  8.6608e-02,  8.6608e-02]],\n",
       " \n",
       "          [[ 8.6608e-02,  8.6608e-02,  8.6608e-02],\n",
       "           [ 8.6608e-02,  8.6608e-02,  8.6608e-02],\n",
       "           [ 8.6608e-02,  8.6608e-02,  8.6608e-02]],\n",
       " \n",
       "          [[ 8.6608e-02,  8.6608e-02,  8.6608e-02],\n",
       "           [ 8.6608e-02,  8.6608e-02,  8.6608e-02],\n",
       "           [ 8.6608e-02,  8.6608e-02,  8.6608e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 7.1065e-01,  7.1065e-01,  7.1065e-01],\n",
       "           [ 7.1065e-01,  7.1065e-01,  7.1065e-01],\n",
       "           [ 7.1065e-01,  7.1065e-01,  7.1065e-01]],\n",
       " \n",
       "          [[ 7.1065e-01,  7.1065e-01,  7.1065e-01],\n",
       "           [ 7.1065e-01,  7.1065e-01,  7.1065e-01],\n",
       "           [ 7.1065e-01,  7.1065e-01,  7.1065e-01]],\n",
       " \n",
       "          [[ 7.1065e-01,  7.1065e-01,  7.1065e-01],\n",
       "           [ 7.1065e-01,  7.1065e-01,  7.1065e-01],\n",
       "           [ 7.1065e-01,  7.1065e-01,  7.1065e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 4.2164e-05,  4.2164e-05,  4.2164e-05],\n",
       "           [ 4.2164e-05,  4.2164e-05,  4.2164e-05],\n",
       "           [ 4.2164e-05,  4.2164e-05,  4.2164e-05]],\n",
       " \n",
       "          [[ 4.2164e-05,  4.2164e-05,  4.2164e-05],\n",
       "           [ 4.2164e-05,  4.2164e-05,  4.2164e-05],\n",
       "           [ 4.2164e-05,  4.2164e-05,  4.2164e-05]],\n",
       " \n",
       "          [[ 4.2164e-05,  4.2164e-05,  4.2164e-05],\n",
       "           [ 4.2164e-05,  4.2164e-05,  4.2164e-05],\n",
       "           [ 4.2164e-05,  4.2164e-05,  4.2164e-05]]],\n",
       " \n",
       " \n",
       "         [[[ 4.7009e-01,  4.7009e-01,  4.7009e-01],\n",
       "           [ 4.7009e-01,  4.7009e-01,  4.7009e-01],\n",
       "           [ 4.7009e-01,  4.7009e-01,  4.7009e-01]],\n",
       " \n",
       "          [[ 4.7009e-01,  4.7009e-01,  4.7009e-01],\n",
       "           [ 4.7009e-01,  4.7009e-01,  4.7009e-01],\n",
       "           [ 4.7009e-01,  4.7009e-01,  4.7009e-01]],\n",
       " \n",
       "          [[ 4.7009e-01,  4.7009e-01,  4.7009e-01],\n",
       "           [ 4.7009e-01,  4.7009e-01,  4.7009e-01],\n",
       "           [ 4.7009e-01,  4.7009e-01,  4.7009e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 5.9080e-01,  5.9080e-01,  5.9080e-01],\n",
       "           [ 5.9080e-01,  5.9080e-01,  5.9080e-01],\n",
       "           [ 5.9080e-01,  5.9080e-01,  5.9080e-01]],\n",
       " \n",
       "          [[ 5.9080e-01,  5.9080e-01,  5.9080e-01],\n",
       "           [ 5.9080e-01,  5.9080e-01,  5.9080e-01],\n",
       "           [ 5.9080e-01,  5.9080e-01,  5.9080e-01]],\n",
       " \n",
       "          [[ 5.9080e-01,  5.9080e-01,  5.9080e-01],\n",
       "           [ 5.9080e-01,  5.9080e-01,  5.9080e-01],\n",
       "           [ 5.9080e-01,  5.9080e-01,  5.9080e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.3910e-02,  1.3910e-02,  1.3910e-02],\n",
       "           [ 1.3910e-02,  1.3910e-02,  1.3910e-02],\n",
       "           [ 1.3910e-02,  1.3910e-02,  1.3910e-02]],\n",
       " \n",
       "          [[ 1.3910e-02,  1.3910e-02,  1.3910e-02],\n",
       "           [ 1.3910e-02,  1.3910e-02,  1.3910e-02],\n",
       "           [ 1.3910e-02,  1.3910e-02,  1.3910e-02]],\n",
       " \n",
       "          [[ 1.3910e-02,  1.3910e-02,  1.3910e-02],\n",
       "           [ 1.3910e-02,  1.3910e-02,  1.3910e-02],\n",
       "           [ 1.3910e-02,  1.3910e-02,  1.3910e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.3742e+00,  1.3742e+00,  1.3742e+00],\n",
       "           [ 1.3742e+00,  1.3742e+00,  1.3742e+00],\n",
       "           [ 1.3742e+00,  1.3742e+00,  1.3742e+00]],\n",
       " \n",
       "          [[ 1.3742e+00,  1.3742e+00,  1.3742e+00],\n",
       "           [ 1.3742e+00,  1.3742e+00,  1.3742e+00],\n",
       "           [ 1.3742e+00,  1.3742e+00,  1.3742e+00]],\n",
       " \n",
       "          [[ 1.3742e+00,  1.3742e+00,  1.3742e+00],\n",
       "           [ 1.3742e+00,  1.3742e+00,  1.3742e+00],\n",
       "           [ 1.3742e+00,  1.3742e+00,  1.3742e+00]]],\n",
       " \n",
       " \n",
       "         [[[-6.3963e-06, -6.3963e-06, -6.3963e-06],\n",
       "           [-6.3963e-06, -6.3963e-06, -6.3963e-06],\n",
       "           [-6.3963e-06, -6.3963e-06, -6.3963e-06]],\n",
       " \n",
       "          [[-6.3963e-06, -6.3963e-06, -6.3963e-06],\n",
       "           [-6.3963e-06, -6.3963e-06, -6.3963e-06],\n",
       "           [-6.3963e-06, -6.3963e-06, -6.3963e-06]],\n",
       " \n",
       "          [[-6.3963e-06, -6.3963e-06, -6.3963e-06],\n",
       "           [-6.3963e-06, -6.3963e-06, -6.3963e-06],\n",
       "           [-6.3963e-06, -6.3963e-06, -6.3963e-06]]],\n",
       " \n",
       " \n",
       "         [[[ 7.9340e-05,  7.9340e-05,  7.9340e-05],\n",
       "           [ 7.9340e-05,  7.9340e-05,  7.9340e-05],\n",
       "           [ 7.9340e-05,  7.9340e-05,  7.9340e-05]],\n",
       " \n",
       "          [[ 7.9340e-05,  7.9340e-05,  7.9340e-05],\n",
       "           [ 7.9340e-05,  7.9340e-05,  7.9340e-05],\n",
       "           [ 7.9340e-05,  7.9340e-05,  7.9340e-05]],\n",
       " \n",
       "          [[ 7.9340e-05,  7.9340e-05,  7.9340e-05],\n",
       "           [ 7.9340e-05,  7.9340e-05,  7.9340e-05],\n",
       "           [ 7.9340e-05,  7.9340e-05,  7.9340e-05]]],\n",
       " \n",
       " \n",
       "         [[[ 5.4609e-05,  5.4609e-05,  5.4609e-05],\n",
       "           [ 5.4609e-05,  5.4609e-05,  5.4609e-05],\n",
       "           [ 5.4609e-05,  5.4609e-05,  5.4609e-05]],\n",
       " \n",
       "          [[ 5.4609e-05,  5.4609e-05,  5.4609e-05],\n",
       "           [ 5.4609e-05,  5.4609e-05,  5.4609e-05],\n",
       "           [ 5.4609e-05,  5.4609e-05,  5.4609e-05]],\n",
       " \n",
       "          [[ 5.4609e-05,  5.4609e-05,  5.4609e-05],\n",
       "           [ 5.4609e-05,  5.4609e-05,  5.4609e-05],\n",
       "           [ 5.4609e-05,  5.4609e-05,  5.4609e-05]]],\n",
       " \n",
       " \n",
       "         [[[ 8.7137e-01,  8.7137e-01,  8.7137e-01],\n",
       "           [ 8.7137e-01,  8.7137e-01,  8.7137e-01],\n",
       "           [ 8.7137e-01,  8.7137e-01,  8.7137e-01]],\n",
       " \n",
       "          [[ 8.7137e-01,  8.7137e-01,  8.7137e-01],\n",
       "           [ 8.7137e-01,  8.7137e-01,  8.7137e-01],\n",
       "           [ 8.7137e-01,  8.7137e-01,  8.7137e-01]],\n",
       " \n",
       "          [[ 8.7137e-01,  8.7137e-01,  8.7137e-01],\n",
       "           [ 8.7137e-01,  8.7137e-01,  8.7137e-01],\n",
       "           [ 8.7137e-01,  8.7137e-01,  8.7137e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 3.9593e-01,  3.9593e-01,  3.9593e-01],\n",
       "           [ 3.9593e-01,  3.9593e-01,  3.9593e-01],\n",
       "           [ 3.9593e-01,  3.9593e-01,  3.9593e-01]],\n",
       " \n",
       "          [[ 3.9593e-01,  3.9593e-01,  3.9593e-01],\n",
       "           [ 3.9593e-01,  3.9593e-01,  3.9593e-01],\n",
       "           [ 3.9593e-01,  3.9593e-01,  3.9593e-01]],\n",
       " \n",
       "          [[ 3.9593e-01,  3.9593e-01,  3.9593e-01],\n",
       "           [ 3.9593e-01,  3.9593e-01,  3.9593e-01],\n",
       "           [ 3.9593e-01,  3.9593e-01,  3.9593e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 6.8521e-01,  6.8521e-01,  6.8521e-01],\n",
       "           [ 6.8521e-01,  6.8521e-01,  6.8521e-01],\n",
       "           [ 6.8521e-01,  6.8521e-01,  6.8521e-01]],\n",
       " \n",
       "          [[ 6.8521e-01,  6.8521e-01,  6.8521e-01],\n",
       "           [ 6.8521e-01,  6.8521e-01,  6.8521e-01],\n",
       "           [ 6.8521e-01,  6.8521e-01,  6.8521e-01]],\n",
       " \n",
       "          [[ 6.8521e-01,  6.8521e-01,  6.8521e-01],\n",
       "           [ 6.8521e-01,  6.8521e-01,  6.8521e-01],\n",
       "           [ 6.8521e-01,  6.8521e-01,  6.8521e-01]]],\n",
       " \n",
       " \n",
       "         [[[-8.9124e-05, -8.9124e-05, -8.9124e-05],\n",
       "           [-8.9124e-05, -8.9124e-05, -8.9124e-05],\n",
       "           [-8.9124e-05, -8.9124e-05, -8.9124e-05]],\n",
       " \n",
       "          [[-8.9124e-05, -8.9124e-05, -8.9124e-05],\n",
       "           [-8.9124e-05, -8.9124e-05, -8.9124e-05],\n",
       "           [-8.9124e-05, -8.9124e-05, -8.9124e-05]],\n",
       " \n",
       "          [[-8.9124e-05, -8.9124e-05, -8.9124e-05],\n",
       "           [-8.9124e-05, -8.9124e-05, -8.9124e-05],\n",
       "           [-8.9124e-05, -8.9124e-05, -8.9124e-05]]],\n",
       " \n",
       " \n",
       "         [[[ 3.7535e-01,  3.7535e-01,  3.7535e-01],\n",
       "           [ 3.7535e-01,  3.7535e-01,  3.7535e-01],\n",
       "           [ 3.7535e-01,  3.7535e-01,  3.7535e-01]],\n",
       " \n",
       "          [[ 3.7535e-01,  3.7535e-01,  3.7535e-01],\n",
       "           [ 3.7535e-01,  3.7535e-01,  3.7535e-01],\n",
       "           [ 3.7535e-01,  3.7535e-01,  3.7535e-01]],\n",
       " \n",
       "          [[ 3.7535e-01,  3.7535e-01,  3.7535e-01],\n",
       "           [ 3.7535e-01,  3.7535e-01,  3.7535e-01],\n",
       "           [ 3.7535e-01,  3.7535e-01,  3.7535e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 6.2291e-01,  6.2291e-01,  6.2291e-01],\n",
       "           [ 6.2291e-01,  6.2291e-01,  6.2291e-01],\n",
       "           [ 6.2291e-01,  6.2291e-01,  6.2291e-01]],\n",
       " \n",
       "          [[ 6.2291e-01,  6.2291e-01,  6.2291e-01],\n",
       "           [ 6.2291e-01,  6.2291e-01,  6.2291e-01],\n",
       "           [ 6.2291e-01,  6.2291e-01,  6.2291e-01]],\n",
       " \n",
       "          [[ 6.2291e-01,  6.2291e-01,  6.2291e-01],\n",
       "           [ 6.2291e-01,  6.2291e-01,  6.2291e-01],\n",
       "           [ 6.2291e-01,  6.2291e-01,  6.2291e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0539e+00,  1.0539e+00,  1.0539e+00],\n",
       "           [ 1.0539e+00,  1.0539e+00,  1.0539e+00],\n",
       "           [ 1.0539e+00,  1.0539e+00,  1.0539e+00]],\n",
       " \n",
       "          [[ 1.0539e+00,  1.0539e+00,  1.0539e+00],\n",
       "           [ 1.0539e+00,  1.0539e+00,  1.0539e+00],\n",
       "           [ 1.0539e+00,  1.0539e+00,  1.0539e+00]],\n",
       " \n",
       "          [[ 1.0539e+00,  1.0539e+00,  1.0539e+00],\n",
       "           [ 1.0539e+00,  1.0539e+00,  1.0539e+00],\n",
       "           [ 1.0539e+00,  1.0539e+00,  1.0539e+00]]],\n",
       " \n",
       " \n",
       "         [[[ 4.5344e-01,  4.5344e-01,  4.5344e-01],\n",
       "           [ 4.5344e-01,  4.5344e-01,  4.5344e-01],\n",
       "           [ 4.5344e-01,  4.5344e-01,  4.5344e-01]],\n",
       " \n",
       "          [[ 4.5344e-01,  4.5344e-01,  4.5344e-01],\n",
       "           [ 4.5344e-01,  4.5344e-01,  4.5344e-01],\n",
       "           [ 4.5344e-01,  4.5344e-01,  4.5344e-01]],\n",
       " \n",
       "          [[ 4.5344e-01,  4.5344e-01,  4.5344e-01],\n",
       "           [ 4.5344e-01,  4.5344e-01,  4.5344e-01],\n",
       "           [ 4.5344e-01,  4.5344e-01,  4.5344e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 7.9825e-01,  7.9825e-01,  7.9825e-01],\n",
       "           [ 7.9825e-01,  7.9825e-01,  7.9825e-01],\n",
       "           [ 7.9825e-01,  7.9825e-01,  7.9825e-01]],\n",
       " \n",
       "          [[ 7.9825e-01,  7.9825e-01,  7.9825e-01],\n",
       "           [ 7.9825e-01,  7.9825e-01,  7.9825e-01],\n",
       "           [ 7.9825e-01,  7.9825e-01,  7.9825e-01]],\n",
       " \n",
       "          [[ 7.9825e-01,  7.9825e-01,  7.9825e-01],\n",
       "           [ 7.9825e-01,  7.9825e-01,  7.9825e-01],\n",
       "           [ 7.9825e-01,  7.9825e-01,  7.9825e-01]]],\n",
       " \n",
       " \n",
       "         [[[-6.9145e-05, -6.9145e-05, -6.9145e-05],\n",
       "           [-6.9145e-05, -6.9145e-05, -6.9145e-05],\n",
       "           [-6.9145e-05, -6.9145e-05, -6.9145e-05]],\n",
       " \n",
       "          [[-6.9145e-05, -6.9145e-05, -6.9145e-05],\n",
       "           [-6.9145e-05, -6.9145e-05, -6.9145e-05],\n",
       "           [-6.9145e-05, -6.9145e-05, -6.9145e-05]],\n",
       " \n",
       "          [[-6.9145e-05, -6.9145e-05, -6.9145e-05],\n",
       "           [-6.9145e-05, -6.9145e-05, -6.9145e-05],\n",
       "           [-6.9145e-05, -6.9145e-05, -6.9145e-05]]],\n",
       " \n",
       " \n",
       "         [[[ 4.8678e-01,  4.8678e-01,  4.8678e-01],\n",
       "           [ 4.8678e-01,  4.8678e-01,  4.8678e-01],\n",
       "           [ 4.8678e-01,  4.8678e-01,  4.8678e-01]],\n",
       " \n",
       "          [[ 4.8678e-01,  4.8678e-01,  4.8678e-01],\n",
       "           [ 4.8678e-01,  4.8678e-01,  4.8678e-01],\n",
       "           [ 4.8678e-01,  4.8678e-01,  4.8678e-01]],\n",
       " \n",
       "          [[ 4.8678e-01,  4.8678e-01,  4.8678e-01],\n",
       "           [ 4.8678e-01,  4.8678e-01,  4.8678e-01],\n",
       "           [ 4.8678e-01,  4.8678e-01,  4.8678e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.9644e-05,  1.9644e-05,  1.9644e-05],\n",
       "           [ 1.9644e-05,  1.9644e-05,  1.9644e-05],\n",
       "           [ 1.9644e-05,  1.9644e-05,  1.9644e-05]],\n",
       " \n",
       "          [[ 1.9644e-05,  1.9644e-05,  1.9644e-05],\n",
       "           [ 1.9644e-05,  1.9644e-05,  1.9644e-05],\n",
       "           [ 1.9644e-05,  1.9644e-05,  1.9644e-05]],\n",
       " \n",
       "          [[ 1.9644e-05,  1.9644e-05,  1.9644e-05],\n",
       "           [ 1.9644e-05,  1.9644e-05,  1.9644e-05],\n",
       "           [ 1.9644e-05,  1.9644e-05,  1.9644e-05]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5480e-01,  1.5480e-01,  1.5480e-01],\n",
       "           [ 1.5480e-01,  1.5480e-01,  1.5480e-01],\n",
       "           [ 1.5480e-01,  1.5480e-01,  1.5480e-01]],\n",
       " \n",
       "          [[ 1.5480e-01,  1.5480e-01,  1.5480e-01],\n",
       "           [ 1.5480e-01,  1.5480e-01,  1.5480e-01],\n",
       "           [ 1.5480e-01,  1.5480e-01,  1.5480e-01]],\n",
       " \n",
       "          [[ 1.5480e-01,  1.5480e-01,  1.5480e-01],\n",
       "           [ 1.5480e-01,  1.5480e-01,  1.5480e-01],\n",
       "           [ 1.5480e-01,  1.5480e-01,  1.5480e-01]]],\n",
       " \n",
       " \n",
       "         [[[-3.9751e-06, -3.9751e-06, -3.9751e-06],\n",
       "           [-3.9751e-06, -3.9751e-06, -3.9751e-06],\n",
       "           [-3.9751e-06, -3.9751e-06, -3.9751e-06]],\n",
       " \n",
       "          [[-3.9751e-06, -3.9751e-06, -3.9751e-06],\n",
       "           [-3.9751e-06, -3.9751e-06, -3.9751e-06],\n",
       "           [-3.9751e-06, -3.9751e-06, -3.9751e-06]],\n",
       " \n",
       "          [[-3.9751e-06, -3.9751e-06, -3.9751e-06],\n",
       "           [-3.9751e-06, -3.9751e-06, -3.9751e-06],\n",
       "           [-3.9751e-06, -3.9751e-06, -3.9751e-06]]],\n",
       " \n",
       " \n",
       "         [[[ 3.1657e-01,  3.1657e-01,  3.1657e-01],\n",
       "           [ 3.1657e-01,  3.1657e-01,  3.1657e-01],\n",
       "           [ 3.1657e-01,  3.1657e-01,  3.1657e-01]],\n",
       " \n",
       "          [[ 3.1657e-01,  3.1657e-01,  3.1657e-01],\n",
       "           [ 3.1657e-01,  3.1657e-01,  3.1657e-01],\n",
       "           [ 3.1657e-01,  3.1657e-01,  3.1657e-01]],\n",
       " \n",
       "          [[ 3.1657e-01,  3.1657e-01,  3.1657e-01],\n",
       "           [ 3.1657e-01,  3.1657e-01,  3.1657e-01],\n",
       "           [ 3.1657e-01,  3.1657e-01,  3.1657e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.8660e-01,  2.8660e-01,  2.8660e-01],\n",
       "           [ 2.8660e-01,  2.8660e-01,  2.8660e-01],\n",
       "           [ 2.8660e-01,  2.8660e-01,  2.8660e-01]],\n",
       " \n",
       "          [[ 2.8660e-01,  2.8660e-01,  2.8660e-01],\n",
       "           [ 2.8660e-01,  2.8660e-01,  2.8660e-01],\n",
       "           [ 2.8660e-01,  2.8660e-01,  2.8660e-01]],\n",
       " \n",
       "          [[ 2.8660e-01,  2.8660e-01,  2.8660e-01],\n",
       "           [ 2.8660e-01,  2.8660e-01,  2.8660e-01],\n",
       "           [ 2.8660e-01,  2.8660e-01,  2.8660e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 5.1152e-05,  5.1152e-05,  5.1152e-05],\n",
       "           [ 5.1152e-05,  5.1152e-05,  5.1152e-05],\n",
       "           [ 5.1152e-05,  5.1152e-05,  5.1152e-05]],\n",
       " \n",
       "          [[ 5.1152e-05,  5.1152e-05,  5.1152e-05],\n",
       "           [ 5.1152e-05,  5.1152e-05,  5.1152e-05],\n",
       "           [ 5.1152e-05,  5.1152e-05,  5.1152e-05]],\n",
       " \n",
       "          [[ 5.1152e-05,  5.1152e-05,  5.1152e-05],\n",
       "           [ 5.1152e-05,  5.1152e-05,  5.1152e-05],\n",
       "           [ 5.1152e-05,  5.1152e-05,  5.1152e-05]]],\n",
       " \n",
       " \n",
       "         [[[ 5.1064e-01,  5.1064e-01,  5.1064e-01],\n",
       "           [ 5.1064e-01,  5.1064e-01,  5.1064e-01],\n",
       "           [ 5.1064e-01,  5.1064e-01,  5.1064e-01]],\n",
       " \n",
       "          [[ 5.1064e-01,  5.1064e-01,  5.1064e-01],\n",
       "           [ 5.1064e-01,  5.1064e-01,  5.1064e-01],\n",
       "           [ 5.1064e-01,  5.1064e-01,  5.1064e-01]],\n",
       " \n",
       "          [[ 5.1064e-01,  5.1064e-01,  5.1064e-01],\n",
       "           [ 5.1064e-01,  5.1064e-01,  5.1064e-01],\n",
       "           [ 5.1064e-01,  5.1064e-01,  5.1064e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.7877e-01,  1.7877e-01,  1.7877e-01],\n",
       "           [ 1.7877e-01,  1.7877e-01,  1.7877e-01],\n",
       "           [ 1.7877e-01,  1.7877e-01,  1.7877e-01]],\n",
       " \n",
       "          [[ 1.7877e-01,  1.7877e-01,  1.7877e-01],\n",
       "           [ 1.7877e-01,  1.7877e-01,  1.7877e-01],\n",
       "           [ 1.7877e-01,  1.7877e-01,  1.7877e-01]],\n",
       " \n",
       "          [[ 1.7877e-01,  1.7877e-01,  1.7877e-01],\n",
       "           [ 1.7877e-01,  1.7877e-01,  1.7877e-01],\n",
       "           [ 1.7877e-01,  1.7877e-01,  1.7877e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 4.2133e-05,  4.2133e-05,  4.2133e-05],\n",
       "           [ 4.2133e-05,  4.2133e-05,  4.2133e-05],\n",
       "           [ 4.2133e-05,  4.2133e-05,  4.2133e-05]],\n",
       " \n",
       "          [[ 4.2133e-05,  4.2133e-05,  4.2133e-05],\n",
       "           [ 4.2133e-05,  4.2133e-05,  4.2133e-05],\n",
       "           [ 4.2133e-05,  4.2133e-05,  4.2133e-05]],\n",
       " \n",
       "          [[ 4.2133e-05,  4.2133e-05,  4.2133e-05],\n",
       "           [ 4.2133e-05,  4.2133e-05,  4.2133e-05],\n",
       "           [ 4.2133e-05,  4.2133e-05,  4.2133e-05]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0055e+00,  1.0055e+00,  1.0055e+00],\n",
       "           [ 1.0055e+00,  1.0055e+00,  1.0055e+00],\n",
       "           [ 1.0055e+00,  1.0055e+00,  1.0055e+00]],\n",
       " \n",
       "          [[ 1.0055e+00,  1.0055e+00,  1.0055e+00],\n",
       "           [ 1.0055e+00,  1.0055e+00,  1.0055e+00],\n",
       "           [ 1.0055e+00,  1.0055e+00,  1.0055e+00]],\n",
       " \n",
       "          [[ 1.0055e+00,  1.0055e+00,  1.0055e+00],\n",
       "           [ 1.0055e+00,  1.0055e+00,  1.0055e+00],\n",
       "           [ 1.0055e+00,  1.0055e+00,  1.0055e+00]]]], device='cuda:0'),\n",
       " 'w_max_thr': Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True),\n",
       " 'w_min_mat': tensor([[[[-9.7084e-05, -9.7084e-05, -9.7084e-05],\n",
       "           [-9.7084e-05, -9.7084e-05, -9.7084e-05],\n",
       "           [-9.7084e-05, -9.7084e-05, -9.7084e-05]],\n",
       " \n",
       "          [[-9.7084e-05, -9.7084e-05, -9.7084e-05],\n",
       "           [-9.7084e-05, -9.7084e-05, -9.7084e-05],\n",
       "           [-9.7084e-05, -9.7084e-05, -9.7084e-05]],\n",
       " \n",
       "          [[-9.7084e-05, -9.7084e-05, -9.7084e-05],\n",
       "           [-9.7084e-05, -9.7084e-05, -9.7084e-05],\n",
       "           [-9.7084e-05, -9.7084e-05, -9.7084e-05]]],\n",
       " \n",
       " \n",
       "         [[[-8.1256e-01, -8.1256e-01, -8.1256e-01],\n",
       "           [-8.1256e-01, -8.1256e-01, -8.1256e-01],\n",
       "           [-8.1256e-01, -8.1256e-01, -8.1256e-01]],\n",
       " \n",
       "          [[-8.1256e-01, -8.1256e-01, -8.1256e-01],\n",
       "           [-8.1256e-01, -8.1256e-01, -8.1256e-01],\n",
       "           [-8.1256e-01, -8.1256e-01, -8.1256e-01]],\n",
       " \n",
       "          [[-8.1256e-01, -8.1256e-01, -8.1256e-01],\n",
       "           [-8.1256e-01, -8.1256e-01, -8.1256e-01],\n",
       "           [-8.1256e-01, -8.1256e-01, -8.1256e-01]]],\n",
       " \n",
       " \n",
       "         [[[-3.5124e-01, -3.5124e-01, -3.5124e-01],\n",
       "           [-3.5124e-01, -3.5124e-01, -3.5124e-01],\n",
       "           [-3.5124e-01, -3.5124e-01, -3.5124e-01]],\n",
       " \n",
       "          [[-3.5124e-01, -3.5124e-01, -3.5124e-01],\n",
       "           [-3.5124e-01, -3.5124e-01, -3.5124e-01],\n",
       "           [-3.5124e-01, -3.5124e-01, -3.5124e-01]],\n",
       " \n",
       "          [[-3.5124e-01, -3.5124e-01, -3.5124e-01],\n",
       "           [-3.5124e-01, -3.5124e-01, -3.5124e-01],\n",
       "           [-3.5124e-01, -3.5124e-01, -3.5124e-01]]],\n",
       " \n",
       " \n",
       "         [[[-4.3474e-05, -4.3474e-05, -4.3474e-05],\n",
       "           [-4.3474e-05, -4.3474e-05, -4.3474e-05],\n",
       "           [-4.3474e-05, -4.3474e-05, -4.3474e-05]],\n",
       " \n",
       "          [[-4.3474e-05, -4.3474e-05, -4.3474e-05],\n",
       "           [-4.3474e-05, -4.3474e-05, -4.3474e-05],\n",
       "           [-4.3474e-05, -4.3474e-05, -4.3474e-05]],\n",
       " \n",
       "          [[-4.3474e-05, -4.3474e-05, -4.3474e-05],\n",
       "           [-4.3474e-05, -4.3474e-05, -4.3474e-05],\n",
       "           [-4.3474e-05, -4.3474e-05, -4.3474e-05]]],\n",
       " \n",
       " \n",
       "         [[[-8.7910e-01, -8.7910e-01, -8.7910e-01],\n",
       "           [-8.7910e-01, -8.7910e-01, -8.7910e-01],\n",
       "           [-8.7910e-01, -8.7910e-01, -8.7910e-01]],\n",
       " \n",
       "          [[-8.7910e-01, -8.7910e-01, -8.7910e-01],\n",
       "           [-8.7910e-01, -8.7910e-01, -8.7910e-01],\n",
       "           [-8.7910e-01, -8.7910e-01, -8.7910e-01]],\n",
       " \n",
       "          [[-8.7910e-01, -8.7910e-01, -8.7910e-01],\n",
       "           [-8.7910e-01, -8.7910e-01, -8.7910e-01],\n",
       "           [-8.7910e-01, -8.7910e-01, -8.7910e-01]]],\n",
       " \n",
       " \n",
       "         [[[-5.8904e-01, -5.8904e-01, -5.8904e-01],\n",
       "           [-5.8904e-01, -5.8904e-01, -5.8904e-01],\n",
       "           [-5.8904e-01, -5.8904e-01, -5.8904e-01]],\n",
       " \n",
       "          [[-5.8904e-01, -5.8904e-01, -5.8904e-01],\n",
       "           [-5.8904e-01, -5.8904e-01, -5.8904e-01],\n",
       "           [-5.8904e-01, -5.8904e-01, -5.8904e-01]],\n",
       " \n",
       "          [[-5.8904e-01, -5.8904e-01, -5.8904e-01],\n",
       "           [-5.8904e-01, -5.8904e-01, -5.8904e-01],\n",
       "           [-5.8904e-01, -5.8904e-01, -5.8904e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.6029e-02, -1.6029e-02, -1.6029e-02],\n",
       "           [-1.6029e-02, -1.6029e-02, -1.6029e-02],\n",
       "           [-1.6029e-02, -1.6029e-02, -1.6029e-02]],\n",
       " \n",
       "          [[-1.6029e-02, -1.6029e-02, -1.6029e-02],\n",
       "           [-1.6029e-02, -1.6029e-02, -1.6029e-02],\n",
       "           [-1.6029e-02, -1.6029e-02, -1.6029e-02]],\n",
       " \n",
       "          [[-1.6029e-02, -1.6029e-02, -1.6029e-02],\n",
       "           [-1.6029e-02, -1.6029e-02, -1.6029e-02],\n",
       "           [-1.6029e-02, -1.6029e-02, -1.6029e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.3783e-01, -2.3783e-01, -2.3783e-01],\n",
       "           [-2.3783e-01, -2.3783e-01, -2.3783e-01],\n",
       "           [-2.3783e-01, -2.3783e-01, -2.3783e-01]],\n",
       " \n",
       "          [[-2.3783e-01, -2.3783e-01, -2.3783e-01],\n",
       "           [-2.3783e-01, -2.3783e-01, -2.3783e-01],\n",
       "           [-2.3783e-01, -2.3783e-01, -2.3783e-01]],\n",
       " \n",
       "          [[-2.3783e-01, -2.3783e-01, -2.3783e-01],\n",
       "           [-2.3783e-01, -2.3783e-01, -2.3783e-01],\n",
       "           [-2.3783e-01, -2.3783e-01, -2.3783e-01]]],\n",
       " \n",
       " \n",
       "         [[[-3.5711e-05, -3.5711e-05, -3.5711e-05],\n",
       "           [-3.5711e-05, -3.5711e-05, -3.5711e-05],\n",
       "           [-3.5711e-05, -3.5711e-05, -3.5711e-05]],\n",
       " \n",
       "          [[-3.5711e-05, -3.5711e-05, -3.5711e-05],\n",
       "           [-3.5711e-05, -3.5711e-05, -3.5711e-05],\n",
       "           [-3.5711e-05, -3.5711e-05, -3.5711e-05]],\n",
       " \n",
       "          [[-3.5711e-05, -3.5711e-05, -3.5711e-05],\n",
       "           [-3.5711e-05, -3.5711e-05, -3.5711e-05],\n",
       "           [-3.5711e-05, -3.5711e-05, -3.5711e-05]]],\n",
       " \n",
       " \n",
       "         [[[-3.8396e-05, -3.8396e-05, -3.8396e-05],\n",
       "           [-3.8396e-05, -3.8396e-05, -3.8396e-05],\n",
       "           [-3.8396e-05, -3.8396e-05, -3.8396e-05]],\n",
       " \n",
       "          [[-3.8396e-05, -3.8396e-05, -3.8396e-05],\n",
       "           [-3.8396e-05, -3.8396e-05, -3.8396e-05],\n",
       "           [-3.8396e-05, -3.8396e-05, -3.8396e-05]],\n",
       " \n",
       "          [[-3.8396e-05, -3.8396e-05, -3.8396e-05],\n",
       "           [-3.8396e-05, -3.8396e-05, -3.8396e-05],\n",
       "           [-3.8396e-05, -3.8396e-05, -3.8396e-05]]],\n",
       " \n",
       " \n",
       "         [[[-3.3553e-05, -3.3553e-05, -3.3553e-05],\n",
       "           [-3.3553e-05, -3.3553e-05, -3.3553e-05],\n",
       "           [-3.3553e-05, -3.3553e-05, -3.3553e-05]],\n",
       " \n",
       "          [[-3.3553e-05, -3.3553e-05, -3.3553e-05],\n",
       "           [-3.3553e-05, -3.3553e-05, -3.3553e-05],\n",
       "           [-3.3553e-05, -3.3553e-05, -3.3553e-05]],\n",
       " \n",
       "          [[-3.3553e-05, -3.3553e-05, -3.3553e-05],\n",
       "           [-3.3553e-05, -3.3553e-05, -3.3553e-05],\n",
       "           [-3.3553e-05, -3.3553e-05, -3.3553e-05]]],\n",
       " \n",
       " \n",
       "         [[[-1.0529e+00, -1.0529e+00, -1.0529e+00],\n",
       "           [-1.0529e+00, -1.0529e+00, -1.0529e+00],\n",
       "           [-1.0529e+00, -1.0529e+00, -1.0529e+00]],\n",
       " \n",
       "          [[-1.0529e+00, -1.0529e+00, -1.0529e+00],\n",
       "           [-1.0529e+00, -1.0529e+00, -1.0529e+00],\n",
       "           [-1.0529e+00, -1.0529e+00, -1.0529e+00]],\n",
       " \n",
       "          [[-1.0529e+00, -1.0529e+00, -1.0529e+00],\n",
       "           [-1.0529e+00, -1.0529e+00, -1.0529e+00],\n",
       "           [-1.0529e+00, -1.0529e+00, -1.0529e+00]]],\n",
       " \n",
       " \n",
       "         [[[-3.9900e-01, -3.9900e-01, -3.9900e-01],\n",
       "           [-3.9900e-01, -3.9900e-01, -3.9900e-01],\n",
       "           [-3.9900e-01, -3.9900e-01, -3.9900e-01]],\n",
       " \n",
       "          [[-3.9900e-01, -3.9900e-01, -3.9900e-01],\n",
       "           [-3.9900e-01, -3.9900e-01, -3.9900e-01],\n",
       "           [-3.9900e-01, -3.9900e-01, -3.9900e-01]],\n",
       " \n",
       "          [[-3.9900e-01, -3.9900e-01, -3.9900e-01],\n",
       "           [-3.9900e-01, -3.9900e-01, -3.9900e-01],\n",
       "           [-3.9900e-01, -3.9900e-01, -3.9900e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.0853e+00, -1.0853e+00, -1.0853e+00],\n",
       "           [-1.0853e+00, -1.0853e+00, -1.0853e+00],\n",
       "           [-1.0853e+00, -1.0853e+00, -1.0853e+00]],\n",
       " \n",
       "          [[-1.0853e+00, -1.0853e+00, -1.0853e+00],\n",
       "           [-1.0853e+00, -1.0853e+00, -1.0853e+00],\n",
       "           [-1.0853e+00, -1.0853e+00, -1.0853e+00]],\n",
       " \n",
       "          [[-1.0853e+00, -1.0853e+00, -1.0853e+00],\n",
       "           [-1.0853e+00, -1.0853e+00, -1.0853e+00],\n",
       "           [-1.0853e+00, -1.0853e+00, -1.0853e+00]]],\n",
       " \n",
       " \n",
       "         [[[-1.4099e-03, -1.4099e-03, -1.4099e-03],\n",
       "           [-1.4099e-03, -1.4099e-03, -1.4099e-03],\n",
       "           [-1.4099e-03, -1.4099e-03, -1.4099e-03]],\n",
       " \n",
       "          [[-1.4099e-03, -1.4099e-03, -1.4099e-03],\n",
       "           [-1.4099e-03, -1.4099e-03, -1.4099e-03],\n",
       "           [-1.4099e-03, -1.4099e-03, -1.4099e-03]],\n",
       " \n",
       "          [[-1.4099e-03, -1.4099e-03, -1.4099e-03],\n",
       "           [-1.4099e-03, -1.4099e-03, -1.4099e-03],\n",
       "           [-1.4099e-03, -1.4099e-03, -1.4099e-03]]],\n",
       " \n",
       " \n",
       "         [[[-8.2915e-01, -8.2915e-01, -8.2915e-01],\n",
       "           [-8.2915e-01, -8.2915e-01, -8.2915e-01],\n",
       "           [-8.2915e-01, -8.2915e-01, -8.2915e-01]],\n",
       " \n",
       "          [[-8.2915e-01, -8.2915e-01, -8.2915e-01],\n",
       "           [-8.2915e-01, -8.2915e-01, -8.2915e-01],\n",
       "           [-8.2915e-01, -8.2915e-01, -8.2915e-01]],\n",
       " \n",
       "          [[-8.2915e-01, -8.2915e-01, -8.2915e-01],\n",
       "           [-8.2915e-01, -8.2915e-01, -8.2915e-01],\n",
       "           [-8.2915e-01, -8.2915e-01, -8.2915e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.1592e-01, -2.1592e-01, -2.1592e-01],\n",
       "           [-2.1592e-01, -2.1592e-01, -2.1592e-01],\n",
       "           [-2.1592e-01, -2.1592e-01, -2.1592e-01]],\n",
       " \n",
       "          [[-2.1592e-01, -2.1592e-01, -2.1592e-01],\n",
       "           [-2.1592e-01, -2.1592e-01, -2.1592e-01],\n",
       "           [-2.1592e-01, -2.1592e-01, -2.1592e-01]],\n",
       " \n",
       "          [[-2.1592e-01, -2.1592e-01, -2.1592e-01],\n",
       "           [-2.1592e-01, -2.1592e-01, -2.1592e-01],\n",
       "           [-2.1592e-01, -2.1592e-01, -2.1592e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.6253e-01, -2.6253e-01, -2.6253e-01],\n",
       "           [-2.6253e-01, -2.6253e-01, -2.6253e-01],\n",
       "           [-2.6253e-01, -2.6253e-01, -2.6253e-01]],\n",
       " \n",
       "          [[-2.6253e-01, -2.6253e-01, -2.6253e-01],\n",
       "           [-2.6253e-01, -2.6253e-01, -2.6253e-01],\n",
       "           [-2.6253e-01, -2.6253e-01, -2.6253e-01]],\n",
       " \n",
       "          [[-2.6253e-01, -2.6253e-01, -2.6253e-01],\n",
       "           [-2.6253e-01, -2.6253e-01, -2.6253e-01],\n",
       "           [-2.6253e-01, -2.6253e-01, -2.6253e-01]]],\n",
       " \n",
       " \n",
       "         [[[-3.8167e-01, -3.8167e-01, -3.8167e-01],\n",
       "           [-3.8167e-01, -3.8167e-01, -3.8167e-01],\n",
       "           [-3.8167e-01, -3.8167e-01, -3.8167e-01]],\n",
       " \n",
       "          [[-3.8167e-01, -3.8167e-01, -3.8167e-01],\n",
       "           [-3.8167e-01, -3.8167e-01, -3.8167e-01],\n",
       "           [-3.8167e-01, -3.8167e-01, -3.8167e-01]],\n",
       " \n",
       "          [[-3.8167e-01, -3.8167e-01, -3.8167e-01],\n",
       "           [-3.8167e-01, -3.8167e-01, -3.8167e-01],\n",
       "           [-3.8167e-01, -3.8167e-01, -3.8167e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.0396e+00, -1.0396e+00, -1.0396e+00],\n",
       "           [-1.0396e+00, -1.0396e+00, -1.0396e+00],\n",
       "           [-1.0396e+00, -1.0396e+00, -1.0396e+00]],\n",
       " \n",
       "          [[-1.0396e+00, -1.0396e+00, -1.0396e+00],\n",
       "           [-1.0396e+00, -1.0396e+00, -1.0396e+00],\n",
       "           [-1.0396e+00, -1.0396e+00, -1.0396e+00]],\n",
       " \n",
       "          [[-1.0396e+00, -1.0396e+00, -1.0396e+00],\n",
       "           [-1.0396e+00, -1.0396e+00, -1.0396e+00],\n",
       "           [-1.0396e+00, -1.0396e+00, -1.0396e+00]]],\n",
       " \n",
       " \n",
       "         [[[-4.4804e-04, -4.4804e-04, -4.4804e-04],\n",
       "           [-4.4804e-04, -4.4804e-04, -4.4804e-04],\n",
       "           [-4.4804e-04, -4.4804e-04, -4.4804e-04]],\n",
       " \n",
       "          [[-4.4804e-04, -4.4804e-04, -4.4804e-04],\n",
       "           [-4.4804e-04, -4.4804e-04, -4.4804e-04],\n",
       "           [-4.4804e-04, -4.4804e-04, -4.4804e-04]],\n",
       " \n",
       "          [[-4.4804e-04, -4.4804e-04, -4.4804e-04],\n",
       "           [-4.4804e-04, -4.4804e-04, -4.4804e-04],\n",
       "           [-4.4804e-04, -4.4804e-04, -4.4804e-04]]],\n",
       " \n",
       " \n",
       "         [[[-8.3687e-01, -8.3687e-01, -8.3687e-01],\n",
       "           [-8.3687e-01, -8.3687e-01, -8.3687e-01],\n",
       "           [-8.3687e-01, -8.3687e-01, -8.3687e-01]],\n",
       " \n",
       "          [[-8.3687e-01, -8.3687e-01, -8.3687e-01],\n",
       "           [-8.3687e-01, -8.3687e-01, -8.3687e-01],\n",
       "           [-8.3687e-01, -8.3687e-01, -8.3687e-01]],\n",
       " \n",
       "          [[-8.3687e-01, -8.3687e-01, -8.3687e-01],\n",
       "           [-8.3687e-01, -8.3687e-01, -8.3687e-01],\n",
       "           [-8.3687e-01, -8.3687e-01, -8.3687e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.2655e-04, -1.2655e-04, -1.2655e-04],\n",
       "           [-1.2655e-04, -1.2655e-04, -1.2655e-04],\n",
       "           [-1.2655e-04, -1.2655e-04, -1.2655e-04]],\n",
       " \n",
       "          [[-1.2655e-04, -1.2655e-04, -1.2655e-04],\n",
       "           [-1.2655e-04, -1.2655e-04, -1.2655e-04],\n",
       "           [-1.2655e-04, -1.2655e-04, -1.2655e-04]],\n",
       " \n",
       "          [[-1.2655e-04, -1.2655e-04, -1.2655e-04],\n",
       "           [-1.2655e-04, -1.2655e-04, -1.2655e-04],\n",
       "           [-1.2655e-04, -1.2655e-04, -1.2655e-04]]],\n",
       " \n",
       " \n",
       "         [[[-8.8655e-01, -8.8655e-01, -8.8655e-01],\n",
       "           [-8.8655e-01, -8.8655e-01, -8.8655e-01],\n",
       "           [-8.8655e-01, -8.8655e-01, -8.8655e-01]],\n",
       " \n",
       "          [[-8.8655e-01, -8.8655e-01, -8.8655e-01],\n",
       "           [-8.8655e-01, -8.8655e-01, -8.8655e-01],\n",
       "           [-8.8655e-01, -8.8655e-01, -8.8655e-01]],\n",
       " \n",
       "          [[-8.8655e-01, -8.8655e-01, -8.8655e-01],\n",
       "           [-8.8655e-01, -8.8655e-01, -8.8655e-01],\n",
       "           [-8.8655e-01, -8.8655e-01, -8.8655e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.2355e-04, -1.2355e-04, -1.2355e-04],\n",
       "           [-1.2355e-04, -1.2355e-04, -1.2355e-04],\n",
       "           [-1.2355e-04, -1.2355e-04, -1.2355e-04]],\n",
       " \n",
       "          [[-1.2355e-04, -1.2355e-04, -1.2355e-04],\n",
       "           [-1.2355e-04, -1.2355e-04, -1.2355e-04],\n",
       "           [-1.2355e-04, -1.2355e-04, -1.2355e-04]],\n",
       " \n",
       "          [[-1.2355e-04, -1.2355e-04, -1.2355e-04],\n",
       "           [-1.2355e-04, -1.2355e-04, -1.2355e-04],\n",
       "           [-1.2355e-04, -1.2355e-04, -1.2355e-04]]],\n",
       " \n",
       " \n",
       "         [[[-1.6212e-01, -1.6212e-01, -1.6212e-01],\n",
       "           [-1.6212e-01, -1.6212e-01, -1.6212e-01],\n",
       "           [-1.6212e-01, -1.6212e-01, -1.6212e-01]],\n",
       " \n",
       "          [[-1.6212e-01, -1.6212e-01, -1.6212e-01],\n",
       "           [-1.6212e-01, -1.6212e-01, -1.6212e-01],\n",
       "           [-1.6212e-01, -1.6212e-01, -1.6212e-01]],\n",
       " \n",
       "          [[-1.6212e-01, -1.6212e-01, -1.6212e-01],\n",
       "           [-1.6212e-01, -1.6212e-01, -1.6212e-01],\n",
       "           [-1.6212e-01, -1.6212e-01, -1.6212e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.0183e+00, -1.0183e+00, -1.0183e+00],\n",
       "           [-1.0183e+00, -1.0183e+00, -1.0183e+00],\n",
       "           [-1.0183e+00, -1.0183e+00, -1.0183e+00]],\n",
       " \n",
       "          [[-1.0183e+00, -1.0183e+00, -1.0183e+00],\n",
       "           [-1.0183e+00, -1.0183e+00, -1.0183e+00],\n",
       "           [-1.0183e+00, -1.0183e+00, -1.0183e+00]],\n",
       " \n",
       "          [[-1.0183e+00, -1.0183e+00, -1.0183e+00],\n",
       "           [-1.0183e+00, -1.0183e+00, -1.0183e+00],\n",
       "           [-1.0183e+00, -1.0183e+00, -1.0183e+00]]],\n",
       " \n",
       " \n",
       "         [[[-5.5657e-05, -5.5657e-05, -5.5657e-05],\n",
       "           [-5.5657e-05, -5.5657e-05, -5.5657e-05],\n",
       "           [-5.5657e-05, -5.5657e-05, -5.5657e-05]],\n",
       " \n",
       "          [[-5.5657e-05, -5.5657e-05, -5.5657e-05],\n",
       "           [-5.5657e-05, -5.5657e-05, -5.5657e-05],\n",
       "           [-5.5657e-05, -5.5657e-05, -5.5657e-05]],\n",
       " \n",
       "          [[-5.5657e-05, -5.5657e-05, -5.5657e-05],\n",
       "           [-5.5657e-05, -5.5657e-05, -5.5657e-05],\n",
       "           [-5.5657e-05, -5.5657e-05, -5.5657e-05]]],\n",
       " \n",
       " \n",
       "         [[[-9.1996e-01, -9.1996e-01, -9.1996e-01],\n",
       "           [-9.1996e-01, -9.1996e-01, -9.1996e-01],\n",
       "           [-9.1996e-01, -9.1996e-01, -9.1996e-01]],\n",
       " \n",
       "          [[-9.1996e-01, -9.1996e-01, -9.1996e-01],\n",
       "           [-9.1996e-01, -9.1996e-01, -9.1996e-01],\n",
       "           [-9.1996e-01, -9.1996e-01, -9.1996e-01]],\n",
       " \n",
       "          [[-9.1996e-01, -9.1996e-01, -9.1996e-01],\n",
       "           [-9.1996e-01, -9.1996e-01, -9.1996e-01],\n",
       "           [-9.1996e-01, -9.1996e-01, -9.1996e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.5027e-01, -1.5027e-01, -1.5027e-01],\n",
       "           [-1.5027e-01, -1.5027e-01, -1.5027e-01],\n",
       "           [-1.5027e-01, -1.5027e-01, -1.5027e-01]],\n",
       " \n",
       "          [[-1.5027e-01, -1.5027e-01, -1.5027e-01],\n",
       "           [-1.5027e-01, -1.5027e-01, -1.5027e-01],\n",
       "           [-1.5027e-01, -1.5027e-01, -1.5027e-01]],\n",
       " \n",
       "          [[-1.5027e-01, -1.5027e-01, -1.5027e-01],\n",
       "           [-1.5027e-01, -1.5027e-01, -1.5027e-01],\n",
       "           [-1.5027e-01, -1.5027e-01, -1.5027e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.7828e-05, -2.7828e-05, -2.7828e-05],\n",
       "           [-2.7828e-05, -2.7828e-05, -2.7828e-05],\n",
       "           [-2.7828e-05, -2.7828e-05, -2.7828e-05]],\n",
       " \n",
       "          [[-2.7828e-05, -2.7828e-05, -2.7828e-05],\n",
       "           [-2.7828e-05, -2.7828e-05, -2.7828e-05],\n",
       "           [-2.7828e-05, -2.7828e-05, -2.7828e-05]],\n",
       " \n",
       "          [[-2.7828e-05, -2.7828e-05, -2.7828e-05],\n",
       "           [-2.7828e-05, -2.7828e-05, -2.7828e-05],\n",
       "           [-2.7828e-05, -2.7828e-05, -2.7828e-05]]],\n",
       " \n",
       " \n",
       "         [[[-3.1211e-01, -3.1211e-01, -3.1211e-01],\n",
       "           [-3.1211e-01, -3.1211e-01, -3.1211e-01],\n",
       "           [-3.1211e-01, -3.1211e-01, -3.1211e-01]],\n",
       " \n",
       "          [[-3.1211e-01, -3.1211e-01, -3.1211e-01],\n",
       "           [-3.1211e-01, -3.1211e-01, -3.1211e-01],\n",
       "           [-3.1211e-01, -3.1211e-01, -3.1211e-01]],\n",
       " \n",
       "          [[-3.1211e-01, -3.1211e-01, -3.1211e-01],\n",
       "           [-3.1211e-01, -3.1211e-01, -3.1211e-01],\n",
       "           [-3.1211e-01, -3.1211e-01, -3.1211e-01]]]], device='cuda:0'),\n",
       " 'w_min_thr': Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True),\n",
       " 'weight': tensor([[[[ 3.3926e-05,  4.8832e-05,  5.1723e-05],\n",
       "           [-1.0173e-06,  1.4792e-05, -1.3877e-05],\n",
       "           [ 6.2059e-08, -2.7080e-05, -3.9607e-05]],\n",
       " \n",
       "          [[-2.4653e-05, -1.5831e-05, -1.8025e-05],\n",
       "           [-7.4998e-05, -5.0562e-05, -8.1501e-05],\n",
       "           [-5.3893e-05, -7.4834e-05, -9.5852e-05]],\n",
       " \n",
       "          [[-5.9384e-05, -5.1323e-05, -5.9283e-05],\n",
       "           [-9.1179e-05, -7.9107e-05, -9.7056e-05],\n",
       "           [-9.2198e-05, -9.7084e-05, -9.6782e-05]]],\n",
       " \n",
       " \n",
       "         [[[-7.6353e-01, -5.6493e-01, -9.9388e-02],\n",
       "           [-2.5349e-01, -2.8053e-01, -1.4948e-01],\n",
       "           [-3.7072e-02, -4.7879e-02,  6.0632e-03]],\n",
       " \n",
       "          [[-8.1256e-01, -6.3405e-01, -1.2998e-01],\n",
       "           [-1.6719e-01, -2.0131e-01, -7.2887e-02],\n",
       "           [ 2.8816e-02, -2.3323e-02,  4.9565e-02]],\n",
       " \n",
       "          [[-4.6741e-01, -3.6335e-01, -8.8470e-02],\n",
       "           [-5.2431e-02, -8.7937e-02, -3.2425e-02],\n",
       "           [-1.4434e-02,  4.0239e-02,  8.6608e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.0910e-01,  4.7629e-01,  2.5074e-01],\n",
       "           [ 5.3795e-01,  7.1065e-01,  3.1545e-01],\n",
       "           [ 3.6902e-01,  4.2501e-01,  1.9579e-01]],\n",
       " \n",
       "          [[-1.8223e-01, -2.0335e-01, -1.0374e-01],\n",
       "           [-1.9377e-01, -1.7482e-01, -1.4427e-01],\n",
       "           [-1.2283e-01, -1.2544e-01, -3.0766e-02]],\n",
       " \n",
       "          [[-3.0735e-01, -3.5055e-01, -2.6941e-01],\n",
       "           [-3.3481e-01, -3.5124e-01, -2.8037e-01],\n",
       "           [-2.8828e-01, -2.6801e-01, -2.3281e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.1557e-05,  6.2053e-06,  1.0648e-05],\n",
       "           [-2.5091e-05, -2.1115e-05, -6.4164e-06],\n",
       "           [-4.3474e-05, -3.8330e-05, -2.8300e-05]],\n",
       " \n",
       "          [[ 2.5888e-05,  2.5187e-05,  2.2839e-05],\n",
       "           [-2.5224e-05, -1.1641e-05,  1.1027e-05],\n",
       "           [-2.7573e-05, -4.1738e-05, -3.4648e-05]],\n",
       " \n",
       "          [[ 4.2164e-05,  2.4549e-05,  2.9787e-05],\n",
       "           [ 8.4856e-06,  9.2610e-06,  2.7057e-05],\n",
       "           [-6.0704e-06, -1.8783e-05, -1.9876e-05]]],\n",
       " \n",
       " \n",
       "         [[[-1.1812e-02, -3.2386e-02,  3.6871e-01],\n",
       "           [ 3.3762e-02, -6.1939e-01,  2.5120e-01],\n",
       "           [ 1.8634e-01, -2.9527e-01,  1.7143e-01]],\n",
       " \n",
       "          [[-2.5725e-02, -6.4152e-02,  4.7009e-01],\n",
       "           [ 8.5168e-03, -8.7910e-01,  3.1790e-01],\n",
       "           [ 2.2234e-01, -3.3017e-01,  1.5299e-01]],\n",
       " \n",
       "          [[-2.4929e-02,  4.3347e-03,  2.3291e-01],\n",
       "           [-7.3127e-03, -3.6414e-01,  1.3936e-01],\n",
       "           [ 9.7613e-02, -1.4163e-01,  7.2931e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.5503e-03,  2.5529e-01, -3.1742e-01],\n",
       "           [ 8.4512e-02,  4.5538e-01, -5.0472e-01],\n",
       "           [ 9.3129e-02,  3.7837e-01, -3.8626e-01]],\n",
       " \n",
       "          [[-7.2636e-03,  3.4578e-01, -4.2748e-01],\n",
       "           [ 9.0714e-02,  5.9080e-01, -5.8904e-01],\n",
       "           [ 1.5058e-02,  3.6403e-01, -3.6200e-01]],\n",
       " \n",
       "          [[-1.2145e-02,  1.6038e-01, -2.4638e-01],\n",
       "           [ 4.3170e-02,  2.2165e-01, -3.2037e-01],\n",
       "           [ 2.8037e-02,  1.8768e-01, -2.7592e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 4.7827e-03, -4.6292e-03,  5.2238e-03],\n",
       "           [ 6.2276e-03, -5.4814e-03,  9.1249e-03],\n",
       "           [ 1.1557e-02,  4.5340e-03,  1.3910e-02]],\n",
       " \n",
       "          [[ 7.7153e-03,  1.8941e-04,  6.7076e-03],\n",
       "           [ 1.0096e-02,  4.8472e-04,  1.0454e-02],\n",
       "           [ 1.0181e-02,  4.7929e-03,  9.0594e-03]],\n",
       " \n",
       "          [[-9.0907e-03, -1.6029e-02, -1.2462e-02],\n",
       "           [-4.5349e-03, -1.4961e-02, -7.6194e-03],\n",
       "           [-5.1449e-03, -1.1845e-02, -1.1118e-02]]],\n",
       " \n",
       " \n",
       "         [[[-9.0489e-02, -1.7051e-01, -1.2021e-01],\n",
       "           [-1.2581e-01, -5.1143e-02, -1.5518e-02],\n",
       "           [-2.3783e-01, -1.2070e-01,  2.5086e-03]],\n",
       " \n",
       "          [[ 8.1513e-02, -9.8530e-02, -1.4559e-01],\n",
       "           [ 8.8384e-02,  1.1014e-02, -1.0361e-01],\n",
       "           [ 2.7827e-02, -1.0149e-01, -1.4572e-01]],\n",
       " \n",
       "          [[ 1.0767e+00,  1.2159e+00,  7.1578e-01],\n",
       "           [ 1.1928e+00,  1.3742e+00,  7.5208e-01],\n",
       "           [ 7.4640e-01,  8.7063e-01,  4.3212e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.3034e-05, -1.0825e-05, -9.2846e-06],\n",
       "           [-9.1074e-06, -1.8495e-05, -2.2803e-05],\n",
       "           [-6.3963e-06, -2.7787e-05, -2.5477e-05]],\n",
       " \n",
       "          [[-3.2974e-05, -2.1725e-05, -2.0423e-05],\n",
       "           [-2.6830e-05, -3.1771e-05, -3.4648e-05],\n",
       "           [-2.3140e-05, -3.5711e-05, -2.3276e-05]],\n",
       " \n",
       "          [[-2.5851e-05, -1.8500e-05, -9.4168e-06],\n",
       "           [-2.7699e-05, -2.7099e-05, -1.7707e-05],\n",
       "           [-8.7538e-06, -3.3177e-05, -2.4983e-05]]],\n",
       " \n",
       " \n",
       "         [[[ 3.9916e-05,  6.9076e-05,  6.2280e-05],\n",
       "           [ 2.5358e-05,  5.4431e-05,  7.9340e-05],\n",
       "           [ 4.0065e-06,  4.1700e-05,  2.6468e-05]],\n",
       " \n",
       "          [[-8.4071e-06,  2.4522e-05,  9.6032e-06],\n",
       "           [-1.0831e-05,  2.1799e-05,  2.3755e-05],\n",
       "           [-3.8396e-05,  1.4549e-05, -4.5770e-06]],\n",
       " \n",
       "          [[ 6.7126e-06,  2.8840e-05,  3.1891e-05],\n",
       "           [-1.2555e-05, -5.4081e-06,  1.0129e-05],\n",
       "           [-3.1754e-05, -8.8748e-06, -2.4933e-05]]],\n",
       " \n",
       " \n",
       "         [[[-5.6138e-06, -2.4085e-05, -3.3553e-05],\n",
       "           [-1.8452e-05, -3.0941e-05,  1.2101e-05],\n",
       "           [-1.3532e-05, -1.1557e-05,  5.9534e-06]],\n",
       " \n",
       "          [[ 3.6730e-05, -1.3248e-05, -8.4136e-06],\n",
       "           [-4.9239e-06,  1.2638e-06,  4.2240e-05],\n",
       "           [ 3.3535e-06,  2.1572e-05,  1.4789e-05]],\n",
       " \n",
       "          [[ 3.0536e-05, -2.2634e-05, -1.0675e-05],\n",
       "           [-9.3290e-07, -6.3852e-06,  5.4609e-05],\n",
       "           [-1.7329e-06,  1.5885e-05,  8.6804e-06]]],\n",
       " \n",
       " \n",
       "         [[[-7.4584e-01,  6.6199e-01,  1.2425e-01],\n",
       "           [-6.5618e-01,  6.5683e-01,  7.6544e-02],\n",
       "           [-2.0962e-01,  2.4300e-01, -5.4869e-02]],\n",
       " \n",
       "          [[-1.0529e+00,  8.7137e-01,  1.6566e-01],\n",
       "           [-8.4024e-01,  8.3752e-01,  9.4997e-02],\n",
       "           [-2.4240e-01,  1.6758e-01, -8.3493e-02]],\n",
       " \n",
       "          [[-4.5522e-01,  3.6809e-01,  2.5921e-02],\n",
       "           [-3.5676e-01,  3.0142e-01, -8.3323e-04],\n",
       "           [-1.4064e-01,  7.8124e-02, -8.8850e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.6193e-01, -3.0276e-01, -2.1264e-01],\n",
       "           [-3.6413e-01, -3.9900e-01, -1.9859e-01],\n",
       "           [-1.6358e-01, -1.9577e-01, -1.0744e-01]],\n",
       " \n",
       "          [[ 9.1855e-02,  8.3728e-02,  1.0091e-02],\n",
       "           [ 3.3113e-02,  3.0277e-02,  3.2249e-02],\n",
       "           [ 5.8468e-02,  4.2165e-02, -1.3469e-02]],\n",
       " \n",
       "          [[ 3.9593e-01,  3.7812e-01,  2.4292e-01],\n",
       "           [ 3.8254e-01,  3.5033e-01,  2.8296e-01],\n",
       "           [ 3.2930e-01,  2.5569e-01,  1.6819e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 4.9501e-01,  5.6993e-01,  1.9788e-01],\n",
       "           [ 5.6139e-01,  6.8521e-01,  3.9982e-02],\n",
       "           [ 4.2600e-01,  4.1148e-01, -7.3908e-02]],\n",
       " \n",
       "          [[-1.0282e+00, -1.0853e+00, -4.8161e-01],\n",
       "           [-1.0628e+00, -9.8168e-01, -3.1745e-01],\n",
       "           [-8.4520e-01, -7.1901e-01, -9.2696e-02]],\n",
       " \n",
       "          [[ 6.4651e-01,  4.7974e-01,  2.8962e-01],\n",
       "           [ 6.2084e-01,  2.5049e-01,  2.9070e-01],\n",
       "           [ 5.5137e-01,  2.7538e-01,  1.8314e-01]]],\n",
       " \n",
       " \n",
       "         [[[-7.4748e-04, -4.7124e-04, -7.7169e-04],\n",
       "           [-9.9780e-04, -9.5299e-04, -1.1348e-03],\n",
       "           [-6.4800e-04, -3.8872e-04, -3.1514e-04]],\n",
       " \n",
       "          [[-7.8387e-04, -6.7489e-04, -1.0597e-03],\n",
       "           [-1.0114e-03, -1.0876e-03, -1.3972e-03],\n",
       "           [-3.8557e-04, -1.0980e-04, -8.9932e-05]],\n",
       " \n",
       "          [[-7.5457e-04, -6.3564e-04, -1.0513e-03],\n",
       "           [-1.0248e-03, -1.0928e-03, -1.4099e-03],\n",
       "           [-2.6409e-04, -8.9124e-05, -1.8142e-04]]],\n",
       " \n",
       " \n",
       "         [[[ 2.2316e-01,  2.1346e-01,  7.3248e-02],\n",
       "           [-5.6490e-01, -5.6756e-01, -1.6207e-01],\n",
       "           [ 2.4825e-01,  2.5954e-01,  6.6131e-02]],\n",
       " \n",
       "          [[ 2.8389e-01,  2.8463e-01,  4.0868e-02],\n",
       "           [-8.2328e-01, -8.2915e-01, -2.2734e-01],\n",
       "           [ 2.8723e-01,  3.7535e-01,  1.0060e-01]],\n",
       " \n",
       "          [[ 1.3315e-01,  1.1714e-01,  6.9819e-02],\n",
       "           [-3.3136e-01, -3.0441e-01, -1.5886e-01],\n",
       "           [ 1.4116e-01,  1.3233e-01,  6.6576e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.6085e-01,  6.2291e-01,  3.2415e-01],\n",
       "           [ 2.3554e-01,  5.2954e-01,  4.1325e-01],\n",
       "           [ 2.8360e-01,  2.5632e-01,  2.7981e-01]],\n",
       " \n",
       "          [[-3.0254e-02,  4.3424e-01, -4.5001e-03],\n",
       "           [-5.2007e-02,  2.0024e-01,  1.2599e-02],\n",
       "           [-3.5927e-02, -1.7135e-01, -1.5328e-01]],\n",
       " \n",
       "          [[-1.9722e-01,  9.6165e-02, -1.8725e-01],\n",
       "           [-1.7937e-01, -5.5362e-02, -1.0531e-01],\n",
       "           [-6.3343e-02, -2.1592e-01, -1.0260e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0539e+00,  1.0522e+00,  6.5942e-01],\n",
       "           [ 9.6242e-01,  1.0347e+00,  6.9542e-01],\n",
       "           [ 5.7616e-01,  5.9412e-01,  4.9451e-01]],\n",
       " \n",
       "          [[ 3.9748e-02, -1.8362e-01, -1.2580e-01],\n",
       "           [-7.5417e-02, -2.2640e-01, -1.0583e-01],\n",
       "           [-6.1385e-02, -1.6755e-01, -4.8314e-02]],\n",
       " \n",
       "          [[-1.2793e-01, -2.6253e-01, -2.4396e-01],\n",
       "           [-2.2596e-01, -2.3029e-01, -1.1892e-01],\n",
       "           [-2.1077e-01, -1.0204e-01, -9.0859e-03]]],\n",
       " \n",
       " \n",
       "         [[[-2.6297e-01, -3.0321e-01, -2.9160e-01],\n",
       "           [-3.5556e-01, -3.8167e-01, -2.9570e-01],\n",
       "           [-2.7041e-01, -2.9097e-01, -1.9798e-01]],\n",
       " \n",
       "          [[-1.3558e-02, -5.9218e-02, -6.8625e-02],\n",
       "           [-3.6624e-02, -7.0952e-02, -7.7360e-03],\n",
       "           [ 1.1963e-03, -4.0188e-02,  5.1894e-02]],\n",
       " \n",
       "          [[ 3.7966e-01,  3.9570e-01,  2.8026e-01],\n",
       "           [ 4.2726e-01,  4.5344e-01,  4.0950e-01],\n",
       "           [ 2.8811e-01,  2.9460e-01,  3.0905e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.4784e-01,  1.5111e-01,  6.6923e-03],\n",
       "           [ 4.9409e-01, -6.6998e-01,  1.1929e-01],\n",
       "           [ 2.6870e-01, -5.2778e-01,  2.7918e-01]],\n",
       " \n",
       "          [[-2.2582e-01,  2.7297e-01, -6.6280e-02],\n",
       "           [ 7.9825e-01, -1.0396e+00,  1.7635e-01],\n",
       "           [ 3.3600e-01, -6.5186e-01,  3.3123e-01]],\n",
       " \n",
       "          [[-8.0496e-02,  6.8517e-02, -3.4240e-02],\n",
       "           [ 3.7895e-01, -6.1010e-01,  2.0907e-01],\n",
       "           [ 4.3151e-02, -5.9399e-02,  4.2906e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.7864e-04, -4.2919e-04, -4.4804e-04],\n",
       "           [-3.0717e-04, -3.0008e-04, -3.4265e-04],\n",
       "           [-1.5933e-04, -1.5952e-04, -1.4650e-04]],\n",
       " \n",
       "          [[-2.7376e-04, -2.7356e-04, -2.8244e-04],\n",
       "           [-2.4955e-04, -2.1792e-04, -2.3353e-04],\n",
       "           [-1.2869e-04, -1.0816e-04, -6.9145e-05]],\n",
       " \n",
       "          [[-1.4584e-04, -1.4609e-04, -1.1416e-04],\n",
       "           [-2.4705e-04, -2.3696e-04, -2.2370e-04],\n",
       "           [-1.6963e-04, -1.5738e-04, -1.2800e-04]]],\n",
       " \n",
       " \n",
       "         [[[-1.6852e-01,  4.8678e-01,  4.3324e-01],\n",
       "           [ 4.3677e-01, -3.5504e-01, -8.0259e-01],\n",
       "           [-2.6118e-01, -1.0131e-01,  3.3889e-01]],\n",
       " \n",
       "          [[-2.7363e-01,  4.4360e-01,  3.7201e-01],\n",
       "           [ 4.4622e-01, -3.7172e-01, -8.3687e-01],\n",
       "           [-2.4179e-01, -1.4866e-02,  4.1207e-01]],\n",
       " \n",
       "          [[-2.8561e-01,  2.0730e-01,  1.9060e-01],\n",
       "           [ 4.0948e-01, -1.0587e-01, -3.9076e-01],\n",
       "           [-1.8427e-01, -6.7886e-02,  1.9889e-01]]],\n",
       " \n",
       " \n",
       "         [[[-4.0502e-06, -1.8488e-05,  6.5893e-06],\n",
       "           [-2.5350e-05, -1.6537e-05,  1.7089e-05],\n",
       "           [ 4.4952e-06,  1.2311e-05,  1.9644e-05]],\n",
       " \n",
       "          [[-3.9374e-06, -1.5783e-05,  5.5191e-06],\n",
       "           [-1.6164e-05, -2.1088e-05,  8.4156e-06],\n",
       "           [ 7.6793e-06,  2.1703e-06,  2.9954e-06]],\n",
       " \n",
       "          [[-1.0368e-04, -1.1061e-04, -8.0007e-05],\n",
       "           [-1.2655e-04, -1.0809e-04, -8.2284e-05],\n",
       "           [-1.0078e-04, -9.8347e-05, -7.5329e-05]]],\n",
       " \n",
       " \n",
       "         [[[-7.2309e-01, -4.3717e-01,  1.3182e-01],\n",
       "           [-4.5982e-01, -2.7150e-01,  1.1698e-01],\n",
       "           [ 1.5211e-01,  9.8999e-02,  9.1658e-02]],\n",
       " \n",
       "          [[-8.8655e-01, -5.2129e-01,  1.3618e-01],\n",
       "           [-5.7329e-01, -3.3404e-01,  1.2610e-01],\n",
       "           [ 1.4391e-01,  1.2880e-01,  1.2032e-01]],\n",
       " \n",
       "          [[-4.2553e-01, -2.4880e-01,  1.3308e-01],\n",
       "           [-3.0552e-01, -1.6448e-01,  1.5480e-01],\n",
       "           [ 4.6423e-02,  5.1070e-02,  1.3183e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.7263e-05, -3.6900e-05, -1.0490e-05],\n",
       "           [-1.0545e-04, -1.0463e-04, -7.9867e-05],\n",
       "           [-4.8817e-05, -2.9624e-05, -3.9751e-06]],\n",
       " \n",
       "          [[-6.6024e-05, -6.7853e-05, -3.9539e-05],\n",
       "           [-1.2355e-04, -1.1842e-04, -9.0917e-05],\n",
       "           [-7.9848e-05, -5.0831e-05, -4.4855e-05]],\n",
       " \n",
       "          [[-5.8100e-05, -3.3799e-05, -5.0920e-06],\n",
       "           [-9.0697e-05, -6.9374e-05, -3.9341e-05],\n",
       "           [-5.9133e-05, -3.4764e-05, -5.0222e-06]]],\n",
       " \n",
       " \n",
       "         [[[-5.6280e-02,  2.1057e-02,  1.1999e-01],\n",
       "           [ 5.1615e-02,  2.4557e-01,  1.5442e-01],\n",
       "           [-7.9976e-02, -1.6212e-01, -1.2684e-01]],\n",
       " \n",
       "          [[-2.2372e-02,  1.7668e-01,  3.1657e-01],\n",
       "           [ 2.5021e-02,  3.0395e-01,  2.9793e-01],\n",
       "           [-1.5051e-01, -7.8282e-02,  1.0538e-01]],\n",
       " \n",
       "          [[-6.5617e-02,  7.6058e-02,  2.1403e-01],\n",
       "           [-4.2881e-02,  1.4509e-01,  1.3066e-01],\n",
       "           [-1.5801e-01, -1.1151e-01,  9.5852e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5930e-01,  2.4711e-01,  1.7585e-01],\n",
       "           [ 2.3790e-01,  2.8660e-01,  2.2708e-01],\n",
       "           [ 9.3694e-02,  1.9598e-01,  1.3328e-01]],\n",
       " \n",
       "          [[-7.1704e-01, -8.5035e-01, -4.5755e-01],\n",
       "           [-7.7286e-01, -1.0183e+00, -6.4734e-01],\n",
       "           [-3.5589e-01, -6.0788e-01, -3.1908e-01]],\n",
       " \n",
       "          [[-1.1449e-01,  9.8909e-02,  7.3894e-02],\n",
       "           [-1.5036e-01, -1.3030e-02,  1.3136e-02],\n",
       "           [-1.6273e-01,  3.8669e-02,  4.9102e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.9448e-05,  2.3619e-05,  2.1449e-05],\n",
       "           [ 5.1152e-05,  3.1632e-05,  2.1212e-05],\n",
       "           [ 4.3059e-05,  4.6227e-05,  3.4883e-05]],\n",
       " \n",
       "          [[-5.4317e-06, -6.0628e-06, -2.6820e-06],\n",
       "           [ 2.2996e-05,  2.7831e-06,  1.4986e-06],\n",
       "           [-6.0755e-07,  1.1868e-05,  1.6881e-05]],\n",
       " \n",
       "          [[-4.5283e-05, -5.5657e-05, -4.9620e-05],\n",
       "           [-1.8977e-05, -4.1498e-05, -3.9803e-05],\n",
       "           [-4.9801e-05, -3.0911e-05, -2.8802e-05]]],\n",
       " \n",
       " \n",
       "         [[[-6.5143e-01, -4.7996e-01,  2.3860e-01],\n",
       "           [ 3.2698e-01,  7.6866e-02,  1.8650e-02],\n",
       "           [ 3.7614e-01,  3.3631e-01, -1.5988e-01]],\n",
       " \n",
       "          [[-9.1996e-01, -6.8183e-01,  2.8547e-01],\n",
       "           [ 5.1064e-01,  1.5285e-01, -6.9270e-02],\n",
       "           [ 3.9401e-01,  4.2624e-01, -2.0206e-01]],\n",
       " \n",
       "          [[-3.6323e-01, -2.9551e-01,  1.2171e-01],\n",
       "           [ 2.6125e-01,  1.0986e-01,  4.9644e-03],\n",
       "           [ 2.1553e-01,  1.7419e-01, -1.0852e-01]]],\n",
       " \n",
       " \n",
       "         [[[-7.6480e-03,  6.8565e-02,  1.4432e-02],\n",
       "           [-5.8811e-02, -8.2450e-02, -1.0213e-01],\n",
       "           [-1.5027e-01, -1.3551e-01, -1.3233e-01]],\n",
       " \n",
       "          [[-5.3524e-02,  3.2519e-02, -6.8398e-02],\n",
       "           [ 4.8721e-02,  8.7519e-02,  2.6076e-02],\n",
       "           [ 4.3700e-02,  1.4745e-01,  1.0002e-01]],\n",
       " \n",
       "          [[-5.6160e-02,  3.5376e-02, -3.8341e-02],\n",
       "           [ 3.9252e-02,  1.1386e-01,  6.9811e-02],\n",
       "           [ 2.3038e-02,  1.7877e-01,  1.4881e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.4235e-05,  2.4268e-05,  1.8922e-05],\n",
       "           [ 1.5574e-05,  2.7006e-05,  1.6032e-05],\n",
       "           [ 1.8987e-05,  4.2133e-05,  3.2662e-05]],\n",
       " \n",
       "          [[-2.4917e-05, -2.7828e-05, -1.8928e-05],\n",
       "           [-1.7725e-05, -1.9816e-05, -2.6981e-05],\n",
       "           [-1.2924e-05,  2.0746e-06, -9.1188e-06]],\n",
       " \n",
       "          [[-7.8252e-06,  1.9655e-06,  6.1377e-06],\n",
       "           [-1.1699e-05, -1.0508e-07,  3.1488e-06],\n",
       "           [-3.3618e-06,  2.2762e-05,  3.3629e-06]]],\n",
       " \n",
       " \n",
       "         [[[ 2.5294e-01,  7.8859e-01,  7.5272e-01],\n",
       "           [-1.0165e-01, -1.8225e-01, -1.3458e-01],\n",
       "           [-1.4382e-01, -1.8132e-01, -1.5332e-01]],\n",
       " \n",
       "          [[ 3.4685e-01,  1.0055e+00,  8.8616e-01],\n",
       "           [-1.5852e-01, -3.1211e-01, -2.3866e-01],\n",
       "           [-1.4293e-01, -2.3013e-01, -2.1281e-01]],\n",
       " \n",
       "          [[ 1.6339e-01,  4.6714e-01,  4.4625e-01],\n",
       "           [-1.2274e-01, -1.8937e-01, -1.6876e-01],\n",
       "           [-1.6350e-01, -1.9343e-01, -1.6454e-01]]]], device='cuda:0')}"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer_load.param_to_quantize[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset='imagenet',num_classes=1000,input_size=224.0\n",
      "8 8 PerLayerAsymPACT\n",
      "This is a quantized Mobilenet with alpha=  1.0  input_size:  224.0  activation bits:  8  weight bits:  8  activation type:  learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:209: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc.bias Error\n",
      "fc.weight Error\n",
      "Model Pretrained Loaded\n",
      "mobilenet_quant_devel(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d_SAME(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d_SAME(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d_SAME(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d_SAME(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d_SAME(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d_SAME(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv2d_SAME(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Conv2d_SAME(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d_SAME(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Conv2d_SAME(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Conv2d_SAME(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Conv2d_SAME(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.__dict__[model_name]\n",
    "model = model(**model_config)\n",
    "if model is None :\n",
    "    print('ERORRR')\n",
    "    \n",
    "#logging.info(\"created model with configuration: %s\", model_config)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a_bits': 4,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 0,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 4,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 1,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 2,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 2,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 3,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 4,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 4,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 4,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 5,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 4,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 6,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 7,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 8,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 9,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 10,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 11,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 12,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 13,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 14,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 4},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 15,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 16,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 4},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 17,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 18,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 4},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 19,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 20,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 4},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 21,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 22,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 23,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 24,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 4},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 25,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 8},\n",
       " {'a_bits': 8,\n",
       "  'fold_type': 'ICN',\n",
       "  'layer': 26,\n",
       "  'quant_type': 'PerChannelsAsymMinMax',\n",
       "  'w_bits': 2},\n",
       " {'layer': 27, 'quant_type': 'PerChannelsAsymMinMax', 'w_bits': 2}]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_add_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PerLayerAsymPACT'"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Folding:  False Batch Folding Delay:  0 Type folding_weights\n",
      "[{'a_bits': 4, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 0}, {'a_bits': 4, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 1}, {'a_bits': 2, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 2}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 3}, {'a_bits': 4, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 4}, {'a_bits': 4, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 5}, {'a_bits': 4, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 6}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 7}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 8}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 9}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 10}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 11}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 12}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 13}, {'a_bits': 8, 'w_bits': 4, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 14}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 15}, {'a_bits': 8, 'w_bits': 4, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 16}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 17}, {'a_bits': 8, 'w_bits': 4, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 18}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 19}, {'a_bits': 8, 'w_bits': 4, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 20}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 21}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 22}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 23}, {'a_bits': 8, 'w_bits': 4, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 24}, {'a_bits': 8, 'w_bits': 8, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 25}, {'a_bits': 8, 'w_bits': 2, 'quant_type': 'PerChannelsAsymMinMax', 'fold_type': 'ICN', 'layer': 26}, {'w_bits': 2, 'quant_type': 'PerChannelsAsymMinMax', 'layer': 27}]\n",
      "PerLayerAsymPACT chennel with ICN on last layer\n",
      "This is the quantized network: \n",
      "mobilenet_quant_devel(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d_SAME(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
      "    (2): Conv2d_SAME(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (3): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
      "    (4): Conv2d_SAME(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (5): ScaledClippedLinearQuantizationChannel(clip_val=3, inplace)\n",
      "    (6): Conv2d_SAME(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "    (7): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (8): Conv2d_SAME(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
      "    (10): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "    (11): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
      "    (12): Conv2d_SAME(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (13): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
      "    (14): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "    (15): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (16): Conv2d_SAME(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (17): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (18): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "    (19): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (20): Conv2d_SAME(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (21): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (22): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "    (23): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (24): Conv2d_SAME(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (25): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (26): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (27): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (28): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (29): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (30): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (31): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (32): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (33): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (34): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (35): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (36): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (37): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (38): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (39): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (40): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (41): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (42): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (43): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (44): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (45): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (46): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "    (47): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (48): Conv2d_SAME(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (49): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (50): Conv2d_SAME(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "    (51): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (52): Conv2d_SAME(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (53): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
      "    (54): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "    (1): ScaledClippedLinearQuantizationChannel(clip_val=False, inplace)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "quantizer = quantization.QuantOp(model, type_quant, weight_bits, batch_fold_delay=0,batch_fold_type=fold_type, act_bits=activ_bits, add_config=quant_add_config  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.batch_fold = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dateset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:209: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "input_eval_transform = transforms.Compose([\n",
    "        transforms.Scale(int(input_size) ),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "input_transform = getattr(model, 'input_transform', input_eval_transform)\n",
    "\n",
    "val_data = get_dataset(dataset, 'val', input_transform['eval'])\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    batch_size=32, shuffle=False,\n",
    "    num_workers=8, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and quantizer!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(quantizer_load.param_to_quantize):\n",
    "    for key in item.keys():\n",
    "        if key in ['bias_bits', 'quant_type','w_min_thr','min_clip', 'w_clip','w_max_thr','max_clip', 'fold_type','w_bits']:\n",
    "            quantizer.param_to_quantize[i][key] = item[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quantizer.generate_deployment_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('module.model.0.0.weight',\n",
       "              tensor([[[[ 3.3926e-05,  4.8832e-05,  5.1723e-05],\n",
       "                        [-1.0173e-06,  1.4792e-05, -1.3877e-05],\n",
       "                        [ 6.2059e-08, -2.7080e-05, -3.9607e-05]],\n",
       "              \n",
       "                       [[-2.4653e-05, -1.5831e-05, -1.8025e-05],\n",
       "                        [-7.4998e-05, -5.0562e-05, -8.1501e-05],\n",
       "                        [-5.3893e-05, -7.4834e-05, -9.5852e-05]],\n",
       "              \n",
       "                       [[-5.9384e-05, -5.1323e-05, -5.9283e-05],\n",
       "                        [-9.1179e-05, -7.9107e-05, -9.7056e-05],\n",
       "                        [-9.2198e-05, -9.7084e-05, -9.6782e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.6353e-01, -5.6493e-01, -9.9388e-02],\n",
       "                        [-2.5349e-01, -2.8053e-01, -1.4948e-01],\n",
       "                        [-3.7072e-02, -4.7879e-02,  6.0632e-03]],\n",
       "              \n",
       "                       [[-8.1256e-01, -6.3405e-01, -1.2998e-01],\n",
       "                        [-1.6719e-01, -2.0131e-01, -7.2887e-02],\n",
       "                        [ 2.8816e-02, -2.3323e-02,  4.9565e-02]],\n",
       "              \n",
       "                       [[-4.6741e-01, -3.6335e-01, -8.8470e-02],\n",
       "                        [-5.2431e-02, -8.7937e-02, -3.2425e-02],\n",
       "                        [-1.4434e-02,  4.0239e-02,  8.6608e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.0910e-01,  4.7629e-01,  2.5074e-01],\n",
       "                        [ 5.3795e-01,  7.1065e-01,  3.1545e-01],\n",
       "                        [ 3.6902e-01,  4.2501e-01,  1.9579e-01]],\n",
       "              \n",
       "                       [[-1.8223e-01, -2.0335e-01, -1.0374e-01],\n",
       "                        [-1.9377e-01, -1.7482e-01, -1.4427e-01],\n",
       "                        [-1.2283e-01, -1.2544e-01, -3.0766e-02]],\n",
       "              \n",
       "                       [[-3.0735e-01, -3.5055e-01, -2.6941e-01],\n",
       "                        [-3.3481e-01, -3.5124e-01, -2.8037e-01],\n",
       "                        [-2.8828e-01, -2.6801e-01, -2.3281e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1557e-05,  6.2053e-06,  1.0648e-05],\n",
       "                        [-2.5091e-05, -2.1115e-05, -6.4164e-06],\n",
       "                        [-4.3474e-05, -3.8330e-05, -2.8300e-05]],\n",
       "              \n",
       "                       [[ 2.5888e-05,  2.5187e-05,  2.2839e-05],\n",
       "                        [-2.5224e-05, -1.1641e-05,  1.1027e-05],\n",
       "                        [-2.7573e-05, -4.1738e-05, -3.4648e-05]],\n",
       "              \n",
       "                       [[ 4.2164e-05,  2.4549e-05,  2.9787e-05],\n",
       "                        [ 8.4856e-06,  9.2610e-06,  2.7057e-05],\n",
       "                        [-6.0704e-06, -1.8783e-05, -1.9876e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1812e-02, -3.2386e-02,  3.6871e-01],\n",
       "                        [ 3.3762e-02, -6.1939e-01,  2.5120e-01],\n",
       "                        [ 1.8634e-01, -2.9527e-01,  1.7143e-01]],\n",
       "              \n",
       "                       [[-2.5725e-02, -6.4152e-02,  4.7009e-01],\n",
       "                        [ 8.5168e-03, -8.7910e-01,  3.1790e-01],\n",
       "                        [ 2.2234e-01, -3.3017e-01,  1.5299e-01]],\n",
       "              \n",
       "                       [[-2.4929e-02,  4.3347e-03,  2.3291e-01],\n",
       "                        [-7.3127e-03, -3.6414e-01,  1.3936e-01],\n",
       "                        [ 9.7613e-02, -1.4163e-01,  7.2931e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.5503e-03,  2.5529e-01, -3.1742e-01],\n",
       "                        [ 8.4512e-02,  4.5538e-01, -5.0472e-01],\n",
       "                        [ 9.3129e-02,  3.7837e-01, -3.8626e-01]],\n",
       "              \n",
       "                       [[-7.2636e-03,  3.4578e-01, -4.2748e-01],\n",
       "                        [ 9.0714e-02,  5.9080e-01, -5.8904e-01],\n",
       "                        [ 1.5058e-02,  3.6403e-01, -3.6200e-01]],\n",
       "              \n",
       "                       [[-1.2145e-02,  1.6038e-01, -2.4638e-01],\n",
       "                        [ 4.3170e-02,  2.2165e-01, -3.2037e-01],\n",
       "                        [ 2.8037e-02,  1.8768e-01, -2.7592e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.7827e-03, -4.6292e-03,  5.2238e-03],\n",
       "                        [ 6.2276e-03, -5.4814e-03,  9.1249e-03],\n",
       "                        [ 1.1557e-02,  4.5340e-03,  1.3910e-02]],\n",
       "              \n",
       "                       [[ 7.7153e-03,  1.8941e-04,  6.7076e-03],\n",
       "                        [ 1.0096e-02,  4.8472e-04,  1.0454e-02],\n",
       "                        [ 1.0181e-02,  4.7929e-03,  9.0594e-03]],\n",
       "              \n",
       "                       [[-9.0907e-03, -1.6029e-02, -1.2462e-02],\n",
       "                        [-4.5349e-03, -1.4961e-02, -7.6194e-03],\n",
       "                        [-5.1449e-03, -1.1845e-02, -1.1118e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.0489e-02, -1.7051e-01, -1.2021e-01],\n",
       "                        [-1.2581e-01, -5.1143e-02, -1.5518e-02],\n",
       "                        [-2.3783e-01, -1.2070e-01,  2.5086e-03]],\n",
       "              \n",
       "                       [[ 8.1513e-02, -9.8530e-02, -1.4559e-01],\n",
       "                        [ 8.8384e-02,  1.1014e-02, -1.0361e-01],\n",
       "                        [ 2.7827e-02, -1.0149e-01, -1.4572e-01]],\n",
       "              \n",
       "                       [[ 1.0767e+00,  1.2159e+00,  7.1578e-01],\n",
       "                        [ 1.1928e+00,  1.3742e+00,  7.5208e-01],\n",
       "                        [ 7.4640e-01,  8.7063e-01,  4.3212e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3034e-05, -1.0825e-05, -9.2846e-06],\n",
       "                        [-9.1074e-06, -1.8495e-05, -2.2803e-05],\n",
       "                        [-6.3963e-06, -2.7787e-05, -2.5477e-05]],\n",
       "              \n",
       "                       [[-3.2974e-05, -2.1725e-05, -2.0423e-05],\n",
       "                        [-2.6830e-05, -3.1771e-05, -3.4648e-05],\n",
       "                        [-2.3140e-05, -3.5711e-05, -2.3276e-05]],\n",
       "              \n",
       "                       [[-2.5851e-05, -1.8500e-05, -9.4168e-06],\n",
       "                        [-2.7699e-05, -2.7099e-05, -1.7707e-05],\n",
       "                        [-8.7538e-06, -3.3177e-05, -2.4983e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.9916e-05,  6.9076e-05,  6.2280e-05],\n",
       "                        [ 2.5358e-05,  5.4431e-05,  7.9340e-05],\n",
       "                        [ 4.0065e-06,  4.1700e-05,  2.6468e-05]],\n",
       "              \n",
       "                       [[-8.4071e-06,  2.4522e-05,  9.6032e-06],\n",
       "                        [-1.0831e-05,  2.1799e-05,  2.3755e-05],\n",
       "                        [-3.8396e-05,  1.4549e-05, -4.5770e-06]],\n",
       "              \n",
       "                       [[ 6.7126e-06,  2.8840e-05,  3.1891e-05],\n",
       "                        [-1.2555e-05, -5.4081e-06,  1.0129e-05],\n",
       "                        [-3.1754e-05, -8.8748e-06, -2.4933e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.6138e-06, -2.4085e-05, -3.3553e-05],\n",
       "                        [-1.8452e-05, -3.0941e-05,  1.2101e-05],\n",
       "                        [-1.3532e-05, -1.1557e-05,  5.9534e-06]],\n",
       "              \n",
       "                       [[ 3.6730e-05, -1.3248e-05, -8.4136e-06],\n",
       "                        [-4.9239e-06,  1.2638e-06,  4.2240e-05],\n",
       "                        [ 3.3535e-06,  2.1572e-05,  1.4789e-05]],\n",
       "              \n",
       "                       [[ 3.0536e-05, -2.2634e-05, -1.0675e-05],\n",
       "                        [-9.3290e-07, -6.3852e-06,  5.4609e-05],\n",
       "                        [-1.7329e-06,  1.5885e-05,  8.6804e-06]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.4584e-01,  6.6199e-01,  1.2425e-01],\n",
       "                        [-6.5618e-01,  6.5683e-01,  7.6544e-02],\n",
       "                        [-2.0962e-01,  2.4300e-01, -5.4869e-02]],\n",
       "              \n",
       "                       [[-1.0529e+00,  8.7137e-01,  1.6566e-01],\n",
       "                        [-8.4024e-01,  8.3752e-01,  9.4997e-02],\n",
       "                        [-2.4240e-01,  1.6758e-01, -8.3493e-02]],\n",
       "              \n",
       "                       [[-4.5522e-01,  3.6809e-01,  2.5921e-02],\n",
       "                        [-3.5676e-01,  3.0142e-01, -8.3323e-04],\n",
       "                        [-1.4064e-01,  7.8124e-02, -8.8850e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.6193e-01, -3.0276e-01, -2.1264e-01],\n",
       "                        [-3.6413e-01, -3.9900e-01, -1.9859e-01],\n",
       "                        [-1.6358e-01, -1.9577e-01, -1.0744e-01]],\n",
       "              \n",
       "                       [[ 9.1855e-02,  8.3728e-02,  1.0091e-02],\n",
       "                        [ 3.3113e-02,  3.0277e-02,  3.2249e-02],\n",
       "                        [ 5.8468e-02,  4.2165e-02, -1.3469e-02]],\n",
       "              \n",
       "                       [[ 3.9593e-01,  3.7812e-01,  2.4292e-01],\n",
       "                        [ 3.8254e-01,  3.5033e-01,  2.8296e-01],\n",
       "                        [ 3.2930e-01,  2.5569e-01,  1.6819e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.9501e-01,  5.6993e-01,  1.9788e-01],\n",
       "                        [ 5.6139e-01,  6.8521e-01,  3.9982e-02],\n",
       "                        [ 4.2600e-01,  4.1148e-01, -7.3908e-02]],\n",
       "              \n",
       "                       [[-1.0282e+00, -1.0853e+00, -4.8161e-01],\n",
       "                        [-1.0628e+00, -9.8168e-01, -3.1745e-01],\n",
       "                        [-8.4520e-01, -7.1901e-01, -9.2696e-02]],\n",
       "              \n",
       "                       [[ 6.4651e-01,  4.7974e-01,  2.8962e-01],\n",
       "                        [ 6.2084e-01,  2.5049e-01,  2.9070e-01],\n",
       "                        [ 5.5137e-01,  2.7538e-01,  1.8314e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.4748e-04, -4.7124e-04, -7.7169e-04],\n",
       "                        [-9.9780e-04, -9.5299e-04, -1.1348e-03],\n",
       "                        [-6.4800e-04, -3.8872e-04, -3.1514e-04]],\n",
       "              \n",
       "                       [[-7.8387e-04, -6.7489e-04, -1.0597e-03],\n",
       "                        [-1.0114e-03, -1.0876e-03, -1.3972e-03],\n",
       "                        [-3.8557e-04, -1.0980e-04, -8.9932e-05]],\n",
       "              \n",
       "                       [[-7.5457e-04, -6.3564e-04, -1.0513e-03],\n",
       "                        [-1.0248e-03, -1.0928e-03, -1.4099e-03],\n",
       "                        [-2.6409e-04, -8.9124e-05, -1.8142e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.2316e-01,  2.1346e-01,  7.3248e-02],\n",
       "                        [-5.6490e-01, -5.6756e-01, -1.6207e-01],\n",
       "                        [ 2.4825e-01,  2.5954e-01,  6.6131e-02]],\n",
       "              \n",
       "                       [[ 2.8389e-01,  2.8463e-01,  4.0868e-02],\n",
       "                        [-8.2328e-01, -8.2915e-01, -2.2734e-01],\n",
       "                        [ 2.8723e-01,  3.7535e-01,  1.0060e-01]],\n",
       "              \n",
       "                       [[ 1.3315e-01,  1.1714e-01,  6.9819e-02],\n",
       "                        [-3.3136e-01, -3.0441e-01, -1.5886e-01],\n",
       "                        [ 1.4116e-01,  1.3233e-01,  6.6576e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6085e-01,  6.2291e-01,  3.2415e-01],\n",
       "                        [ 2.3554e-01,  5.2954e-01,  4.1325e-01],\n",
       "                        [ 2.8360e-01,  2.5632e-01,  2.7981e-01]],\n",
       "              \n",
       "                       [[-3.0254e-02,  4.3424e-01, -4.5001e-03],\n",
       "                        [-5.2007e-02,  2.0024e-01,  1.2599e-02],\n",
       "                        [-3.5927e-02, -1.7135e-01, -1.5328e-01]],\n",
       "              \n",
       "                       [[-1.9722e-01,  9.6165e-02, -1.8725e-01],\n",
       "                        [-1.7937e-01, -5.5362e-02, -1.0531e-01],\n",
       "                        [-6.3343e-02, -2.1592e-01, -1.0260e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0539e+00,  1.0522e+00,  6.5942e-01],\n",
       "                        [ 9.6242e-01,  1.0347e+00,  6.9542e-01],\n",
       "                        [ 5.7616e-01,  5.9412e-01,  4.9451e-01]],\n",
       "              \n",
       "                       [[ 3.9748e-02, -1.8362e-01, -1.2580e-01],\n",
       "                        [-7.5417e-02, -2.2640e-01, -1.0583e-01],\n",
       "                        [-6.1385e-02, -1.6755e-01, -4.8314e-02]],\n",
       "              \n",
       "                       [[-1.2793e-01, -2.6253e-01, -2.4396e-01],\n",
       "                        [-2.2596e-01, -2.3029e-01, -1.1892e-01],\n",
       "                        [-2.1077e-01, -1.0204e-01, -9.0859e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.6297e-01, -3.0321e-01, -2.9160e-01],\n",
       "                        [-3.5556e-01, -3.8167e-01, -2.9570e-01],\n",
       "                        [-2.7041e-01, -2.9097e-01, -1.9798e-01]],\n",
       "              \n",
       "                       [[-1.3558e-02, -5.9218e-02, -6.8625e-02],\n",
       "                        [-3.6624e-02, -7.0952e-02, -7.7360e-03],\n",
       "                        [ 1.1963e-03, -4.0188e-02,  5.1894e-02]],\n",
       "              \n",
       "                       [[ 3.7966e-01,  3.9570e-01,  2.8026e-01],\n",
       "                        [ 4.2726e-01,  4.5344e-01,  4.0950e-01],\n",
       "                        [ 2.8811e-01,  2.9460e-01,  3.0905e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.4784e-01,  1.5111e-01,  6.6923e-03],\n",
       "                        [ 4.9409e-01, -6.6998e-01,  1.1929e-01],\n",
       "                        [ 2.6870e-01, -5.2778e-01,  2.7918e-01]],\n",
       "              \n",
       "                       [[-2.2582e-01,  2.7297e-01, -6.6280e-02],\n",
       "                        [ 7.9825e-01, -1.0396e+00,  1.7635e-01],\n",
       "                        [ 3.3600e-01, -6.5186e-01,  3.3123e-01]],\n",
       "              \n",
       "                       [[-8.0496e-02,  6.8517e-02, -3.4240e-02],\n",
       "                        [ 3.7895e-01, -6.1010e-01,  2.0907e-01],\n",
       "                        [ 4.3151e-02, -5.9399e-02,  4.2906e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.7864e-04, -4.2919e-04, -4.4804e-04],\n",
       "                        [-3.0717e-04, -3.0008e-04, -3.4265e-04],\n",
       "                        [-1.5933e-04, -1.5952e-04, -1.4650e-04]],\n",
       "              \n",
       "                       [[-2.7376e-04, -2.7356e-04, -2.8244e-04],\n",
       "                        [-2.4955e-04, -2.1792e-04, -2.3353e-04],\n",
       "                        [-1.2869e-04, -1.0816e-04, -6.9145e-05]],\n",
       "              \n",
       "                       [[-1.4584e-04, -1.4609e-04, -1.1416e-04],\n",
       "                        [-2.4705e-04, -2.3696e-04, -2.2370e-04],\n",
       "                        [-1.6963e-04, -1.5738e-04, -1.2800e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.6852e-01,  4.8678e-01,  4.3324e-01],\n",
       "                        [ 4.3677e-01, -3.5504e-01, -8.0259e-01],\n",
       "                        [-2.6118e-01, -1.0131e-01,  3.3889e-01]],\n",
       "              \n",
       "                       [[-2.7363e-01,  4.4360e-01,  3.7201e-01],\n",
       "                        [ 4.4622e-01, -3.7172e-01, -8.3687e-01],\n",
       "                        [-2.4179e-01, -1.4866e-02,  4.1207e-01]],\n",
       "              \n",
       "                       [[-2.8561e-01,  2.0730e-01,  1.9060e-01],\n",
       "                        [ 4.0948e-01, -1.0587e-01, -3.9076e-01],\n",
       "                        [-1.8427e-01, -6.7886e-02,  1.9889e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.0502e-06, -1.8488e-05,  6.5893e-06],\n",
       "                        [-2.5350e-05, -1.6537e-05,  1.7089e-05],\n",
       "                        [ 4.4952e-06,  1.2311e-05,  1.9644e-05]],\n",
       "              \n",
       "                       [[-3.9374e-06, -1.5783e-05,  5.5191e-06],\n",
       "                        [-1.6164e-05, -2.1088e-05,  8.4156e-06],\n",
       "                        [ 7.6793e-06,  2.1703e-06,  2.9954e-06]],\n",
       "              \n",
       "                       [[-1.0368e-04, -1.1061e-04, -8.0007e-05],\n",
       "                        [-1.2655e-04, -1.0809e-04, -8.2284e-05],\n",
       "                        [-1.0078e-04, -9.8347e-05, -7.5329e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.2309e-01, -4.3717e-01,  1.3182e-01],\n",
       "                        [-4.5982e-01, -2.7150e-01,  1.1698e-01],\n",
       "                        [ 1.5211e-01,  9.8999e-02,  9.1658e-02]],\n",
       "              \n",
       "                       [[-8.8655e-01, -5.2129e-01,  1.3618e-01],\n",
       "                        [-5.7329e-01, -3.3404e-01,  1.2610e-01],\n",
       "                        [ 1.4391e-01,  1.2880e-01,  1.2032e-01]],\n",
       "              \n",
       "                       [[-4.2553e-01, -2.4880e-01,  1.3308e-01],\n",
       "                        [-3.0552e-01, -1.6448e-01,  1.5480e-01],\n",
       "                        [ 4.6423e-02,  5.1070e-02,  1.3183e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7263e-05, -3.6900e-05, -1.0490e-05],\n",
       "                        [-1.0545e-04, -1.0463e-04, -7.9867e-05],\n",
       "                        [-4.8817e-05, -2.9624e-05, -3.9751e-06]],\n",
       "              \n",
       "                       [[-6.6024e-05, -6.7853e-05, -3.9539e-05],\n",
       "                        [-1.2355e-04, -1.1842e-04, -9.0917e-05],\n",
       "                        [-7.9848e-05, -5.0831e-05, -4.4855e-05]],\n",
       "              \n",
       "                       [[-5.8100e-05, -3.3799e-05, -5.0920e-06],\n",
       "                        [-9.0697e-05, -6.9374e-05, -3.9341e-05],\n",
       "                        [-5.9133e-05, -3.4764e-05, -5.0222e-06]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.6280e-02,  2.1057e-02,  1.1999e-01],\n",
       "                        [ 5.1615e-02,  2.4557e-01,  1.5442e-01],\n",
       "                        [-7.9976e-02, -1.6212e-01, -1.2684e-01]],\n",
       "              \n",
       "                       [[-2.2372e-02,  1.7668e-01,  3.1657e-01],\n",
       "                        [ 2.5021e-02,  3.0395e-01,  2.9793e-01],\n",
       "                        [-1.5051e-01, -7.8282e-02,  1.0538e-01]],\n",
       "              \n",
       "                       [[-6.5617e-02,  7.6058e-02,  2.1403e-01],\n",
       "                        [-4.2881e-02,  1.4509e-01,  1.3066e-01],\n",
       "                        [-1.5801e-01, -1.1151e-01,  9.5852e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5930e-01,  2.4711e-01,  1.7585e-01],\n",
       "                        [ 2.3790e-01,  2.8660e-01,  2.2708e-01],\n",
       "                        [ 9.3694e-02,  1.9598e-01,  1.3328e-01]],\n",
       "              \n",
       "                       [[-7.1704e-01, -8.5035e-01, -4.5755e-01],\n",
       "                        [-7.7286e-01, -1.0183e+00, -6.4734e-01],\n",
       "                        [-3.5589e-01, -6.0788e-01, -3.1908e-01]],\n",
       "              \n",
       "                       [[-1.1449e-01,  9.8909e-02,  7.3894e-02],\n",
       "                        [-1.5036e-01, -1.3030e-02,  1.3136e-02],\n",
       "                        [-1.6273e-01,  3.8669e-02,  4.9102e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.9448e-05,  2.3619e-05,  2.1449e-05],\n",
       "                        [ 5.1152e-05,  3.1632e-05,  2.1212e-05],\n",
       "                        [ 4.3059e-05,  4.6227e-05,  3.4883e-05]],\n",
       "              \n",
       "                       [[-5.4317e-06, -6.0628e-06, -2.6820e-06],\n",
       "                        [ 2.2996e-05,  2.7831e-06,  1.4986e-06],\n",
       "                        [-6.0755e-07,  1.1868e-05,  1.6881e-05]],\n",
       "              \n",
       "                       [[-4.5283e-05, -5.5657e-05, -4.9620e-05],\n",
       "                        [-1.8977e-05, -4.1498e-05, -3.9803e-05],\n",
       "                        [-4.9801e-05, -3.0911e-05, -2.8802e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.5143e-01, -4.7996e-01,  2.3860e-01],\n",
       "                        [ 3.2698e-01,  7.6866e-02,  1.8650e-02],\n",
       "                        [ 3.7614e-01,  3.3631e-01, -1.5988e-01]],\n",
       "              \n",
       "                       [[-9.1996e-01, -6.8183e-01,  2.8547e-01],\n",
       "                        [ 5.1064e-01,  1.5285e-01, -6.9270e-02],\n",
       "                        [ 3.9401e-01,  4.2624e-01, -2.0206e-01]],\n",
       "              \n",
       "                       [[-3.6323e-01, -2.9551e-01,  1.2171e-01],\n",
       "                        [ 2.6125e-01,  1.0986e-01,  4.9644e-03],\n",
       "                        [ 2.1553e-01,  1.7419e-01, -1.0852e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.6480e-03,  6.8565e-02,  1.4432e-02],\n",
       "                        [-5.8811e-02, -8.2450e-02, -1.0213e-01],\n",
       "                        [-1.5027e-01, -1.3551e-01, -1.3233e-01]],\n",
       "              \n",
       "                       [[-5.3524e-02,  3.2519e-02, -6.8398e-02],\n",
       "                        [ 4.8721e-02,  8.7519e-02,  2.6076e-02],\n",
       "                        [ 4.3700e-02,  1.4745e-01,  1.0002e-01]],\n",
       "              \n",
       "                       [[-5.6160e-02,  3.5376e-02, -3.8341e-02],\n",
       "                        [ 3.9252e-02,  1.1386e-01,  6.9811e-02],\n",
       "                        [ 2.3038e-02,  1.7877e-01,  1.4881e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4235e-05,  2.4268e-05,  1.8922e-05],\n",
       "                        [ 1.5574e-05,  2.7006e-05,  1.6032e-05],\n",
       "                        [ 1.8987e-05,  4.2133e-05,  3.2662e-05]],\n",
       "              \n",
       "                       [[-2.4917e-05, -2.7828e-05, -1.8928e-05],\n",
       "                        [-1.7725e-05, -1.9816e-05, -2.6981e-05],\n",
       "                        [-1.2924e-05,  2.0746e-06, -9.1188e-06]],\n",
       "              \n",
       "                       [[-7.8252e-06,  1.9655e-06,  6.1377e-06],\n",
       "                        [-1.1699e-05, -1.0508e-07,  3.1488e-06],\n",
       "                        [-3.3618e-06,  2.2762e-05,  3.3629e-06]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.5294e-01,  7.8859e-01,  7.5272e-01],\n",
       "                        [-1.0165e-01, -1.8225e-01, -1.3458e-01],\n",
       "                        [-1.4382e-01, -1.8132e-01, -1.5332e-01]],\n",
       "              \n",
       "                       [[ 3.4685e-01,  1.0055e+00,  8.8616e-01],\n",
       "                        [-1.5852e-01, -3.1211e-01, -2.3866e-01],\n",
       "                        [-1.4293e-01, -2.3013e-01, -2.1281e-01]],\n",
       "              \n",
       "                       [[ 1.6339e-01,  4.6714e-01,  4.4625e-01],\n",
       "                        [-1.2274e-01, -1.8937e-01, -1.6876e-01],\n",
       "                        [-1.6350e-01, -1.9343e-01, -1.6454e-01]]]], device='cuda:0')),\n",
       "             ('module.model.0.1.weight',\n",
       "              tensor([0.5586, 0.9879, 1.1681, 0.8917, 0.8562, 0.8794, 1.2988, 0.4998, 0.9651,\n",
       "                      0.3025, 0.9336, 1.0216, 2.5379, 0.5707, 0.9593, 1.1815, 1.4119, 0.7776,\n",
       "                      4.2705, 0.3510, 0.9537, 0.5394, 1.2036, 1.1973, 0.9948, 1.2695, 0.6585,\n",
       "                      0.9505, 1.1331, 1.1147, 0.9222, 0.9476], device='cuda:0')),\n",
       "             ('module.model.0.1.bias',\n",
       "              tensor([-1.2336,  2.4457,  0.2383, -0.4244,  1.6574,  2.2337,  0.6558,  2.9957,\n",
       "                      -0.3670, -1.2619, -0.8959,  2.4735,  1.9676,  1.7710,  0.7243,  3.2365,\n",
       "                      -1.4895,  2.6021,  0.5804,  2.2393, -0.5690, -0.0251, -2.2157,  3.4039,\n",
       "                      -2.1242,  2.2968,  4.9991, -0.4414,  2.4995,  1.5125, -0.3663,  3.3310],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.0.1.running_mean',\n",
       "              tensor([ 1.8120e-04,  1.7072e-01,  2.4804e-01, -9.6133e-06, -2.6728e-03,\n",
       "                      -8.4004e-03,  1.1219e-02, -4.7592e-01,  6.3952e-05, -1.3772e-05,\n",
       "                      -1.7591e-05, -6.5754e-03, -3.4615e-01, -1.7396e-01,  2.0383e-03,\n",
       "                       4.0459e-02, -6.0367e-03,  2.6236e-01, -4.5059e-01, -4.4378e-03,\n",
       "                       5.4704e-04,  8.4171e-03,  1.7401e-04,  2.0181e-01,  1.5054e-04,\n",
       "                      -1.7247e-01,  2.1020e-02,  5.6599e-05, -1.1024e-02, -1.1778e-01,\n",
       "                       4.9550e-06, -7.8730e-02], device='cuda:0')),\n",
       "             ('module.model.0.1.running_var',\n",
       "              tensor([3.5887e-07, 1.0679e+00, 3.5384e-01, 5.6285e-09, 3.7234e-02, 3.5334e-02,\n",
       "                      8.0824e-04, 1.0691e+00, 8.6616e-08, 5.2713e-08, 2.2332e-09, 1.0577e-01,\n",
       "                      6.4778e-01, 6.8181e-01, 9.5853e-05, 1.1689e-01, 1.6784e-01, 1.1040e+00,\n",
       "                      1.0398e+00, 3.7268e-02, 9.2535e-06, 3.9187e-02, 2.4694e-07, 1.4192e+00,\n",
       "                      5.7937e-07, 6.8027e-01, 4.3856e-01, 1.6526e-08, 4.9724e-02, 1.5024e-01,\n",
       "                      3.4661e-09, 3.2313e-01], device='cuda:0')),\n",
       "             ('module.model.0.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.0.2.clip_val', tensor([6.5764], device='cuda:0')),\n",
       "             ('module.model.1.0.weight',\n",
       "              tensor([[[[ 1.9302e+00,  1.3625e+00,  2.1403e+00],\n",
       "                        [ 4.5900e-01,  9.9429e-01,  1.5504e+00],\n",
       "                        [ 1.7504e+00,  6.5618e-01,  1.3216e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.6417e+00,  5.8919e+00,  2.2381e+00],\n",
       "                        [ 1.3972e+00,  1.3753e+00,  2.3458e+00],\n",
       "                        [-4.4354e+00, -6.8448e+00, -4.2336e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.7330e-01,  1.4126e+00, -4.3988e-01],\n",
       "                        [ 1.3708e+00,  4.3422e+00,  7.3441e-01],\n",
       "                        [-3.9496e-01,  2.3296e-01, -7.2121e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.6464e+00, -2.5504e+00, -2.9712e+00],\n",
       "                        [-1.2154e+00, -2.2671e+00, -1.7325e+00],\n",
       "                        [-1.5393e+00, -2.2813e+00, -1.4683e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7206e+00,  1.0883e+00,  1.0154e-01],\n",
       "                        [ 3.2462e+00, -1.5267e+00, -1.4538e-02],\n",
       "                        [ 2.6119e-01,  3.4065e-02, -1.9752e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.5198e+00, -5.3251e+00, -4.7478e-01],\n",
       "                        [ 3.0801e+00, -4.0174e+00,  9.9758e-01],\n",
       "                        [-6.4253e-01,  1.4463e+00,  1.9677e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8119e+00, -1.7216e+00, -1.6375e+00],\n",
       "                        [-2.9288e+00, -1.2654e+00, -5.0557e-01],\n",
       "                        [-2.6009e+00, -2.3887e+00, -1.4115e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.9438e-01,  1.4191e-02,  4.5762e-01],\n",
       "                        [ 8.7551e-01,  5.9290e+00,  1.9145e-01],\n",
       "                        [ 8.5534e-04,  6.6525e-01,  8.0606e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2434e+00,  1.4647e+00,  1.5015e+00],\n",
       "                        [ 1.6779e+00,  1.1835e+00,  1.7163e+00],\n",
       "                        [ 3.3644e-01,  8.0083e-01,  1.1933e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7602e+00,  2.2959e+00,  2.1474e+00],\n",
       "                        [ 2.3600e+00,  2.0164e+00,  1.4759e+00],\n",
       "                        [ 4.4810e-01,  1.2496e-01, -3.3640e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.6475e-01, -4.1241e-01,  1.3181e-01],\n",
       "                        [ 3.1775e-01,  1.1903e+00,  1.5901e-01],\n",
       "                        [ 2.9137e-01,  1.9058e+00,  2.3859e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.2597e-01,  7.9222e-01,  2.4329e-01],\n",
       "                        [ 3.4233e+00, -8.4445e+00,  1.1788e+00],\n",
       "                        [-4.0575e-01,  1.3710e+00,  3.7497e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.0819e-01, -8.9439e-03, -1.7689e-02],\n",
       "                        [ 4.8673e-01, -3.7321e+00,  8.3517e-01],\n",
       "                        [-1.0458e-01,  9.9976e-01, -4.9680e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0879e+00, -1.5240e+00,  6.3925e-01],\n",
       "                        [-2.9158e+00,  6.7935e+00, -1.5974e+00],\n",
       "                        [ 7.3991e-01,  1.2914e-01,  1.3443e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.1027e-01,  6.3710e-01,  3.0803e-01],\n",
       "                        [-3.9553e-01, -1.0906e-01, -2.3735e-01],\n",
       "                        [ 1.4546e+00,  3.1524e-04,  2.6636e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.2777e-01, -1.3357e+01,  1.1383e+00],\n",
       "                        [ 1.0348e+00,  1.0539e+01,  1.1486e-02],\n",
       "                        [-5.4426e-01,  2.8641e+00, -7.9289e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.5597e-01,  1.6142e+00,  1.6518e-01],\n",
       "                        [ 3.5840e+00,  5.3022e+00,  1.9107e+00],\n",
       "                        [ 1.9035e+00,  4.3955e+00,  2.0680e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.2396e-01, -8.9525e-01,  3.8385e-01],\n",
       "                        [-2.1699e+00, -7.1240e+00, -1.7742e+00],\n",
       "                        [ 1.1339e+00, -3.1729e+00,  1.6554e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.6572e-01, -1.0710e+00,  7.0673e-01],\n",
       "                        [-8.4620e-01, -2.4221e+00,  3.7408e-01],\n",
       "                        [ 5.4663e-01, -2.7559e-01,  2.9903e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.0854e-01,  4.6011e+00, -1.5235e-01],\n",
       "                        [-6.8006e-01, -3.4050e+00,  5.8908e-02],\n",
       "                        [-2.4877e-02, -8.1771e-02,  1.1751e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8235e-01, -1.4401e+00, -2.0026e+00],\n",
       "                        [-5.0806e-01, -3.5156e-01, -7.8403e-01],\n",
       "                        [ 9.8905e-01,  7.2798e-02,  1.5387e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9617e+00, -2.9548e+00, -2.0259e-01],\n",
       "                        [-2.2802e+00, -2.1519e+00, -6.6620e-01],\n",
       "                        [-1.0109e+00, -7.9036e-01,  4.6184e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6152e+00,  1.5298e+00,  1.1150e+00],\n",
       "                        [ 1.9201e+00,  1.4021e+00,  8.5969e-01],\n",
       "                        [ 9.0924e-01,  8.0287e-01, -7.6861e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.6043e-02,  1.9788e-01, -1.5626e-01],\n",
       "                        [ 1.5169e+01,  4.5272e-01, -1.5900e+01],\n",
       "                        [ 5.7337e+00,  1.1794e+00, -6.5140e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3317e+00, -1.3011e+00, -1.0505e+00],\n",
       "                        [ 6.6771e-01,  1.3866e+00, -8.1926e-02],\n",
       "                        [ 1.6689e+00,  1.4735e+00, -5.6742e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.6698e-01,  7.1196e-01, -1.2137e-01],\n",
       "                        [-3.4179e+00, -1.9503e+00,  6.8726e-01],\n",
       "                        [-1.6029e-01,  9.3935e-01, -6.8687e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.2708e-01, -5.0198e-01,  1.2918e-02],\n",
       "                        [-1.9845e+00, -4.3292e+00,  6.1212e-01],\n",
       "                        [ 8.8443e-01,  7.5214e-01, -9.5491e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.2400e-01,  4.6090e-02, -8.4560e-01],\n",
       "                        [-5.6461e-01, -7.5807e-01, -1.2660e+00],\n",
       "                        [-7.2909e-01, -1.3064e+00, -1.7099e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.0506e-01, -1.6492e+00,  8.8585e-02],\n",
       "                        [-2.4197e+00,  7.1894e+00, -6.3814e-01],\n",
       "                        [ 4.2787e-01, -9.9123e-01, -6.5640e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3497e+00, -2.1284e+00,  3.5847e-01],\n",
       "                        [-7.7972e-01, -1.6937e+00, -1.9166e-01],\n",
       "                        [ 1.3518e+00,  2.9364e-01,  1.1712e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3533e+00, -1.6044e+00, -6.9005e-01],\n",
       "                        [-1.6174e+00, -2.4297e+00, -1.8268e+00],\n",
       "                        [-2.2875e+00, -2.7252e+00, -1.3794e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.4381e-01, -6.9611e-01,  5.0691e-01],\n",
       "                        [-9.0573e+00, -1.0896e+01,  1.0601e+00],\n",
       "                        [ 9.0445e+00,  1.1105e+01, -9.7453e-01]]]], device='cuda:0')),\n",
       "             ('module.model.1.1.weight',\n",
       "              tensor([-0.0201,  1.4926,  0.4733,  0.0793,  1.0690,  0.9095,  0.9401,  0.5208,\n",
       "                       0.0554,  0.4527,  0.1931,  0.9306,  0.6016,  0.5810, -0.0945,  1.7161,\n",
       "                       0.3885,  0.4797,  0.7414,  1.0216,  0.5042,  0.5170,  0.6964,  2.8046,\n",
       "                       0.4234,  2.6618,  0.5661,  0.4655,  0.8053,  0.6836,  0.1987,  2.0924],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.1.1.bias',\n",
       "              tensor([-1.1711,  0.6936,  0.8053, -0.4234,  1.6950,  1.1063, -0.8237,  3.1678,\n",
       "                      -0.8526, -0.7858, -0.9610,  3.6124,  2.1233,  1.8707, -0.4440,  2.2020,\n",
       "                       0.3092,  3.2648,  0.5580,  1.3247, -0.7746,  4.4368, -1.0032,  1.7464,\n",
       "                       0.1925, -2.9495,  1.2712, -0.9094,  2.9132, -0.0361, -0.9508,  4.2938],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.1.1.running_mean',\n",
       "              tensor([ 2.3262e-43,  4.3140e-01,  3.8674e-01, -2.3262e-43,  2.5391e-01,\n",
       "                       7.2073e-01, -9.9198e+00,  1.6858e+01,  2.3262e-43,  2.3262e-43,\n",
       "                       2.3262e-43, -4.6497e+00, -2.8805e+00,  3.5119e+00,  5.8195e-01,\n",
       "                       5.4152e-02,  1.1656e+00, -2.5659e+01, -5.0691e+00,  8.4675e-01,\n",
       "                      -2.3262e-43, -8.3659e-01,  2.3262e-43,  1.0046e-01,  2.3262e-43,\n",
       "                      -9.6779e+00, -2.5855e+01, -2.3262e-43,  1.8013e+00, -6.4376e+00,\n",
       "                      -2.3262e-43,  2.2490e-01], device='cuda:0')),\n",
       "             ('module.model.1.1.running_var',\n",
       "              tensor([2.3262e-43, 5.0027e+01, 1.0654e+00, 2.3262e-43, 5.8525e+00, 6.8189e+01,\n",
       "                      1.3622e+02, 9.3367e+00, 2.3262e-43, 2.3262e-43, 2.3262e-43, 4.3557e+01,\n",
       "                      5.6071e+00, 2.1018e+00, 1.3523e-01, 1.4564e+02, 1.0558e+01, 6.6691e+01,\n",
       "                      3.3783e+01, 3.8380e+00, 2.3262e-43, 3.3117e+00, 2.3262e-43, 2.3912e+02,\n",
       "                      2.3262e-43, 3.5391e+01, 9.3586e+00, 2.3262e-43, 3.9613e+01, 2.7556e+01,\n",
       "                      2.3262e-43, 1.5106e+02], device='cuda:0')),\n",
       "             ('module.model.1.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.1.2.clip_val', tensor([6.4698], device='cuda:0')),\n",
       "             ('module.model.2.0.weight', tensor([[[[ 7.1991e-06]],\n",
       "              \n",
       "                       [[ 2.2636e-01]],\n",
       "              \n",
       "                       [[ 1.2890e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-8.3699e-02]],\n",
       "              \n",
       "                       [[ 7.6510e-05]],\n",
       "              \n",
       "                       [[ 4.4205e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6838e-05]],\n",
       "              \n",
       "                       [[ 7.8640e-03]],\n",
       "              \n",
       "                       [[-8.2099e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.8280e-01]],\n",
       "              \n",
       "                       [[-1.0161e-05]],\n",
       "              \n",
       "                       [[-8.2191e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9332e-05]],\n",
       "              \n",
       "                       [[ 6.3775e-02]],\n",
       "              \n",
       "                       [[ 1.5190e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.0982e-01]],\n",
       "              \n",
       "                       [[-3.2549e-05]],\n",
       "              \n",
       "                       [[ 1.8434e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 2.6654e-05]],\n",
       "              \n",
       "                       [[ 1.9700e-01]],\n",
       "              \n",
       "                       [[-2.6702e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.9127e-03]],\n",
       "              \n",
       "                       [[-4.0458e-05]],\n",
       "              \n",
       "                       [[ 5.3284e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.2205e-06]],\n",
       "              \n",
       "                       [[-6.2195e-02]],\n",
       "              \n",
       "                       [[-2.2324e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.6125e-03]],\n",
       "              \n",
       "                       [[-2.0939e-05]],\n",
       "              \n",
       "                       [[-3.9655e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.7623e-06]],\n",
       "              \n",
       "                       [[-1.3872e+00]],\n",
       "              \n",
       "                       [[-3.9437e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.5189e-01]],\n",
       "              \n",
       "                       [[ 1.5303e-05]],\n",
       "              \n",
       "                       [[-7.3340e-01]]]], device='cuda:0')),\n",
       "             ('module.model.2.1.weight',\n",
       "              tensor([0.5796, 0.8111, 1.1039, 0.5462, 0.7519, 2.7461, 0.8117, 1.3221, 0.9808,\n",
       "                      1.8197, 1.0505, 0.5907, 1.7955, 5.5237, 1.4518, 1.4039, 1.1520, 1.7451,\n",
       "                      1.7668, 1.0717, 2.6180, 0.7088, 1.0086, 2.4872, 0.7591, 1.7587, 2.7584,\n",
       "                      1.0718, 2.6241, 2.9590, 1.4663, 1.2157, 1.1968, 3.0526, 0.9921, 1.5097,\n",
       "                      1.3668, 0.2813, 1.2089, 0.7890, 1.1843, 0.9979, 1.1122, 1.1365, 0.1209,\n",
       "                      0.8966, 1.6731, 1.0442, 3.0541, 0.8917, 2.4494, 0.9694, 2.2390, 0.9402,\n",
       "                      0.8702, 1.8187, 0.6165, 1.1064, 1.2081, 2.0268, 0.1118, 1.3093, 3.4571,\n",
       "                      2.0148], device='cuda:0')),\n",
       "             ('module.model.2.1.bias',\n",
       "              tensor([ 1.7265,  0.0439, -0.0410,  1.4265,  0.6352,  1.4496,  2.6643,  0.1298,\n",
       "                       0.1331,  1.6991,  0.0867,  1.5613, -0.1287,  1.7711,  1.6332,  0.1476,\n",
       "                       1.0326,  0.6432, -0.9288, -2.6370,  0.3934,  0.7808,  0.8817, -0.0535,\n",
       "                       1.6520,  1.3248,  3.0505,  2.5080,  0.7838,  1.9677,  0.7559,  0.3397,\n",
       "                       0.4582,  3.2585, -0.1189,  1.4744,  0.0164, -1.4958,  1.3682,  0.0069,\n",
       "                       0.0178, -0.0691,  1.1683,  2.9533, -1.4235,  1.2228,  1.2514,  3.0078,\n",
       "                       0.3352, -1.3889,  0.0204,  1.7801, -0.0888, -0.1007, -0.0921,  0.1548,\n",
       "                       1.2128,  2.8885,  0.4482,  2.6727, -1.3493,  0.5922, -0.0636, -0.0285],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.2.1.running_mean',\n",
       "              tensor([-3.5767e-01,  1.0021e+00, -5.3673e+00,  2.3220e-01,  3.9882e-01,\n",
       "                       9.9757e-01,  1.1327e+00,  4.3383e+00, -3.8350e+00, -5.2900e-01,\n",
       "                       5.4872e-03,  8.5220e-01,  2.0975e+00,  1.9606e+00,  2.9673e+00,\n",
       "                       6.4451e-01, -1.2379e+00, -1.0682e+00, -1.9743e-01, -4.3482e-01,\n",
       "                       3.3063e+00,  1.5357e+00, -4.7516e-01, -6.3721e+00, -1.4136e+00,\n",
       "                       3.5855e+00, -3.3532e+00, -2.6134e+00, -6.4052e+00, -5.1216e-01,\n",
       "                      -9.9114e-01,  2.0449e+00, -4.7150e+00,  3.2884e+00,  2.0172e-05,\n",
       "                       8.7666e-01,  4.1354e+00,  2.4424e-04, -2.4266e+00,  1.4732e+00,\n",
       "                      -3.5434e+00, -1.3847e-04, -8.6110e-01,  3.0267e+00,  6.2999e-06,\n",
       "                      -2.1956e+00,  2.1475e+00,  3.4668e+00,  1.0689e-01,  6.1469e-01,\n",
       "                      -1.0852e+00,  5.0736e-01,  4.2871e-02, -3.5537e-05, -1.3332e+00,\n",
       "                       3.7426e+00,  4.9717e-01, -2.7584e+00, -2.0562e+00,  1.0044e+00,\n",
       "                       5.9035e-04,  2.4016e+00,  4.0975e+00, -4.0584e+00], device='cuda:0')),\n",
       "             ('module.model.2.1.running_var',\n",
       "              tensor([2.6702e-01, 5.1175e-01, 1.4103e+00, 2.8470e-01, 2.1199e-01, 6.0689e-01,\n",
       "                      1.6474e-01, 1.4468e+00, 5.1904e-01, 4.0496e-01, 6.3107e-03, 3.4565e-01,\n",
       "                      1.3975e+00, 3.9279e+00, 4.4684e-01, 1.3435e+00, 3.8952e-01, 3.5496e-01,\n",
       "                      1.6195e-01, 9.4432e-02, 1.6785e+00, 6.1577e-01, 6.4220e-01, 1.9328e+00,\n",
       "                      1.2534e+00, 9.3588e-01, 4.3682e-01, 1.9666e+00, 3.0930e+00, 3.5161e-01,\n",
       "                      6.0667e-01, 5.8752e-01, 7.8165e-01, 5.1754e-01, 2.0311e-09, 4.0478e-01,\n",
       "                      1.6192e+00, 3.1907e-08, 4.3964e-01, 9.4205e-01, 1.2280e+00, 1.7762e-08,\n",
       "                      6.7168e-01, 2.4414e+00, 7.9373e-09, 3.2393e-01, 4.4826e-01, 2.2440e+00,\n",
       "                      1.2798e+00, 2.0785e-01, 2.6270e+00, 3.8770e-01, 1.7539e+00, 6.4112e-09,\n",
       "                      8.2071e-01, 3.1733e+00, 3.1800e-01, 1.9028e+00, 6.2285e-01, 3.3741e-01,\n",
       "                      2.4802e-08, 5.3685e-01, 1.4974e+00, 3.1574e+00], device='cuda:0')),\n",
       "             ('module.model.2.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.2.2.clip_val', tensor([5.3852], device='cuda:0')),\n",
       "             ('module.model.3.0.weight',\n",
       "              tensor([[[[-3.8909e-01, -7.2013e-01, -1.9051e-01],\n",
       "                        [-4.5161e-01,  2.5857e+00,  1.7117e+00],\n",
       "                        [ 1.4019e-02, -1.0570e+00, -1.0194e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.1883e-01,  6.2246e-01,  3.1392e-01],\n",
       "                        [ 8.8129e-01,  1.3241e+00,  5.9893e-01],\n",
       "                        [ 6.9769e-01,  1.0062e+00,  4.7550e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0633e+00,  1.4895e+00,  5.1611e-01],\n",
       "                        [ 1.1788e+00,  1.9875e+00,  1.0393e+00],\n",
       "                        [ 3.9032e-01,  9.6963e-01,  6.0925e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.4559e-01,  1.7807e-01,  4.4458e-01],\n",
       "                        [-1.3971e+00,  1.8628e+00,  1.1347e+00],\n",
       "                        [-1.7610e+00,  1.0270e+00,  9.1864e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.4273e-01,  2.3912e-01,  4.1912e-02],\n",
       "                        [-2.2416e+00,  9.1384e-01,  1.3629e+00],\n",
       "                        [-1.5536e+00,  5.0634e-01,  9.0734e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.9093e-01, -6.3597e-01,  1.3110e-01],\n",
       "                        [-8.2357e-01, -6.2526e-01,  2.3669e-01],\n",
       "                        [ 1.2300e-03,  2.0359e-01,  2.5564e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.0283e-01, -2.8618e+00,  1.3526e+00],\n",
       "                        [ 2.2537e+00, -1.8894e+00,  6.8430e-01],\n",
       "                        [ 1.2058e-01,  1.0227e-01, -9.5936e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.2077e-01,  3.2037e-01,  3.7189e-02],\n",
       "                        [ 1.3052e+00,  1.9999e+00,  8.4295e-01],\n",
       "                        [ 1.0558e+00,  1.4546e+00,  6.1378e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1164e+00, -8.8484e-01,  1.5516e-01],\n",
       "                        [-9.5684e-01, -1.2558e+00, -4.2907e-01],\n",
       "                        [ 6.7391e-01,  8.5592e-01,  4.8781e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.8989e-01,  5.5725e-01,  2.2299e-01],\n",
       "                        [ 2.9917e+00,  1.0281e+00, -8.6198e-01],\n",
       "                        [-2.4496e+00, -2.1966e+00,  5.7459e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.3986e-01, -2.5522e-01, -9.0922e-01],\n",
       "                        [ 8.5635e-02, -1.5574e-01, -2.4871e-01],\n",
       "                        [-4.3456e-01, -7.4101e-03, -3.2992e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.5424e-01, -1.0421e+00, -4.1793e-01],\n",
       "                        [-2.7549e-01, -6.3757e-01, -1.6040e-01],\n",
       "                        [ 3.5070e-01,  3.8409e-01,  3.3921e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.1535e-01,  1.1975e+00,  4.2779e-01],\n",
       "                        [ 1.0842e+00,  1.5142e+00,  1.0094e+00],\n",
       "                        [ 4.8933e-01,  9.0456e-01,  6.6673e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.5750e-01, -9.4872e-01, -7.9921e-01],\n",
       "                        [-9.9673e-01, -1.3431e+00, -9.7992e-01],\n",
       "                        [-2.9699e-01, -3.6534e-01, -3.7886e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.4558e-01, -1.5892e+00, -1.1719e+00],\n",
       "                        [ 2.1182e+00,  3.8956e+00,  1.3868e+00],\n",
       "                        [-3.7151e-01, -4.2668e-01, -4.4370e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0508e+00,  1.3701e+00,  7.0248e-01],\n",
       "                        [ 1.1114e+00,  1.1572e+00,  3.9920e-01],\n",
       "                        [ 5.7929e-01,  2.8961e-01,  1.1648e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7477e+00,  1.2525e+00, -1.0794e-02],\n",
       "                        [-1.4610e+00,  5.4537e-01,  1.1994e+00],\n",
       "                        [-1.4116e+00, -1.7047e+00, -3.6076e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3366e+00,  1.7861e+00,  2.5536e-01],\n",
       "                        [ 2.3106e+00,  3.7467e-01, -1.1164e+00],\n",
       "                        [ 2.7735e-01, -1.4642e+00, -5.1786e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0584e+00,  1.3375e+00,  2.4137e-01],\n",
       "                        [ 8.1092e-01,  6.8157e-01, -1.4943e-01],\n",
       "                        [-8.7646e-01, -9.7937e-01, -3.6529e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.9933e-01,  1.4945e+00,  8.4458e-01],\n",
       "                        [ 3.2270e-01,  1.4440e+00,  7.4745e-01],\n",
       "                        [-3.8820e-01,  2.1656e-03, -1.6676e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.7327e-01, -8.3640e-01, -3.1619e-01],\n",
       "                        [-1.1657e+00, -1.4181e+00, -9.9792e-01],\n",
       "                        [-6.6973e-01, -1.1223e+00, -6.9784e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.6670e+00,  2.4630e+00, -5.2822e-01],\n",
       "                        [ 1.0451e+00,  2.7523e-01,  1.1937e-01],\n",
       "                        [-1.4978e+00, -2.2894e+00,  2.1449e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.6369e-01,  2.3222e+00,  1.0144e+00],\n",
       "                        [ 5.1734e-01,  1.4275e+00,  2.8196e-01],\n",
       "                        [-2.2068e-01, -1.4679e+00, -7.9905e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.6254e-01, -8.4219e-01, -4.9341e-01],\n",
       "                        [-1.1369e+00, -1.6859e+00, -1.0017e+00],\n",
       "                        [-1.0380e+00, -1.5792e+00, -6.6386e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7510e+00, -3.1391e+00, -3.7006e-01],\n",
       "                        [ 1.6549e+00,  5.4990e-01, -7.2293e-01],\n",
       "                        [ 2.8562e+00,  1.8565e+00, -1.9220e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9697e+00, -2.1434e+00, -2.3304e-01],\n",
       "                        [ 2.0525e-01,  1.9968e-01, -2.2340e-01],\n",
       "                        [ 1.7024e+00,  2.0357e+00,  1.4176e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3499e+00,  1.3571e+00, -6.4151e-02],\n",
       "                        [-2.0213e+00,  2.2228e+00, -1.1190e-02],\n",
       "                        [-9.5908e-01,  8.9595e-01,  4.1397e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7418e+00,  4.6277e+00,  7.3580e-01],\n",
       "                        [-1.0950e+00, -2.5092e-01,  3.4149e-01],\n",
       "                        [-2.2850e+00, -4.1104e+00, -1.0022e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.3324e-01, -1.0367e+00, -2.8716e-01],\n",
       "                        [-1.2417e+00, -1.5554e+00, -5.3494e-01],\n",
       "                        [-4.8025e-01, -7.7488e-01, -2.4795e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.8011e-01,  8.2644e-01, -8.2195e-01],\n",
       "                        [-9.9152e-01,  2.6504e+00, -1.3398e+00],\n",
       "                        [-5.8106e-01,  1.4735e+00, -8.8195e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.3814e-01, -3.6708e-01,  4.6930e-02],\n",
       "                        [ 6.3930e-01,  1.7008e+00,  3.4617e-01],\n",
       "                        [ 1.2318e+00,  1.0943e+00, -6.0283e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.1074e-01,  1.8591e+00,  1.8216e+00],\n",
       "                        [ 2.0722e-01,  9.3296e-01,  5.3758e-01],\n",
       "                        [-3.7145e-01, -1.0066e+00, -1.2188e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2223e-01,  1.4174e-01,  5.1162e-02],\n",
       "                        [ 1.3347e+00,  1.6996e+00,  5.4173e-01],\n",
       "                        [ 8.1068e-01,  1.1805e+00,  6.5179e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.2170e-01,  1.3620e+00, -7.4742e-01],\n",
       "                        [-7.4931e-01,  2.5994e+00, -1.4250e+00],\n",
       "                        [-5.9009e-01,  1.5829e+00, -1.0282e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.0389e-01, -8.8527e-01, -4.7661e-01],\n",
       "                        [-5.0067e-02, -5.8282e-01, -2.1236e-02],\n",
       "                        [-5.3961e-01, -4.8539e-01, -7.5217e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.3147e+00, -3.0685e-01, -8.2122e-01],\n",
       "                        [ 2.5667e+00, -3.4962e-01, -1.2315e+00],\n",
       "                        [ 3.3789e-01, -1.2490e-01, -5.9767e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3915e+00,  1.9026e+00,  1.1752e+00],\n",
       "                        [ 1.6880e+00,  1.6454e+00,  9.1196e-01],\n",
       "                        [ 3.7457e-01,  3.3312e-01,  3.2596e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.1787e-01,  4.7011e-01,  5.5692e-02],\n",
       "                        [ 2.9026e-01,  2.1678e-01,  6.7158e-02],\n",
       "                        [ 4.6783e-01,  6.1039e-01,  1.3297e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4791e+00,  1.8710e+00, -6.4747e-01],\n",
       "                        [ 9.4241e-01,  1.2074e+00, -3.9510e-01],\n",
       "                        [-1.1864e+00, -1.1274e+00,  2.1752e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.6278e-01,  1.0443e+00,  5.7882e-01],\n",
       "                        [ 1.3765e+00,  1.8834e+00,  9.3236e-01],\n",
       "                        [ 8.7443e-01,  1.2185e+00,  5.2360e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.0801e-01, -1.2903e+00, -7.0047e-01],\n",
       "                        [-1.0484e+00, -1.7738e+00, -8.6521e-01],\n",
       "                        [-4.7913e-01, -7.2251e-01, -2.5290e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.7896e-02,  9.7495e-01,  7.1689e-01],\n",
       "                        [ 9.7837e-01,  2.3998e-01,  1.9888e-01],\n",
       "                        [ 4.0985e-01, -4.5156e-01,  4.9733e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.6691e-01,  3.3341e-01,  1.2217e-01],\n",
       "                        [-1.7143e+00, -8.2129e-01,  5.1048e-01],\n",
       "                        [-8.5422e-01, -4.1077e-01,  4.0740e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4394e+00, -5.8969e+00, -2.1608e+00],\n",
       "                        [ 3.8364e-01,  6.5349e-01,  1.8052e-01],\n",
       "                        [ 2.6391e+00,  5.6107e+00,  1.7626e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.6790e-01,  1.0568e-01,  8.9811e-01],\n",
       "                        [ 5.1824e-01, -1.3863e-01,  2.4778e+00],\n",
       "                        [ 9.8216e-02, -6.1086e-01,  5.5341e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6910e+00,  3.5781e-01, -1.1607e+00],\n",
       "                        [ 2.0323e+00,  9.2059e-01, -1.4918e+00],\n",
       "                        [ 6.1964e-01,  7.0172e-01, -7.5694e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4314e+00,  2.8807e+00, -1.7451e-01],\n",
       "                        [ 8.6481e-01,  1.4553e+00, -1.7946e-01],\n",
       "                        [-1.1273e+00, -1.5606e+00, -8.9874e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.6333e+00,  2.0868e-01,  3.7283e+00],\n",
       "                        [-4.9019e+00,  6.6398e-01,  4.5051e+00],\n",
       "                        [-1.1095e+00,  4.7731e-01,  7.4635e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.1981e-01, -9.5803e-01, -4.8811e-01],\n",
       "                        [-9.9389e-01, -1.2913e+00, -7.3280e-01],\n",
       "                        [-5.5442e-01, -7.1775e-01, -1.8768e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.8338e-01,  7.8299e-01,  4.3179e-01],\n",
       "                        [ 8.7040e-01,  1.3387e+00,  8.3697e-01],\n",
       "                        [ 4.8005e-01,  7.4136e-01,  4.2451e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.4612e-01,  1.5153e+00,  9.7926e-01],\n",
       "                        [ 4.9449e-01,  9.7088e-01,  9.6151e-01],\n",
       "                        [ 6.2680e-02,  1.8449e-01,  5.0802e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3662e+00,  2.1464e+00,  9.1918e-01],\n",
       "                        [-7.2589e-01, -2.2016e+00, -6.3234e-01],\n",
       "                        [-1.0603e-01, -1.8806e-01,  4.1683e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.6950e-01, -9.5411e-01, -6.2226e-01],\n",
       "                        [-1.0847e+00, -1.3895e+00, -8.6883e-01],\n",
       "                        [-6.1639e-01, -8.3647e-01, -2.9077e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1710e+00, -1.4731e+00, -1.4006e+00],\n",
       "                        [-3.1104e+00, -1.9099e+00, -1.7115e+00],\n",
       "                        [-2.0638e+00, -1.8565e+00, -1.1140e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.4238e-01,  7.8937e-01,  3.2343e-01],\n",
       "                        [ 8.6849e-01,  1.2779e+00,  7.1617e-01],\n",
       "                        [ 4.0568e-01,  7.5451e-01,  4.9806e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.3307e-01, -5.3670e-01, -2.1723e-01],\n",
       "                        [-7.1213e-01, -8.9172e-01, -5.2730e-01],\n",
       "                        [-4.8851e-01, -9.2994e-01, -3.9963e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.1294e-01, -1.0266e+00,  3.9878e-02],\n",
       "                        [-8.4442e-01, -1.5658e+00, -5.5942e-01],\n",
       "                        [-1.2157e-01, -5.1604e-01, -3.9244e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.3661e+00,  2.7699e-01, -3.4538e+00],\n",
       "                        [ 4.9024e+00,  3.3227e-01, -5.0536e+00],\n",
       "                        [ 1.1854e+00,  8.0950e-02, -1.0132e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.8398e-02, -2.2236e+00, -1.6560e+00],\n",
       "                        [ 1.5150e-01,  7.9129e-01,  8.0288e-01],\n",
       "                        [ 1.5592e-01,  3.0851e-01,  6.6003e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.3250e-01, -2.2611e+00,  2.1583e-01],\n",
       "                        [-1.4060e+00, -1.3220e+00,  1.4993e+00],\n",
       "                        [-9.1510e-01,  1.3735e+00,  1.6449e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.9581e-01, -7.8119e-02,  2.9295e-01],\n",
       "                        [ 8.9596e-01,  6.0455e-01,  1.1289e+00],\n",
       "                        [ 6.4749e-01,  1.2162e+00, -2.6764e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7700e+00,  1.8446e+00,  1.8779e-01],\n",
       "                        [-8.6047e-01, -1.1110e+00, -3.4262e-01],\n",
       "                        [-2.3451e-01, -1.6327e-01,  1.0464e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.8617e-01, -9.9012e-01, -4.9330e-01],\n",
       "                        [-9.5153e-01, -1.4227e+00, -1.0011e+00],\n",
       "                        [-4.4737e-01, -9.1986e-01, -6.6820e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.3131e-01,  7.6267e-01,  2.0159e-01],\n",
       "                        [ 1.5471e+00,  1.7910e+00,  4.6766e-01],\n",
       "                        [ 8.4756e-01,  1.0117e+00,  3.8103e-01]]]], device='cuda:0')),\n",
       "             ('module.model.3.1.weight',\n",
       "              tensor([0.9464, 0.6763, 0.8080, 0.6577, 0.9386, 0.8258, 1.2677, 0.7741, 1.2027,\n",
       "                      1.1021, 0.6254, 1.7386, 0.8007, 1.0848, 1.0856, 0.8415, 0.9879, 0.8675,\n",
       "                      0.4918, 0.5017, 2.7795, 0.6984, 0.6899, 0.9986, 1.4140, 1.1276, 1.3950,\n",
       "                      2.9157, 2.1586, 0.9806, 0.7644, 0.5891, 0.8421, 1.2275, 0.5758, 0.9080,\n",
       "                      0.8564, 0.7521, 0.6630, 0.7023, 0.8640, 0.1068, 0.8967, 3.1229, 0.6423,\n",
       "                      0.9099, 0.7694, 3.4775, 2.2190, 0.3728, 0.9335, 0.7770, 1.1213, 0.5314,\n",
       "                      0.6479, 2.0894, 0.6157, 3.1950, 1.2212, 0.8772, 0.8195, 0.9207, 1.3415,\n",
       "                      0.9953], device='cuda:0')),\n",
       "             ('module.model.3.1.bias',\n",
       "              tensor([ 1.3701,  0.5306,  1.5709,  1.5670,  1.0730,  1.1551,  0.6941,  0.6322,\n",
       "                       0.6772,  4.8149, -0.5230, -1.6196,  0.9555,  1.2058,  0.4899,  0.7171,\n",
       "                       1.0318,  1.4564,  0.1039, -0.5550, -0.7161,  1.6319,  0.7516,  1.2393,\n",
       "                       0.7775,  0.3389,  0.1625,  5.7523, -0.3394,  3.3615,  0.7078,  0.6072,\n",
       "                       0.9564, -0.0151, -0.3930,  0.7840,  0.8391, -0.7413,  1.2198,  0.5085,\n",
       "                       4.6174, -0.7314,  1.1086,  0.5079, -0.8266,  0.6154,  1.0176,  0.3000,\n",
       "                      -0.3118,  0.7089,  1.7641,  0.3827,  1.1173, -0.3678,  0.5509,  0.1057,\n",
       "                       2.6068,  5.7900,  1.0824,  2.5463, -0.9634,  0.6180,  0.6834,  1.0629],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.3.1.running_mean',\n",
       "              tensor([-2.3670e-01,  4.9638e-01,  7.1153e-01,  2.5199e-01, -1.0706e-01,\n",
       "                      -2.8028e+00,  4.2106e-01,  1.2823e+00, -3.1816e-01, -2.4670e-01,\n",
       "                      -3.7845e-02, -1.5480e+00,  1.4524e+00, -9.1303e+00,  2.4833e+00,\n",
       "                       1.5351e+00, -1.3587e-01,  7.7941e-01,  1.0130e-01,  7.3310e-03,\n",
       "                      -4.5536e+00,  1.4257e-01,  7.4991e-01, -3.2577e+00,  2.0598e-01,\n",
       "                      -7.4880e-01,  2.5173e-01, -1.1919e-01, -4.8126e+00,  1.8843e-01,\n",
       "                       8.2520e-01,  3.7712e-01,  1.2758e+00,  2.3369e+00, -2.3262e-43,\n",
       "                       1.4863e+00,  1.4655e+00,  2.3262e-43,  1.6344e+00,  3.8361e-01,\n",
       "                      -9.2070e-01,  2.3262e-43, -1.3076e+00, -8.4215e-01, -2.3262e-43,\n",
       "                       5.7373e-01,  3.6282e+00, -1.8464e-01, -3.8952e+00,  3.5889e-03,\n",
       "                       2.7966e+00,  2.7323e-01, -2.0240e+00, -2.3262e-43,  2.5541e-01,\n",
       "                      -1.4593e+00, -1.4629e+00,  6.1539e-01, -3.1717e-01, -4.2117e+00,\n",
       "                       2.3262e-43,  2.0623e-01, -4.0854e+00,  2.1062e+00], device='cuda:0')),\n",
       "             ('module.model.3.1.running_var',\n",
       "              tensor([1.1779e+01, 1.9336e+00, 3.1214e+00, 6.1962e+00, 4.4751e+00, 1.0575e+01,\n",
       "                      1.8779e+01, 7.0803e+00, 2.2039e+00, 4.8187e+01, 4.1864e-02, 4.9986e+00,\n",
       "                      6.8442e+00, 9.0843e+01, 5.2845e+01, 1.0489e+01, 1.1875e+01, 8.4332e+00,\n",
       "                      5.2469e-01, 2.2134e-02, 3.2710e+01, 2.1189e+00, 5.8938e+00, 2.2200e+01,\n",
       "                      8.4685e+00, 7.8697e+00, 5.9737e+01, 2.8417e+01, 3.2087e+01, 6.4813e+01,\n",
       "                      5.8750e+00, 2.6454e+00, 6.2073e+00, 6.9134e+01, 2.3262e-43, 2.2574e+01,\n",
       "                      1.0590e+01, 2.3262e-43, 1.2523e+01, 1.5230e+00, 4.2367e+00, 2.3262e-43,\n",
       "                      6.8894e+00, 5.3247e+01, 2.3262e-43, 9.2376e+00, 3.6063e+01, 3.5620e+01,\n",
       "                      2.3596e+01, 9.1299e-03, 1.3069e+01, 8.2150e+00, 9.8345e+00, 2.3262e-43,\n",
       "                      6.3267e-01, 5.7964e+00, 7.8930e+00, 4.0753e+01, 7.3745e+00, 3.1499e+01,\n",
       "                      2.3262e-43, 5.7767e+00, 2.4498e+01, 1.3922e+01], device='cuda:0')),\n",
       "             ('module.model.3.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.3.2.clip_val', tensor([6.3233], device='cuda:0')),\n",
       "             ('module.model.4.0.weight', tensor([[[[ 0.0785]],\n",
       "              \n",
       "                       [[-0.0370]],\n",
       "              \n",
       "                       [[-0.2942]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0407]],\n",
       "              \n",
       "                       [[ 0.2888]],\n",
       "              \n",
       "                       [[ 0.2139]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0373]],\n",
       "              \n",
       "                       [[ 0.1709]],\n",
       "              \n",
       "                       [[-0.0022]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0179]],\n",
       "              \n",
       "                       [[-0.0275]],\n",
       "              \n",
       "                       [[ 0.1004]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0135]],\n",
       "              \n",
       "                       [[-0.0191]],\n",
       "              \n",
       "                       [[-0.0168]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0025]],\n",
       "              \n",
       "                       [[-0.0410]],\n",
       "              \n",
       "                       [[ 0.0032]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.2842]],\n",
       "              \n",
       "                       [[-0.0316]],\n",
       "              \n",
       "                       [[ 0.0021]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.2568]],\n",
       "              \n",
       "                       [[-0.0093]],\n",
       "              \n",
       "                       [[-0.1615]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0365]],\n",
       "              \n",
       "                       [[ 0.0529]],\n",
       "              \n",
       "                       [[-0.1196]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0211]],\n",
       "              \n",
       "                       [[-0.1452]],\n",
       "              \n",
       "                       [[-0.0995]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0062]],\n",
       "              \n",
       "                       [[ 0.5850]],\n",
       "              \n",
       "                       [[ 0.2322]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0351]],\n",
       "              \n",
       "                       [[-0.2418]],\n",
       "              \n",
       "                       [[ 0.0729]]]], device='cuda:0')),\n",
       "             ('module.model.4.1.weight',\n",
       "              tensor([1.0045, 1.3574, 0.8998, 1.4196, 1.5456, 1.5724, 0.6147, 1.1640, 1.6754,\n",
       "                      1.1690, 0.6306, 1.4182, 0.7004, 1.2202, 0.9969, 1.1795, 1.5017, 0.6348,\n",
       "                      0.7631, 0.6541, 0.9356, 1.5494, 1.1515, 1.1164, 0.9600, 0.3683, 1.1288,\n",
       "                      1.0936, 0.8501, 0.7842, 1.6434, 0.8019, 0.5289, 1.1916, 1.1470, 1.3937,\n",
       "                      0.4832, 1.1702, 1.6586, 0.8290, 0.6847, 1.0775, 1.4811, 1.3269, 0.6362,\n",
       "                      0.4765, 1.4019, 0.8451, 0.7146, 0.5852, 1.0077, 1.0238, 0.2614, 1.4289,\n",
       "                      0.8129, 1.0829, 1.0768, 0.8068, 0.7130, 1.0223, 1.0167, 1.3200, 1.1303,\n",
       "                      0.9374, 1.5185, 1.0731, 1.1057, 1.5796, 1.2054, 0.4716, 1.1458, 1.2751,\n",
       "                      0.7476, 1.0832, 1.1399, 0.9964, 1.2092, 0.7051, 1.5195, 1.0227, 0.8224,\n",
       "                      0.6676, 1.6592, 1.0445, 1.4009, 1.0017, 1.3665, 1.4763, 0.8039, 1.7968,\n",
       "                      1.2840, 0.7220, 1.2340, 1.3832, 1.4813, 0.9742, 0.3255, 1.3173, 1.3194,\n",
       "                      2.2518, 0.9622, 1.6217, 0.5453, 1.2652, 1.0829, 1.0996, 1.0556, 1.0326,\n",
       "                      0.5952, 0.9706, 1.4268, 1.2411, 1.2944, 1.2653, 0.9622, 1.2138, 1.1380,\n",
       "                      1.3738, 0.8275, 0.7776, 1.2226, 0.6585, 1.2693, 1.3754, 1.0700, 1.1963,\n",
       "                      0.6618, 0.7614], device='cuda:0')),\n",
       "             ('module.model.4.1.bias',\n",
       "              tensor([ 1.3136, -0.7799, -0.7159,  0.7453,  0.0687,  0.1194,  1.1209,  1.0927,\n",
       "                      -0.2420,  1.3238,  1.3201,  0.3171,  1.3263,  0.4903,  0.5552,  0.4355,\n",
       "                       0.5744,  1.6951,  1.1624,  1.4253,  0.1310,  0.5200,  0.5405,  1.2293,\n",
       "                       1.0692,  1.2903,  0.6400, -1.9241,  0.8165,  2.3062,  1.3944,  1.8225,\n",
       "                       1.1533,  1.1606,  0.7806, -0.0689,  1.1224,  0.9714, -0.8149,  1.2362,\n",
       "                       1.2688,  0.7847, -0.1819,  0.5273,  1.0604,  1.2406, -0.1351,  1.0232,\n",
       "                       1.8681,  1.1200,  0.6626,  0.8398,  1.9170,  0.2702, -0.6802,  1.3364,\n",
       "                      -0.2774,  2.6540,  1.4142,  0.8227,  0.8232,  1.6573,  0.7193,  0.5158,\n",
       "                       0.5249,  0.2479, -2.4131,  0.3765, -0.4915,  1.3530,  0.7268, -0.0149,\n",
       "                       1.0921,  0.3700, -0.2545,  0.7952,  0.8529,  1.0480,  1.3008,  0.5516,\n",
       "                       1.2655,  1.6119,  0.3966,  1.0163, -0.0787,  1.1193,  0.2546,  0.3444,\n",
       "                       0.8622,  0.2082,  0.8931,  0.8867,  1.1246, -0.2859,  0.6346,  0.7803,\n",
       "                       2.7346,  0.5417,  0.8480,  0.4408,  0.9949,  1.8265,  2.0027, -0.1699,\n",
       "                       0.5388,  0.4821,  0.1644,  0.0541,  1.5654,  1.0302, -0.1000,  0.5688,\n",
       "                       1.1038,  0.1296,  1.1071,  1.1409,  0.0555,  0.3374,  0.9701,  1.6164,\n",
       "                       0.7356,  2.3197,  0.0368,  0.5890,  0.9035,  0.8478,  1.9297,  1.0409],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.4.1.running_mean',\n",
       "              tensor([-2.5915e+00,  6.5017e-01,  1.1137e+00,  2.3504e-02,  2.1515e+00,\n",
       "                      -1.0438e+00,  1.9630e+00, -8.6238e-01,  4.7303e+00, -2.0641e+00,\n",
       "                      -1.0220e+00,  9.9438e-01,  5.4230e+00,  7.3336e-01, -3.0529e-01,\n",
       "                      -1.2423e+00, -2.6136e+00, -6.1871e-01, -1.5010e+00,  2.4939e+00,\n",
       "                       1.1998e+00,  3.1294e+00,  6.4816e-01,  1.2470e+00,  5.1426e-01,\n",
       "                      -1.4210e+00, -2.7529e-01, -2.6078e-01,  8.7641e-01,  6.5423e-01,\n",
       "                      -2.1053e-01, -8.7719e-01, -2.2570e+00,  1.6506e+00,  2.2402e-01,\n",
       "                       2.0539e-01, -1.2032e+00, -4.3926e-01,  4.0474e+00, -1.9157e-01,\n",
       "                      -8.3191e-01,  3.3443e-01, -2.7624e+00, -2.0440e+00, -2.2800e+00,\n",
       "                      -3.7638e+00, -1.4654e+00,  2.9814e+00,  3.3240e+00,  1.2022e+00,\n",
       "                       1.5694e+00,  1.5542e+00,  6.9261e-01,  9.7819e-01,  2.8277e-01,\n",
       "                      -5.3517e-01, -4.0613e-01,  2.0484e+00,  4.5261e-02, -1.4793e+00,\n",
       "                       3.1962e-02, -1.9112e+00, -4.6558e-01, -8.7095e-01,  1.2539e+00,\n",
       "                      -2.0621e+00,  1.0063e+00,  2.8131e-01,  1.2814e+00, -1.3976e+00,\n",
       "                      -2.5719e+00, -3.8265e-01, -6.4669e-02, -2.4000e-03,  2.8124e-01,\n",
       "                      -2.8779e+00,  1.1943e+00,  7.9503e-01,  1.6425e+00,  5.9143e-01,\n",
       "                       4.1256e+00, -3.3163e+00,  2.5828e-01,  7.7609e-01, -1.1110e+00,\n",
       "                       1.5012e+00,  3.9045e+00,  6.7622e-01, -1.1626e+00, -3.4466e-01,\n",
       "                       6.2512e-01, -1.5474e-01,  6.0709e-01,  1.5030e+00,  6.3974e-01,\n",
       "                       6.1609e-02,  2.2163e-01,  2.6299e-01,  1.9739e+00,  2.5906e+00,\n",
       "                      -2.9327e+00, -9.4008e-01,  2.0121e+00, -5.4866e-01, -1.3388e+00,\n",
       "                      -1.1605e+00,  5.4816e+00,  2.5912e+00,  1.2430e+00,  6.4759e-01,\n",
       "                       1.4265e+00,  8.3542e-01,  7.8263e-02, -9.6593e-01, -2.6045e-01,\n",
       "                      -1.2496e+00, -1.6827e-01, -2.4742e+00,  5.5353e-01,  5.2722e-01,\n",
       "                       2.4398e+00,  1.3572e+00,  2.2767e+00, -5.3108e-02,  9.8379e-02,\n",
       "                      -2.7579e+00, -4.3430e-01, -1.2088e+00], device='cuda:0')),\n",
       "             ('module.model.4.1.running_var',\n",
       "              tensor([2.9207, 0.3576, 0.2548, 0.5475, 0.9227, 0.5556, 0.3341, 0.6864, 2.0997,\n",
       "                      0.3578, 1.8487, 0.4358, 1.4771, 0.8865, 0.8373, 0.4513, 0.5969, 0.4218,\n",
       "                      0.6009, 0.4581, 0.6371, 0.9613, 0.2791, 0.3994, 1.1248, 2.9624, 0.4770,\n",
       "                      0.1135, 0.3704, 0.4295, 0.4297, 1.0130, 0.3225, 0.4487, 0.9495, 0.2165,\n",
       "                      0.6294, 0.4016, 0.9207, 1.2987, 0.4312, 0.6963, 0.9308, 1.1771, 0.7265,\n",
       "                      3.6651, 1.2732, 0.6404, 0.6616, 0.3413, 0.5585, 0.8010, 0.5686, 1.1560,\n",
       "                      0.2871, 0.4929, 0.2323, 3.3011, 0.5234, 0.5449, 1.1871, 3.9871, 0.8902,\n",
       "                      0.2470, 0.2950, 0.6341, 0.2472, 0.4767, 0.3951, 0.3951, 1.8430, 0.3544,\n",
       "                      0.6070, 0.4990, 0.2862, 1.3365, 0.6199, 0.9511, 0.6068, 0.4160, 1.3378,\n",
       "                      1.3739, 1.0896, 0.3778, 0.3369, 0.4569, 1.4095, 0.6177, 0.3810, 0.8304,\n",
       "                      0.5188, 0.7123, 1.1056, 1.2456, 0.4496, 0.4091, 1.4859, 0.7182, 0.6231,\n",
       "                      1.8007, 1.3051, 0.4878, 1.3360, 0.1454, 0.5993, 0.7349, 1.5659, 0.9871,\n",
       "                      0.4457, 0.3546, 0.3990, 1.0720, 0.7705, 0.2158, 0.8654, 0.4308, 0.5390,\n",
       "                      0.4049, 0.4043, 0.4271, 0.6466, 0.6880, 1.8084, 0.4997, 1.0853, 1.0538,\n",
       "                      0.4646, 2.8101], device='cuda:0')),\n",
       "             ('module.model.4.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.4.2.clip_val', tensor([5.9336], device='cuda:0')),\n",
       "             ('module.model.5.0.weight',\n",
       "              tensor([[[[-2.1955e-01,  4.5004e-01, -4.4335e-01],\n",
       "                        [-9.1050e-02,  4.9866e+00, -1.3171e+00],\n",
       "                        [-4.0392e-01, -1.0000e+00,  3.1766e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.3467e-01, -5.6767e-01,  1.0145e-02],\n",
       "                        [-2.5808e-01, -8.6264e-01, -2.4664e-02],\n",
       "                        [-1.8538e-01, -3.0152e-01,  5.2584e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.3806e-02,  4.6402e-01, -3.2355e-01],\n",
       "                        [ 5.1852e-01,  1.2999e+00, -4.0585e-01],\n",
       "                        [-1.5238e-01, -2.5684e-01, -2.3287e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 5.2417e-01,  3.3403e+00, -5.4843e-01],\n",
       "                        [ 1.4919e-01,  1.6903e+00, -4.8180e-01],\n",
       "                        [ 1.4088e-01, -3.5859e-02, -2.4694e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3165e+00, -3.4263e+00,  1.0032e+00],\n",
       "                        [-1.2407e+00,  1.7240e+00, -2.4210e-01],\n",
       "                        [-3.7967e-01,  1.4821e+00, -8.5568e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0408e+00,  2.7636e+00,  2.0831e-02],\n",
       "                        [-1.5497e+00, -2.0162e+00, -7.1039e-01],\n",
       "                        [ 2.9044e-01,  1.6677e-01,  3.4373e-01]]]], device='cuda:0')),\n",
       "             ('module.model.5.1.weight',\n",
       "              tensor([0.9441, 0.4269, 0.5312, 1.2539, 0.8114, 1.1293, 0.5622, 0.5906, 0.6028,\n",
       "                      1.2925, 1.2191, 0.9465, 1.0773, 1.4200, 0.5837, 0.8996, 1.4064, 1.6704,\n",
       "                      0.5428, 1.5497, 0.5446, 1.2099, 0.4008, 1.2034, 0.8038, 1.2480, 0.9698,\n",
       "                      0.4674, 1.3433, 1.1151, 0.9639, 1.7142, 1.0225, 1.3007, 1.0668, 1.0786,\n",
       "                      0.6738, 1.0671, 2.1239, 1.5901, 1.4051, 0.9760, 0.7343, 0.7727, 0.8093,\n",
       "                      1.2728, 1.0706, 0.6303, 0.5614, 0.9903, 0.4834, 1.0132, 1.3944, 0.7201,\n",
       "                      0.5342, 1.4697, 0.8020, 2.1261, 1.5571, 0.7499, 0.9565, 0.9250, 0.9598,\n",
       "                      0.9355, 1.2343, 0.8047, 0.5781, 0.9529, 0.4300, 0.8730, 0.9868, 0.6330,\n",
       "                      0.8946, 0.7322, 1.3775, 1.0834, 0.7741, 0.9058, 1.3287, 0.7760, 1.1094,\n",
       "                      0.8041, 1.0640, 1.1941, 0.5404, 1.0038, 0.7837, 0.9757, 1.1113, 1.5331,\n",
       "                      1.3103, 0.9067, 1.4180, 0.9947, 1.3177, 0.4951, 0.8364, 0.6804, 1.7416,\n",
       "                      0.6808, 1.0813, 1.7412, 1.6718, 0.5395, 1.3970, 0.8098, 0.9132, 0.6908,\n",
       "                      1.1500, 0.9583, 0.8638, 0.8251, 1.5620, 0.3759, 1.1674, 1.3793, 0.5198,\n",
       "                      0.4348, 0.9920, 1.5163, 0.7728, 1.2913, 0.6275, 0.5695, 0.8565, 0.7206,\n",
       "                      1.5206, 1.2449], device='cuda:0')),\n",
       "             ('module.model.5.1.bias',\n",
       "              tensor([ 1.9527e+00,  2.0950e+00, -3.1849e-01,  2.3192e-02,  8.8430e-01,\n",
       "                       4.0235e-01,  5.5557e-01,  5.0521e-01,  2.2203e+00,  3.3612e-02,\n",
       "                       1.2528e+00,  5.3113e-01,  2.8156e+00,  8.8300e-02,  5.8288e-01,\n",
       "                       5.5097e-01, -7.8544e-02, -5.4153e-02,  1.4434e-01,  4.2926e-01,\n",
       "                       4.6850e-01, -7.5339e-02,  1.6294e+00,  1.1968e-02,  2.9721e+00,\n",
       "                      -3.4087e-01,  2.7526e-01, -1.0324e+00,  1.9033e-01,  2.8752e-01,\n",
       "                       2.3524e+00,  4.8352e-01,  5.2097e-01,  1.3131e+00,  8.0732e-01,\n",
       "                       1.3163e+00,  2.5716e-01,  7.2937e-01, -5.4466e-02, -1.0907e-01,\n",
       "                       1.9903e+00,  1.7322e-02,  8.0768e-01,  2.0409e+00,  3.8223e+00,\n",
       "                      -2.9038e-01,  4.0906e-01,  1.4752e+00,  6.2171e-01,  1.1534e+00,\n",
       "                       1.1100e+00,  2.1420e-01, -3.8913e-01,  7.7812e-01, -1.3493e-01,\n",
       "                       9.7508e-03, -9.1541e-01,  2.7892e+00,  5.6399e-01,  2.5523e-01,\n",
       "                       4.7652e-01,  2.6596e+00, -4.1819e-02,  7.0203e-01, -8.6828e-03,\n",
       "                      -1.3184e-01, -1.7463e+00, -1.3097e-01,  1.0319e+00, -1.7855e-02,\n",
       "                       5.7415e-02,  1.1789e+00,  4.2879e-01,  5.5148e-01,  3.7493e-01,\n",
       "                       3.8376e-01,  2.2372e+00,  3.1274e+00,  1.5312e-02,  4.8385e-01,\n",
       "                       3.5825e-01,  2.2076e+00,  5.6870e-02,  4.0665e-01,  1.9981e+00,\n",
       "                      -3.4088e-01,  1.3675e+00,  5.5845e-01,  1.8118e+00,  4.4348e-03,\n",
       "                       5.7859e-01,  7.5791e-01, -1.1240e-01,  4.0894e-01, -2.7515e-03,\n",
       "                       1.0978e+00,  1.9918e+00,  3.0511e+00,  1.8210e-01,  2.0519e-01,\n",
       "                       4.2568e-01,  1.7119e-02, -2.4707e-01,  4.1932e-02,  1.0217e-01,\n",
       "                       1.7533e-01,  9.5426e-02,  4.2371e-01,  7.5796e-02, -3.5000e-01,\n",
       "                       6.4998e-01,  5.2954e-01, -5.9457e-03,  5.2079e-01,  6.8908e-01,\n",
       "                       4.1638e-02,  1.3367e-01,  1.2046e+00,  2.0501e+00,  1.5201e+00,\n",
       "                       1.0346e+00,  4.2409e-01,  7.2739e-01,  5.5176e-01,  4.2573e-01,\n",
       "                       1.8291e+00,  1.4956e+00,  1.7872e+00], device='cuda:0')),\n",
       "             ('module.model.5.1.running_mean',\n",
       "              tensor([ 2.4414e+00, -5.5322e-01,  9.4422e-02,  5.2436e-01,  1.6577e+00,\n",
       "                      -2.1261e+00, -7.0714e-01, -2.0307e+00, -8.0458e-01,  5.3478e-01,\n",
       "                       1.5446e-01,  1.2158e-01,  3.1319e+00,  9.4813e-01,  1.0062e+00,\n",
       "                       5.9957e-01,  1.1519e+00, -1.8186e-02, -1.1743e+00,  2.6060e-01,\n",
       "                       3.7990e-01,  2.4888e+00, -9.8602e-01,  2.7233e-01,  6.6849e-01,\n",
       "                       7.5500e+00, -1.7040e-01,  8.7809e-03,  2.1175e-01,  9.6182e-01,\n",
       "                       7.2939e-01, -2.6348e-01, -2.1247e-02, -3.4358e-02, -1.9710e+00,\n",
       "                      -2.8612e-01,  1.6262e+00,  4.8924e-01,  7.9388e-02, -3.5258e-01,\n",
       "                      -1.8741e-01,  1.4009e-01,  1.0331e+00, -7.1114e-01, -2.6630e-01,\n",
       "                       4.0923e+00,  6.7418e-01,  7.7977e-01, -5.1584e+00, -1.1177e-01,\n",
       "                      -1.7536e+00,  2.5577e+00, -3.6095e+00, -1.1765e+00,  1.5180e-01,\n",
       "                       7.7755e-01,  3.0817e-01,  1.3572e-01, -1.1386e-02, -1.6224e+00,\n",
       "                       2.4809e+00, -1.8496e+00,  8.8233e-01, -2.4982e-01, -1.3575e-01,\n",
       "                       3.5067e-01,  8.5946e-04,  3.2026e-01, -3.8220e-01,  5.9200e-03,\n",
       "                       2.1376e+00,  2.0234e-01, -1.5872e+00,  7.6904e-01, -5.5629e-01,\n",
       "                       2.5335e+00, -2.9979e-01,  1.3788e-02,  8.8135e-01,  6.1242e-01,\n",
       "                       6.8736e-01,  3.6575e+00, -2.9600e-01,  1.7843e-01,  8.3793e-02,\n",
       "                       2.1065e+00,  9.7161e-02,  6.0050e-01, -1.2366e-02, -1.7298e-01,\n",
       "                       1.1245e+00,  5.4737e-01,  9.2231e-01,  5.0090e-01,  9.5157e-02,\n",
       "                      -1.3213e+00,  2.9443e+00, -2.4040e-01, -1.5026e-01,  2.6889e+00,\n",
       "                       6.8334e-01,  7.5663e-02, -8.5901e+00, -7.6872e-01,  5.3264e-01,\n",
       "                       2.2328e-01,  6.4647e-01,  1.4268e+00, -3.1237e-01, -1.9422e+00,\n",
       "                      -1.3953e+00, -1.4051e+00,  6.1462e-01, -2.1979e+00, -1.9171e+00,\n",
       "                       3.5053e-01,  5.0979e-01, -2.4134e+00,  6.8705e-01,  5.8578e-02,\n",
       "                       3.0658e-01,  4.1472e-01, -7.8830e-01, -1.0682e+00,  2.6289e+00,\n",
       "                       3.4910e+00, -1.1840e+00,  3.0870e-01], device='cuda:0')),\n",
       "             ('module.model.5.1.running_var',\n",
       "              tensor([1.0119e+01, 1.0324e+00, 1.9517e-01, 7.0674e+00, 8.1620e+00, 1.3366e+01,\n",
       "                      8.5045e-01, 3.8147e+00, 1.9271e+00, 1.5979e+01, 3.2536e+00, 8.1679e+00,\n",
       "                      4.5763e+00, 7.4271e+00, 4.3354e+00, 3.4279e+00, 1.2856e+01, 4.3216e+00,\n",
       "                      5.7022e-01, 6.7721e+00, 3.1279e+00, 1.4659e+01, 1.6927e+00, 1.2325e+01,\n",
       "                      3.2163e+00, 2.2268e+00, 4.0854e+00, 5.7168e-02, 2.7196e+00, 1.1964e+01,\n",
       "                      2.2297e+01, 7.1058e+00, 1.4185e+00, 1.5941e+01, 7.7617e+00, 4.2110e+00,\n",
       "                      9.8020e-01, 1.0375e+01, 5.9310e+00, 6.9155e+00, 3.9975e+00, 3.8961e+00,\n",
       "                      3.5281e+00, 6.4239e+00, 2.5230e+00, 7.9822e-01, 4.2697e+00, 4.4193e+00,\n",
       "                      4.3064e+00, 2.6953e+00, 2.9852e+00, 1.1225e+01, 6.9791e-01, 4.0494e+00,\n",
       "                      2.5622e-01, 1.1852e+01, 5.3345e-01, 1.1306e+01, 5.1759e+00, 1.5683e+00,\n",
       "                      3.0623e+00, 1.3537e+01, 5.0434e+00, 5.4135e+00, 1.4480e+01, 1.2096e+00,\n",
       "                      5.6286e-03, 5.7269e+00, 5.2653e-01, 2.5152e+00, 4.3466e+00, 6.6316e+00,\n",
       "                      1.2484e+00, 1.9609e+00, 3.3419e+00, 6.4698e+00, 1.9040e+00, 2.6001e+00,\n",
       "                      1.0929e+01, 6.1408e+00, 6.7369e+00, 5.6272e+00, 2.2104e+00, 9.3104e+00,\n",
       "                      6.5580e+00, 4.7535e+00, 4.0032e+00, 6.3980e+00, 2.5242e+00, 4.0206e+00,\n",
       "                      1.1387e+01, 2.2342e+00, 4.0720e+00, 2.4442e+00, 8.3690e+00, 3.0337e+00,\n",
       "                      2.0226e+00, 2.7100e+00, 1.6892e+01, 8.6417e+00, 3.5157e+00, 2.8324e+01,\n",
       "                      6.0964e+00, 1.2072e+00, 5.0983e+00, 7.4096e+00, 1.5056e+00, 4.6510e+00,\n",
       "                      1.0591e+00, 2.2342e+00, 5.6217e+00, 2.1804e+00, 1.7577e+01, 5.4041e+00,\n",
       "                      8.0124e+00, 1.0582e+01, 1.6689e+00, 6.4915e+00, 3.6803e+00, 7.9079e+00,\n",
       "                      1.0110e+01, 1.8348e+01, 1.7581e+00, 2.2929e+00, 3.5366e+00, 1.4766e+01,\n",
       "                      5.8864e+00, 2.2322e+00], device='cuda:0')),\n",
       "             ('module.model.5.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.5.2.clip_val', tensor([5.6410], device='cuda:0')),\n",
       "             ('module.model.6.0.weight', tensor([[[[-0.0444]],\n",
       "              \n",
       "                       [[ 0.0204]],\n",
       "              \n",
       "                       [[-0.0047]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0289]],\n",
       "              \n",
       "                       [[ 0.0183]],\n",
       "              \n",
       "                       [[ 0.0322]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1866]],\n",
       "              \n",
       "                       [[-0.1031]],\n",
       "              \n",
       "                       [[ 0.0927]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0296]],\n",
       "              \n",
       "                       [[ 0.0092]],\n",
       "              \n",
       "                       [[-0.0263]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0004]],\n",
       "              \n",
       "                       [[-0.0399]],\n",
       "              \n",
       "                       [[-0.0688]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0231]],\n",
       "              \n",
       "                       [[-0.0218]],\n",
       "              \n",
       "                       [[ 0.0144]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.2145]],\n",
       "              \n",
       "                       [[-0.0462]],\n",
       "              \n",
       "                       [[-0.0247]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1183]],\n",
       "              \n",
       "                       [[-0.0069]],\n",
       "              \n",
       "                       [[ 0.0364]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1181]],\n",
       "              \n",
       "                       [[ 0.0764]],\n",
       "              \n",
       "                       [[ 0.0016]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0058]],\n",
       "              \n",
       "                       [[ 0.0065]],\n",
       "              \n",
       "                       [[-0.0874]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0394]],\n",
       "              \n",
       "                       [[-0.1862]],\n",
       "              \n",
       "                       [[ 0.0405]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0568]],\n",
       "              \n",
       "                       [[-0.0612]],\n",
       "              \n",
       "                       [[-0.1247]]]], device='cuda:0')),\n",
       "             ('module.model.6.1.weight',\n",
       "              tensor([0.9250, 1.0584, 1.9038, 1.8567, 1.7010, 0.8224, 1.6902, 1.5131, 1.1531,\n",
       "                      0.5877, 1.7498, 0.5213, 1.5132, 1.4316, 1.1928, 1.1119, 1.8156, 1.3098,\n",
       "                      1.6312, 1.4183, 0.7931, 1.6732, 1.4766, 1.4951, 1.5054, 1.7531, 1.2889,\n",
       "                      1.0695, 1.3991, 1.6363, 1.5855, 1.4430, 0.7780, 1.3819, 1.8006, 1.1030,\n",
       "                      1.4128, 0.6205, 1.0906, 1.0119, 2.0043, 0.3745, 1.6881, 0.5333, 1.4339,\n",
       "                      0.5480, 1.5783, 2.3248, 1.5701, 0.3852, 1.8342, 1.1500, 1.7250, 1.3882,\n",
       "                      1.4753, 1.4329, 0.5117, 1.5490, 1.1240, 1.6271, 0.8934, 0.9639, 0.9983,\n",
       "                      1.7599, 1.5093, 1.4515, 2.4477, 1.4007, 1.4352, 1.3456, 0.4129, 1.5490,\n",
       "                      1.1489, 0.6328, 1.3625, 1.2604, 0.8147, 0.5221, 1.2251, 0.4706, 0.7468,\n",
       "                      1.0308, 0.6944, 0.6479, 0.4876, 1.8528, 1.4902, 1.8271, 1.0369, 1.3815,\n",
       "                      1.2227, 1.3634, 0.8047, 0.9587, 0.4338, 1.6431, 0.7752, 1.4252, 0.6056,\n",
       "                      1.6062, 1.3214, 1.2152, 1.5062, 1.4968, 1.3571, 1.2187, 1.0055, 0.8750,\n",
       "                      0.4599, 0.4434, 0.6805, 1.8995, 1.4029, 0.5735, 1.5166, 2.1247, 2.0265,\n",
       "                      1.8473, 0.5721, 0.6647, 1.3298, 0.4560, 0.9191, 1.3040, 0.9557, 0.9069,\n",
       "                      1.2718, 1.3705], device='cuda:0')),\n",
       "             ('module.model.6.1.bias',\n",
       "              tensor([-0.3775,  0.2553,  0.1690,  1.0123, -0.4044,  1.0047,  0.7899, -0.4161,\n",
       "                       1.7052,  1.3769, -0.3365,  1.2390,  0.4631, -0.1690,  0.9168, -0.5815,\n",
       "                       0.2233,  0.6368, -0.0853,  0.0291,  0.6264, -0.3642,  0.0608, -0.6493,\n",
       "                      -0.5194, -0.2445, -0.0964,  1.1054, -0.2147,  0.0733, -0.5575, -0.7523,\n",
       "                       1.1032, -0.4678, -0.7542, -0.3392, -0.1825,  1.6332,  0.4375, -0.1698,\n",
       "                      -0.3310,  1.4968, -0.1244,  0.5703, -0.5005,  1.0805, -0.2575,  1.3500,\n",
       "                      -0.5594,  1.3515, -0.3342, -0.3793, -0.8047, -0.6633, -0.4259,  0.6980,\n",
       "                       1.1274, -0.2305, -0.4191,  0.7854,  1.0711, -0.2492,  1.0460,  0.0224,\n",
       "                      -0.2194, -1.4940,  0.3343, -0.1509,  0.5446, -0.4661,  1.1354, -0.4162,\n",
       "                       0.8787,  1.4196, -0.3117, -0.3885, -0.6403,  1.2888, -0.5649,  1.0259,\n",
       "                       1.3555, -0.0448,  0.9849,  1.7824,  1.0226,  0.0059, -0.2924, -1.0601,\n",
       "                      -0.7551, -1.2703, -0.1169, -0.5701,  0.7777, -0.1675,  1.2478,  0.6530,\n",
       "                       0.9609, -0.5850,  1.8788,  0.3704, -0.3268, -0.0759, -0.1931, -0.2085,\n",
       "                       0.2272, -0.7585,  1.2362,  1.0019,  1.1595,  1.2553,  1.6390, -0.4643,\n",
       "                       0.3739,  1.4609, -0.6983,  0.2520,  1.9208, -0.9712,  1.4905,  1.3633,\n",
       "                      -0.4240,  2.1513,  0.2111, -0.1907,  0.7379,  1.0602, -0.1620, -1.3252],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.6.1.running_mean',\n",
       "              tensor([ 0.8261,  1.4980,  1.5279,  1.7864, -0.0250,  0.5202, -0.4253,  0.3000,\n",
       "                       0.9361,  0.6109, -2.2384, -0.6344,  1.0191, -0.9499, -1.4439,  1.4736,\n",
       "                       1.2980, -1.6676, -1.2762,  0.9164,  0.6362,  0.0471, -1.3437,  0.7342,\n",
       "                      -0.9842, -1.8579,  1.5227,  1.5042, -1.5691,  0.3582,  0.0980, -0.8081,\n",
       "                       1.0307, -2.6941, -2.6432,  1.8314,  0.5512, -0.6204,  2.0966,  4.2852,\n",
       "                       1.8586, -2.0490, -0.9094,  0.3434,  0.1295, -0.6087, -3.2525,  0.5357,\n",
       "                       2.0377,  0.0715, -2.1937,  2.5220, -1.8910, -1.5680, -1.5290, -1.8831,\n",
       "                      -1.8098,  0.0160,  0.0419, -2.3470, -0.1657, -0.6576,  1.7579, -3.6400,\n",
       "                      -0.8791, -0.8961,  1.4065,  0.2571, -0.5297,  3.0027,  1.9070,  2.5636,\n",
       "                      -3.1001,  0.4370, -0.7247,  1.6542,  0.4344, -1.2286,  1.8649,  1.5652,\n",
       "                       0.1814,  3.1159, -2.1172,  0.5735,  1.2183,  1.0878, -0.3771, -1.9674,\n",
       "                      -1.9199, -0.7970, -1.6905,  4.3613,  0.8469,  1.3523,  1.1400,  0.4664,\n",
       "                       1.5992, -0.7647,  0.0992,  0.2404,  2.8518,  0.8824, -0.1556, -1.1218,\n",
       "                       1.3304,  3.7451, -0.3407,  0.7954,  0.5753,  1.1087,  1.2334, -1.4061,\n",
       "                      -1.3396,  1.3174, -1.4027, -0.1897,  0.9392, -3.1217,  0.3226,  0.1804,\n",
       "                       1.2299,  3.4770, -1.2579, -2.3137,  1.1588,  0.9730,  0.0337, -0.2710],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.6.1.running_var',\n",
       "              tensor([0.6428, 0.8681, 0.9454, 1.2272, 1.2677, 1.1646, 0.4371, 1.5314, 1.1786,\n",
       "                      0.9632, 1.6349, 0.3022, 0.9905, 1.0862, 1.6475, 0.6777, 1.4397, 0.9862,\n",
       "                      1.7596, 1.0221, 1.2671, 1.6931, 1.4502, 1.0601, 1.4524, 2.8067, 1.1922,\n",
       "                      0.8913, 1.2135, 1.1527, 1.2100, 2.0568, 0.7449, 1.4285, 3.6245, 0.8924,\n",
       "                      1.2976, 1.5734, 1.5437, 0.8658, 1.3776, 0.5985, 1.1271, 0.2936, 1.3623,\n",
       "                      0.5160, 1.4220, 1.2769, 1.4443, 0.9653, 1.2543, 1.0960, 1.1983, 1.8465,\n",
       "                      1.0919, 0.8853, 0.9421, 1.0855, 0.8227, 1.3030, 0.6070, 0.5739, 0.4872,\n",
       "                      2.2030, 1.2250, 0.5695, 1.6490, 0.8746, 0.7057, 1.2605, 0.6186, 1.1100,\n",
       "                      1.2817, 0.9940, 1.2325, 1.1864, 0.4598, 0.4607, 1.3348, 0.6149, 0.5417,\n",
       "                      1.0467, 0.4012, 1.0381, 0.8332, 1.1751, 1.7728, 3.2672, 1.1147, 1.0846,\n",
       "                      1.0638, 0.8848, 0.5370, 0.4005, 0.5561, 0.9354, 0.8204, 1.7150, 0.6216,\n",
       "                      1.0144, 1.3594, 1.1431, 1.7560, 1.7011, 1.5362, 1.3020, 1.2355, 0.7826,\n",
       "                      0.6923, 0.4871, 1.1052, 2.0163, 1.8066, 1.1885, 1.3462, 0.7969, 1.7946,\n",
       "                      2.5324, 0.8891, 1.0006, 1.3264, 1.1595, 0.8048, 1.1845, 0.4111, 0.8601,\n",
       "                      0.3711, 1.3725], device='cuda:0')),\n",
       "             ('module.model.6.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.6.2.clip_val', tensor([5.3681], device='cuda:0')),\n",
       "             ('module.model.7.0.weight', tensor([[[[ 0.6894,  0.8641,  0.4929],\n",
       "                        [ 0.8024,  0.8955,  0.5088],\n",
       "                        [ 0.3592,  0.4494,  0.2313]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.5499,  0.8271,  0.4219],\n",
       "                        [ 0.7200,  0.9898,  0.5124],\n",
       "                        [ 0.3983,  0.5397,  0.2941]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.4476, -0.7613, -0.5199],\n",
       "                        [-0.7145, -1.0141, -0.6367],\n",
       "                        [-0.4377, -0.6237, -0.2997]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0612,  1.4363,  0.5299],\n",
       "                        [-0.1193, -0.4410, -0.6123],\n",
       "                        [-0.6847, -0.9285, -0.3976]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.5173,  0.6201,  0.3535],\n",
       "                        [ 0.8688,  0.9374,  0.4087],\n",
       "                        [ 0.5201,  0.4963,  0.1131]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.7872, -0.8789, -0.4077],\n",
       "                        [-0.9427, -1.1592, -0.5853],\n",
       "                        [-0.4177, -0.5815, -0.2882]]]], device='cuda:0')),\n",
       "             ('module.model.7.1.weight',\n",
       "              tensor([0.7451, 0.7618, 1.2960, 2.4901, 0.7922, 1.0193, 0.7343, 0.7678, 2.2324,\n",
       "                      0.9215, 0.7270, 1.0164, 2.3165, 0.8123, 0.9664, 0.6433, 2.9575, 0.8066,\n",
       "                      0.8234, 0.8736, 1.0465, 2.9440, 0.7050, 0.7188, 0.7670, 0.8364, 0.9090,\n",
       "                      0.8934, 1.1995, 0.8731, 0.9094, 0.6699, 1.4253, 0.7403, 0.8430, 0.7988,\n",
       "                      0.7876, 1.3885, 0.9861, 0.7424, 0.9413, 1.4038, 1.4439, 0.9143, 0.6996,\n",
       "                      1.0520, 1.1904, 2.0694, 0.9111, 0.8419, 0.8572, 0.8183, 0.7093, 0.6741,\n",
       "                      0.8122, 0.9975, 0.8774, 0.9698, 0.7358, 1.1289, 1.6113, 0.6802, 0.7981,\n",
       "                      0.8706, 1.1539, 0.4808, 2.7794, 0.7828, 1.0404, 0.6960, 1.8269, 1.2679,\n",
       "                      0.8349, 0.8966, 0.7462, 0.8330, 0.6455, 1.0635, 0.8712, 0.7439, 1.3342,\n",
       "                      0.8184, 0.9723, 0.9652, 0.8234, 1.8531, 0.6435, 0.7376, 0.5939, 1.0011,\n",
       "                      0.7439, 0.8268, 1.3178, 0.8986, 1.4349, 1.2051, 1.4388, 0.8336, 0.8399,\n",
       "                      1.7115, 0.7957, 0.9283, 0.8111, 0.7930, 0.9450, 0.7454, 2.4162, 1.0454,\n",
       "                      1.3394, 1.0277, 1.3422, 2.2408, 0.8050, 0.8712, 0.7892, 1.9831, 2.8204,\n",
       "                      0.7612, 0.8326, 2.3887, 0.7655, 0.9774, 0.7446, 0.9226, 0.9870, 0.7826,\n",
       "                      0.5990, 0.6648], device='cuda:0')),\n",
       "             ('module.model.7.1.bias',\n",
       "              tensor([ 0.5802,  1.1679,  0.5611, -0.5643,  1.2501,  1.9057,  2.6393,  0.8610,\n",
       "                      -1.0412,  2.1852,  1.0356,  0.6599, -0.8958,  0.7712,  2.6412,  0.6059,\n",
       "                      -1.7831,  1.8161,  1.2498,  1.1758,  1.1476, -1.2430,  3.3810,  1.1293,\n",
       "                       4.1471,  0.9833,  0.7511,  1.3547,  0.5244,  1.0008,  1.2525,  0.7586,\n",
       "                      -0.3606,  0.8872,  0.8391,  0.7513,  1.5409,  1.0563,  0.9808,  0.7425,\n",
       "                       0.7618, -0.1040,  0.2674, -0.1144,  3.2715,  0.0761,  0.9350, -0.3330,\n",
       "                       0.7276,  2.3047,  2.4808,  0.6533,  3.5609,  1.1353,  1.6919,  0.9330,\n",
       "                       2.0634,  0.9870,  0.7470,  0.5506, -0.5701,  0.5493,  2.7020,  1.8269,\n",
       "                       0.6533,  0.3829, -1.2481,  1.0363,  1.2703,  3.7991, -0.4620,  0.6542,\n",
       "                       1.7068,  2.0008,  0.7113,  0.6902,  0.6522,  0.0332,  0.7829,  1.7319,\n",
       "                      -0.2684,  1.1385,  0.0375,  2.1210,  1.8892, -0.0734,  2.4326,  0.8844,\n",
       "                       0.4280, -0.5999,  1.1420,  0.9067, -0.6470,  0.6294, -0.4904,  0.3520,\n",
       "                      -0.4074,  0.6022,  1.0368, -0.1674,  0.7925,  0.7126,  1.1990,  1.0102,\n",
       "                       4.4542,  0.8758, -0.9437,  0.3496, -0.1245,  0.3101,  0.6683, -0.5644,\n",
       "                       2.6369,  1.9324,  0.7473, -0.5588, -2.0488,  1.4066,  2.7458, -1.1395,\n",
       "                       3.8724,  2.2900,  0.7819,  0.7083, -0.0054,  3.0697,  0.5306,  5.0948],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.7.1.running_mean',\n",
       "              tensor([  0.8478,   2.1400,  -3.4365,  -5.8889,  -1.7981,   5.9416,   1.5674,\n",
       "                        1.6122, -10.0061,   7.0510,   1.7663,  -4.5118,  -4.0590,   2.1027,\n",
       "                       -4.9840,   0.6876,  -3.5439,  -3.9169,   2.4824,  -2.4077,   3.7957,\n",
       "                       -1.9711,  -2.2338,   1.2629,  -1.5049,   2.3000,   1.9136,  -4.5682,\n",
       "                       -1.3945,   2.8433,  -1.3012,   0.6779,  -0.4430,   1.3512,   1.7072,\n",
       "                        1.3042,   2.3368,  -8.2131,   2.9994,   1.1887,   3.0020,   0.4969,\n",
       "                       -2.2922,  -0.0786,  -1.2676,   0.2577,  -1.9133,  -7.6555,   1.5178,\n",
       "                        6.9225,  -2.1588,   1.0061,  -1.3404,   0.6276,  -1.1569,  -4.2424,\n",
       "                        5.9666,  -2.0170,   0.9822,  -5.3509,  -4.2721,   0.7919,   2.5355,\n",
       "                        2.9195,  -1.6845,   0.2367,  -2.8277,   1.9668,  -3.6887,  -1.2134,\n",
       "                       -0.4860,  -1.7111,   4.5373,   9.8131,   1.4220,   1.1493,   0.3825,\n",
       "                        0.0445,   1.0908,   4.7533,  -5.3640,   1.8219,  -0.1935,  11.8843,\n",
       "                        4.2441,  -2.9859,  -1.3076,   0.7656,   0.4854,   0.1539,   1.5424,\n",
       "                        1.1573,  -0.1518,   0.8390,  -1.9910,  -5.1904,  -0.6213,   1.1921,\n",
       "                       -9.6353,  -3.3161,   1.4198,   1.8053,   2.1504,   2.1656,  -2.7919,\n",
       "                        1.0275,  -5.3158,  -0.2441,  -1.8913,  -0.9697,  -7.6776,  -2.5153,\n",
       "                       -3.2182,   7.4224,   0.9506,  -3.8015,  -9.9535,   1.1117,   6.9249,\n",
       "                       -5.9327,  -1.3247, -11.3085,   1.4563,   1.6709,  -0.1121,   0.1261,\n",
       "                        1.5270,  -0.4018], device='cuda:0')),\n",
       "             ('module.model.7.1.running_var',\n",
       "              tensor([ 1.6620,  3.3603, 12.3506, 26.8060,  2.3756, 13.5585, 12.2606,  2.6930,\n",
       "                      30.5627,  6.0771,  2.0118,  2.0565, 14.0654,  4.5624,  8.4925,  0.8953,\n",
       "                       9.9035,  4.3384,  2.7368,  3.9788, 11.4455,  4.2161,  2.8037,  2.0736,\n",
       "                       3.3706,  3.9031,  4.7501,  4.9615,  1.7213,  6.7165,  1.8370,  0.8427,\n",
       "                       5.4890,  1.7159,  3.7242,  3.3262,  4.2533,  6.2788, 11.1337,  1.6329,\n",
       "                      12.4444,  1.9427,  4.6572,  1.2479,  1.9270,  1.6353,  3.4836, 36.6584,\n",
       "                       5.2449,  2.0099,  4.6371,  1.9536,  2.1075,  0.6958,  1.1005,  6.7889,\n",
       "                       4.9757,  3.7797,  1.4123,  6.0354,  4.0990,  1.2044,  8.8360,  3.1996,\n",
       "                       2.5591,  0.2842,  8.7713,  4.8176,  7.1739,  2.2345,  2.2656,  3.4358,\n",
       "                       5.3656, 10.1649,  2.0135,  2.6303,  0.7702,  1.6456,  2.4961,  2.3092,\n",
       "                       4.2895,  5.1856,  1.7472, 11.0542,  3.0518,  9.5858,  1.0673,  0.8844,\n",
       "                       0.5179,  0.1044,  2.0510,  3.2923,  1.0805,  3.3904,  1.1069,  9.1833,\n",
       "                       1.8151,  2.9640,  4.4818,  5.7742,  2.6650,  6.2840,  2.8648,  3.2746,\n",
       "                       7.1491,  1.7666, 13.7333,  1.8605,  1.5162,  1.3501,  6.6448,  5.9483,\n",
       "                       2.4930,  3.6406,  1.1048, 12.2627, 25.1599,  1.2778,  2.5608,  6.0225,\n",
       "                       2.0159,  2.8976,  2.3883,  4.1742,  1.4817,  2.7045,  3.0707,  0.4884],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.7.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.7.2.clip_val', tensor([5.8726], device='cuda:0')),\n",
       "             ('module.model.8.0.weight', tensor([[[[-0.0804]],\n",
       "              \n",
       "                       [[-0.4415]],\n",
       "              \n",
       "                       [[ 0.0850]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0147]],\n",
       "              \n",
       "                       [[ 0.0938]],\n",
       "              \n",
       "                       [[ 0.0142]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0608]],\n",
       "              \n",
       "                       [[ 0.0805]],\n",
       "              \n",
       "                       [[-0.0596]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0201]],\n",
       "              \n",
       "                       [[ 0.0451]],\n",
       "              \n",
       "                       [[ 0.0075]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1146]],\n",
       "              \n",
       "                       [[-0.0115]],\n",
       "              \n",
       "                       [[ 0.1007]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0090]],\n",
       "              \n",
       "                       [[-0.1881]],\n",
       "              \n",
       "                       [[-0.0703]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2062]],\n",
       "              \n",
       "                       [[-0.0125]],\n",
       "              \n",
       "                       [[-0.0402]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0154]],\n",
       "              \n",
       "                       [[-0.0731]],\n",
       "              \n",
       "                       [[ 0.0215]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0432]],\n",
       "              \n",
       "                       [[ 0.0120]],\n",
       "              \n",
       "                       [[ 0.0324]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1088]],\n",
       "              \n",
       "                       [[ 0.0171]],\n",
       "              \n",
       "                       [[ 0.0078]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0095]],\n",
       "              \n",
       "                       [[-0.2729]],\n",
       "              \n",
       "                       [[-0.2068]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0411]],\n",
       "              \n",
       "                       [[ 0.0608]],\n",
       "              \n",
       "                       [[ 0.0445]]]], device='cuda:0')),\n",
       "             ('module.model.8.1.weight',\n",
       "              tensor([0.9489, 0.3893, 1.0024, 1.3708, 0.9465, 1.1251, 1.0048, 1.1598, 0.6188,\n",
       "                      0.8834, 1.1865, 1.2009, 0.6548, 1.1177, 0.9195, 0.4276, 1.0321, 0.8377,\n",
       "                      0.6448, 0.9781, 1.0357, 0.5149, 0.6604, 0.4503, 1.1952, 0.7153, 0.9521,\n",
       "                      0.6746, 0.9361, 0.6841, 1.1030, 0.6922, 0.8957, 0.8710, 0.3178, 0.7598,\n",
       "                      0.3451, 1.0575, 1.0621, 0.8844, 0.8319, 0.7068, 1.0601, 1.0673, 0.6914,\n",
       "                      0.3563, 0.9204, 1.1839, 0.4843, 0.5939, 0.7994, 0.9979, 0.9247, 1.2400,\n",
       "                      0.5459, 0.5040, 1.3833, 1.1336, 0.9840, 0.7817, 0.2701, 0.3282, 0.4046,\n",
       "                      0.5825, 0.7918, 0.9176, 0.9636, 1.0470, 0.9745, 1.0977, 1.1331, 0.2877,\n",
       "                      0.8109, 0.5821, 0.6561, 0.7155, 1.3483, 0.4616, 0.6418, 0.6999, 0.7027,\n",
       "                      0.6852, 0.9438, 0.4417, 0.5422, 0.8659, 1.0275, 0.8242, 0.4547, 0.3782,\n",
       "                      0.5079, 0.7266, 0.8926, 0.5551, 0.9035, 1.1403, 0.2216, 1.0018, 1.2518,\n",
       "                      0.4437, 1.2073, 0.9973, 1.1765, 0.6469, 0.8599, 1.1154, 1.1350, 0.4034,\n",
       "                      0.9573, 0.6953, 1.0033, 0.7814, 1.0497, 1.0233, 0.5907, 1.1702, 0.8618,\n",
       "                      1.1018, 0.5149, 1.2633, 0.5829, 0.6341, 1.0354, 0.6151, 0.8546, 1.1430,\n",
       "                      0.7991, 0.8824, 0.5646, 0.9589, 0.7152, 0.8789, 1.0400, 0.7003, 0.7431,\n",
       "                      0.7476, 1.8857, 0.9074, 1.0961, 0.4456, 0.5671, 0.3736, 1.1102, 0.6803,\n",
       "                      0.5058, 0.4153, 0.4069, 0.7119, 0.8809, 0.9890, 0.7963, 0.8520, 1.6402,\n",
       "                      1.0230, 0.8236, 1.0594, 0.6039, 0.6783, 0.8559, 0.9893, 0.7298, 0.7728,\n",
       "                      0.7351, 0.6795, 0.9659, 1.1628, 0.6265, 1.1376, 0.8009, 0.8809, 0.4134,\n",
       "                      1.1581, 0.6725, 1.1821, 0.7835, 1.4238, 0.9550, 0.6417, 0.4101, 1.5251,\n",
       "                      0.8065, 1.0064, 0.8581, 0.3026, 1.0984, 0.6493, 1.0257, 0.6698, 1.0096,\n",
       "                      0.8296, 1.0787, 0.6874, 0.4412, 0.7731, 0.6469, 0.3872, 1.0143, 0.7831,\n",
       "                      0.8655, 1.0185, 1.1614, 0.5109, 0.6570, 0.9969, 1.2570, 1.2963, 0.4087,\n",
       "                      0.9363, 0.6634, 0.3260, 1.1391, 1.0324, 1.1162, 0.6611, 0.7405, 0.9348,\n",
       "                      0.6503, 0.4129, 0.4147, 0.6383, 0.2733, 0.7934, 0.4695, 1.0948, 1.1047,\n",
       "                      0.8613, 1.2471, 0.8078, 0.8949, 0.3967, 1.2402, 1.1399, 0.9173, 1.0752,\n",
       "                      0.5893, 0.6166, 0.9731, 0.3747, 0.5989, 0.6685, 0.9343, 0.6218, 1.2027,\n",
       "                      0.9133, 0.5440, 0.9971, 0.4966, 0.9151, 0.7799, 0.5126, 0.6680, 0.6438,\n",
       "                      0.7737, 0.6740, 0.9500, 0.2518], device='cuda:0')),\n",
       "             ('module.model.8.1.bias',\n",
       "              tensor([ 0.8392,  1.2741,  0.7999,  0.5768,  1.4114,  0.5554,  0.4386,  0.3158,\n",
       "                       1.2479,  0.9107,  0.0944, -0.2944,  1.2340,  0.4741,  1.6919,  1.1825,\n",
       "                       0.7230,  0.2069,  0.9638,  1.2898,  0.9311,  1.3460,  1.1079,  1.3157,\n",
       "                      -0.2808,  1.2313,  1.1318,  1.0188,  0.7394,  1.0586,  0.4970,  1.3414,\n",
       "                      -0.7467,  1.9408,  1.2087,  0.9459,  1.1563,  0.9493,  0.4983,  1.5907,\n",
       "                       1.3354,  1.4021,  0.2651,  0.1142,  1.2316,  1.3453,  0.7379,  0.3681,\n",
       "                       1.1184,  1.2134,  1.0748,  1.0494,  0.5011,  0.3488,  1.0251,  1.4393,\n",
       "                      -0.2410,  0.3979,  1.1212,  0.8617,  1.2094,  1.1470,  1.2027,  1.1470,\n",
       "                       1.1902,  0.9171,  0.7517,  1.0429,  1.3481, -0.5189,  0.4442,  1.2873,\n",
       "                      -0.8121,  1.0184,  1.3508,  1.0949,  0.6856,  1.2039,  0.9047,  1.3163,\n",
       "                       1.2093,  1.0475, -0.5138,  1.4758,  1.4651,  1.1947, -0.4208,  0.8725,\n",
       "                       1.1445,  1.1754,  1.1488,  0.8575,  0.8406,  0.9683,  1.1694,  0.4903,\n",
       "                       1.2279, -1.7555, -0.0287,  1.3617,  0.0846,  0.5206, -0.1681,  1.0605,\n",
       "                       1.0277,  0.2602,  0.5491,  1.5649,  0.9193,  1.1215,  0.4748,  1.1629,\n",
       "                       0.4430,  0.8940,  1.2816,  0.6797,  1.3863,  0.2241,  1.1738,  0.4705,\n",
       "                       1.1424,  1.4066,  0.0642,  1.2690,  0.8840, -0.2629,  0.7440,  1.4935,\n",
       "                       1.1639,  0.8499,  1.6920,  0.8672,  0.1635,  1.5458,  1.4799,  0.8499,\n",
       "                       1.0983,  0.6285,  0.2472,  1.3281,  1.2811,  1.3379, -0.3833,  0.9870,\n",
       "                       1.0736,  1.0670,  1.1549,  1.0585,  1.1104,  0.9398,  1.0412,  1.0762,\n",
       "                       1.4011,  0.7736,  1.3063,  0.7455,  1.5576,  1.2865,  0.9172,  0.7977,\n",
       "                       0.9386,  1.3205,  1.0164,  1.2307,  0.6420, -0.4422,  1.2311,  0.4133,\n",
       "                       0.7743,  1.0344,  1.2767,  0.0278,  1.2879,  0.3119,  1.0673, -0.0080,\n",
       "                       0.6249,  1.0728,  1.3803,  0.8250,  0.8931,  0.8561,  0.7018,  1.1917,\n",
       "                      -0.0604,  1.2107,  0.7176,  1.3395,  0.1271,  1.6105, -0.0186,  1.0147,\n",
       "                       1.1496,  1.3174,  1.1634,  1.3484,  0.4854,  1.1114, -0.9100,  0.9118,\n",
       "                      -0.2491,  1.0129,  1.1250,  0.6889, -0.1390, -0.2005,  1.6534,  0.8250,\n",
       "                       1.0805,  1.5343,  0.1712,  0.5701,  0.0213,  1.0993,  1.0446,  0.6978,\n",
       "                       1.1445,  1.2375,  1.1627,  1.2677,  1.6746,  0.7996,  1.3057,  0.5670,\n",
       "                       0.4989,  0.7892, -0.1393,  1.2270,  0.7978,  1.3558,  0.2317, -0.5292,\n",
       "                       0.7907,  0.2667,  0.9792,  0.8856,  0.8175,  1.1183,  1.2421,  1.0054,\n",
       "                      -0.0909,  1.1513, -0.4621,  0.9858,  1.4962, -0.2059,  1.1686,  0.7554,\n",
       "                       0.3249,  1.1536,  1.1332,  1.6905,  1.2034,  0.9799,  0.9477,  1.1656],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.8.1.running_mean',\n",
       "              tensor([ 2.0330e+00, -1.0351e+00, -1.8956e+00,  1.5715e+00, -2.3319e-01,\n",
       "                       9.5039e-01, -1.6635e+00, -1.7144e-01,  7.6263e-01,  1.0619e+00,\n",
       "                       1.1937e+00,  5.4684e-01,  1.2756e+00, -9.3380e-01, -7.7158e-02,\n",
       "                       7.0792e-01,  7.6245e-01,  8.6942e-02,  1.7461e+00,  2.3606e+00,\n",
       "                       2.2266e-02,  6.4959e-01,  6.6701e-01,  5.7293e-01,  9.5217e-01,\n",
       "                       5.6212e-01,  2.4142e+00, -5.5037e-01,  1.6157e+00, -1.0736e+00,\n",
       "                      -1.3498e+00,  1.0298e-01, -1.9002e+00,  8.2203e-02, -1.8528e+00,\n",
       "                       1.6153e+00,  7.3737e-01,  7.3448e-01,  1.7880e-01, -1.7167e-01,\n",
       "                       1.4701e+00, -1.9075e-01,  9.5072e-01, -8.5216e-01,  1.2883e-01,\n",
       "                       8.8858e-01, -2.0137e-02,  2.0206e+00, -1.5615e+00, -3.3140e-01,\n",
       "                      -5.1954e-01, -1.7822e+00,  8.0671e-01,  1.7183e+00, -7.3619e-01,\n",
       "                       1.8635e+00,  1.4616e+00, -2.7952e+00,  7.1421e-01, -5.9421e-01,\n",
       "                      -2.9406e-01, -1.8309e-01, -3.3132e-01, -1.0480e+00, -2.4978e-01,\n",
       "                       1.0762e+00, -1.2693e+00,  2.0005e-01,  6.3876e-01,  2.8377e+00,\n",
       "                      -1.9995e-01,  1.1539e+00,  6.7982e-01,  1.2325e+00,  1.1943e+00,\n",
       "                      -5.3480e-01, -1.4396e+00, -2.2201e+00, -2.7021e-02,  9.3845e-01,\n",
       "                       3.0534e-01, -2.6964e-01,  1.0073e+00,  7.6688e-01,  6.6179e-01,\n",
       "                      -1.4446e+00, -9.0161e-01, -1.0360e+00,  1.1557e+00, -3.5362e+00,\n",
       "                      -1.4183e+00,  5.8511e-01,  1.0174e+00,  3.8560e-01,  1.4626e+00,\n",
       "                      -1.4194e+00, -3.0852e-01,  1.2858e+00,  8.6388e-01,  1.6644e+00,\n",
       "                       5.3001e-01, -1.7822e+00,  1.8331e-01,  8.7493e-02, -2.6857e-01,\n",
       "                      -6.7373e-01, -2.4496e-01, -2.3297e+00,  2.7028e-01,  1.0878e+00,\n",
       "                       1.9205e+00,  1.5654e+00,  1.2388e+00,  2.9742e-01,  3.0852e-01,\n",
       "                       3.1824e+00, -2.8208e-01, -4.4591e-01, -2.2551e+00, -3.1415e+00,\n",
       "                      -1.1873e+00,  1.6888e+00,  6.7279e-01, -5.5971e-01, -1.3329e+00,\n",
       "                       4.1934e-01,  2.4862e+00,  3.8329e-01, -7.2715e-01, -4.9893e-01,\n",
       "                      -1.4318e+00, -1.0311e+00,  1.2079e-01, -7.4453e-01,  1.0684e+00,\n",
       "                      -8.6002e-01,  4.3470e-01, -3.0006e-01, -9.5531e-01,  4.6016e-02,\n",
       "                       7.1881e-01,  2.7860e-01, -4.6693e-02,  4.0036e-01,  3.0380e+00,\n",
       "                      -4.0649e+00,  4.8995e-01,  1.2400e+00, -2.4199e-02, -5.8711e-01,\n",
       "                       5.0002e-01, -2.7409e-01, -1.1659e+00,  3.0518e-01,  1.7014e+00,\n",
       "                      -1.0536e+00, -4.9028e-01, -1.3607e+00, -7.4810e-01,  7.3272e-01,\n",
       "                       1.3412e+00, -1.9445e+00,  1.5489e-01, -5.0439e-01,  1.3286e+00,\n",
       "                      -1.5124e+00, -8.4275e-04,  5.3785e-01,  1.8614e+00,  4.0366e-01,\n",
       "                      -3.0627e-01,  3.5960e-01, -1.9532e+00,  2.4687e+00,  1.6193e+00,\n",
       "                      -1.1947e+00,  5.0683e-01,  5.8484e-01, -1.2780e+00,  6.4017e-01,\n",
       "                       4.1135e-01,  2.3425e+00,  3.4614e-01, -3.3050e+00,  1.5228e+00,\n",
       "                       7.6326e-01,  7.3411e-01, -2.2753e+00,  1.4951e+00, -7.0861e-01,\n",
       "                      -9.0913e-01,  7.8065e-01, -1.1472e+00,  1.7174e+00, -9.0459e-01,\n",
       "                       5.0391e-01, -3.0163e+00,  7.1763e-01,  2.3433e-01,  2.1608e+00,\n",
       "                       1.6658e-01,  8.5922e-02, -3.0876e-01, -3.6342e-01, -8.3144e-01,\n",
       "                      -2.8375e+00, -1.0017e-01,  2.3557e+00,  1.0594e+00,  1.3333e+00,\n",
       "                       4.7127e-01, -3.8791e-01,  1.8513e+00, -8.9806e-03,  2.1637e-01,\n",
       "                      -1.0125e-01,  3.0009e+00,  5.9997e-01,  6.7961e-01, -1.0572e-01,\n",
       "                       2.3594e+00, -8.5288e-01, -1.0623e+00,  2.0113e+00, -2.2788e+00,\n",
       "                       1.1195e+00,  1.9197e+00,  5.0102e-01,  3.7238e-01,  1.9208e+00,\n",
       "                       1.5926e+00,  1.7063e+00, -2.2658e+00,  5.3009e-02,  2.5693e+00,\n",
       "                       1.9186e+00,  1.3128e+00, -4.1480e-01,  2.2515e+00, -1.1927e-01,\n",
       "                      -1.3360e+00, -4.4253e-01,  7.5707e-02,  3.4542e-02,  1.5847e+00,\n",
       "                       1.1808e+00, -9.2219e-01,  1.7862e+00, -2.9220e+00, -7.9511e-01,\n",
       "                       5.6188e-01, -5.4404e-02,  1.4286e+00,  2.7266e+00,  1.2010e+00,\n",
       "                      -1.0671e+00], device='cuda:0')),\n",
       "             ('module.model.8.1.running_var',\n",
       "              tensor([2.0294, 0.9163, 0.5514, 0.7455, 0.8784, 0.8451, 0.6605, 0.7624, 1.4062,\n",
       "                      0.4091, 1.5192, 0.2988, 0.7529, 0.4674, 0.8071, 1.0152, 0.7766, 0.2644,\n",
       "                      1.0040, 1.9558, 0.5576, 1.6860, 1.1926, 0.4523, 1.0768, 1.1837, 0.8269,\n",
       "                      0.4772, 0.9278, 0.6670, 1.4347, 1.3982, 0.5344, 0.8164, 0.5105, 0.5700,\n",
       "                      0.4301, 0.6684, 0.5878, 1.2797, 0.8649, 1.3531, 0.6760, 0.8319, 1.2322,\n",
       "                      0.3376, 0.7480, 1.9871, 0.6974, 0.7975, 0.5505, 0.7562, 0.5133, 1.6783,\n",
       "                      0.6271, 0.9658, 1.1562, 1.0065, 0.6222, 0.5948, 0.5619, 0.7007, 0.4251,\n",
       "                      0.3618, 0.6094, 0.5713, 0.6358, 0.6723, 0.8517, 0.8305, 0.4526, 0.9462,\n",
       "                      0.4654, 0.6596, 0.6035, 0.6012, 0.8981, 0.5473, 0.4065, 0.7058, 0.9766,\n",
       "                      1.5405, 0.6362, 1.3066, 1.3143, 0.8472, 0.4028, 0.7302, 0.5094, 0.5611,\n",
       "                      0.5773, 0.6838, 0.4445, 0.3901, 0.8895, 0.9206, 0.4677, 0.7796, 0.6881,\n",
       "                      0.6881, 0.4926, 0.8314, 0.8983, 0.6726, 0.9972, 0.4006, 0.7918, 0.6414,\n",
       "                      0.9576, 0.9778, 0.6496, 1.2708, 1.0920, 1.8794, 0.6987, 1.0196, 0.6091,\n",
       "                      0.2907, 1.6146, 0.5377, 1.2809, 1.3957, 0.8319, 0.7644, 0.5712, 0.2766,\n",
       "                      0.7297, 0.9988, 0.5041, 0.5627, 1.1161, 0.9607, 0.3697, 1.9436, 1.4720,\n",
       "                      0.7704, 0.5037, 0.3396, 0.6653, 1.0188, 0.9982, 1.2133, 1.1234, 0.6743,\n",
       "                      1.5961, 0.6877, 0.3378, 0.4223, 0.6751, 1.2781, 0.5820, 0.9453, 0.4483,\n",
       "                      1.5760, 0.7532, 1.0482, 2.3830, 0.2948, 0.4707, 0.7172, 0.6340, 0.5899,\n",
       "                      1.0903, 0.6468, 0.9356, 0.3903, 0.5902, 0.8830, 0.5967, 0.8700, 1.6587,\n",
       "                      0.9251, 0.7785, 0.8486, 0.8133, 1.0184, 0.8506, 0.2546, 0.5335, 0.4127,\n",
       "                      1.1710, 1.2928, 0.5956, 0.6730, 0.5320, 0.3602, 0.8289, 0.9769, 0.4176,\n",
       "                      0.9507, 0.5814, 0.3063, 0.3003, 0.6529, 0.7305, 0.7597, 0.7268, 0.7285,\n",
       "                      0.6640, 0.4696, 0.3405, 0.4188, 0.5998, 1.0881, 0.7695, 1.0404, 0.7811,\n",
       "                      1.0142, 0.4247, 0.6975, 1.0579, 0.5906, 1.0731, 0.4053, 0.8492, 0.5779,\n",
       "                      1.8726, 0.5329, 0.3385, 1.2930, 1.5059, 0.5206, 2.0924, 0.6595, 0.4206,\n",
       "                      0.6048, 0.5801, 0.8996, 0.3973, 0.6814, 0.6676, 0.8631, 0.6193, 0.6358,\n",
       "                      1.3786, 0.6283, 0.7980, 0.5305, 2.2548, 0.5492, 0.3597, 0.4045, 0.4349,\n",
       "                      1.2012, 1.6438, 0.3704, 0.8204, 0.6980, 0.4720, 0.9019, 0.9098, 1.3063,\n",
       "                      0.5806, 1.1522, 0.8946, 0.3651], device='cuda:0')),\n",
       "             ('module.model.8.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.8.2.clip_val', tensor([5.4219], device='cuda:0')),\n",
       "             ('module.model.9.0.weight', tensor([[[[-0.0843, -0.2290, -0.0678],\n",
       "                        [-0.1839,  2.9336,  0.0275],\n",
       "                        [-0.2248,  0.0802,  0.0647]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.7536, -0.2370,  0.6937],\n",
       "                        [-0.0387, -0.7649, -0.0928],\n",
       "                        [ 0.6312,  0.0308,  0.5681]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2290, -0.5608, -0.1252],\n",
       "                        [-0.6565, -0.7453, -0.4928],\n",
       "                        [-0.0626, -0.3975, -0.0393]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1123,  0.1792,  0.0112],\n",
       "                        [ 0.2949,  2.6179, -0.3317],\n",
       "                        [ 0.0084, -0.6449,  0.0929]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1115, -1.6799, -0.0206],\n",
       "                        [ 0.1862,  2.6634, -0.1611],\n",
       "                        [-0.4081, -1.0619,  0.2194]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0196, -0.2194, -0.0088],\n",
       "                        [-0.3369, -1.6377,  0.0696],\n",
       "                        [ 0.0687,  0.0978,  0.0034]]]], device='cuda:0')),\n",
       "             ('module.model.9.1.weight',\n",
       "              tensor([0.7514, 0.8627, 1.5035, 0.8263, 1.5861, 1.2635, 0.8354, 0.8859, 1.4001,\n",
       "                      1.1792, 1.0967, 1.1359, 1.3841, 0.6073, 1.1021, 1.1747, 1.1023, 0.6554,\n",
       "                      1.4358, 0.8308, 1.2393, 1.3326, 1.3821, 1.5516, 0.8308, 1.4532, 1.4091,\n",
       "                      1.2007, 1.0588, 0.9964, 1.3395, 1.3894, 0.8078, 1.3806, 1.3132, 0.5956,\n",
       "                      0.9866, 1.1148, 0.9206, 1.3676, 1.3179, 1.6592, 0.9718, 1.1325, 1.1612,\n",
       "                      1.3254, 0.9232, 0.6643, 1.4727, 1.3742, 1.1770, 1.1297, 0.8370, 1.3841,\n",
       "                      1.3355, 1.0870, 1.5601, 0.5312, 1.1726, 1.0326, 1.8397, 0.6353, 1.3141,\n",
       "                      1.1443, 1.3677, 1.0467, 0.7276, 1.3035, 1.1412, 0.8564, 1.0585, 1.7715,\n",
       "                      0.8128, 1.3241, 0.8725, 1.2842, 1.4357, 1.0848, 1.2174, 0.9820, 1.4550,\n",
       "                      1.3724, 0.5542, 2.0875, 0.8140, 1.5010, 0.6506, 1.3364, 0.9737, 1.6079,\n",
       "                      0.9372, 0.7667, 1.0910, 1.0619, 1.2675, 0.9835, 0.8816, 0.8661, 0.5798,\n",
       "                      1.5712, 0.5394, 0.6570, 0.6790, 1.2134, 1.2688, 1.0490, 0.5699, 1.2663,\n",
       "                      1.3905, 1.5334, 0.5852, 1.4958, 1.1108, 1.0579, 0.9964, 0.6183, 1.4144,\n",
       "                      0.6900, 1.6457, 1.5599, 0.9845, 1.0297, 0.9911, 1.2412, 1.1871, 0.4775,\n",
       "                      0.7331, 0.9526, 1.4212, 1.0889, 1.0546, 0.9180, 0.8688, 1.8875, 1.2185,\n",
       "                      0.9540, 1.0650, 0.8410, 0.8822, 1.6068, 1.5254, 1.2763, 0.7701, 1.2546,\n",
       "                      0.7534, 0.8709, 1.1939, 1.0411, 1.3668, 1.1255, 1.1098, 1.1965, 0.9540,\n",
       "                      0.8049, 1.4831, 0.5156, 2.0559, 1.2600, 1.0160, 0.5719, 0.4944, 1.1040,\n",
       "                      1.3259, 1.2700, 0.8506, 1.2314, 1.6359, 0.8432, 1.1658, 1.1364, 1.0188,\n",
       "                      1.0032, 0.9368, 0.9539, 1.3627, 1.2232, 0.4703, 1.1579, 1.5083, 1.0225,\n",
       "                      0.7486, 1.4261, 0.8051, 0.4690, 0.5707, 1.1998, 1.3893, 1.4508, 0.6673,\n",
       "                      1.6601, 0.8560, 1.2187, 1.0686, 0.8050, 1.1773, 0.7208, 0.4991, 1.3055,\n",
       "                      0.4629, 1.2471, 0.4807, 1.1441, 1.2571, 1.1200, 0.5855, 1.6668, 1.5246,\n",
       "                      0.6847, 1.3134, 0.6839, 0.8021, 0.7856, 0.9011, 1.2445, 1.1453, 1.1943,\n",
       "                      1.6111, 1.2463, 1.3026, 1.6617, 0.8610, 1.3147, 0.8100, 1.1079, 1.2276,\n",
       "                      1.4736, 1.5493, 0.7815, 1.2744, 1.6357, 0.7440, 0.9511, 0.7511, 0.8964,\n",
       "                      0.9237, 0.8774, 1.4728, 1.4044, 0.9264, 0.9868, 1.1692, 1.3960, 0.7491,\n",
       "                      1.2952, 0.9193, 0.9052, 0.8337, 1.1589, 0.8202, 1.8688, 1.2600, 1.0056,\n",
       "                      1.2213, 0.8241, 1.1488, 0.6039], device='cuda:0')),\n",
       "             ('module.model.9.1.bias',\n",
       "              tensor([ 7.1310e-01,  4.6332e-01, -5.9909e-01,  2.0871e+00, -3.5500e-01,\n",
       "                      -3.1372e-01,  2.6841e-01,  1.6975e-02, -9.6389e-02, -5.8751e-02,\n",
       "                      -3.9234e-01, -1.2188e-03, -6.0288e-01,  4.3262e-01,  5.8599e-01,\n",
       "                       6.1300e-01,  3.1683e-02,  1.9666e-01, -2.4258e-01,  2.1978e+00,\n",
       "                       4.2456e-02, -5.7227e-03, -2.8974e-01, -8.6576e-01,  1.4862e-01,\n",
       "                      -1.7131e-01, -1.6671e-01, -1.1115e-01,  5.9807e-02,  9.6430e-01,\n",
       "                       9.3103e-01, -5.2833e-02, -5.1119e-01, -1.1633e-01, -7.1393e-01,\n",
       "                       5.0657e-01,  1.4305e-02,  1.1520e-02, -6.7452e-02, -1.3043e-01,\n",
       "                      -3.1956e-02, -3.5149e-04, -7.4497e-02,  9.9495e-01,  1.4319e-01,\n",
       "                      -2.2855e-01,  3.8855e-01,  5.6823e-01, -4.1445e-01, -5.3159e-01,\n",
       "                      -1.3034e-01,  9.6448e-02,  6.5688e-03,  7.1913e-01, -2.8632e-01,\n",
       "                      -1.2946e-01,  3.9712e-01,  1.1652e+00,  1.5874e-01,  8.8336e-01,\n",
       "                      -2.1787e-01,  1.2443e+00, -2.5450e-01, -3.3700e-03, -9.9802e-02,\n",
       "                      -1.1060e-01,  3.7899e-01, -1.9723e-01,  2.2208e-01, -1.9556e-01,\n",
       "                      -5.3216e-02,  1.4575e-01, -1.4239e+00, -3.5700e-01,  2.6516e+00,\n",
       "                      -2.8488e-01, -1.1580e-01,  1.1786e+00, -1.9332e-01,  1.0144e+00,\n",
       "                       1.1720e-02, -3.9327e-01,  1.9105e-01, -4.9696e-01,  1.2490e+00,\n",
       "                      -2.2388e-01,  2.2025e-01, -1.6261e-02,  8.5005e-02, -8.6096e-01,\n",
       "                       1.6593e+00,  3.5412e-01, -1.6791e-02,  5.1789e-02,  3.3514e-02,\n",
       "                       4.4773e-02,  8.3013e-02, -1.1705e+00,  6.9572e-01, -1.0472e-01,\n",
       "                       5.5891e-01,  3.3958e-01,  3.0644e-01, -7.0090e-02,  3.8309e-01,\n",
       "                      -6.6122e-02,  1.1099e+00,  5.7349e-01, -2.0200e-01,  3.2513e-01,\n",
       "                       5.3793e-01, -2.2588e-01, -6.5794e-02,  3.2665e-01, -1.6654e-01,\n",
       "                       6.5313e-01, -2.7564e-01,  1.5874e-01, -2.1323e-01, -2.2836e-01,\n",
       "                       1.2225e+00,  3.3282e+00, -3.3666e-01, -1.0372e-01, -3.3654e-02,\n",
       "                       7.6085e-01,  3.3247e-01,  2.4306e+00, -1.5757e-01, -8.0784e-02,\n",
       "                       2.0226e+00,  7.2891e-02,  2.0891e-01, -4.2115e-01, -2.1649e-02,\n",
       "                       3.7852e-01,  2.9231e-01,  7.6200e-01,  3.4248e-01, -7.1849e-01,\n",
       "                      -3.6031e-01,  1.5827e-01,  2.8570e-01, -6.5526e-02,  1.7734e+00,\n",
       "                       6.1178e-01, -8.6910e-02,  7.3326e-02, -2.6893e-01,  1.4151e-01,\n",
       "                      -5.6412e-01,  2.0171e-01,  8.8487e-01,  3.6496e-01, -3.0851e-01,\n",
       "                       1.4314e+00, -3.1158e-01,  5.8108e-03,  3.6170e-01,  1.4881e+00,\n",
       "                       5.3430e-01,  2.4861e-01, -2.3954e-01, -1.3393e-01,  4.1355e-01,\n",
       "                      -1.2327e-01, -6.2590e-01,  1.6989e-01, -5.7693e-01,  3.2359e-02,\n",
       "                       2.5227e-01, -3.0366e-01,  1.3154e+00, -2.0816e-02, -3.4735e-01,\n",
       "                       5.1892e-01,  5.7973e-01, -1.8110e-01,  1.1921e-01,  3.1523e-01,\n",
       "                       1.4087e-01, -2.4349e-01,  1.0863e+00,  8.8736e-01,  1.1581e-01,\n",
       "                      -1.0493e-01, -2.7619e-01, -5.8416e-01,  3.9441e-01, -3.1231e-01,\n",
       "                      -3.9828e-01,  9.1460e-03,  2.3755e-02,  3.3948e+00, -4.6537e-02,\n",
       "                       2.4192e+00,  1.3818e+00, -2.4826e-01,  1.0385e-01,  2.7189e-02,\n",
       "                       7.0926e-01, -1.2725e-01, -1.2345e-01,  4.3649e-01,  2.4538e+00,\n",
       "                       2.4768e-01, -5.1071e-01,  1.4747e+00, -1.5105e-01,  5.2894e-01,\n",
       "                       1.6434e-02,  2.2400e+00, -4.2613e-02, -2.8569e-01,  2.3056e-01,\n",
       "                      -5.3958e-02, -3.1816e-01,  2.9194e-01, -2.4104e-01, -3.7753e-01,\n",
       "                       6.8033e-01, -3.7306e-01,  1.7000e+00,  1.0212e-01, -1.0135e-01,\n",
       "                      -5.3229e-01, -3.3663e-01,  2.0771e-01, -5.8378e-01, -8.7440e-01,\n",
       "                       6.0506e-01, -3.4131e-01,  5.0570e-01, -6.3925e-01,  3.0251e-01,\n",
       "                       6.5617e-01, -3.2377e-01, -4.4735e-02,  1.8619e+00,  4.2353e-01,\n",
       "                      -3.1768e-01, -1.1468e-01,  3.2519e-01, -2.4703e-02,  6.1253e-01,\n",
       "                      -9.8756e-01,  2.4106e-01, -1.9278e-01,  2.5848e-02, -1.2981e+00,\n",
       "                      -1.9495e-01,  1.7635e+00, -7.0094e-01,  1.3280e+00,  9.1547e-02,\n",
       "                       1.3016e+00], device='cuda:0')),\n",
       "             ('module.model.9.1.running_mean',\n",
       "              tensor([ 2.1269,  1.7796, -2.7869,  0.4565,  0.0881,  0.2698,  1.3260,  1.3108,\n",
       "                      -0.2274,  0.2677,  1.1065, -0.9036, -2.8348, -2.2491, -1.4112, -2.6160,\n",
       "                       0.0794,  0.4741, -0.4396,  2.2094,  0.7629, -4.2325, -2.9495,  1.2755,\n",
       "                       0.6313, -0.4185, -0.0795,  0.1493, -1.5401,  0.1307, -1.8067,  0.0255,\n",
       "                       0.1760,  0.1255, -2.1365, -1.8670, -2.8313,  0.1877, -1.8412, -1.6523,\n",
       "                       0.2065,  0.2270,  0.0995, -1.2199, -0.1097, -0.6959, -1.6467,  1.2371,\n",
       "                      -0.0511,  2.9998, -0.1264,  0.0432,  0.1533, -2.5949, -0.7164, -4.2441,\n",
       "                      -1.2199, -1.7217, -0.0719, -0.1375, -0.8513, -2.5334, -0.0443,  0.1726,\n",
       "                      -0.2056, -2.6720,  1.7261,  0.0617, -0.1344,  0.3852,  0.3223, -0.2589,\n",
       "                       0.1529, -0.3226, -0.5764, -0.1253, -2.5153,  0.2590,  0.3428, -0.6785,\n",
       "                      -0.5699,  2.0278,  0.2152, -0.1261, -3.0690,  0.3334,  0.3742, -0.1636,\n",
       "                      -1.7949, -2.5082,  0.3231,  2.3341,  0.1171,  0.7005,  0.0180,  1.8310,\n",
       "                      -2.7225,  0.0852, -1.2073, -1.9811, -1.3018,  1.6033,  0.6785,  0.2721,\n",
       "                       0.1696, -1.5930, -2.0176,  0.6213, -0.1585, -0.7656,  1.1539, -0.1298,\n",
       "                       1.0819, -2.7460, -4.0702, -2.0273,  0.7616,  0.8661, -3.2032, -1.9229,\n",
       "                       1.7485,  0.6256,  1.0670, -0.0239,  0.3255, -0.7470,  2.1034,  0.1040,\n",
       "                      -0.0843, -1.8771, -0.0626,  1.4000,  0.3861, -0.1068, -4.5732, -2.0450,\n",
       "                       0.3519, -0.0617,  1.0057, -3.6896, -0.3869, -4.1894,  0.6118, -0.1189,\n",
       "                       1.9895, -1.9271,  0.5404,  0.5032, -0.5988, -2.4254,  1.3775, -0.4294,\n",
       "                       0.7415,  1.8983,  0.6204, -1.8431, -0.0499,  0.1666,  0.3189, -2.9728,\n",
       "                      -2.1843,  0.1079, -0.1946, -0.1663,  2.2166, -0.6737,  0.3442,  1.6367,\n",
       "                       0.4299, -0.0377, -2.4851,  1.4901, -0.0689,  1.8035, -1.1605, -1.6086,\n",
       "                       2.1469, -0.2916, -0.2093,  0.4826,  2.3218, -2.4216,  1.0473, -3.6497,\n",
       "                       1.0173,  0.8898, -0.2560,  2.8716,  0.9829,  0.2135,  0.6391, -0.3244,\n",
       "                       0.5929,  0.1884, -0.0883,  2.6185, -1.7251, -0.0855,  0.0934,  0.1138,\n",
       "                      -0.9078, -0.1987, -0.1263, -2.2189, -1.0596, -1.1931, -0.3640,  2.4766,\n",
       "                      -0.4640, -4.8701,  1.4766,  0.0225,  1.0443, -1.2766,  0.1142, -0.2268,\n",
       "                      -0.2598, -0.1378, -1.1357, -0.2016, -4.8873,  1.2144,  3.1422, -2.2057,\n",
       "                      -1.5330, -0.4521, -1.2895, -3.2276, -0.0904, -2.9428, -1.7402,  0.4695,\n",
       "                       1.5187,  1.6868,  1.2382,  1.3965, -0.2834, -0.2143,  2.5678, -1.5861,\n",
       "                      -0.3848, -0.0707, -0.7249, -0.0801,  2.1958,  0.4217, -2.3061,  1.2613,\n",
       "                       1.2022, -2.4103,  2.0197,  0.1836, -0.3074,  2.2093,  0.0646, -2.1603],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.9.1.running_var',\n",
       "              tensor([ 4.6095,  0.7320,  3.5232,  4.6760,  3.6237,  1.4738,  3.3366,  3.7567,\n",
       "                       2.8796,  4.1702,  1.6833,  1.2182,  1.4428,  3.1617,  6.7628,  1.0477,\n",
       "                       3.1163,  1.3255,  1.7295,  5.0694,  6.6078,  2.2211,  1.3015,  1.3421,\n",
       "                       1.9687,  4.2532,  3.4851,  1.5183,  1.5977,  1.5960,  6.8681,  2.9349,\n",
       "                       0.1711,  3.2891,  0.2835,  1.5582,  0.5915,  3.6962,  1.5199,  2.4553,\n",
       "                       5.0985,  3.0920,  3.9873,  3.7609,  2.3070,  0.7662,  2.1970,  2.4506,\n",
       "                       0.5611,  3.4797,  2.8193,  2.4154,  2.5759, 10.4957,  0.7297,  1.4852,\n",
       "                       3.6262,  2.7580,  7.0117,  2.1946,  1.0177,  0.4492,  0.9786,  1.4199,\n",
       "                       3.0709,  2.2228,  4.0571,  3.7147,  4.8666,  0.6814,  3.5628,  0.6081,\n",
       "                       0.1614,  1.1173,  2.1587,  1.6076,  5.5472,  0.8428,  1.1820,  2.8217,\n",
       "                       3.7507,  3.0011,  0.2460,  1.8337,  1.6244,  3.1253,  0.8757,  1.6885,\n",
       "                       0.5705,  0.5068,  1.1480,  2.2593,  4.0682,  1.6590,  5.3205,  6.8316,\n",
       "                       0.1937,  0.1592,  1.8174,  1.6858,  0.9827,  2.1314,  1.0782,  1.5953,\n",
       "                       3.2277,  1.7080,  3.2367,  1.0164,  2.8372,  2.6293,  1.7219,  2.0882,\n",
       "                       1.4768,  3.5000,  2.1327,  2.1102,  4.4689,  1.9690,  1.8484,  3.9952,\n",
       "                       1.7614,  1.2217,  1.7542,  2.5607,  3.5801,  0.7481,  2.0914,  4.5307,\n",
       "                       1.4537,  2.5499,  2.9869,  5.0892,  1.7048,  3.6441,  2.5376,  1.5066,\n",
       "                       5.9680,  2.8879,  1.8929,  1.1887,  2.5250,  1.3160,  1.6519,  1.3719,\n",
       "                       1.2731,  0.3706,  0.6743,  2.8955,  2.7108,  3.3801,  3.1646,  2.3755,\n",
       "                      10.3836,  4.4288,  2.3310,  1.7395,  2.9624,  3.5476,  3.5017,  4.2545,\n",
       "                       1.4087,  4.2573,  2.1856,  2.4207,  3.4844,  0.6494,  1.8963,  4.5290,\n",
       "                       1.7167,  3.8665,  0.6810,  1.9346,  2.2320,  4.4566,  1.3086,  4.2458,\n",
       "                       2.9225,  2.1783,  1.5763,  5.9801,  3.6557,  4.0640,  2.8146,  0.4694,\n",
       "                       1.5055,  3.8840,  1.7840,  4.7326,  2.1865,  3.6485,  0.9037,  2.1197,\n",
       "                       1.0386,  2.7895,  2.1461,  0.9774,  1.5262,  2.5417,  0.1708,  4.8693,\n",
       "                       0.7781,  1.2995,  1.7437,  4.5613,  1.2061,  3.4615,  1.1129,  2.8283,\n",
       "                       2.1005,  0.5641,  1.7844,  2.6604,  2.8383,  1.4919,  1.7715,  0.9794,\n",
       "                       2.4496,  0.9861,  0.6420,  1.6912,  0.5657,  2.0822,  1.9140,  3.1072,\n",
       "                       1.9755,  2.3247,  1.4877,  2.8908,  2.5563,  0.4888,  3.2203,  0.8080,\n",
       "                       2.9885,  1.2505,  0.9863,  1.7943,  1.7106,  0.6952,  2.4336,  1.8730,\n",
       "                       2.8770,  1.6967,  0.6245,  2.8709,  2.3131,  0.3915,  0.7038,  4.1392,\n",
       "                       1.9454,  0.7361,  3.1189,  2.7459,  2.1827,  2.7872,  2.1631,  0.1993],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.9.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.9.2.clip_val', tensor([5.7710], device='cuda:0')),\n",
       "             ('module.model.10.0.weight', tensor([[[[-0.0080]],\n",
       "              \n",
       "                       [[ 0.0267]],\n",
       "              \n",
       "                       [[ 0.0326]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0349]],\n",
       "              \n",
       "                       [[ 0.0080]],\n",
       "              \n",
       "                       [[-0.1053]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0268]],\n",
       "              \n",
       "                       [[-0.1172]],\n",
       "              \n",
       "                       [[-0.0516]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0350]],\n",
       "              \n",
       "                       [[ 0.1915]],\n",
       "              \n",
       "                       [[-0.0074]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0513]],\n",
       "              \n",
       "                       [[-0.0424]],\n",
       "              \n",
       "                       [[-0.0712]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0381]],\n",
       "              \n",
       "                       [[-0.1053]],\n",
       "              \n",
       "                       [[-0.0165]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0119]],\n",
       "              \n",
       "                       [[-0.0366]],\n",
       "              \n",
       "                       [[-0.0102]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1367]],\n",
       "              \n",
       "                       [[-0.0071]],\n",
       "              \n",
       "                       [[ 0.0460]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0021]],\n",
       "              \n",
       "                       [[ 0.0509]],\n",
       "              \n",
       "                       [[ 0.0230]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0563]],\n",
       "              \n",
       "                       [[ 0.0243]],\n",
       "              \n",
       "                       [[-0.0016]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0532]],\n",
       "              \n",
       "                       [[-0.0810]],\n",
       "              \n",
       "                       [[-0.0406]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1207]],\n",
       "              \n",
       "                       [[-0.0012]],\n",
       "              \n",
       "                       [[-0.0105]]]], device='cuda:0')),\n",
       "             ('module.model.10.1.weight',\n",
       "              tensor([1.0829, 0.9835, 0.8730, 1.0049, 1.1641, 0.3439, 0.5179, 0.7833, 0.4102,\n",
       "                      1.1204, 1.1714, 0.9301, 0.4629, 0.4235, 0.8527, 0.6074, 1.0719, 0.3856,\n",
       "                      1.1767, 0.9710, 0.4071, 0.4466, 1.0632, 0.9191, 0.5755, 1.1385, 1.0545,\n",
       "                      0.8546, 1.0662, 1.2175, 1.2967, 0.5330, 0.7571, 1.0101, 1.5030, 1.0800,\n",
       "                      1.0885, 1.1378, 1.0941, 1.0752, 0.8129, 1.0694, 1.0833, 0.7990, 1.0694,\n",
       "                      1.0831, 1.2263, 0.8838, 1.0537, 0.3917, 0.6791, 0.9861, 0.7394, 0.6863,\n",
       "                      1.0644, 1.1591, 1.3062, 1.0724, 1.1342, 1.0602, 0.9003, 0.8580, 1.0067,\n",
       "                      0.9797, 1.0530, 0.7420, 0.7761, 1.3057, 1.3092, 0.4523, 0.9763, 1.2663,\n",
       "                      1.1930, 1.0931, 1.1778, 1.1913, 1.0983, 0.5313, 1.2183, 1.0580, 1.1878,\n",
       "                      1.0740, 1.2104, 1.1641, 1.0349, 1.2043, 0.4945, 1.1649, 0.4618, 1.2318,\n",
       "                      1.1267, 1.0380, 0.5374, 1.1191, 0.5283, 1.0611, 0.8772, 1.2383, 0.7788,\n",
       "                      1.1286, 1.1300, 1.2010, 1.3222, 1.1375, 0.8799, 0.9131, 1.4921, 0.4585,\n",
       "                      0.9077, 0.4300, 1.2662, 0.6623, 0.4420, 1.1758, 1.1818, 0.8200, 1.2540,\n",
       "                      0.8430, 0.8743, 0.5512, 1.4696, 0.9454, 0.9728, 1.0922, 1.3247, 1.2303,\n",
       "                      1.2846, 0.9916, 0.8677, 0.4692, 0.9814, 1.1112, 1.1844, 1.2038, 1.2205,\n",
       "                      1.1608, 1.1363, 1.0109, 1.0982, 0.4584, 1.1295, 1.2744, 0.7304, 0.6167,\n",
       "                      1.0754, 0.9510, 0.4715, 0.9790, 0.9908, 0.9531, 1.0760, 0.9156, 0.6502,\n",
       "                      1.1083, 1.3049, 1.0788, 0.7737, 1.0870, 0.8683, 0.1872, 1.1860, 1.0741,\n",
       "                      1.0752, 1.0480, 1.0480, 0.3691, 0.3937, 1.1807, 1.2701, 1.1634, 1.1803,\n",
       "                      0.8298, 1.0522, 0.9920, 0.8980, 1.4383, 1.1299, 0.7960, 1.0322, 0.8136,\n",
       "                      0.7133, 0.7217, 1.0245, 0.9338, 1.0189, 1.1908, 1.1476, 1.3045, 1.1990,\n",
       "                      0.8375, 0.9029, 0.6049, 1.0741, 1.2902, 1.2556, 1.1052, 0.4547, 0.8126,\n",
       "                      1.2065, 1.0468, 0.4787, 0.6626, 1.0703, 1.3206, 0.4126, 1.0727, 0.8804,\n",
       "                      0.3759, 1.1790, 1.2259, 1.1215, 0.9959, 1.0675, 0.5517, 1.2729, 0.8375,\n",
       "                      1.0743, 0.5350, 0.5102, 1.2615, 0.3547, 0.7896, 1.0265, 1.1211, 0.9255,\n",
       "                      0.8566, 1.1035, 1.0286, 1.4208, 1.3067, 1.1082, 0.4900, 1.1771, 1.0229,\n",
       "                      1.1031, 0.9636, 0.4862, 0.9016, 0.6394, 1.0518, 1.1529, 1.1559, 1.2411,\n",
       "                      1.2028, 1.1038, 0.7534, 1.1658, 1.2702, 1.3184, 0.8056, 1.1176, 0.8121,\n",
       "                      1.2303, 1.1369, 0.8101, 1.1627], device='cuda:0')),\n",
       "             ('module.model.10.1.bias',\n",
       "              tensor([-1.8155e-01,  6.0792e-01, -3.9411e-01,  5.1146e-01,  3.9637e-01,\n",
       "                       1.2388e+00,  1.0137e+00,  8.6395e-01,  1.1229e+00, -3.3991e-01,\n",
       "                       1.4932e-01, -6.5231e-01,  1.2258e+00,  1.1498e+00,  7.8277e-01,\n",
       "                       1.0572e+00, -1.3593e-01,  1.1597e+00, -3.3417e-01,  5.1030e-01,\n",
       "                       1.2776e+00,  1.2489e+00, -1.0455e+00, -9.4628e-01,  1.0869e+00,\n",
       "                      -1.0426e+00, -5.9258e-01,  9.0851e-01, -6.8484e-01, -6.5218e-01,\n",
       "                      -1.1003e-01,  1.4274e+00,  1.0968e+00,  4.1458e-01, -6.6307e-01,\n",
       "                      -5.8837e-01, -9.8404e-01,  7.0422e-02, -6.2978e-01,  2.5235e-01,\n",
       "                      -1.5682e+00, -3.7550e-01, -8.6254e-01, -8.1002e-01,  2.7226e-01,\n",
       "                      -4.7431e-01, -1.6139e-01, -8.7681e-01, -6.4727e-01,  1.1709e+00,\n",
       "                      -3.8624e-01,  4.3553e-01,  8.6391e-01,  1.1170e+00, -7.3785e-01,\n",
       "                      -1.2498e-01,  9.1310e-02, -1.9143e-02, -1.4071e-01, -4.6399e-01,\n",
       "                       8.2654e-01, -1.4934e+00,  5.3928e-01, -9.5358e-01, -1.7082e-01,\n",
       "                       7.9610e-01, -1.3757e+00, -3.7534e-01, -2.0404e-01,  1.4742e+00,\n",
       "                      -8.9930e-01, -4.2253e-01,  6.3596e-01, -8.7157e-01, -4.1499e-01,\n",
       "                      -3.8892e-01, -8.6658e-01,  1.3622e+00,  5.4124e-01, -4.6307e-01,\n",
       "                      -3.2986e-01,  8.7407e-01, -5.4971e-02,  1.7252e-01, -4.7374e-01,\n",
       "                      -4.1257e-02,  1.1241e+00, -3.7884e-01,  1.0200e+00,  2.8747e-04,\n",
       "                      -7.2935e-02, -3.6427e-01,  1.2258e+00, -6.2722e-01,  1.4758e+00,\n",
       "                       5.0603e-02,  9.0571e-01, -4.5574e-01,  8.4880e-01, -3.2227e-01,\n",
       "                       3.4209e-01, -9.6028e-01, -9.5321e-01, -9.8240e-01,  9.7994e-01,\n",
       "                      -6.0023e-01,  5.9826e-02,  1.1156e+00,  7.5387e-01,  1.0651e+00,\n",
       "                       1.7633e-01,  5.4416e-01,  1.2364e+00, -5.8433e-01,  1.0030e-01,\n",
       "                       9.5350e-01, -1.8420e-01, -6.3791e-01,  8.6402e-01,  1.4750e+00,\n",
       "                      -2.9258e-01,  4.8898e-01,  5.7633e-01, -3.8469e-01, -6.2745e-01,\n",
       "                      -5.6769e-01, -3.2634e-01, -6.3865e-01,  6.8960e-01,  1.3146e+00,\n",
       "                       8.4928e-01, -5.8902e-01, -1.4314e-01, -8.0537e-02, -8.8636e-02,\n",
       "                      -6.1557e-01, -3.2754e-01, -2.2081e-01,  1.5502e-02,  1.0736e+00,\n",
       "                       1.0173e-01, -3.5870e-01,  1.0257e+00,  4.5192e-01,  5.1896e-01,\n",
       "                       5.8771e-01,  9.6391e-01, -8.3950e-01, -4.8226e-01, -9.2859e-01,\n",
       "                      -1.7024e-01, -1.9542e-01,  9.8362e-01,  8.6571e-02,  2.0855e-01,\n",
       "                       6.5632e-01,  1.4915e+00, -5.0510e-01,  6.1018e-01,  1.8755e+00,\n",
       "                       2.6512e-01, -6.6055e-01, -5.8757e-01,  2.8542e-01,  7.9190e-01,\n",
       "                       1.2830e+00,  1.1025e+00,  2.2719e-01,  2.5712e-01, -8.6958e-01,\n",
       "                      -1.4464e+00,  1.3292e+00, -6.8754e-01,  4.1197e-01, -1.1745e+00,\n",
       "                       1.7909e-03,  3.3297e-02,  9.0491e-01, -3.3303e-01,  9.1567e-01,\n",
       "                       1.1825e+00,  8.5725e-01,  5.9177e-01, -1.0763e+00, -8.1763e-01,\n",
       "                      -4.0631e-01,  5.9254e-02, -2.9776e-01, -3.6806e-02,  7.0557e-01,\n",
       "                      -6.1162e-01,  8.8438e-01, -4.3373e-01, -3.1833e-01, -6.0464e-03,\n",
       "                      -7.7566e-01,  1.2633e+00,  9.6953e-01, -1.0824e+00,  5.9963e-01,\n",
       "                       1.1167e+00,  9.1011e-01,  2.4679e-01,  8.1651e-02,  1.2970e+00,\n",
       "                       2.2275e-01,  7.6357e-01,  1.0705e+00, -1.3100e+00, -4.9345e-01,\n",
       "                       5.0573e-01, -4.9658e-01, -6.4219e-01,  9.6000e-01, -1.6167e-01,\n",
       "                       1.0564e+00, -4.9203e-01,  1.2152e+00,  1.1170e+00, -6.9164e-01,\n",
       "                       1.0420e+00,  1.0448e+00, -8.6725e-01, -8.3678e-01,  6.6429e-01,\n",
       "                      -6.7365e-01, -7.6805e-01, -7.1652e-02,  2.3674e-01, -4.8011e-01,\n",
       "                      -6.0370e-01,  1.1115e+00,  9.5248e-02, -8.8187e-01,  2.1994e-01,\n",
       "                       6.8265e-01,  1.1072e+00,  8.0696e-01,  1.4909e+00, -1.0376e+00,\n",
       "                      -6.0300e-01, -5.7589e-01, -2.0260e-01, -3.6112e-01,  1.9544e-01,\n",
       "                       8.5193e-01,  3.3837e-01, -3.8941e-01, -1.9333e-01,  9.5606e-01,\n",
       "                      -9.7296e-01,  9.3187e-01, -5.7962e-02, -6.1505e-01,  8.1809e-01,\n",
       "                       2.4815e-01], device='cuda:0')),\n",
       "             ('module.model.10.1.running_mean',\n",
       "              tensor([-1.2752e+00,  7.1903e-01, -3.8507e-01, -3.4033e-01, -4.7543e-01,\n",
       "                       1.0129e-01,  1.2973e-01, -5.5928e-01, -1.2981e-01,  3.8460e-01,\n",
       "                      -4.6330e-01,  1.2930e+00, -1.3049e-01, -7.2852e-02, -9.0096e-01,\n",
       "                       1.3943e-01,  9.7053e-01,  1.9939e+00, -1.2017e+00,  3.9985e-01,\n",
       "                      -1.3476e+00,  1.1192e+00, -9.5692e-01, -1.6068e+00,  1.8690e+00,\n",
       "                      -1.0189e+00, -5.0052e-01,  2.0943e-01, -1.4427e+00, -4.5372e-01,\n",
       "                       5.5936e-02,  2.0780e+00,  9.3289e-01,  3.2496e-01, -1.5094e+00,\n",
       "                       1.6020e+00, -9.0917e-01,  6.2392e-01, -2.6914e+00,  8.7917e-01,\n",
       "                      -3.8920e-01,  3.3889e-01,  1.0440e+00,  3.2781e-01,  6.7366e-01,\n",
       "                      -9.0088e-01, -8.5316e-01,  3.9204e-01, -1.6991e-02,  1.2759e+00,\n",
       "                       9.0098e-01,  4.2501e-01,  7.4979e-01,  1.2107e+00, -7.0184e-01,\n",
       "                      -5.6174e-01,  1.4294e+00, -3.3009e-01,  4.9275e-01,  2.6844e-01,\n",
       "                      -4.5807e-01, -2.2358e-01,  8.7360e-01, -6.8935e-01,  3.9302e-01,\n",
       "                       3.0247e-03,  1.6535e+00, -1.6150e+00, -3.4913e-01,  1.0328e+00,\n",
       "                       1.2266e-01,  7.4408e-01, -1.9981e-01, -4.5025e-01, -2.0798e-01,\n",
       "                       6.6296e-01,  3.1935e-02,  1.4198e+00,  9.1796e-01, -8.4356e-01,\n",
       "                       3.8628e-01, -7.0529e-01, -4.2187e-01, -9.9358e-01,  1.4712e-01,\n",
       "                      -6.8844e-01, -1.2639e+00,  9.2817e-01,  8.7681e-01,  1.8555e-01,\n",
       "                       2.3889e-01, -1.5824e-01,  1.6177e+00,  1.6877e+00,  2.2269e+00,\n",
       "                      -1.0565e+00,  1.4162e+00,  3.7592e-01,  1.2868e-01, -6.0936e-01,\n",
       "                       6.4807e-02, -1.0491e+00, -2.3513e+00, -4.4729e-01,  6.6231e-01,\n",
       "                       2.2681e+00,  1.2564e+00,  1.6492e+00, -1.7080e-01,  8.2216e-02,\n",
       "                       2.8166e-01,  1.6892e+00, -1.6128e-01, -6.8562e-01, -1.0592e+00,\n",
       "                       1.1718e+00,  1.4610e+00,  6.5524e-01,  1.3833e+00,  2.5695e+00,\n",
       "                       2.8771e-01, -1.0045e+00, -2.7643e-01, -8.9806e-02, -6.0448e-01,\n",
       "                       1.9841e+00, -3.0826e+00,  1.2477e+00,  3.2686e-01, -8.4898e-02,\n",
       "                      -7.3484e-01,  1.8229e+00,  4.5525e-01,  7.4518e-01, -1.4291e+00,\n",
       "                       2.6075e-01,  4.9836e-01,  1.9740e+00,  1.2465e+00,  1.6019e+00,\n",
       "                       1.7131e-01, -6.3909e-01, -9.3185e-02,  7.4003e-01,  1.4129e+00,\n",
       "                       1.3162e-01,  6.0182e-01,  1.2013e+00,  8.2948e-02, -4.4616e-01,\n",
       "                       1.1661e+00,  1.0509e+00, -1.0579e-01, -1.9426e-01,  3.3676e-01,\n",
       "                       6.5743e-01, -4.0381e-01,  2.3346e+00, -4.4068e-02, -1.4340e-01,\n",
       "                      -2.0746e-01, -2.1415e-01,  6.5811e-01, -2.2936e+00,  3.3711e+00,\n",
       "                       1.1027e+00,  8.2340e-01,  9.2766e-01, -5.5201e-01, -1.2272e-03,\n",
       "                      -9.5717e-01,  2.8666e+00,  4.0206e-01,  3.1697e+00,  2.1509e-04,\n",
       "                       1.4543e-01,  4.3018e-01,  8.5427e-01,  2.0001e+00,  5.1923e-02,\n",
       "                      -5.9469e-01,  4.6142e-01,  5.3258e-01, -1.9296e-01, -2.7733e-01,\n",
       "                       1.7050e+00, -2.1011e-01, -7.5126e-01, -5.1384e-01,  7.6277e-01,\n",
       "                       1.9435e-01,  1.0544e+00,  3.2383e-01, -1.1392e-01,  1.8034e+00,\n",
       "                      -4.1873e-01,  2.4238e-01,  1.1280e+00,  1.9419e-01, -1.9172e-01,\n",
       "                       3.4719e-01,  3.0812e-02,  4.2328e-01,  2.7246e-01, -1.5923e+00,\n",
       "                      -9.5374e-01,  9.4668e-01,  8.7625e-01, -8.5119e-01, -2.3526e+00,\n",
       "                       1.2843e+00,  7.3408e-01, -4.7131e-01,  7.9033e-01,  9.0217e-01,\n",
       "                       6.6688e-01,  2.3568e+00,  8.0854e-01,  2.1984e+00,  4.7805e-01,\n",
       "                       8.3040e-01, -2.7453e-02, -1.0959e+00,  9.6665e-01,  7.5740e-01,\n",
       "                      -4.1237e-02, -1.6788e+00,  8.6319e-01,  7.8120e-01,  1.1857e+00,\n",
       "                      -1.0544e+00,  7.2367e-01,  9.2045e-01, -1.5962e+00,  2.2197e-01,\n",
       "                       1.3549e+00,  2.3579e-01,  1.0286e+00,  9.2552e-01,  1.3830e-01,\n",
       "                       1.2221e+00,  1.1750e-01, -1.9460e-01,  2.7123e-01, -1.7298e+00,\n",
       "                      -1.2391e+00,  4.1950e-01, -1.3750e+00, -7.5128e-01, -7.6600e-01,\n",
       "                      -8.3819e-01, -1.9475e-01,  1.2320e+00, -8.9676e-02,  4.5809e-01,\n",
       "                      -3.8544e-01], device='cuda:0')),\n",
       "             ('module.model.10.1.running_var',\n",
       "              tensor([1.0843, 0.7849, 0.5278, 0.4393, 0.7874, 0.6058, 0.3810, 0.6017, 0.6145,\n",
       "                      0.8551, 1.0880, 0.8041, 0.5749, 0.4309, 0.8061, 0.3756, 0.5988, 0.5748,\n",
       "                      1.1283, 0.5872, 0.4552, 0.8541, 0.8381, 1.1139, 0.8915, 1.2259, 0.5457,\n",
       "                      0.6927, 1.0239, 0.9802, 1.3692, 0.5909, 0.8275, 0.8817, 2.5561, 0.9009,\n",
       "                      0.8052, 0.4579, 1.1152, 0.6025, 0.5017, 1.0873, 0.8947, 0.4806, 1.7022,\n",
       "                      0.7422, 1.1009, 0.4370, 0.6842, 0.5335, 0.6103, 0.4552, 0.4669, 0.7784,\n",
       "                      0.7560, 1.4493, 1.6356, 0.7600, 1.3687, 0.7412, 0.8640, 0.5445, 0.6854,\n",
       "                      0.7482, 0.6502, 0.2523, 1.0117, 1.4586, 1.8972, 1.0331, 0.6587, 1.0242,\n",
       "                      0.8994, 1.1057, 0.9370, 1.1783, 0.9107, 0.7394, 0.7015, 0.7582, 0.9075,\n",
       "                      0.9744, 0.5256, 0.7837, 0.6666, 0.4023, 0.4977, 1.0609, 0.4983, 0.5352,\n",
       "                      0.7160, 0.7112, 0.5942, 1.1764, 0.7638, 0.6546, 0.6705, 1.1169, 0.9697,\n",
       "                      1.2141, 1.0626, 1.0409, 1.6669, 1.0809, 0.9268, 0.8610, 2.2241, 0.7047,\n",
       "                      0.7087, 0.3182, 0.7374, 0.3991, 0.6310, 1.3444, 1.4173, 0.9979, 1.4140,\n",
       "                      0.7207, 1.0720, 0.9372, 1.5945, 0.6175, 0.7231, 0.7970, 1.2565, 0.9836,\n",
       "                      2.1641, 0.8199, 0.4006, 0.7014, 0.9745, 0.7955, 1.3136, 1.0137, 1.3715,\n",
       "                      0.7341, 0.5521, 0.5983, 0.5669, 0.6520, 0.9079, 1.2115, 0.7301, 0.4816,\n",
       "                      0.8602, 0.2957, 0.3571, 0.5920, 0.7134, 0.6425, 0.6989, 0.3698, 0.5922,\n",
       "                      0.5226, 0.5998, 0.6678, 1.0390, 0.7167, 0.5388, 0.5139, 0.6704, 0.5591,\n",
       "                      0.7887, 0.6006, 1.6527, 0.6897, 0.5538, 0.6032, 1.1056, 0.8359, 0.6633,\n",
       "                      2.0225, 0.9121, 0.7554, 0.5338, 2.2481, 0.7990, 0.6398, 0.4495, 0.6483,\n",
       "                      0.8786, 0.7611, 0.8644, 0.5638, 0.6445, 1.5063, 0.7301, 1.3711, 0.7609,\n",
       "                      0.5924, 0.8897, 0.4598, 0.7240, 1.4004, 1.6585, 0.7710, 0.3459, 0.8594,\n",
       "                      1.2755, 0.8906, 0.5494, 0.3207, 0.8639, 0.8581, 0.7674, 0.7714, 0.5468,\n",
       "                      0.4770, 0.7481, 1.0658, 0.8877, 0.6734, 0.7695, 0.5714, 0.9274, 0.8067,\n",
       "                      0.7921, 1.0695, 0.8135, 1.8245, 0.4802, 0.4951, 0.6799, 0.7382, 0.6070,\n",
       "                      0.5650, 0.7039, 0.5661, 0.7761, 1.0012, 0.6883, 0.5866, 0.6621, 0.9455,\n",
       "                      0.6031, 0.9432, 0.5057, 0.7602, 0.7654, 0.4431, 1.2718, 0.9138, 0.7661,\n",
       "                      0.8744, 0.7927, 0.6937, 1.0166, 0.8698, 0.9425, 0.5201, 1.1525, 0.6590,\n",
       "                      1.1455, 1.0569, 0.7117, 1.0306], device='cuda:0')),\n",
       "             ('module.model.10.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.10.2.clip_val', tensor([5.0651], device='cuda:0')),\n",
       "             ('module.model.11.0.weight',\n",
       "              tensor([[[[ 0.6272,  0.8802,  0.4453],\n",
       "                        [ 0.9006,  1.2251,  0.5642],\n",
       "                        [ 0.5198,  0.6733,  0.3417]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0870, -0.5150, -0.4721],\n",
       "                        [-0.5118, -0.8144, -0.4324],\n",
       "                        [-0.4394, -0.3636, -0.0656]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3937,  0.8521,  0.7457],\n",
       "                        [-0.0956,  0.6533,  0.7224],\n",
       "                        [-0.4343,  0.0956,  0.3665]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4935,  0.6807,  0.3601],\n",
       "                        [ 0.6852,  0.8974,  0.4155],\n",
       "                        [ 0.3553,  0.4416,  0.2151]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3554, -0.4258, -0.2715],\n",
       "                        [-0.5683, -0.7645, -0.4319],\n",
       "                        [-0.2829, -0.4174, -0.1796]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0719,  1.1308,  0.2842],\n",
       "                        [ 0.8892,  0.7660,  0.0250],\n",
       "                        [ 0.0122, -0.2586, -0.2942]]]], device='cuda:0')),\n",
       "             ('module.model.11.1.weight',\n",
       "              tensor([0.8599, 0.7809, 0.6674, 1.3921, 1.6954, 0.8726, 0.9823, 0.9393, 1.1464,\n",
       "                      1.1275, 0.8600, 1.2854, 1.1547, 1.2437, 0.8472, 0.5910, 0.8161, 0.7482,\n",
       "                      0.6284, 0.8455, 1.6453, 1.6800, 0.5919, 0.6354, 2.0896, 0.6487, 0.8940,\n",
       "                      2.2207, 0.7191, 0.7425, 0.7769, 1.6165, 1.3124, 0.8342, 0.7216, 0.8149,\n",
       "                      0.5899, 1.0844, 0.7253, 1.2651, 0.5861, 0.6784, 0.6875, 0.6798, 0.8303,\n",
       "                      0.7452, 0.6890, 0.6711, 0.6525, 0.8494, 0.6467, 0.6010, 0.6587, 0.9183,\n",
       "                      0.5993, 0.7727, 0.9846, 0.8651, 0.8382, 0.6647, 1.0144, 0.5031, 1.3583,\n",
       "                      0.6808, 0.5988, 0.9878, 0.6601, 0.7880, 0.8258, 1.7356, 0.6754, 0.7231,\n",
       "                      2.7842, 0.6844, 0.6294, 0.7778, 0.6825, 2.6415, 2.0930, 0.6201, 0.6646,\n",
       "                      2.5532, 1.2362, 0.9253, 0.7130, 1.1440, 0.8489, 0.6763, 1.1626, 1.3097,\n",
       "                      1.1656, 0.7265, 1.5877, 0.8281, 2.1256, 0.7478, 1.0674, 0.7130, 1.1430,\n",
       "                      0.7804, 0.8813, 0.6345, 0.7184, 0.8056, 1.0174, 1.1565, 0.9985, 0.8031,\n",
       "                      0.8670, 0.9744, 1.2274, 0.6444, 0.8403, 0.7610, 0.8597, 1.7456, 0.8424,\n",
       "                      0.6643, 0.9245, 2.4088, 0.8446, 0.8200, 0.8727, 0.7358, 0.7651, 0.7202,\n",
       "                      0.9613, 0.6254, 1.0653, 1.2595, 1.4321, 0.6780, 0.6889, 0.7397, 0.7118,\n",
       "                      0.6893, 0.6681, 0.6753, 0.8204, 0.9306, 1.6283, 0.7052, 1.0551, 1.0063,\n",
       "                      1.2501, 1.0248, 0.8623, 0.6250, 0.5793, 0.5724, 0.8162, 0.5464, 0.8796,\n",
       "                      1.0692, 1.3255, 2.0032, 1.9779, 0.7606, 0.5951, 1.0073, 1.4674, 0.6495,\n",
       "                      0.5990, 0.7220, 1.0758, 0.7809, 0.7784, 0.8631, 2.2545, 0.5674, 0.9033,\n",
       "                      1.1163, 0.7555, 0.7412, 0.5872, 0.9906, 1.1645, 1.1070, 0.8203, 1.6611,\n",
       "                      1.1096, 0.8885, 1.4590, 0.6263, 0.6557, 1.2868, 1.1885, 0.7966, 1.2573,\n",
       "                      0.7965, 0.7470, 0.8070, 0.6570, 0.7325, 0.8347, 0.6674, 0.6581, 1.2210,\n",
       "                      0.6566, 2.2714, 1.1723, 0.6025, 0.6930, 1.6911, 0.8981, 0.5840, 1.0731,\n",
       "                      0.7968, 0.6139, 0.8026, 1.9543, 0.7158, 0.7754, 1.2219, 2.3717, 1.1044,\n",
       "                      0.9080, 0.9804, 0.8805, 0.8294, 0.9675, 1.1272, 0.5941, 0.8262, 0.6683,\n",
       "                      0.4841, 0.5762, 0.6678, 1.2779, 0.7479, 0.7007, 0.9869, 1.0616, 0.7765,\n",
       "                      0.6177, 0.7796, 0.9031, 1.3643, 1.3227, 0.6434, 0.8129, 0.7419, 0.7419,\n",
       "                      0.6958, 0.7025, 1.1029, 1.1233, 0.6676, 0.7626, 1.4541, 0.6847, 1.2590,\n",
       "                      0.8209, 0.7166, 1.5007, 0.7449], device='cuda:0')),\n",
       "             ('module.model.11.1.bias',\n",
       "              tensor([ 1.2301,  0.8190,  0.2205, -0.1942, -0.2479,  0.4352,  0.3440,  0.8257,\n",
       "                       0.1840,  1.3579,  1.1186, -0.6417,  0.2055, -0.2627,  1.4051,  1.6976,\n",
       "                       1.0586,  1.7788,  1.4811,  0.7025, -0.9771, -0.2559,  0.4350,  0.3535,\n",
       "                      -0.7972,  0.3603,  1.4602, -1.0383,  0.4413,  0.5571,  0.9081, -0.2923,\n",
       "                      -0.4377,  1.4088,  0.8010,  0.4563,  0.5218,  0.4157,  0.5882,  0.6071,\n",
       "                       0.1820,  0.7244,  0.5051,  0.2196,  2.2145,  0.4542,  2.3206,  0.2697,\n",
       "                       0.4054,  0.6449,  0.7957,  1.0660,  1.3500,  0.6515,  0.9029,  2.8228,\n",
       "                       0.8414,  0.7022,  0.7410,  0.3226,  0.3911,  0.2320,  0.6387,  0.4336,\n",
       "                       2.3461, -0.4820,  0.2340,  0.9226,  0.9725, -0.4554,  0.7361,  0.8375,\n",
       "                      -1.3040,  0.3992,  0.9302,  0.6035,  0.3107, -1.7088, -0.8086,  0.9297,\n",
       "                       1.0587, -0.9511,  0.3090,  0.6319,  0.4301,  0.4601,  0.5022,  0.8695,\n",
       "                      -0.5690, -0.0854,  0.9047,  0.5798, -0.0261, -0.0079, -1.0498,  0.7561,\n",
       "                       0.4113,  0.7070,  0.4205,  0.8834,  0.8018,  0.7564,  0.4485,  0.4450,\n",
       "                       0.4445, -0.1128,  0.8952,  2.1866,  0.9387, -0.4417,  0.4317,  0.2175,\n",
       "                       2.5662,  0.6686,  0.8895, -0.0642,  0.5758,  0.5593,  1.0085, -1.3120,\n",
       "                       0.7324,  1.0391,  0.9090,  2.9734,  0.6495,  0.4687,  1.0744,  0.6922,\n",
       "                      -0.2610,  0.0252,  0.3177,  0.4405,  1.4771,  1.0089,  1.6754,  0.9823,\n",
       "                       0.5213,  0.7166,  0.9705,  0.7413, -0.1234,  2.1403, -0.0845, -0.3420,\n",
       "                       1.1541,  0.0254,  0.1316,  0.5643,  2.4556,  0.2526,  0.1382,  0.3142,\n",
       "                       0.6962,  1.1501,  0.0214, -0.2023, -0.9136,  0.5442,  1.7179,  0.6893,\n",
       "                       0.1353,  1.1504,  0.4589,  0.7817,  1.2964,  2.1130,  0.9177,  0.7494,\n",
       "                      -0.5278,  0.3824,  0.6477,  2.0069,  0.4053,  0.8956,  0.2427,  0.8125,\n",
       "                       1.0670, -0.0515,  0.4365, -0.1862,  0.1903,  0.7037,  0.1628,  0.2570,\n",
       "                       0.3785, -0.1206,  0.6677,  0.8660,  1.0991,  0.3818,  0.4578,  0.1605,\n",
       "                       1.0066,  0.8364,  0.9180,  2.4667,  1.0074,  0.5671,  0.4149, -0.6973,\n",
       "                       0.1003,  1.4172,  2.0655, -0.2338,  1.7551,  1.6204, -0.2631,  0.8771,\n",
       "                       0.2756,  0.5430, -0.1526,  0.5637,  0.3889,  0.2564, -0.7072, -0.0926,\n",
       "                       0.4377,  2.5038,  1.2171,  0.5866, -0.1875, -0.1552,  2.7288,  0.4680,\n",
       "                       1.8860,  2.2900,  0.3782,  0.5011,  0.3733,  0.8153,  0.6714, -0.2341,\n",
       "                       0.5182,  0.4436,  2.2711,  1.0250,  0.5826,  0.2508, -0.3872,  0.3092,\n",
       "                       0.6338,  0.5071,  0.4376,  0.6161,  1.6821,  0.6830,  0.6900,  0.7394,\n",
       "                       0.7806, -0.7061,  0.3867,  0.3359,  1.9916,  0.7641, -0.2134,  1.7439],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.11.1.running_mean',\n",
       "              tensor([ 1.9278, -2.6841,  0.5483, -2.3550, -2.5741, -4.1644, -3.2749, -3.3745,\n",
       "                      -3.7503, -1.1429,  2.1189,  0.4193, -4.5408, -4.0151, -3.1731, -0.6077,\n",
       "                      -1.1198, -4.4965, -1.1099, -0.5975, -0.3725, -5.0451,  0.3019,  0.2116,\n",
       "                      -4.1424,  0.4625, -0.7794, -3.6418,  0.5631,  1.1464,  2.2385, -6.1365,\n",
       "                      -0.2082, -2.2015,  1.1869,  0.9259,  0.4578, -1.5049,  0.7763, -2.0056,\n",
       "                       0.1822,  1.1303,  0.6716,  0.2745, -2.0569,  0.9704, -1.5518,  0.3810,\n",
       "                       0.7007, -4.0403,  0.5627, -0.3826, -2.7770, -3.8564,  0.8255, -1.5433,\n",
       "                       2.4661,  1.9098,  1.9903,  0.6592, -3.4836,  0.1145, -2.5711,  0.4170,\n",
       "                      -1.1746,  0.7991,  0.1181,  1.2811,  1.7338, -6.4021,  0.4389,  1.3771,\n",
       "                      -3.4690,  0.5883,  1.2680,  1.5853,  0.5042, -5.8272, -2.8208,  0.9485,\n",
       "                       1.4817, -4.2204, -1.5937, -1.9399,  0.7638, -1.4634, -3.6721,  1.2208,\n",
       "                      -0.4917, -1.8330, -1.4838,  1.0462, -5.1463,  0.7465, -6.2944,  1.8034,\n",
       "                      -3.3112,  1.3367,  3.5468,  1.5395, -2.3572,  0.5404,  0.8545,  0.4854,\n",
       "                      -3.6526,  0.4952,  3.6109,  4.5169, -2.8862, -0.3114, -2.1212,  0.4952,\n",
       "                      -4.9385,  1.0861,  2.6501, -3.7023,  1.7742,  0.4606, -3.2257, -6.5309,\n",
       "                       2.4755, -2.0745, -2.4338, -1.2491,  0.9459,  0.9748,  1.4569,  0.5273,\n",
       "                       0.8230, -4.7275, -3.8843,  0.9237, -1.4322,  1.7729, -1.6372,  1.2049,\n",
       "                       1.0943,  1.0313, -1.4008, -3.4651, -1.5996, -1.3437, -0.3225, -0.2545,\n",
       "                      -2.9588, -1.2312,  0.1428,  0.5371, -0.6707,  0.3367,  1.2830,  0.4924,\n",
       "                      -3.2310, -1.5026, -2.3606, -3.0468, -6.4226,  0.8886, -1.7460,  1.7883,\n",
       "                      -2.1990,  0.8032,  0.7353,  2.4285,  4.5306, -5.0656, -3.8011, -2.2202,\n",
       "                      -2.6568,  0.7082,  0.4329,  8.3937,  0.7128,  2.3229,  0.2132,  3.2971,\n",
       "                      -1.6137, -0.3535,  0.8824, -3.5186, -4.8514, -2.8347, -2.6134,  0.2608,\n",
       "                       0.5147,  1.1844, -1.6202,  1.8016, -1.8121,  0.2292,  0.7590,  0.1956,\n",
       "                       1.0103,  1.4184,  2.5132, -0.8409,  0.5060, -3.9340,  0.6352, -2.9565,\n",
       "                      -4.1450, -0.0092, -1.8722, -2.2177, -5.4185, -1.8720, -0.0874, -3.3173,\n",
       "                       0.3054,  1.3971, -2.7084,  0.7271,  0.6730, -3.0182, -1.5540, -0.3058,\n",
       "                       1.2308,  6.5245, -4.3774,  0.7717, -0.0884, -0.0677, -0.5089,  0.7393,\n",
       "                       0.0924, -0.3594,  0.5945,  1.0959, -2.5104,  1.3919,  1.0093, -0.1966,\n",
       "                      -1.7245,  0.5598, -1.5807, -2.9361, -3.9032, -3.4895, -6.0829,  0.4545,\n",
       "                       0.9808,  0.9790,  1.7044,  1.2320, -1.8291, -3.1620, -2.3925,  1.5445,\n",
       "                       1.9774,  0.1818,  0.4696, -3.7998,  2.3370,  0.8831, -2.9278,  2.1594],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.11.1.running_var',\n",
       "              tensor([ 5.8335,  1.8847,  0.7934,  2.1363,  3.1239,  0.8375,  1.5084,  3.2051,\n",
       "                       0.9413,  2.0884,  3.0595,  0.6998,  1.5564,  0.9439,  3.7731,  0.8742,\n",
       "                       1.3532,  0.9915,  0.7916,  2.6531,  0.9929,  1.6819,  0.2467,  0.1922,\n",
       "                       2.1381,  0.7431,  1.6103,  5.1657,  0.6020,  1.7710,  3.1583,  2.0816,\n",
       "                       1.5853,  3.3070,  1.0409,  2.1816,  0.5168,  1.9406,  1.1671,  3.8530,\n",
       "                       0.4475,  0.9043,  0.8542,  0.4562,  2.5672,  1.6764,  1.5177,  0.6673,\n",
       "                       0.9059,  0.8153,  0.6548,  2.1427,  2.3944,  3.6853,  1.3795,  2.0440,\n",
       "                       6.7493,  4.1757,  7.9800,  0.6511,  2.8510,  0.1308,  4.7101,  0.6085,\n",
       "                       1.0496,  1.2481,  0.1173,  1.0876,  3.2114,  2.0195,  0.5804,  2.0572,\n",
       "                       8.6273,  0.9549,  1.4164,  6.3873,  0.7710,  2.7065,  5.8313,  0.6905,\n",
       "                       2.2234, 10.3368,  2.5110,  2.2370,  1.6104,  2.2255,  1.2006,  1.6209,\n",
       "                       0.4529,  2.2628,  2.6264,  1.6443,  2.2173,  1.0947,  2.3404,  2.6669,\n",
       "                       4.2750,  2.1338,  5.1748,  2.6287,  2.2747,  0.4089,  1.5261,  0.7216,\n",
       "                       3.1059,  1.3987, 14.0191,  1.8404,  3.3532,  0.4225,  5.2239,  0.8203,\n",
       "                       1.8737,  1.4814,  8.0491,  5.1739,  3.7225,  0.4499,  4.3500,  2.6799,\n",
       "                       9.2880,  1.9236,  3.2485,  2.1723,  0.9395,  1.1001,  1.6670,  0.3892,\n",
       "                       1.7202,  2.1175,  7.3328,  1.0196,  0.9892,  2.7322,  1.4059,  2.3423,\n",
       "                       1.0534,  2.1253,  1.5261,  1.0322,  1.9448,  1.3335,  1.8293,  0.5032,\n",
       "                       6.2973,  1.9933,  0.4267,  0.7553,  0.5332,  0.3476,  2.3910,  0.7537,\n",
       "                       2.4033,  4.5733,  3.2255,  7.8610,  6.4221,  1.7620,  1.8501,  1.5576,\n",
       "                       3.9474,  1.2408,  0.6615,  4.7208, 11.8703,  0.9488,  0.7554,  3.3555,\n",
       "                       5.5617,  1.0605,  1.1949, 20.4475,  1.2866,  3.6495,  0.2320, 12.4176,\n",
       "                       2.8087,  1.6362,  2.1362,  4.9878,  4.0210,  2.3176,  3.9528,  0.2817,\n",
       "                       0.6398,  2.5486,  2.7254,  3.8614,  5.2901,  1.2696,  1.2965,  0.8457,\n",
       "                       1.3822,  1.3438,  6.8131,  1.6685,  0.9914,  3.7877,  1.0852,  6.7493,\n",
       "                       1.3242,  1.1087,  2.0771,  3.5434,  1.9388,  0.9187,  1.6125,  0.6273,\n",
       "                       0.3987,  3.3934,  4.9876,  1.0494,  0.8573,  1.4928,  2.7541,  2.4668,\n",
       "                       3.7827,  5.6073,  1.3324,  0.9942,  0.2750,  2.0689,  0.5135,  1.2006,\n",
       "                       1.7015,  0.2727,  0.6956,  1.2939,  8.1268,  2.0359,  1.9618,  0.9479,\n",
       "                       2.3574,  1.0737,  2.8850,  2.6294,  1.2283,  5.1634,  3.8038,  0.8770,\n",
       "                       2.5071,  1.7514,  3.3976,  1.8324,  2.4869,  2.0918,  2.9580,  1.9490,\n",
       "                       3.8076,  1.9104,  0.6063,  4.2279,  4.6777,  1.2437,  2.9541,  4.6870],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.11.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.11.2.clip_val', tensor([5.8785], device='cuda:0')),\n",
       "             ('module.model.12.0.weight', tensor([[[[-4.4575e-05]],\n",
       "              \n",
       "                       [[-8.8843e-03]],\n",
       "              \n",
       "                       [[-2.8199e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.0063e-01]],\n",
       "              \n",
       "                       [[-6.6207e-02]],\n",
       "              \n",
       "                       [[ 8.1928e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.2264e-02]],\n",
       "              \n",
       "                       [[ 3.1179e-04]],\n",
       "              \n",
       "                       [[-1.0499e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.6213e-02]],\n",
       "              \n",
       "                       [[ 9.0472e-02]],\n",
       "              \n",
       "                       [[ 1.1438e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.9065e-02]],\n",
       "              \n",
       "                       [[ 6.0209e-02]],\n",
       "              \n",
       "                       [[-4.9785e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.8695e-02]],\n",
       "              \n",
       "                       [[-3.8309e-02]],\n",
       "              \n",
       "                       [[ 4.2271e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-8.2036e-02]],\n",
       "              \n",
       "                       [[-6.8122e-02]],\n",
       "              \n",
       "                       [[-1.8235e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.7215e-02]],\n",
       "              \n",
       "                       [[-2.1702e-02]],\n",
       "              \n",
       "                       [[ 5.2232e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.2768e-02]],\n",
       "              \n",
       "                       [[ 1.6829e-01]],\n",
       "              \n",
       "                       [[-4.8940e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.0469e-02]],\n",
       "              \n",
       "                       [[ 1.7036e-03]],\n",
       "              \n",
       "                       [[ 3.8395e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.8253e-02]],\n",
       "              \n",
       "                       [[ 4.2836e-02]],\n",
       "              \n",
       "                       [[-1.5966e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.3863e-02]],\n",
       "              \n",
       "                       [[ 4.4791e-02]],\n",
       "              \n",
       "                       [[-3.0403e-02]]]], device='cuda:0')),\n",
       "             ('module.model.12.1.weight',\n",
       "              tensor([1.2109, 1.1372, 1.0272, 1.0539, 0.6047, 0.7741, 1.0328, 0.9555, 0.4470,\n",
       "                      1.0650, 0.8663, 0.6069, 0.5914, 0.4056, 0.6727, 0.6019, 0.8049, 0.7580,\n",
       "                      0.7669, 0.2655, 0.6493, 0.8325, 1.2156, 1.1623, 1.0716, 0.5163, 0.6617,\n",
       "                      0.5616, 0.6862, 1.1827, 0.7264, 0.6239, 1.0935, 0.5675, 0.6009, 0.7462,\n",
       "                      0.3395, 1.0914, 0.9423, 1.2093, 1.0731, 0.7825, 0.3204, 1.0252, 0.8295,\n",
       "                      0.7869, 0.6616, 0.6139, 0.6409, 0.5392, 0.6415, 1.1229, 0.6918, 1.0228,\n",
       "                      0.7079, 1.1298, 1.2922, 0.9790, 0.8069, 0.6553, 1.0040, 1.0389, 0.6393,\n",
       "                      1.0635, 1.0539, 0.6038, 0.7216, 0.3285, 1.0404, 0.5212, 0.5821, 0.4300,\n",
       "                      0.8075, 0.5279, 0.8969, 0.8917, 0.8214, 0.5118, 0.4785, 0.7901, 1.0412,\n",
       "                      0.9541, 0.7931, 1.8037, 0.8167, 0.8155, 0.8073, 0.8031, 0.7441, 0.6263,\n",
       "                      1.0750, 0.9896, 0.9556, 1.2673, 0.7720, 1.0042, 0.7546, 0.6242, 0.9780,\n",
       "                      0.5704, 0.5544, 0.8802, 0.7184, 1.1069, 0.6042, 0.9811, 1.0872, 0.4920,\n",
       "                      0.7290, 0.7679, 0.8494, 0.3872, 1.1791, 0.9961, 0.5665, 0.6426, 0.7242,\n",
       "                      1.0877, 1.0501, 0.6820, 0.9825, 0.6580, 1.0889, 0.9895, 0.7421, 0.6913,\n",
       "                      0.9337, 2.2623, 1.0194, 1.0374, 0.7194, 1.1471, 0.5887, 0.5835, 0.6193,\n",
       "                      0.7209, 0.9407, 0.6017, 0.7164, 0.7060, 0.4795, 0.5941, 0.6325, 1.0776,\n",
       "                      1.0472, 0.6847, 0.6935, 0.4625, 0.6686, 1.0738, 0.5516, 0.7652, 0.9845,\n",
       "                      1.0163, 0.9153, 1.0364, 0.7847, 0.3270, 0.8231, 0.7323, 0.5279, 0.6007,\n",
       "                      0.5335, 1.0117, 0.8345, 0.7026, 0.8653, 0.6908, 0.7501, 0.6763, 1.1313,\n",
       "                      0.4857, 0.4798, 0.6452, 1.1039, 0.7236, 0.7126, 0.8194, 0.7274, 0.7111,\n",
       "                      0.5252, 0.7927, 1.2065, 0.5805, 0.4103, 0.8721, 0.4691, 0.7384, 1.1512,\n",
       "                      0.7315, 1.0308, 1.8990, 1.0172, 0.4830, 0.6284, 0.7269, 0.6090, 0.6290,\n",
       "                      0.7388, 0.4091, 0.6212, 0.4186, 0.5370, 0.6408, 0.6966, 0.9046, 0.8183,\n",
       "                      0.8301, 0.6190, 0.4951, 1.0629, 0.7953, 0.7093, 1.2892, 0.8534, 1.1097,\n",
       "                      0.6790, 1.0002, 0.5775, 1.0993, 0.5546, 0.9895, 0.7588, 0.6175, 1.1750,\n",
       "                      1.0828, 0.7170, 0.5754, 1.0244, 0.8596, 0.5748, 0.6534, 0.9003, 0.5877,\n",
       "                      1.0256, 1.3478, 0.3658, 0.5875, 0.7465, 0.5405, 0.3838, 1.0107, 0.6544,\n",
       "                      0.5758, 0.5880, 0.3532, 0.7450, 1.0621, 1.0783, 1.0300, 1.2048, 0.6198,\n",
       "                      0.9544, 0.4136, 0.9659, 0.6009, 0.9766, 0.5813, 0.8133, 0.9934, 0.7613,\n",
       "                      0.8919, 0.7418, 0.6156, 0.7356, 0.6194, 0.4257, 1.0050, 0.7190, 1.1411,\n",
       "                      0.7946, 0.3653, 1.2488, 0.7878, 0.6500, 0.9208, 0.7507, 0.7913, 0.5823,\n",
       "                      1.1143, 0.5641, 0.8893, 1.0867, 1.0221, 0.9004, 1.0855, 0.4813, 1.0666,\n",
       "                      0.6788, 0.6838, 0.9368, 0.7250, 1.0732, 0.5363, 0.3626, 0.9445, 0.8063,\n",
       "                      0.8860, 0.5984, 1.0425, 0.6287, 0.9145, 1.0531, 0.9961, 0.8167, 0.9316,\n",
       "                      1.1089, 0.6244, 1.0264, 1.1161, 0.9478, 1.0361, 0.8710, 0.7858, 1.1263,\n",
       "                      0.6445, 0.7355, 0.9304, 0.6175, 0.5872, 0.5051, 0.8574, 0.9512, 0.8910,\n",
       "                      0.7428, 0.4414, 0.3848, 0.5334, 0.8569, 0.8152, 0.8557, 1.0799, 0.6464,\n",
       "                      0.6522, 0.6338, 0.9633, 0.8095, 0.5682, 1.0687, 0.9739, 0.9540, 0.8296,\n",
       "                      0.9616, 0.8851, 1.0142, 0.9960, 1.0143, 0.8511, 0.9366, 1.0269, 0.3960,\n",
       "                      1.0527, 0.9500, 0.9548, 0.7162, 0.5218, 0.8174, 0.7232, 0.4842, 0.7422,\n",
       "                      0.7934, 1.0287, 1.0947, 0.7652, 0.7860, 0.6016, 0.8208, 0.7105, 1.1645,\n",
       "                      0.6224, 0.8299, 0.7980, 0.8490, 1.1217, 0.2844, 0.4314, 1.1209, 0.5265,\n",
       "                      0.7705, 0.6794, 0.4757, 0.5880, 0.8021, 0.3974, 0.5787, 0.9376, 0.7494,\n",
       "                      0.8708, 0.6531, 0.6778, 1.1799, 0.8588, 0.5522, 1.0618, 0.4830, 0.6767,\n",
       "                      0.9327, 0.8340, 0.9188, 1.0686, 0.8227, 1.0189, 1.0266, 0.9887, 0.7956,\n",
       "                      1.0148, 1.0546, 0.4802, 1.1361, 1.0023, 0.8982, 1.1560, 0.6524, 0.9871,\n",
       "                      0.9218, 0.9564, 0.7947, 0.9557, 1.8359, 0.3657, 1.4744, 0.5640, 1.0195,\n",
       "                      0.6389, 1.1117, 1.1472, 0.4964, 0.5598, 0.7208, 0.9264, 0.9826, 0.9638,\n",
       "                      0.6989, 0.8055, 1.0253, 1.0794, 0.5968, 0.4846, 1.0121, 0.9065, 0.5323,\n",
       "                      1.0137, 0.8733, 0.6713, 0.9342, 0.5989, 1.0186, 0.6698, 0.9966, 0.5161,\n",
       "                      0.8265, 0.8221, 0.6495, 1.0057, 1.1121, 0.8364, 0.2636, 1.0590, 0.5691,\n",
       "                      1.0233, 0.9534, 0.9728, 0.4157, 0.5288, 1.0644, 1.0714, 0.8647, 1.2190,\n",
       "                      1.0293, 0.7386, 0.8362, 0.9651, 1.4080, 0.7320, 0.7247, 0.9605, 0.5818,\n",
       "                      0.8649, 0.4617, 0.5279, 0.7285, 0.7642, 0.5709, 1.0546, 0.9925, 0.9813,\n",
       "                      0.6357, 0.6231, 0.9147, 0.7354, 1.0363, 1.0767, 0.5917, 0.5970, 1.0088,\n",
       "                      0.6580, 1.2100, 1.0721, 0.5991, 0.9421, 0.8417, 0.7985, 0.6002, 0.3166,\n",
       "                      0.5805, 0.5921, 0.6618, 0.5792, 1.1279, 0.9740, 0.6779, 0.6390],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.12.1.bias',\n",
       "              tensor([ 1.9885e-01, -1.7400e-01,  4.0039e-02,  3.6581e-01,  9.3635e-01,\n",
       "                       1.0313e+00,  1.5781e-01,  5.5253e-01,  1.0121e+00,  2.2759e-01,\n",
       "                       9.2334e-01,  1.5553e+00,  1.2430e+00,  1.2038e+00,  1.0095e+00,\n",
       "                       9.6358e-01,  6.9955e-01,  1.1099e+00,  9.9031e-01,  1.1166e+00,\n",
       "                       8.6413e-01,  8.1899e-01,  5.3739e-02, -1.4018e-01, -2.2346e-01,\n",
       "                       1.0536e+00,  9.6486e-01,  1.2088e+00,  8.7363e-01,  2.7445e-01,\n",
       "                       9.2834e-01,  1.1019e+00, -1.6278e-01,  9.8240e-01,  1.0457e+00,\n",
       "                       8.5118e-01,  1.1101e+00,  2.3076e-01, -4.7784e-01, -8.4771e-03,\n",
       "                       4.9875e-01,  9.3500e-01,  1.0410e+00,  2.3348e-01,  7.8423e-01,\n",
       "                       9.6442e-01,  1.1397e+00,  9.2213e-01,  1.0918e+00,  1.0548e+00,\n",
       "                       9.1748e-01,  1.1948e-01,  1.1602e+00, -3.8790e-02,  8.5126e-01,\n",
       "                       2.6322e-01,  1.1985e-01,  3.5603e-01,  9.7173e-01,  1.0096e+00,\n",
       "                      -2.6481e-01, -3.2733e-01,  9.2545e-01, -2.1110e-01, -3.3764e-01,\n",
       "                       1.0931e+00,  1.1160e+00,  1.2871e+00,  5.0937e-02,  1.1279e+00,\n",
       "                       1.0317e+00,  1.1598e+00,  1.0013e+00,  1.0365e+00,  9.0969e-01,\n",
       "                      -8.0114e-01,  1.0125e+00,  1.0459e+00,  1.0110e+00, -6.8005e-01,\n",
       "                       7.1809e-01,  4.6648e-01,  7.2581e-01,  1.6402e-01,  1.4340e+00,\n",
       "                      -7.8949e-01,  7.2831e-01,  8.3133e-01,  9.8336e-01,  1.0900e+00,\n",
       "                       5.3966e-01,  5.3738e-01,  7.8521e-01,  4.6290e-01,  1.1174e+00,\n",
       "                       6.1746e-01,  8.9164e-01,  1.0327e+00,  5.1496e-01,  1.0043e+00,\n",
       "                       1.2554e+00,  7.9276e-01,  9.1841e-01,  3.6462e-01,  9.9238e-01,\n",
       "                       1.2506e+00,  3.2095e-01,  1.0511e+00,  1.1676e+00,  8.4129e-01,\n",
       "                       1.1247e+00,  1.0933e+00,  5.4635e-02,  3.7318e-01,  1.0642e+00,\n",
       "                       8.4745e-01,  1.1695e+00,  4.6824e-01,  3.8348e-01,  1.0083e+00,\n",
       "                       3.7977e-01,  1.0863e+00,  3.6573e-01, -6.0536e-02,  1.2379e+00,\n",
       "                       8.3982e-01,  5.1857e-01,  6.2164e-02,  6.3830e-01, -1.4919e-01,\n",
       "                       8.7499e-01, -2.9365e-01,  1.3441e+00,  9.4898e-01,  1.0260e+00,\n",
       "                       9.6405e-01,  8.4202e-01,  9.1305e-01,  9.0851e-01,  8.6719e-01,\n",
       "                       1.0552e+00,  1.1708e+00,  9.4192e-01,  3.9289e-01,  3.5624e-01,\n",
       "                       1.1369e+00,  1.1502e+00,  1.2377e+00,  9.8629e-01,  6.4711e-01,\n",
       "                       1.0667e+00,  1.0534e+00, -2.9126e-01,  1.0520e-01,  8.0521e-01,\n",
       "                       4.4392e-02,  9.9809e-01,  1.3048e+00,  6.7591e-01,  9.5075e-01,\n",
       "                       1.1994e+00,  1.1317e+00,  1.3188e+00,  4.1614e-01,  9.4342e-01,\n",
       "                       8.5958e-01,  5.7292e-01,  7.9530e-01,  1.2228e+00,  1.0497e+00,\n",
       "                       4.8546e-01,  1.1787e+00,  1.0332e+00,  9.5655e-01,  1.4200e-01,\n",
       "                       9.3194e-01,  9.7894e-01, -6.8250e-01,  9.9223e-01,  1.2412e+00,\n",
       "                       1.0080e+00,  7.7379e-01,  4.6873e-01,  1.0928e+00,  1.1121e+00,\n",
       "                       7.2555e-01,  1.2092e+00,  1.0171e+00,  2.1729e-01,  8.2208e-01,\n",
       "                       1.0052e-01,  8.3986e-02,  4.7014e-01,  1.0794e+00,  1.3151e+00,\n",
       "                       1.2852e+00,  8.9239e-01,  1.3592e+00,  1.1741e+00,  1.1476e+00,\n",
       "                       1.0603e+00,  1.0815e+00,  1.0178e+00,  1.0539e+00,  9.2930e-01,\n",
       "                       5.2298e-01,  8.9910e-01,  1.0917e+00,  1.0628e+00,  1.0643e+00,\n",
       "                       6.3273e-01,  1.0069e+00,  7.5558e-01,  2.3135e-01,  1.3848e+00,\n",
       "                      -2.3061e-01,  9.9791e-01,  5.3054e-01,  1.0818e+00, -2.6757e-01,\n",
       "                       1.1322e+00,  5.9644e-01,  1.2276e+00,  1.1317e+00, -5.4351e-02,\n",
       "                       1.9934e-01,  9.4287e-01,  1.2016e+00,  4.0208e-01,  9.0745e-01,\n",
       "                       1.1047e+00,  9.6594e-01,  7.2208e-01,  1.1172e+00,  1.4453e-01,\n",
       "                       7.0590e-01,  1.1061e+00,  1.2745e+00,  1.1170e+00,  1.0591e+00,\n",
       "                       1.1163e+00,  3.7535e-01,  9.3877e-01,  1.1868e+00,  1.0159e+00,\n",
       "                       1.0762e+00,  1.1420e+00,  3.7494e-01,  2.9402e-01, -3.1447e-01,\n",
       "                       4.0510e-01,  9.8436e-01,  4.0181e-01,  1.0289e+00,  6.3393e-01,\n",
       "                       9.8624e-01,  5.2247e-01,  9.7001e-01,  9.0207e-01,  5.0941e-01,\n",
       "                       1.0615e+00,  8.6222e-01,  9.5333e-01,  1.0064e+00,  1.1017e+00,\n",
       "                       9.1473e-01,  1.0484e+00,  1.0658e+00,  8.7419e-01,  4.1249e-01,\n",
       "                       8.1273e-01,  1.2479e+00, -4.0284e-02,  1.0904e+00,  1.2933e+00,\n",
       "                       3.8494e-01,  8.7831e-01,  8.3028e-01,  9.3684e-01, -4.7372e-01,\n",
       "                       1.3473e+00,  8.7537e-01, -8.0429e-02, -3.0775e-01, -6.3237e-01,\n",
       "                      -7.7741e-02,  1.0930e+00,  1.7244e-01,  9.3671e-01,  1.4185e+00,\n",
       "                       6.0859e-01,  7.7277e-01,  2.4811e-01,  9.4330e-01,  1.0716e+00,\n",
       "                       5.8716e-01,  9.8437e-01,  6.9509e-01,  1.0421e+00, -1.5916e-01,\n",
       "                       1.1351e+00,  6.9755e-01,  9.1705e-01,  3.0837e-01,  1.0018e+00,\n",
       "                       1.0067e+00,  5.4674e-01,  9.4997e-01, -6.4317e-02,  4.1631e-01,\n",
       "                       7.1539e-01, -4.6946e-01,  6.6335e-01,  9.5735e-01,  2.0870e-01,\n",
       "                       1.1991e+00,  1.1839e+00,  4.6067e-01,  1.0668e+00,  1.0489e+00,\n",
       "                       1.1608e+00,  7.7102e-01, -5.9149e-01, -6.0881e-01,  9.3491e-01,\n",
       "                       9.8887e-01,  1.1169e+00,  1.0193e+00,  6.6303e-01,  1.3003e+00,\n",
       "                       7.1045e-01,  1.6675e-01,  9.4171e-01,  1.0452e+00,  8.8876e-01,\n",
       "                       7.5306e-02,  8.9241e-01,  9.6810e-01, -8.6707e-03, -1.6933e-01,\n",
       "                       4.7647e-01,  7.6125e-01,  4.8593e-01,  9.1887e-01,  6.6243e-01,\n",
       "                       3.5397e-01,  4.5931e-02, -5.8507e-01,  5.1748e-01,  4.1691e-01,\n",
       "                       1.0376e+00, -1.3813e-02,  8.5009e-01,  9.8102e-01,  7.3964e-01,\n",
       "                       1.3960e+00,  8.9621e-01,  7.8882e-01,  1.0267e+00,  1.3121e+00,\n",
       "                       6.9216e-01,  3.4083e-01, -9.7023e-02,  9.3556e-01,  1.0525e+00,\n",
       "                       9.5670e-01,  8.6050e-01,  1.0304e+00,  3.3974e-01,  9.4233e-01,\n",
       "                       9.5716e-01,  1.0113e+00,  7.1896e-01,  8.9896e-01,  1.4198e+00,\n",
       "                       1.0691e+00,  2.7939e-01,  1.0348e+00,  8.9004e-01,  7.5055e-01,\n",
       "                       1.2793e+00,  1.0806e+00,  7.7595e-01,  1.0865e+00,  1.2997e+00,\n",
       "                       9.5121e-01,  1.1593e+00, -6.1823e-01,  1.1968e+00,  8.4695e-01,\n",
       "                      -3.7950e-01,  5.5230e-01,  1.0095e+00,  3.7561e-01,  1.1079e+00,\n",
       "                       9.2403e-01,  6.0310e-01,  1.3037e+00,  8.3584e-01, -3.4645e-01,\n",
       "                       8.4942e-01,  2.2799e-01,  6.1178e-04,  3.2800e-01,  7.3326e-01,\n",
       "                       5.1111e-01, -1.2632e-02,  1.2021e+00, -3.6883e-01,  4.1904e-01,\n",
       "                       5.6706e-01, -4.4398e-02,  8.9935e-01, -3.0494e-01,  1.4106e-01,\n",
       "                      -5.8089e-01,  7.7624e-01,  7.0701e-01,  4.3511e-01,  1.2601e+00,\n",
       "                       3.7901e-01,  1.0481e+00,  5.5305e-01,  1.0570e+00,  3.2272e-01,\n",
       "                       2.2185e-01,  1.0269e+00,  1.2944e+00,  8.8178e-01,  9.8001e-01,\n",
       "                       8.4816e-02,  1.6502e-01,  1.0769e+00,  8.1147e-01, -1.8732e-01,\n",
       "                      -4.4209e-02,  9.7745e-01,  9.7227e-01,  1.1450e+00,  1.0543e+00,\n",
       "                       1.0660e+00,  3.2182e-01, -6.1585e-01,  1.1802e+00,  7.8259e-01,\n",
       "                       9.1071e-01,  1.6202e-01,  1.0029e+00,  2.6713e-01,  9.9519e-01,\n",
       "                       1.0022e+00,  9.9100e-01,  1.1834e+00,  7.1041e-02,  1.0736e-01,\n",
       "                       7.7888e-01,  1.0658e+00,  5.6641e-03,  9.6347e-01,  2.5851e-01,\n",
       "                       4.4232e-01,  3.8982e-01,  1.1340e+00,  1.1028e+00,  9.0558e-02,\n",
       "                       3.3549e-01,  7.3823e-01,  2.8225e-02,  2.8411e-01,  9.3229e-01,\n",
       "                       1.1463e+00, -6.0665e-01,  6.3755e-02,  1.2315e+00,  1.0930e+00,\n",
       "                      -4.2944e-01,  1.2339e+00,  6.1216e-01,  1.0739e+00,  1.5283e+00,\n",
       "                       1.2990e+00,  7.3782e-01,  1.1997e+00, -2.1201e-01,  6.6501e-01,\n",
       "                       9.0196e-01,  1.0969e+00,  1.2275e+00,  5.5377e-01,  1.2223e+00,\n",
       "                      -4.4741e-01, -1.0666e-01,  9.0946e-01,  1.2369e+00,  3.2982e-01,\n",
       "                       1.1742e+00,  1.4597e-01, -1.3060e-01,  1.0941e+00,  4.8894e-01,\n",
       "                       8.5635e-01,  8.0261e-01,  9.6559e-01,  1.1119e+00,  1.1975e+00,\n",
       "                       1.1750e+00,  1.1493e+00,  1.0130e+00, -1.3949e-01,  4.1753e-01,\n",
       "                       1.3098e+00,  9.8831e-01], device='cuda:0')),\n",
       "             ('module.model.12.1.running_mean',\n",
       "              tensor([-2.9512e+00,  4.1112e-01, -1.6736e+00, -3.6190e-01,  7.6022e-01,\n",
       "                       6.0364e-01, -1.6556e+00, -3.4362e-01, -1.0468e+00,  1.6299e+00,\n",
       "                       1.0235e+00,  1.0602e+00,  1.0465e-01, -1.9913e+00, -1.7301e-01,\n",
       "                      -6.8253e-02,  3.7353e-01, -1.0468e+00, -8.8107e-01,  3.9309e-01,\n",
       "                       8.5767e-01,  2.0012e+00, -9.4196e-01, -1.2782e+00,  1.2325e+00,\n",
       "                      -1.5748e+00,  7.5087e-02, -1.3159e+00, -1.1952e+00,  2.7188e-01,\n",
       "                      -3.2900e-01, -5.5295e-02,  3.6011e-01, -3.6397e-01, -1.1878e+00,\n",
       "                      -7.6671e-01,  4.4116e-01, -1.1135e-02,  9.9699e-01,  1.7890e-01,\n",
       "                       8.6745e-01, -3.9475e-01,  6.0043e-01, -8.7655e-01,  1.6787e+00,\n",
       "                      -2.5864e-01,  1.7913e+00,  4.3164e-01,  8.3137e-01, -1.2074e+00,\n",
       "                      -8.5892e-01, -8.2720e-01,  1.4404e-01,  3.4699e-01, -1.0650e-01,\n",
       "                      -1.6472e+00, -6.4475e-01, -6.8491e-01, -4.4160e-01,  8.0286e-01,\n",
       "                       5.3076e-01,  1.3597e+00, -1.7689e-01, -7.7614e-01,  1.0313e+00,\n",
       "                      -1.2144e+00,  1.2404e+00,  2.0782e-01,  8.3062e-01,  1.1283e+00,\n",
       "                       7.9463e-01, -8.4842e-01,  8.1343e-01,  1.5507e-02, -3.2444e-01,\n",
       "                      -9.9543e-01, -9.1818e-01,  5.7258e-01, -3.3178e-01,  1.6027e+00,\n",
       "                       1.5181e-01, -3.5068e-01, -1.6722e+00,  9.5676e-01,  2.4244e+00,\n",
       "                      -1.4234e+00, -5.6345e-01, -1.2618e+00, -2.8942e-01,  2.1993e-01,\n",
       "                       1.9372e+00, -7.3377e-01, -1.0694e+00, -4.5245e-01,  1.3267e+00,\n",
       "                       6.5469e-01,  4.1087e-01,  4.6739e-01,  6.8754e-01, -1.8901e-02,\n",
       "                      -1.9165e+00, -1.6894e-01,  1.7964e-01,  7.4325e-04, -9.5652e-01,\n",
       "                       4.3401e-01, -1.6035e+00,  8.9614e-01,  5.5604e-01, -8.8354e-01,\n",
       "                      -6.9533e-01,  3.2503e-01,  5.0830e-01, -9.8985e-01,  5.7954e-01,\n",
       "                       1.4940e+00, -5.9917e-01, -3.4992e-01, -3.1333e-01,  8.9333e-01,\n",
       "                       6.0078e-01, -3.4022e-01,  1.1995e+00, -1.0465e+00,  2.5043e-01,\n",
       "                       8.8270e-01,  9.5688e-01, -2.8463e-01, -1.9989e+00,  3.9685e-01,\n",
       "                      -1.1469e+00, -1.3686e+00,  1.5664e+00, -3.6639e-01,  4.1024e-01,\n",
       "                       1.4197e+00, -9.9289e-02, -6.5749e-02, -4.2308e-01,  2.8865e-01,\n",
       "                      -1.9714e-01, -4.2664e-02,  1.0023e+00, -6.2411e-01,  4.3881e-01,\n",
       "                       1.6571e-03, -8.0635e-01, -1.2347e-01, -1.1361e+00, -1.2431e+00,\n",
       "                      -1.6182e+00, -8.7371e-01,  9.8377e-01, -1.4660e+00,  4.6907e-01,\n",
       "                      -6.2670e-01,  2.8219e-01, -2.6093e-01,  7.6395e-01,  8.5591e-01,\n",
       "                      -6.9096e-01, -1.9428e+00, -5.6827e-01, -1.7803e+00, -7.3725e-01,\n",
       "                      -9.3343e-01, -6.4837e-01, -1.0242e+00,  2.8147e+00,  1.6852e-01,\n",
       "                       8.2259e-01, -4.7281e-02, -6.1380e-01,  2.5520e+00,  1.0529e+00,\n",
       "                      -1.4760e-01,  1.4496e+00,  1.2687e+00, -5.1689e-01, -5.2642e-01,\n",
       "                       1.1380e+00, -1.3456e+00, -4.1292e-01, -1.5391e-02,  1.4601e+00,\n",
       "                      -1.5613e-03,  6.9453e-02, -4.5655e-01, -8.1262e-01,  9.9653e-01,\n",
       "                      -1.7822e-01,  5.9504e-01, -8.1755e-01, -1.6990e+00, -1.9487e+00,\n",
       "                       1.4200e-01,  5.5489e-01, -2.0767e-01, -1.1371e+00,  9.2980e-01,\n",
       "                      -1.6763e+00, -4.5674e-01,  7.0726e-01,  4.3677e-01, -1.9564e+00,\n",
       "                      -1.1783e+00,  1.2650e+00, -6.0495e-01,  6.0587e-01, -3.0295e-01,\n",
       "                      -1.1595e+00, -1.5205e+00,  7.0655e-01, -5.7424e-01,  1.4634e+00,\n",
       "                      -7.8372e-01, -9.1645e-01, -4.4823e-03, -9.8868e-01,  1.1391e-01,\n",
       "                      -6.1983e-01, -2.4274e+00,  9.8979e-01, -9.2172e-01, -1.6164e+00,\n",
       "                      -4.2683e-01,  4.3453e-01, -1.5108e+00,  6.8762e-01,  6.0292e-02,\n",
       "                      -9.2339e-01, -1.9571e-01, -8.7894e-01, -1.4272e-01,  5.3603e-01,\n",
       "                      -1.2643e-01,  1.9352e-01, -1.3015e+00, -2.2026e-01, -6.7092e-01,\n",
       "                      -9.2386e-01,  1.3037e+00,  1.6959e+00,  7.6374e-01, -9.3573e-01,\n",
       "                       4.9431e-01,  1.1127e+00,  1.7731e-02, -3.7139e-01, -2.8316e+00,\n",
       "                       7.8853e-01, -3.7254e-01, -9.3478e-01, -1.0473e+00,  1.1227e+00,\n",
       "                      -1.6231e-01,  2.8451e-01, -1.0741e+00,  2.9296e-01, -1.1656e+00,\n",
       "                       5.6562e-03, -7.6830e-01, -1.5259e+00,  1.1373e+00, -3.8371e-01,\n",
       "                      -1.3783e+00,  1.4922e+00, -4.3329e-01, -1.2128e+00, -1.5383e+00,\n",
       "                       7.5448e-01, -1.7346e-01, -4.4477e-01, -1.4229e+00,  1.6679e+00,\n",
       "                      -5.0985e-01, -6.9981e-01,  7.2376e-01, -6.2977e-01, -3.0853e-01,\n",
       "                      -1.8068e+00,  1.6339e-01,  6.0660e-01, -1.8118e+00, -1.3080e+00,\n",
       "                      -1.1203e+00, -4.1940e-01, -9.8941e-01,  2.6093e+00, -2.1724e+00,\n",
       "                       5.7078e-01,  4.3256e-01,  2.1294e-01,  2.9082e-02,  3.0064e-03,\n",
       "                      -5.7728e-03,  6.9831e-01,  4.0758e-01, -1.0413e+00,  3.0524e-02,\n",
       "                       7.7702e-02, -5.7871e-03, -6.3550e-01, -7.7595e-01,  1.7238e+00,\n",
       "                      -3.8020e-01, -2.8616e-02,  6.7013e-01, -4.3079e-01,  1.9140e-01,\n",
       "                      -1.2410e-01, -8.6881e-01, -1.0263e+00, -7.0136e-01, -3.6371e-01,\n",
       "                      -8.6738e-03,  6.0235e-01,  2.6337e-01, -1.2438e-01, -4.7211e-01,\n",
       "                      -5.0340e-02,  1.2098e+00, -5.1227e-01, -3.1029e+00,  9.5900e-01,\n",
       "                      -3.5488e-01, -2.8657e-01,  1.9623e-01,  8.1515e-01,  9.5437e-01,\n",
       "                      -7.3118e-01, -8.2586e-01,  1.0886e+00, -1.4725e+00,  1.1717e+00,\n",
       "                       4.5368e-01, -1.7145e+00,  1.5901e+00, -8.9866e-01,  9.5204e-01,\n",
       "                       6.9300e-02, -3.5096e-01, -1.0448e-01,  8.1392e-01,  1.7854e+00,\n",
       "                      -3.7322e-01,  3.1758e-01, -6.8298e-01, -1.7007e+00, -1.1439e+00,\n",
       "                      -3.1831e+00,  1.0993e+00, -7.7600e-01,  1.4843e+00,  4.6008e-01,\n",
       "                       1.1840e+00, -9.4661e-02,  2.4728e-01, -1.9344e+00, -1.5122e+00,\n",
       "                      -7.0280e-01,  1.5517e-01,  9.6593e-01,  3.5851e-01,  6.4614e-01,\n",
       "                      -1.8693e+00, -1.9255e+00, -1.2112e+00,  2.1983e-01,  5.3188e-01,\n",
       "                      -5.0445e-01, -6.5779e-01, -1.8654e+00, -7.8602e-01,  1.7662e+00,\n",
       "                      -7.8348e-01,  2.9027e-01, -1.5455e+00, -1.7739e+00, -9.9510e-01,\n",
       "                       2.0430e-01, -6.4155e-01,  1.5459e-01, -6.4553e-01, -7.5047e-01,\n",
       "                       3.0159e-01, -1.3819e+00,  4.9468e-01, -9.8977e-01,  1.6095e+00,\n",
       "                      -2.3395e+00, -4.4829e-01,  1.2703e+00, -1.2436e+00, -1.6790e+00,\n",
       "                      -1.5118e+00, -2.1638e+00, -1.9647e+00,  1.7073e-02,  2.6643e-01,\n",
       "                      -1.1400e+00,  1.4407e+00, -5.7946e-01,  1.3281e-01,  7.0523e-01,\n",
       "                      -4.1676e-01,  2.0300e+00,  1.0241e-01,  1.1012e+00, -1.0600e+00,\n",
       "                      -1.5292e+00,  1.0249e+00, -1.2840e+00,  5.6223e-01, -5.1781e-01,\n",
       "                      -1.4786e+00, -4.8278e-01,  4.4115e-01, -3.5011e-01, -1.9441e+00,\n",
       "                      -1.1824e+00,  5.3157e-01,  2.6833e-01, -2.0099e+00, -2.6873e-01,\n",
       "                      -3.3022e-01, -9.8327e-01, -7.1090e-01, -1.6511e+00, -1.0189e-02,\n",
       "                      -1.5685e+00,  6.2624e-01, -1.2841e+00,  6.8848e-01, -1.2950e+00,\n",
       "                      -7.7438e-01,  5.2237e-01, -6.7849e-01,  1.0032e-01,  1.0112e+00,\n",
       "                       6.9872e-01,  2.6297e-02,  1.3440e-01,  1.6676e+00, -7.1017e-01,\n",
       "                      -1.4875e+00, -1.9324e+00,  2.5833e+00, -4.5572e-01,  1.9738e-02,\n",
       "                       1.5105e+00,  1.2649e+00, -5.4019e-01, -3.9744e-01, -1.0256e-01,\n",
       "                       2.7808e-01, -5.6293e-01, -1.2424e+00, -1.1007e+00, -2.5310e-01,\n",
       "                       1.1695e+00, -7.0261e-01, -4.6928e-01, -1.9198e-01,  3.0837e-01,\n",
       "                      -1.0170e+00,  4.3216e-01,  8.1809e-02,  6.7060e-01, -6.0949e-01,\n",
       "                       8.7296e-01, -9.4998e-01, -1.4260e+00, -1.5804e+00, -2.1217e+00,\n",
       "                       3.2622e-01,  2.0531e-01, -2.0863e+00, -1.8268e-02,  1.4167e+00,\n",
       "                      -2.7777e-01, -5.6964e-01, -7.7463e-02,  4.7849e-01, -7.2791e-01,\n",
       "                      -1.2982e-01,  1.9723e+00,  4.1969e-01,  2.1508e+00, -8.6638e-02,\n",
       "                      -9.9323e-02, -2.9399e+00, -2.3720e+00,  7.7315e-01, -8.5068e-01,\n",
       "                       2.4257e+00,  7.7581e-02, -1.7170e+00,  8.7226e-01,  8.6406e-01,\n",
       "                      -7.1859e-01, -5.9087e-01, -5.4113e-01, -2.4744e+00,  4.3175e-01,\n",
       "                      -1.3129e+00,  2.0044e+00, -6.0328e-01, -2.4175e+00, -6.6836e-01,\n",
       "                       7.4576e-01, -7.7365e-01], device='cuda:0')),\n",
       "             ('module.model.12.1.running_var',\n",
       "              tensor([0.5559, 0.6028, 0.3972, 0.6152, 0.6925, 0.7726, 1.1990, 0.6647, 0.8868,\n",
       "                      0.7459, 0.5379, 1.6416, 1.7337, 0.8655, 0.5815, 0.7340, 0.5500, 0.9459,\n",
       "                      0.8605, 0.5257, 0.4522, 0.8109, 0.8595, 0.5619, 0.3196, 0.6849, 0.5711,\n",
       "                      0.8346, 0.2841, 0.6879, 0.3271, 0.5756, 0.4175, 0.2445, 0.4187, 1.0447,\n",
       "                      0.6834, 0.4522, 0.5588, 0.4779, 0.7106, 0.7709, 0.3698, 0.3517, 0.9170,\n",
       "                      0.4792, 0.8691, 0.5275, 1.5126, 0.6010, 0.8777, 0.5613, 0.9710, 0.5184,\n",
       "                      0.4219, 1.6299, 0.4946, 0.5141, 0.6650, 0.9562, 0.4621, 1.1159, 0.6901,\n",
       "                      0.5043, 0.6968, 0.7821, 0.9432, 0.3848, 0.5092, 0.9683, 0.6316, 0.7431,\n",
       "                      0.4103, 0.6747, 0.4856, 0.3277, 1.2342, 0.9579, 0.5944, 0.2871, 1.5093,\n",
       "                      0.9395, 0.4465, 0.4122, 1.5143, 0.4717, 0.7310, 1.0118, 0.5907, 0.9073,\n",
       "                      0.4467, 0.3911, 0.8853, 0.4136, 0.6139, 0.8067, 0.3828, 0.5829, 0.4782,\n",
       "                      0.5878, 1.4896, 0.5477, 0.6379, 0.4297, 1.1758, 1.0378, 0.8037, 0.4494,\n",
       "                      0.9213, 0.5842, 0.8016, 0.6026, 1.1155, 0.3532, 0.8506, 0.5321, 1.3849,\n",
       "                      0.4957, 0.5449, 0.8972, 0.6903, 0.4919, 0.5831, 0.2609, 2.5136, 0.6566,\n",
       "                      0.5147, 0.7211, 0.4854, 0.4038, 0.8208, 0.5130, 2.4835, 0.5472, 0.4882,\n",
       "                      0.6451, 0.6494, 0.5607, 1.0732, 0.5955, 0.5199, 0.6329, 0.4164, 0.6259,\n",
       "                      0.3613, 0.8560, 1.5788, 1.0839, 0.2680, 0.6984, 0.6441, 0.6673, 0.3618,\n",
       "                      0.4518, 1.0419, 0.4660, 0.6296, 1.2572, 0.4236, 0.5490, 0.8857, 0.4595,\n",
       "                      0.8385, 0.4263, 0.6578, 0.5049, 0.6258, 0.3829, 0.9676, 1.5949, 0.7685,\n",
       "                      0.7787, 0.6136, 0.7190, 0.3450, 0.5068, 0.4997, 0.5637, 1.0994, 0.7294,\n",
       "                      0.8729, 0.4342, 0.7006, 0.6593, 0.9859, 0.2911, 0.9375, 0.8949, 1.7402,\n",
       "                      0.3728, 0.3077, 0.5202, 0.4423, 1.5343, 1.5527, 1.6366, 0.6753, 0.9885,\n",
       "                      0.8516, 1.1354, 0.5656, 0.9042, 0.4126, 0.9549, 0.9267, 0.6438, 0.4999,\n",
       "                      0.8203, 0.6242, 1.0884, 1.0760, 0.8167, 0.3510, 0.4951, 1.8063, 0.6047,\n",
       "                      0.5361, 0.6747, 0.5416, 0.6328, 0.8100, 0.3663, 1.2478, 1.1195, 0.4704,\n",
       "                      0.3992, 0.5282, 1.9424, 0.8563, 0.7248, 0.7514, 0.5254, 0.4763, 0.4719,\n",
       "                      0.6036, 0.3086, 0.8230, 0.9498, 0.6051, 0.7267, 0.2862, 0.3792, 0.5098,\n",
       "                      0.5959, 0.3748, 0.5457, 1.6191, 0.5001, 0.5541, 0.5646, 0.3576, 0.7024,\n",
       "                      0.9147, 0.8948, 0.8226, 0.7340, 0.4375, 0.9276, 0.7064, 0.4590, 0.7823,\n",
       "                      0.4162, 0.6283, 0.4426, 0.9207, 1.1154, 0.3606, 1.3610, 1.4395, 0.9669,\n",
       "                      0.5556, 1.2322, 0.4856, 1.0349, 1.2425, 0.5511, 0.6402, 1.4478, 0.6208,\n",
       "                      0.3618, 1.2378, 0.9358, 0.6294, 0.8954, 0.7277, 1.1252, 0.7064, 0.4575,\n",
       "                      0.6986, 0.8628, 0.3586, 0.5869, 0.5799, 0.4772, 0.3269, 0.4937, 0.6873,\n",
       "                      0.5627, 0.8651, 0.5445, 0.6558, 0.5695, 0.6144, 0.7935, 0.6442, 1.6577,\n",
       "                      0.6232, 0.7891, 0.3509, 0.3332, 0.6357, 1.3398, 0.4968, 0.6251, 0.2551,\n",
       "                      1.1774, 1.2676, 0.6498, 0.3896, 0.8421, 0.4667, 0.7620, 0.3669, 0.5969,\n",
       "                      1.0505, 0.3825, 0.5471, 0.4680, 0.5061, 1.3569, 0.7408, 0.5524, 0.6636,\n",
       "                      0.5965, 0.3497, 0.3367, 0.9391, 0.4832, 0.4007, 0.2941, 0.3312, 0.9771,\n",
       "                      0.4247, 1.6372, 0.9233, 0.3522, 0.6219, 0.4371, 0.5796, 0.8090, 0.7430,\n",
       "                      0.4376, 0.2946, 0.7506, 0.4863, 1.1339, 0.5773, 0.3773, 0.8670, 1.1025,\n",
       "                      0.4884, 0.4193, 1.0000, 0.4230, 1.0170, 0.5152, 1.2199, 1.0611, 0.7136,\n",
       "                      0.5310, 0.4947, 0.7274, 0.5349, 0.7097, 1.5595, 0.6502, 0.3833, 0.3755,\n",
       "                      0.7312, 0.5619, 0.8541, 0.8986, 0.7349, 0.3769, 1.3157, 0.8386, 1.0670,\n",
       "                      0.3485, 1.3421, 0.5018, 0.5327, 0.4428, 0.5247, 0.4561, 0.4009, 0.7843,\n",
       "                      0.3425, 0.9935, 1.0847, 0.4540, 0.7017, 0.3857, 0.3526, 0.4281, 0.3787,\n",
       "                      0.5928, 0.6502, 1.2684, 0.4849, 0.6915, 0.5405, 0.4265, 0.5808, 0.4323,\n",
       "                      0.4349, 0.4220, 0.6399, 1.5050, 0.5616, 0.4077, 0.3125, 0.9471, 0.3249,\n",
       "                      0.5227, 0.7801, 0.3603, 0.7625, 1.4728, 0.5708, 0.9687, 0.5792, 0.5362,\n",
       "                      1.0103, 0.5938, 0.7676, 0.4862, 0.3656, 0.3224, 2.1649, 0.9967, 0.6051,\n",
       "                      0.5398, 0.8014, 0.6773, 0.6024, 0.4907, 0.2921, 0.4267, 0.2878, 0.8724,\n",
       "                      1.0444, 1.1598, 0.7270, 0.5334, 0.7946, 0.5935, 0.5787, 0.5197, 0.5240,\n",
       "                      0.5917, 0.6798, 0.4830, 0.9640, 0.5158, 0.8064, 0.5457, 0.6281, 0.2411,\n",
       "                      1.0263, 0.5422, 0.9421, 0.4373, 0.4930, 1.2203, 0.5871, 0.3998, 1.1691,\n",
       "                      0.9944, 0.6397, 2.3696, 1.4027, 0.3893, 0.2726, 0.6750, 0.8644, 1.0794,\n",
       "                      0.8959, 0.8471, 0.4266, 1.1973, 0.9068, 0.5718, 0.4983, 0.6099, 0.5758,\n",
       "                      0.7606, 0.9153, 0.5483, 0.7853, 0.3828, 0.9430, 0.6944, 0.5228, 0.8077,\n",
       "                      0.9861, 0.6258, 1.0593, 0.4973, 0.8591, 0.5404, 2.1527, 0.3625],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.12.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.12.2.clip_val', tensor([5.4906], device='cuda:0')),\n",
       "             ('module.model.13.0.weight',\n",
       "              tensor([[[[-0.3811, -0.3612, -0.1964],\n",
       "                        [-0.3108, -0.1642, -0.2148],\n",
       "                        [-0.3424, -0.3401, -0.2447]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.4464, -0.0743, -0.3365],\n",
       "                        [-0.1408,  0.0275, -0.1301],\n",
       "                        [-0.3529, -0.1454, -0.3441]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1162,  0.4200,  0.0039],\n",
       "                        [ 0.4942,  0.9939,  0.2735],\n",
       "                        [ 0.0521,  0.2315, -0.0200]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0247,  0.3759, -0.0673],\n",
       "                        [-0.1588,  1.7913, -0.2305],\n",
       "                        [ 0.1221,  0.0365,  0.1147]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0748, -0.2695,  0.0109],\n",
       "                        [-0.3983, -1.2663, -0.4228],\n",
       "                        [-0.0126, -0.2922, -0.0660]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3050, -0.9574, -0.4289],\n",
       "                        [ 0.3081, -0.1562, -0.9236],\n",
       "                        [ 0.6624,  1.0742,  0.2926]]]], device='cuda:0')),\n",
       "             ('module.model.13.1.weight',\n",
       "              tensor([1.1582, 0.8780, 0.7516, 0.7729, 1.4211, 1.5276, 0.7956, 1.0774, 0.9203,\n",
       "                      0.9958, 1.5486, 1.5995, 1.8220, 1.3380, 1.5194, 0.8177, 1.7303, 1.4936,\n",
       "                      1.4989, 1.3927, 0.8507, 0.7919, 1.0787, 1.0035, 0.5603, 1.1629, 1.2812,\n",
       "                      0.9471, 1.2729, 1.1674, 1.0883, 0.8779, 0.9692, 1.2619, 1.1399, 0.8951,\n",
       "                      1.1859, 1.3662, 0.9166, 1.3612, 0.8438, 1.4454, 0.9814, 0.9984, 1.2260,\n",
       "                      1.3930, 1.0586, 1.2125, 1.4388, 0.9543, 1.4163, 0.5964, 1.3867, 0.7967,\n",
       "                      1.0962, 0.7650, 0.9003, 0.8562, 1.3508, 0.8187, 0.8902, 0.9404, 1.4969,\n",
       "                      0.8939, 0.5640, 1.3613, 1.3192, 1.9424, 0.8971, 1.5283, 1.2562, 1.3008,\n",
       "                      1.0582, 1.5498, 1.8866, 1.1712, 0.8952, 1.6644, 1.4102, 0.8193, 1.3592,\n",
       "                      0.9448, 1.1693, 0.9881, 1.7483, 0.4174, 1.0071, 1.5311, 1.5717, 0.8732,\n",
       "                      0.7667, 1.1327, 1.4617, 1.3176, 1.3732, 1.4459, 0.9654, 1.0906, 0.9185,\n",
       "                      1.5590, 1.8113, 1.1445, 1.3917, 1.4100, 1.5431, 1.7659, 0.7471, 1.4399,\n",
       "                      1.1773, 1.4281, 1.2525, 1.2437, 0.5926, 1.2759, 0.8414, 1.0002, 1.5201,\n",
       "                      1.5635, 0.7033, 1.5326, 0.7839, 1.3866, 1.4711, 0.6358, 1.6417, 0.9846,\n",
       "                      0.7283, 0.7468, 1.0338, 1.1748, 1.3673, 1.1293, 1.9619, 1.4668, 1.1476,\n",
       "                      1.2851, 0.8118, 1.1074, 0.7802, 1.3031, 1.5158, 1.9184, 1.2530, 1.0858,\n",
       "                      0.7357, 0.9320, 1.6414, 1.6619, 0.9738, 1.2154, 0.8278, 1.6170, 0.7874,\n",
       "                      0.8477, 1.3965, 1.1023, 1.2992, 1.4950, 1.0360, 1.3781, 0.9255, 0.9721,\n",
       "                      1.5224, 0.8201, 1.0133, 1.0594, 1.2371, 1.2384, 1.4200, 1.5893, 1.1775,\n",
       "                      1.8100, 0.7583, 1.4369, 1.0502, 1.1804, 1.6358, 0.8476, 1.5252, 1.3407,\n",
       "                      1.0986, 1.4446, 1.6984, 1.3156, 1.4198, 0.9557, 1.8490, 1.2441, 1.3762,\n",
       "                      1.0800, 0.6672, 0.7412, 1.2626, 1.3117, 1.5756, 1.5421, 1.2526, 1.6812,\n",
       "                      1.4475, 1.3894, 0.9930, 0.9247, 0.8958, 1.3730, 0.9602, 1.0153, 1.0680,\n",
       "                      1.3691, 1.3070, 1.9205, 1.3047, 1.2252, 1.0818, 0.6976, 1.6992, 0.7284,\n",
       "                      0.9830, 1.8438, 1.1301, 0.6353, 1.5591, 1.1347, 1.8805, 1.4462, 0.8774,\n",
       "                      1.0546, 1.5431, 1.7220, 0.8896, 1.0601, 0.8516, 1.4488, 1.4493, 1.0887,\n",
       "                      0.7311, 1.1951, 1.0370, 1.6310, 0.9583, 0.9084, 1.0349, 0.7863, 0.8290,\n",
       "                      1.5139, 1.1657, 1.5597, 1.4094, 1.4329, 1.6840, 0.9891, 1.0622, 1.2284,\n",
       "                      0.7922, 1.4685, 1.2226, 0.9231, 0.9956, 1.4758, 0.9669, 1.0396, 1.3491,\n",
       "                      1.3834, 1.4562, 1.6646, 1.2188, 0.9274, 1.1003, 1.2444, 0.9910, 0.8556,\n",
       "                      1.0675, 2.2099, 1.4450, 1.0048, 1.7083, 1.0318, 1.5476, 1.5024, 1.2234,\n",
       "                      1.1186, 1.5133, 1.5721, 0.9981, 0.8401, 1.1787, 0.5657, 1.3791, 0.7783,\n",
       "                      1.7618, 1.5453, 1.1463, 1.0683, 0.7898, 0.8718, 1.1727, 1.4658, 1.4664,\n",
       "                      0.8413, 2.0032, 0.7401, 1.3589, 1.1108, 1.4402, 0.6737, 1.4007, 0.9591,\n",
       "                      0.8753, 1.4803, 0.5900, 0.9909, 1.7218, 0.8289, 1.2307, 0.7689, 1.5480,\n",
       "                      1.4800, 1.4262, 1.0872, 1.3619, 1.8357, 1.7811, 1.0159, 0.6493, 0.8112,\n",
       "                      1.2751, 1.2905, 0.9772, 0.9008, 1.3890, 1.5782, 1.2492, 1.0215, 1.2348,\n",
       "                      1.4196, 1.3554, 0.6702, 0.8771, 0.9887, 0.8714, 0.8035, 0.8119, 1.4774,\n",
       "                      0.8781, 1.6090, 0.8073, 1.1992, 0.7412, 0.7448, 1.1867, 0.8274, 0.8599,\n",
       "                      0.6155, 1.0895, 0.8362, 0.9901, 2.0414, 1.3095, 0.9265, 1.4585, 1.6406,\n",
       "                      1.3734, 1.2767, 0.8135, 1.2103, 1.8888, 1.1720, 1.4030, 1.3578, 1.0077,\n",
       "                      1.2169, 1.3459, 1.6363, 1.3081, 1.7955, 1.8609, 1.1298, 1.5729, 1.5164,\n",
       "                      1.5436, 0.7796, 1.1942, 1.7583, 0.8237, 0.9009, 2.3269, 1.5401, 1.6943,\n",
       "                      0.7450, 1.4094, 0.8875, 1.3959, 1.1282, 1.2474, 0.9400, 1.4602, 1.5269,\n",
       "                      1.5199, 1.3385, 1.4128, 0.7025, 1.3906, 0.8160, 0.8596, 0.8636, 0.9856,\n",
       "                      1.1610, 0.7960, 1.5499, 1.1486, 0.5404, 0.8592, 0.7413, 1.0437, 0.8011,\n",
       "                      2.0969, 0.9369, 1.3932, 1.4787, 0.9354, 1.7328, 0.9409, 1.6202, 1.4391,\n",
       "                      0.9790, 0.9330, 0.6644, 1.3302, 1.5233, 1.2602, 0.8767, 0.9069, 0.9391,\n",
       "                      1.4488, 1.2692, 0.6332, 0.6515, 1.3134, 1.3054, 0.8690, 1.2091, 1.4205,\n",
       "                      1.1582, 0.6784, 1.4206, 1.3151, 1.4531, 0.7492, 0.9186, 0.9235, 1.2474,\n",
       "                      1.5076, 1.3157, 1.6538, 0.8980, 0.5886, 0.7095, 0.8493, 0.7788, 1.0540,\n",
       "                      0.6356, 0.7343, 0.8690, 1.5428, 1.6273, 0.8360, 1.3960, 1.0526, 1.3219,\n",
       "                      1.1396, 1.3805, 1.6195, 0.6339, 0.7640, 1.3518, 1.2881, 0.7084, 1.5894,\n",
       "                      0.8785, 0.9297, 2.1200, 1.6341, 1.2116, 1.2715, 0.6912, 0.5772, 1.4476,\n",
       "                      1.3587, 1.5009, 0.7260, 1.4636, 0.5535, 0.7947, 1.2680, 1.5888, 0.7401,\n",
       "                      1.3953, 1.5266, 0.7333, 0.8567, 0.6607, 1.4315, 1.1817, 0.7670, 0.8941,\n",
       "                      1.6605, 0.9239, 1.8158, 1.3725, 0.5256, 1.0116, 1.8733, 1.1478],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.13.1.bias',\n",
       "              tensor([-1.4358e-01,  7.3020e-02, -2.7836e-02,  8.0882e-01, -3.0174e-01,\n",
       "                      -5.0473e-01,  2.0060e-01, -9.4922e-02,  5.4811e-01,  2.4383e-01,\n",
       "                      -5.4279e-01,  9.7506e-01, -1.7107e-01, -5.0751e-01, -7.2560e-01,\n",
       "                       3.7901e-01, -1.6502e+00, -3.1203e-02, -4.8735e-01, -6.6211e-01,\n",
       "                       4.1318e-01,  1.7051e+00,  1.5097e-01,  4.4250e-01,  8.3686e-01,\n",
       "                       4.9708e-01,  6.0273e-02,  2.3124e+00, -1.8426e-01,  2.2472e-01,\n",
       "                      -4.4283e-02,  1.6474e+00,  1.5805e-01, -4.2458e-01,  1.0645e-01,\n",
       "                       4.6845e-01,  1.3998e-01, -2.2741e-01, -4.0324e-01,  9.1020e-02,\n",
       "                       4.5694e-01, -4.7517e-01,  3.2105e-02,  2.2641e-01, -3.8886e-02,\n",
       "                      -4.0546e-01,  6.3143e-01, -1.5409e-01,  2.2752e-01,  7.0002e-01,\n",
       "                      -2.1882e-01,  9.5223e-01,  1.2882e-01, -2.8938e-01, -6.0516e-02,\n",
       "                       1.5394e+00, -4.7782e-01, -3.2742e-02, -2.2301e-01,  1.5378e+00,\n",
       "                      -7.4626e-01, -1.5981e-01, -5.4954e-01,  1.1243e-01,  3.2155e-01,\n",
       "                      -1.4316e-01, -4.9295e-02, -6.9629e-01,  9.9595e-02, -1.9791e-01,\n",
       "                       1.3414e-01,  1.6307e-01,  1.2516e-01, -4.0108e-01, -1.0810e+00,\n",
       "                      -3.5538e-01,  1.8353e+00, -4.6247e-01,  1.5810e-02, -4.3182e-01,\n",
       "                       2.9874e-01,  1.1174e-01, -4.6172e-01, -6.5989e-01, -1.6759e-01,\n",
       "                       1.4412e-01, -1.1653e-01, -5.8896e-01, -7.2652e-01,  2.4737e+00,\n",
       "                       2.1355e+00, -1.5052e-01, -4.3576e-01, -4.7661e-01, -2.7629e-01,\n",
       "                       1.0180e-01,  1.8787e-01,  9.2640e-02, -5.7465e-02, -4.6203e-01,\n",
       "                       8.2385e-04,  1.2515e-01, -4.5942e-01, -3.1055e-01, -7.5461e-02,\n",
       "                      -8.0125e-01,  1.4268e+00, -4.4374e-01,  1.8742e-01, -6.0720e-01,\n",
       "                      -1.6825e-02,  7.9277e-01,  1.2246e+00, -9.4912e-01,  1.4902e+00,\n",
       "                       1.2412e-01, -1.1484e-01, -7.6534e-01,  3.7212e-01, -1.1531e-01,\n",
       "                       1.3304e-01, -2.5739e-01,  2.1817e-02,  1.5843e-01,  1.0065e+00,\n",
       "                       7.0508e-01,  3.4903e-01,  3.3275e-02, -1.2294e-01,  3.1529e-01,\n",
       "                      -2.7524e-01, -2.6214e-01, -7.5545e-01, -5.3518e-01, -1.4800e-02,\n",
       "                      -1.9054e-01,  1.5465e+00, -1.3665e-01,  1.6242e+00,  4.7346e-02,\n",
       "                      -2.0274e-01, -1.0430e+00, -2.2222e-01,  5.8346e-02,  7.2258e-02,\n",
       "                       8.3088e-01,  1.6210e-03,  7.1720e-01,  9.7464e-02, -2.2999e-03,\n",
       "                       1.6127e+00, -7.9245e-01, -5.1948e-01, -1.4633e-01, -7.0945e-02,\n",
       "                      -8.8273e-01, -9.9568e-02, -5.0068e-01,  6.1989e-02, -2.6926e-01,\n",
       "                       1.8728e+00,  4.6422e-01, -5.9723e-02, -1.7510e-02,  1.6774e-01,\n",
       "                       3.6404e-01,  6.1219e-02, -2.6364e-01, -6.3784e-02, -1.7929e-02,\n",
       "                       1.1273e-01, -4.7406e-01,  1.1921e+00, -4.3176e-01, -6.3761e-01,\n",
       "                       1.9471e-01, -7.5321e-01, -8.4009e-01, -3.1374e-01,  1.1071e-01,\n",
       "                       4.6782e-02, -9.3351e-01, -1.0212e+00,  1.6891e-01, -7.0889e-01,\n",
       "                       8.5824e-02, -2.8445e-01, -4.4071e-02, -1.0860e+00,  6.4474e-02,\n",
       "                       1.0908e-01,  2.8459e-01, -2.1169e-01,  1.2978e-01,  3.5556e-01,\n",
       "                       1.6412e-01, -1.8731e-01, -4.2523e-02, -2.3877e-02, -3.4267e-01,\n",
       "                       8.8855e-01,  4.3138e-01,  1.0510e+00, -3.8098e-02,  4.1002e-01,\n",
       "                      -2.0301e-01,  1.4184e-01,  1.6365e-02, -1.0955e-02, -5.7316e-01,\n",
       "                      -5.6561e-01, -7.0477e-02, -1.2082e-01,  9.3804e-02, -3.4205e-03,\n",
       "                       9.1558e-01,  3.6608e-01, -5.7882e-01,  1.2215e-01,  4.3857e-01,\n",
       "                      -8.6707e-02, -4.0404e-01, -4.6605e-01,  1.5534e-01,  4.1289e-01,\n",
       "                      -6.9232e-02, -7.4649e-01,  7.3612e-01,  9.0384e-02,  3.3796e-02,\n",
       "                       8.9086e-01, -3.6035e-01, -1.2034e+00,  7.2796e-01,  7.1819e-02,\n",
       "                      -3.5238e-01,  3.4338e-01, -1.9620e-01,  6.4935e-01,  1.3653e+00,\n",
       "                       2.3596e-01,  3.2233e-01,  1.5326e+00, -6.3018e-01, -1.4952e-01,\n",
       "                      -2.6070e-01,  1.8545e-01, -4.3268e-01, -2.1748e-01, -4.7953e-01,\n",
       "                      -3.0871e-02, -2.4454e-01,  3.3193e-01, -3.7919e-01, -1.9923e-01,\n",
       "                       9.3255e-01, -9.9502e-02, -4.0028e-02,  2.2414e-01, -8.4949e-02,\n",
       "                      -2.8769e-01, -9.9900e-01, -4.3735e-01, -1.0593e+00,  2.6821e-01,\n",
       "                       6.0864e-01,  3.1349e-01,  8.9338e-02,  3.0376e-01,  2.6399e-01,\n",
       "                      -1.6260e-01, -2.4913e-01, -1.0848e-02,  5.1807e-01, -4.3103e-02,\n",
       "                       1.1903e-01, -4.7953e-01, -6.7930e-01,  6.0210e-02,  1.9386e-01,\n",
       "                       4.6627e-01, -3.7089e-01,  7.0061e-02, -1.5804e-01, -1.1143e+00,\n",
       "                       1.7462e+00,  4.9314e-01,  8.3265e-01, -8.5231e-01, -2.0213e-01,\n",
       "                      -2.6023e-01,  8.1487e-01,  1.0402e+00,  7.2059e-01, -3.0621e-01,\n",
       "                      -5.3027e-01, -3.8218e-01,  3.4690e-01, -8.9021e-01, -4.6788e-01,\n",
       "                       1.4586e-01, -1.0892e-01, -4.0303e-01,  3.4288e-01, -1.6422e-01,\n",
       "                       2.0217e+00,  1.1274e+00, -5.4782e-01,  7.7155e-01,  1.1916e-02,\n",
       "                      -5.9592e-01, -9.2549e-02, -2.2163e-01,  1.1279e+00, -4.9008e-01,\n",
       "                       4.4751e-01,  7.4705e-02,  1.0287e-01, -4.3637e-01, -5.4552e-01,\n",
       "                      -7.2152e-01,  4.9601e-01,  1.6009e+00, -5.0905e-01, -1.1242e-02,\n",
       "                      -3.7431e-02, -8.8923e-02,  5.7192e-01, -4.0922e-01, -7.5985e-02,\n",
       "                      -1.9158e-01, -7.4205e-01, -1.9732e-04, -3.3661e-01, -4.0316e-01,\n",
       "                       2.3555e-01,  1.5850e+00, -1.8069e-01, -2.3320e-01, -5.2183e-01,\n",
       "                       1.1081e+00, -6.6698e-01,  2.4767e-01, -1.6713e-01,  1.5048e+00,\n",
       "                      -2.8938e-01,  3.1028e-01, -2.9331e-01, -1.0491e-01,  1.9629e-01,\n",
       "                       8.2372e-01,  2.4615e-01, -1.6848e-01,  2.0044e+00,  1.0540e-01,\n",
       "                      -4.1695e-01, -2.9109e-01,  1.0411e+00,  5.8603e-02, -1.7599e-01,\n",
       "                      -1.7604e-01, -1.4158e-01,  4.1579e-02,  1.0929e-01, -8.6037e-01,\n",
       "                       1.8812e-01, -5.7252e-01,  4.1588e-01,  1.4893e-01, -1.2908e-01,\n",
       "                      -2.4704e-01, -5.6154e-01, -8.3397e-01, -7.9248e-01, -1.1548e+00,\n",
       "                      -1.0961e-02, -5.5922e-01, -9.7708e-01, -5.1905e-01,  7.2002e-01,\n",
       "                       1.9487e-01, -6.3729e-01,  1.5648e+00, -6.2180e-02, -1.0210e+00,\n",
       "                      -3.3898e-01, -2.8664e-01,  6.1495e-02, -3.6350e-02,  2.5641e-01,\n",
       "                      -4.8205e-02, -1.7508e-01, -6.1967e-02,  1.0544e-01, -9.4009e-01,\n",
       "                      -5.1116e-01, -5.3894e-01,  1.1105e-02, -2.3432e-01,  4.7167e-01,\n",
       "                      -2.3893e-01, -8.0597e-02, -3.9120e-01,  1.7771e-02,  5.2652e-03,\n",
       "                       5.0193e-01,  2.9421e-02,  8.1695e-01,  2.2144e-01,  1.2196e+00,\n",
       "                       4.7791e-01,  2.6707e-01,  5.2760e-01,  4.0463e-01, -7.6465e-01,\n",
       "                      -6.5463e-01, -3.3142e-01,  7.9207e-02,  1.2760e-01, -1.3265e+00,\n",
       "                      -1.5696e-01, -4.0634e-01, -4.9535e-01,  6.4975e-01, -6.4899e-02,\n",
       "                       7.1837e-01,  2.1272e-01,  4.6795e-01, -4.8337e-01,  2.0127e+00,\n",
       "                      -4.9643e-01,  8.0920e-01, -2.2126e-01, -1.6908e-01,  4.0053e-02,\n",
       "                       3.0163e-01, -5.9979e-01, -1.7828e-01,  1.0326e+00,  3.4223e-01,\n",
       "                      -5.0675e-01,  6.9547e-01, -1.2325e-01, -1.5338e-01, -1.8191e-01,\n",
       "                      -2.7745e-01,  9.2046e-03,  1.4749e-01, -4.7464e-01,  1.9366e-01,\n",
       "                      -3.1762e-01, -4.3678e-02, -3.5501e-01,  6.6583e-01,  3.2354e-01,\n",
       "                       3.8887e-01,  2.8258e-01,  2.1190e-01,  3.2429e-01,  6.0226e-01,\n",
       "                       3.2243e-01,  6.1279e-01,  2.6310e-01, -4.3798e-01, -6.0608e-02,\n",
       "                      -2.4599e-01,  8.1598e-02, -1.3779e-01,  2.3847e-01, -7.3346e-01,\n",
       "                      -3.2867e-01,  1.2539e+00,  1.8079e-01,  4.4753e-01, -1.0592e-01,\n",
       "                       3.3281e-01,  2.6412e-03,  3.7921e-01,  5.1266e-01,  3.5867e-01,\n",
       "                      -9.9802e-02, -2.7497e-01, -2.1021e-01,  1.1170e+00,  2.1684e+00,\n",
       "                      -3.2049e-01,  3.4413e-02,  1.6299e-01,  2.6911e-01, -7.7974e-02,\n",
       "                       4.6116e-01, -1.2557e-01, -2.8946e-01, -4.4436e-01,  5.1659e-01,\n",
       "                      -5.4504e-02,  2.2172e-01, -1.6611e-01,  1.8247e+00,  5.0312e-01,\n",
       "                      -3.8904e-01,  2.1844e-01,  4.7448e-01,  5.9363e-01, -2.1210e-01,\n",
       "                       8.0724e-01, -6.3982e-01, -4.6645e-01,  1.8920e+00, -1.2032e-01,\n",
       "                      -6.9904e-01, -1.2401e-01], device='cuda:0')),\n",
       "             ('module.model.13.1.running_mean',\n",
       "              tensor([-1.2142, -0.6088,  0.9455, -1.5083, -0.3974, -0.4403,  0.8558,  1.7999,\n",
       "                      -1.7423,  0.2300, -0.6356, -0.0592, -0.5100, -2.1705,  1.9253, -1.1864,\n",
       "                       0.1946, -0.2345, -0.3079, -1.9341, -1.8403,  0.3405, -1.0305, -0.7977,\n",
       "                      -0.7061, -0.1168,  0.1186,  1.1774,  0.8458, -1.3081, -0.0129,  0.1643,\n",
       "                      -0.7051,  0.7358,  0.6125,  1.3630, -1.9502, -0.9934,  0.2419, -1.1686,\n",
       "                      -1.3565, -0.3531, -1.9315, -0.0798, -1.9643,  1.1361, -0.0367, -0.2097,\n",
       "                      -0.2267,  0.9436, -0.2076, -1.1247,  0.0164,  0.6942,  0.1884,  0.7886,\n",
       "                       1.3022,  0.5151, -0.2159,  1.7474,  0.3304,  0.4900, -1.6389, -0.1931,\n",
       "                       0.3025,  0.0239, -0.0960, -0.0885,  0.0776,  0.0094, -0.3404, -0.3077,\n",
       "                       0.0103, -0.2339, -1.0310,  0.2144,  0.7396, -0.3229, -0.1698,  0.0758,\n",
       "                       0.1233,  1.0016,  2.0766,  1.4046, -0.6630,  0.0858,  0.9057, -0.2165,\n",
       "                      -0.5420, -0.3123,  0.0540, -0.0168, -0.4102, -1.4863, -0.3530, -1.8004,\n",
       "                       0.3035,  0.0769,  0.3027,  0.0811, -0.3673, -0.2101, -0.3432, -1.4653,\n",
       "                      -0.1430, -0.7171,  0.7303, -0.2857,  0.1107,  0.0849,  0.0687, -0.2858,\n",
       "                       0.6117,  0.8152,  1.1873, -1.6321, -0.0598, -1.2741, -1.2799, -0.1086,\n",
       "                       0.9783, -0.1835, -1.5375,  0.5379, -0.3667, -0.2189,  1.5738,  2.1690,\n",
       "                      -0.0441, -0.5123, -0.2289, -0.2160, -3.1769,  1.5178,  0.9365, -0.1695,\n",
       "                       1.1031,  0.3519,  0.9265, -0.4983, -0.1652, -0.4580, -0.2205,  0.0413,\n",
       "                       0.2045,  0.3553,  0.1018, -0.0659,  0.0190, -2.0282,  1.3974, -0.9993,\n",
       "                       0.5093,  0.6658, -0.1069,  0.6279, -0.1801, -2.3663, -1.4238, -0.2549,\n",
       "                       0.8710,  0.5093, -0.3920,  1.0141,  0.1472,  0.1967,  0.1239, -0.3745,\n",
       "                      -0.0956, -0.2442, -0.0354, -0.5061, -1.1622, -0.0338,  1.1655,  0.0166,\n",
       "                      -0.4966,  0.1938, -0.2397, -0.2604,  2.5918,  1.5962, -0.4091, -0.1271,\n",
       "                      -2.0295,  0.1316, -0.4894, -0.0932,  1.4641,  0.1980,  0.6374,  1.4534,\n",
       "                      -1.5724, -1.9284, -0.0501,  0.0274,  1.8198, -0.2992, -0.0932, -1.8909,\n",
       "                       0.7571, -1.4953,  0.5246, -0.0440, -1.4227,  1.5859,  0.0213, -0.2848,\n",
       "                       0.0997, -0.3971,  2.7152,  0.3182,  0.5951,  1.6239, -0.4538, -0.4724,\n",
       "                       1.4955, -1.4376,  0.3790,  0.7683, -0.0759,  1.1703, -0.6429, -0.1047,\n",
       "                      -0.9700, -0.9232, -2.0895, -0.0298,  1.5472,  0.2492,  0.1383, -0.3213,\n",
       "                       0.8825, -0.0821,  0.6441, -0.0992, -1.8802, -0.2778,  0.1865,  1.3958,\n",
       "                       0.4227, -1.2722,  0.9461, -1.9198,  2.2482, -0.2930, -0.2104, -1.1474,\n",
       "                      -1.0946,  0.6822, -0.2420,  0.3772,  1.0681, -0.0734, -0.0454,  1.7531,\n",
       "                      -1.3059, -0.0240,  0.0708, -0.1594, -0.3979,  1.8886, -0.4212, -0.0705,\n",
       "                       0.1139,  1.6753,  1.0378, -0.3458,  1.5773, -0.9359,  0.4064, -0.5221,\n",
       "                      -0.9297,  0.7049, -0.6709, -0.1084, -0.7019,  3.0189, -0.3191, -0.4627,\n",
       "                      -0.3167, -0.4139,  0.0733,  0.4012,  0.3115, -0.9013, -0.1147,  0.2762,\n",
       "                       0.1420, -0.2268, -0.2679,  0.0366, -1.0264,  1.6370, -2.2980, -0.2391,\n",
       "                      -0.3300,  0.6945, -0.4978,  0.6859,  0.0910,  0.4809, -0.6347,  0.9685,\n",
       "                      -0.0169,  0.2335,  0.1232, -1.6377, -0.5000, -1.6779, -0.7308,  0.3422,\n",
       "                      -1.1914,  0.0960, -1.0104, -0.1035, -0.0242,  0.2412, -0.2180, -0.4917,\n",
       "                      -0.1072, -1.4568,  0.0218,  0.3347,  0.0432,  1.2962, -2.1629,  0.8970,\n",
       "                      -1.8245, -0.3122, -0.1202,  1.4129, -0.3610, -0.2849, -0.2047,  0.7956,\n",
       "                       0.7368, -1.6888,  1.0383,  0.3930, -0.0547,  2.6321,  0.0763, -2.3165,\n",
       "                       0.8828, -0.3943,  0.5506,  0.1969, -0.3031,  1.1538, -1.9345,  0.6533,\n",
       "                      -0.0886,  0.5459,  1.4908, -0.5477, -0.9775,  0.0655, -0.1091, -0.5029,\n",
       "                      -0.7050, -1.1396,  0.5189,  0.0367, -0.2939,  1.0080,  3.2179, -0.1048,\n",
       "                       0.2656, -0.0899, -0.2694, -0.4386,  0.5267, -0.9464, -3.4386, -1.5541,\n",
       "                      -1.1487,  1.2415, -0.3061,  1.4001,  0.8014, -0.2055,  0.4472, -2.5342,\n",
       "                      -0.4039, -0.3619, -0.4890,  0.3051, -0.0496, -1.9968, -0.5231,  0.0511,\n",
       "                      -0.9551, -1.2777,  2.1352, -1.7032, -1.4291, -0.0472, -1.8248, -0.4121,\n",
       "                      -0.2456,  0.8373,  0.7053,  0.9891,  0.2524, -1.5934,  0.8505, -0.2628,\n",
       "                      -0.5723, -0.9773,  1.5833,  0.5424,  0.0884,  0.0490, -0.7302,  0.3293,\n",
       "                      -0.5208,  0.0794,  0.4777, -3.2917, -0.1633, -0.4243, -1.2505,  0.2104,\n",
       "                       1.4861, -1.1393, -0.2326, -0.2704,  0.4094,  0.2005,  1.2020, -0.0315,\n",
       "                      -0.0094, -0.0597,  0.7954,  0.9315,  0.5123, -0.0911,  0.5512,  0.1922,\n",
       "                      -1.4661,  0.1144,  0.2005, -0.0732,  0.1146, -0.2944,  0.5099, -2.2304,\n",
       "                       0.7473, -0.1819, -0.3132,  0.1543, -0.2530,  0.0989,  0.9563, -1.0947,\n",
       "                      -1.3843,  0.8935,  1.5145,  1.2380,  1.0388,  0.0461, -0.2324, -0.2237,\n",
       "                       0.5260, -1.3313, -0.0941, -0.9225,  0.1392,  0.5747, -0.3322,  0.0314,\n",
       "                       1.0022, -0.2851, -0.0786,  0.5079, -0.4583,  1.2440, -1.9731, -0.3393,\n",
       "                      -0.3433, -0.2784,  0.9975,  0.3851, -1.0905, -2.6944, -0.1835, -0.5064,\n",
       "                       1.2372, -0.1130,  0.5845,  0.7870,  1.2797, -0.2877,  0.9932, -0.0692,\n",
       "                      -1.2270,  0.8310,  1.0405,  1.3019, -0.0304,  0.1854, -2.1415, -1.7693,\n",
       "                      -0.3002,  0.5399, -0.2273, -2.0326, -0.8045,  1.1668, -3.2957, -0.2158],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.13.1.running_var',\n",
       "              tensor([1.3216, 0.7498, 1.1093, 2.2351, 1.0873, 2.2260, 1.1585, 3.2499, 0.4065,\n",
       "                      1.7877, 2.4224, 5.5282, 1.8489, 0.4079, 2.4042, 0.5752, 0.8229, 3.2729,\n",
       "                      2.1193, 0.2276, 1.0049, 1.2124, 1.3437, 0.9723, 0.5787, 1.6077, 2.2685,\n",
       "                      1.4235, 1.6458, 1.9245, 2.0064, 1.2215, 0.5893, 0.8885, 1.4789, 1.5794,\n",
       "                      0.4093, 0.8892, 0.3630, 1.7438, 1.8788, 1.1883, 0.2130, 1.0671, 2.0874,\n",
       "                      2.4256, 2.7552, 1.3298, 2.7328, 0.8000, 0.6927, 1.4947, 2.5870, 0.5969,\n",
       "                      0.9550, 1.7070, 3.0914, 0.9272, 2.8833, 2.1909, 0.3917, 0.7353, 1.0725,\n",
       "                      0.8070, 0.5217, 1.3921, 3.0336, 0.8248, 2.3709, 0.6078, 1.6243, 1.2819,\n",
       "                      1.9859, 0.8720, 4.3270, 0.5098, 2.3197, 0.6921, 0.7653, 0.1109, 3.5383,\n",
       "                      2.1666, 2.9606, 3.7277, 6.3684, 0.0383, 1.3031, 1.2618, 0.8385, 1.0518,\n",
       "                      2.2116, 2.2224, 1.3043, 1.6933, 2.4216, 2.9389, 2.7729, 0.7630, 1.4290,\n",
       "                      1.4219, 1.9142, 3.3288, 1.7439, 1.4964, 1.2992, 4.2219, 2.3511, 1.0835,\n",
       "                      1.3577, 1.5160, 2.2423, 1.1109, 1.1813, 2.0996, 1.0186, 0.7836, 2.3945,\n",
       "                      1.1358, 1.3202, 1.8157, 1.3215, 1.8831, 3.1642, 0.6226, 3.6081, 0.7695,\n",
       "                      1.8773, 9.0447, 2.7542, 0.3815, 1.3040, 1.0258, 1.3614, 1.3977, 1.4737,\n",
       "                      2.0658, 2.6622, 0.8503, 1.3026, 1.3412, 1.1263, 1.0721, 1.4967, 2.2517,\n",
       "                      2.4030, 1.4043, 2.4494, 1.9630, 2.1228, 2.8891, 1.4184, 1.0981, 0.6079,\n",
       "                      0.8904, 3.1361, 0.9302, 3.5708, 0.3389, 1.0636, 2.1668, 1.1048, 1.3498,\n",
       "                      1.9934, 1.6851, 2.3044, 1.8779, 3.8013, 1.0778, 3.7242, 1.7092, 1.6944,\n",
       "                      1.4860, 0.5650, 1.2158, 1.6922, 1.4225, 1.5520, 0.1651, 1.8819, 4.0531,\n",
       "                      1.9610, 2.4711, 3.0704, 2.0870, 0.3801, 2.1084, 1.4725, 1.7961, 2.1863,\n",
       "                      1.2850, 1.1976, 6.7863, 1.4164, 0.8719, 2.6311, 4.2507, 1.3655, 3.2303,\n",
       "                      2.9657, 0.3787, 1.3620, 0.3331, 0.8780, 1.6106, 0.8420, 1.6881, 1.6326,\n",
       "                      5.0093, 1.0463, 1.0116, 5.1756, 1.7821, 0.8895, 3.0161, 5.8908, 0.6505,\n",
       "                      1.9993, 1.4422, 0.9485, 1.2140, 1.4541, 2.3958, 2.4633, 2.9234, 1.0092,\n",
       "                      0.7167, 1.3513, 2.5568, 2.3656, 1.7634, 0.6612, 1.8012, 2.4546, 1.7141,\n",
       "                      0.8566, 4.5735, 0.3927, 2.1920, 1.5829, 1.3167, 0.4293, 1.1087, 1.2899,\n",
       "                      0.9435, 1.7084, 0.4949, 3.3619, 1.0969, 1.2343, 1.0016, 2.4559, 0.7086,\n",
       "                      1.1653, 0.2086, 1.4614, 2.2198, 1.0607, 1.0331, 2.5221, 2.3558, 2.4819,\n",
       "                      4.7108, 1.9995, 0.9406, 4.1358, 1.4363, 0.5846, 6.9957, 1.9399, 0.8979,\n",
       "                      1.6922, 0.9822, 1.3269, 1.9101, 3.4342, 2.3967, 1.2267, 2.9143, 0.8312,\n",
       "                      0.3320, 3.5656, 3.0317, 1.3156, 0.4468, 0.4011, 1.1973, 1.7451, 2.1788,\n",
       "                      1.3843, 4.2112, 2.0487, 0.6805, 1.6740, 1.1559, 0.3887, 3.1215, 2.2050,\n",
       "                      2.8007, 1.1674, 0.6857, 2.8147, 2.5783, 4.1215, 1.1273, 1.6743, 1.4279,\n",
       "                      6.5133, 0.5646, 0.3763, 2.2773, 2.6889, 0.3154, 0.8438, 1.0976, 0.9596,\n",
       "                      2.9322, 3.4399, 3.5481, 1.4833, 1.6035, 1.6619, 1.6312, 1.4027, 0.3263,\n",
       "                      2.0862, 0.8241, 0.4076, 0.7899, 1.5407, 4.2182, 0.8389, 2.4439, 1.3662,\n",
       "                      1.9867, 0.6858, 1.2453, 2.1296, 0.5809, 1.6942, 0.5797, 1.3890, 3.0006,\n",
       "                      2.5344, 2.9290, 2.7253, 1.7173, 0.7742, 0.1291, 1.8771, 1.8120, 0.4282,\n",
       "                      0.9606, 1.8374, 4.6910, 1.3418, 2.1648, 1.3414, 0.8098, 0.6794, 4.2264,\n",
       "                      1.1812, 0.9070, 0.7141, 0.8962, 2.1276, 1.1605, 3.5318, 3.1445, 3.0804,\n",
       "                      1.2461, 2.8999, 2.9219, 1.7862, 5.6420, 0.4187, 0.3940, 0.9726, 1.2630,\n",
       "                      1.7258, 1.5025, 0.6207, 1.2415, 1.4370, 0.4548, 1.7894, 4.4901, 3.2151,\n",
       "                      0.3806, 1.9087, 1.1576, 0.3947, 1.4420, 0.6498, 1.3235, 1.5265, 0.8911,\n",
       "                      1.0790, 5.4885, 1.5217, 0.3418, 1.7258, 1.2011, 1.1097, 1.3808, 1.6647,\n",
       "                      2.5449, 0.9785, 1.7868, 0.5477, 1.5537, 3.1232, 2.3778, 0.7756, 0.9548,\n",
       "                      3.0142, 0.3838, 0.8554, 3.5626, 6.9531, 0.5686, 2.7037, 1.2655, 0.9511,\n",
       "                      0.9474, 3.2391, 1.1285, 0.7614, 4.8820, 1.4328, 2.0802, 1.9793, 1.9730,\n",
       "                      1.6283, 2.2830, 0.8874, 1.5766, 0.9580, 0.8205, 2.3546, 1.8589, 0.5698,\n",
       "                      1.7169, 0.2512, 2.7761, 3.7354, 0.8142, 0.9695, 1.0181, 1.4393, 1.0265,\n",
       "                      2.0217, 2.9883, 2.4142, 2.6608, 1.6083, 0.8455, 0.1072, 1.6213, 1.2969,\n",
       "                      1.5454, 1.5255, 1.0208, 1.1822, 1.1148, 0.9663, 1.6057, 1.8969, 0.9317,\n",
       "                      2.7469, 1.5628, 3.9184, 1.7079, 3.4197, 5.8442, 2.9650, 0.8977, 2.0936,\n",
       "                      1.6223, 0.6206, 4.3108, 2.8767, 0.9804, 2.4397, 0.6873, 0.8830, 2.7597,\n",
       "                      2.3653, 2.5520, 2.1103, 3.3047, 0.4244, 1.2511, 1.0631, 1.8583, 1.4957,\n",
       "                      1.9787, 2.4377, 1.2862, 1.3137, 2.3086, 1.7247, 3.9310, 0.9054, 0.3112,\n",
       "                      1.6767, 1.1579, 1.8877, 0.8311, 0.7722, 2.0198, 1.9670, 1.7994],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.13.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.13.2.clip_val', tensor([5.9824], device='cuda:0')),\n",
       "             ('module.model.14.0.weight', tensor([[[[-0.0143]],\n",
       "              \n",
       "                       [[-0.0274]],\n",
       "              \n",
       "                       [[ 0.0492]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0347]],\n",
       "              \n",
       "                       [[ 0.0039]],\n",
       "              \n",
       "                       [[ 0.0337]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0275]],\n",
       "              \n",
       "                       [[-0.0582]],\n",
       "              \n",
       "                       [[ 0.0623]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0207]],\n",
       "              \n",
       "                       [[ 0.1213]],\n",
       "              \n",
       "                       [[ 0.0220]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0319]],\n",
       "              \n",
       "                       [[-0.0792]],\n",
       "              \n",
       "                       [[-0.0643]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0259]],\n",
       "              \n",
       "                       [[ 0.1147]],\n",
       "              \n",
       "                       [[ 0.0117]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0242]],\n",
       "              \n",
       "                       [[ 0.0311]],\n",
       "              \n",
       "                       [[ 0.0616]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0116]],\n",
       "              \n",
       "                       [[ 0.0518]],\n",
       "              \n",
       "                       [[ 0.0182]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0475]],\n",
       "              \n",
       "                       [[-0.0025]],\n",
       "              \n",
       "                       [[-0.0031]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0136]],\n",
       "              \n",
       "                       [[-0.0309]],\n",
       "              \n",
       "                       [[ 0.0814]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1304]],\n",
       "              \n",
       "                       [[-0.0103]],\n",
       "              \n",
       "                       [[ 0.0416]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0221]],\n",
       "              \n",
       "                       [[-0.0482]],\n",
       "              \n",
       "                       [[-0.0363]]]], device='cuda:0')),\n",
       "             ('module.model.14.1.weight',\n",
       "              tensor([0.9327, 0.8386, 0.7096, 1.0971, 0.8600, 0.9646, 1.0344, 1.0304, 1.0734,\n",
       "                      1.0648, 0.8150, 0.7177, 1.0259, 0.8866, 0.9178, 0.9097, 1.1000, 0.8899,\n",
       "                      1.0610, 0.8534, 0.8045, 1.0316, 1.0866, 0.9998, 0.5559, 1.0304, 0.3068,\n",
       "                      0.8786, 0.8124, 1.0771, 0.7612, 1.1540, 0.7887, 0.8328, 0.6347, 1.0487,\n",
       "                      0.9838, 0.5695, 0.4791, 1.0361, 1.0327, 1.0322, 0.9917, 0.9672, 0.9572,\n",
       "                      1.0889, 1.1009, 0.9132, 0.9223, 0.9718, 1.0083, 0.4627, 1.0690, 1.0490,\n",
       "                      0.9938, 1.0589, 0.9797, 1.0557, 0.4505, 0.7535, 0.7026, 0.9659, 0.5909,\n",
       "                      0.7988, 0.4121, 0.7848, 0.9444, 1.1332, 1.1589, 1.0915, 0.9245, 0.7598,\n",
       "                      0.5577, 0.9802, 1.0142, 1.0719, 1.0686, 1.0547, 1.0847, 0.6111, 1.0669,\n",
       "                      1.0775, 1.0203, 1.1027, 1.1320, 1.0088, 1.0026, 1.0055, 1.0185, 1.0478,\n",
       "                      0.8053, 0.7247, 1.0491, 1.0496, 1.0000, 0.9554, 1.0200, 0.5859, 1.0231,\n",
       "                      1.0025, 1.0508, 1.0645, 0.8073, 0.8920, 0.5383, 0.8691, 0.8383, 0.9504,\n",
       "                      1.1184, 0.9983, 1.0831, 1.0214, 0.9683, 0.9903, 0.9939, 1.0567, 0.8610,\n",
       "                      0.5303, 0.9598, 1.0062, 0.6994, 1.0584, 0.7082, 0.7216, 0.9822, 0.8820,\n",
       "                      1.0234, 0.9055, 0.6117, 0.8537, 0.6677, 0.9624, 1.0076, 0.7975, 0.8709,\n",
       "                      0.7754, 1.0221, 1.4738, 0.9786, 1.2133, 1.0446, 0.8783, 0.7588, 0.7605,\n",
       "                      0.9692, 1.0610, 0.8522, 1.1105, 0.5223, 1.0561, 1.0669, 0.6686, 1.0179,\n",
       "                      0.8947, 1.0317, 1.0646, 0.9522, 1.0474, 0.9137, 1.0616, 0.7582, 0.3938,\n",
       "                      1.2072, 0.7970, 0.9547, 0.6885, 0.8492, 0.8197, 0.9873, 0.5026, 0.9916,\n",
       "                      1.0086, 1.0476, 1.5239, 0.7217, 0.9375, 0.5231, 1.0990, 0.5186, 0.9656,\n",
       "                      0.9900, 0.9880, 1.0172, 1.0324, 1.0646, 0.5878, 0.9102, 1.0050, 0.9622,\n",
       "                      1.0344, 1.1175, 1.1300, 1.0231, 0.3965, 0.8173, 1.0795, 1.2047, 0.4170,\n",
       "                      0.9500, 0.8495, 0.9351, 1.0418, 0.9557, 0.8476, 1.0167, 1.0215, 1.0484,\n",
       "                      0.7406, 1.0113, 0.7846, 1.0488, 0.3882, 1.0303, 0.9724, 0.9824, 1.0766,\n",
       "                      1.1015, 0.7518, 1.0610, 1.1347, 1.0213, 0.7090, 0.4638, 0.5353, 0.5458,\n",
       "                      1.1178, 0.4887, 0.4100, 0.7766, 0.9872, 1.0757, 1.1871, 0.9788, 0.8995,\n",
       "                      1.0385, 1.0295, 0.9522, 0.9194, 1.0338, 0.6598, 1.0175, 1.0276, 0.8179,\n",
       "                      0.2938, 0.9187, 1.1534, 1.0652, 0.3031, 0.8741, 0.5517, 1.0163, 0.8722,\n",
       "                      0.3782, 0.8283, 1.0300, 0.7644, 0.5610, 0.6052, 0.9791, 0.8716, 1.1158,\n",
       "                      0.7576, 0.8342, 1.0539, 1.0950, 0.7393, 1.0468, 1.0054, 0.9876, 1.1066,\n",
       "                      0.9983, 0.5742, 0.7375, 0.7690, 1.0212, 0.3437, 0.3534, 1.0352, 1.0205,\n",
       "                      1.0559, 0.9550, 1.1422, 1.0445, 0.8324, 1.0547, 1.7643, 1.0931, 0.8874,\n",
       "                      0.5852, 0.9762, 0.7304, 1.0729, 0.9910, 1.0590, 1.1643, 1.0323, 0.9178,\n",
       "                      0.9605, 1.0802, 1.0700, 1.0455, 0.6590, 0.6079, 0.9651, 0.6344, 1.0503,\n",
       "                      1.0532, 0.8671, 0.9750, 1.0433, 1.0513, 1.0480, 1.1166, 0.8334, 0.7367,\n",
       "                      1.1518, 0.8390, 1.0612, 0.7664, 0.6472, 0.9858, 1.0447, 0.4413, 0.6182,\n",
       "                      0.4813, 0.8016, 0.8552, 1.0519, 0.7621, 0.9311, 1.4997, 1.0247, 0.5685,\n",
       "                      1.0765, 1.0775, 0.9658, 0.7929, 0.9341, 0.6993, 1.0341, 0.9383, 0.3776,\n",
       "                      1.1287, 1.0330, 0.9372, 0.9907, 1.0318, 0.9973, 1.2086, 1.0203, 1.0615,\n",
       "                      1.0234, 1.0880, 0.5209, 1.1060, 1.0436, 0.9792, 0.9360, 0.6043, 1.0671,\n",
       "                      0.5234, 0.4429, 0.4709, 1.0710, 1.0341, 1.0911, 0.6218, 0.9816, 0.7515,\n",
       "                      0.4901, 1.0414, 0.9725, 0.9016, 0.8270, 0.5451, 1.0049, 1.0173, 0.8053,\n",
       "                      0.7676, 0.6269, 1.0060, 0.8363, 0.8189, 1.0902, 0.9209, 0.9326, 1.0279,\n",
       "                      0.7224, 1.0365, 0.8240, 0.9161, 0.9845, 1.1023, 1.0440, 0.7718, 1.0499,\n",
       "                      0.9765, 1.0076, 0.5934, 1.0598, 0.5854, 0.8406, 0.6670, 0.4236, 1.0399,\n",
       "                      1.0249, 0.4598, 1.0503, 1.0068, 1.1874, 0.8588, 0.9821, 0.5738, 0.6287,\n",
       "                      0.8175, 0.7610, 0.9495, 0.8582, 0.6071, 0.4590, 0.7743, 0.5734, 1.1255,\n",
       "                      0.9268, 0.3994, 0.9591, 1.1791, 0.5472, 1.0397, 1.1735, 1.0953, 1.1215,\n",
       "                      0.9759, 1.0320, 0.9773, 0.9838, 0.5005, 0.3796, 1.0517, 0.4883, 1.1541,\n",
       "                      0.4352, 1.0527, 0.8734, 1.0197, 1.1156, 1.0110, 0.9423, 1.0148, 1.0250,\n",
       "                      0.6099, 0.8709, 1.0205, 0.5817, 1.1773, 0.7895, 1.0020, 0.8048, 0.3338,\n",
       "                      0.9348, 1.0401, 0.7025, 1.3254, 1.2276, 1.0426, 1.0632, 0.9292, 0.7277,\n",
       "                      1.0092, 0.9020, 0.7212, 0.4946, 0.4685, 0.5581, 0.5230, 0.9043, 1.2211,\n",
       "                      0.2723, 1.0144, 0.9964, 0.8626, 1.1414, 0.8610, 0.7996, 1.0226, 0.6449,\n",
       "                      0.7500, 0.7417, 1.0739, 1.0555, 0.9935, 0.8250, 0.4426, 0.8383, 0.8942,\n",
       "                      1.0427, 1.2601, 0.6488, 0.7796, 1.0414, 0.8827, 0.4666, 1.1494, 0.8950,\n",
       "                      1.0686, 0.9778, 0.5611, 1.0793, 0.9941, 1.0444, 0.9872, 1.1528],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.14.1.bias',\n",
       "              tensor([ 0.4987, -0.6140,  0.7780, -0.3040,  0.7545,  0.5519, -0.1481,  0.0370,\n",
       "                       0.2589, -0.0663, -0.8534,  0.9622,  0.3038,  0.7793,  0.4962, -0.3999,\n",
       "                      -0.0741,  0.7887,  0.2077,  0.6460,  0.8768,  0.1912, -0.5387, -0.4976,\n",
       "                       1.1545,  0.4047,  1.0334,  1.0388,  0.7719, -0.3571,  0.7229, -0.5271,\n",
       "                       0.7127,  0.8170,  1.0127, -0.5562,  0.3092,  1.0486,  1.0074,  0.1255,\n",
       "                       0.1662,  0.3833, -0.2918,  0.5118,  0.4236,  0.2239, -0.2424, -0.5247,\n",
       "                       0.5416, -0.4320, -0.3962,  0.9720, -0.2835,  0.2009,  0.3412,  0.1772,\n",
       "                      -0.4856, -0.3954,  0.9792,  0.7566,  0.7578,  0.4156,  0.9201,  1.0988,\n",
       "                       1.0522,  0.7530, -0.5100,  0.2012, -0.5251, -0.1091,  0.5373,  0.7107,\n",
       "                       0.8793,  0.5961, -0.1490,  0.3542,  0.0565, -0.0641,  0.0471,  0.9986,\n",
       "                      -0.1336, -0.4504, -0.2307,  0.1063, -0.0872,  0.2629, -0.5100, -0.3525,\n",
       "                       0.2657, -0.1879,  0.6440,  0.8721,  0.0729,  0.0185, -0.3168,  0.3980,\n",
       "                       1.1870,  0.8248,  0.1313, -0.3555,  0.1294, -0.0131,  0.6785,  0.5681,\n",
       "                       1.2162,  1.2501,  0.6951,  0.4381, -0.1871, -0.5617, -0.2572,  0.2819,\n",
       "                      -0.4764, -0.2753,  0.2238,  0.6108,  0.6016,  1.1508,  0.5259,  0.4350,\n",
       "                       0.8104, -0.2951,  0.8084,  0.9089, -0.3180,  0.5235, -0.1884,  0.5314,\n",
       "                       1.0591, -0.2052,  0.9092,  0.5140,  0.3683,  0.8123, -0.6531,  0.7895,\n",
       "                      -0.2309,  0.5283, -0.4096, -0.0137,  0.0918,  0.5859,  0.7044,  0.7013,\n",
       "                      -0.3183,  0.1142, -0.7638,  0.2065,  1.0256, -0.0593,  0.2133,  0.8495,\n",
       "                      -0.2473,  0.5249, -0.5613, -0.0260,  0.1774, -0.0271,  0.5473, -0.4611,\n",
       "                       0.7490,  1.0761, -0.0031,  0.7663,  0.3737,  0.9538,  0.7416, -0.8489,\n",
       "                       0.3193,  0.9513,  0.2431, -0.2240,  0.1767,  1.7009,  0.7829, -0.5029,\n",
       "                       0.9776, -0.1138,  1.0125,  0.3356, -0.3278,  0.3935, -0.2514,  0.3398,\n",
       "                       0.4984,  0.9962, -0.5495,  0.2573,  0.5000,  0.2791,  0.3109,  0.1538,\n",
       "                      -0.2274,  1.0151,  0.6808,  0.1174,  0.2834,  1.1081, -0.4896,  0.9039,\n",
       "                       0.4504,  0.1565, -0.4290,  0.8926, -0.2661, -0.2493,  0.0609, -0.7940,\n",
       "                       0.1453,  0.7736,  0.4119,  1.0066,  0.8742, -0.4601,  0.3105, -0.1110,\n",
       "                       0.0407,  0.8281, -0.1956, -0.2921, -0.0994,  0.7937,  0.9706,  1.1725,\n",
       "                       1.2728, -0.1626,  0.9554,  1.0915,  0.7395, -0.4533,  0.0603, -0.3564,\n",
       "                       0.0105,  0.5238,  0.0455, -0.0331,  0.6112,  0.4897,  0.1827,  0.6643,\n",
       "                       0.3272,  0.0928,  0.4532,  1.3276,  0.5496,  0.6495, -0.0332,  1.0910,\n",
       "                       0.7037,  1.3088,  0.0802,  0.7233,  1.0889, -0.6805,  0.3241,  0.7659,\n",
       "                       1.2074,  0.9922, -0.2857,  0.5813, -0.1499,  0.7629,  0.6080,  0.1584,\n",
       "                       0.0243,  0.8071, -0.0141,  0.3251, -0.3690,  1.8613,  0.4236,  0.8908,\n",
       "                       0.7528,  0.9407,  0.1697,  1.1557,  1.1488,  0.0946,  0.1521,  0.2845,\n",
       "                      -0.3449, -0.1939, -0.0727,  0.7086, -0.3309,  0.0420,  0.0228,  0.6156,\n",
       "                       0.9207,  0.3641,  0.8483,  0.1206,  0.4691,  0.0552, -0.4229, -0.0323,\n",
       "                       0.5997,  0.5201,  0.3631, -0.0146, -0.1183,  0.8439,  0.8989, -0.4323,\n",
       "                       0.9722, -0.1690,  0.2283,  0.6780, -0.4328,  0.1962, -0.1801, -0.4139,\n",
       "                      -0.1139,  0.6444,  0.7995,  0.1951,  0.7532, -0.2136,  0.4337,  1.0421,\n",
       "                      -0.4119,  0.0936,  0.9886,  0.9617,  1.0227, -0.7790,  0.5716, -0.4463,\n",
       "                       0.7422,  0.1019,  0.9435,  0.9492,  0.9051,  0.3973,  0.0135,  0.4731,\n",
       "                       0.6733, -0.4446,  0.7828, -0.1327,  0.5338,  1.1376, -0.4269,  0.6799,\n",
       "                       0.9320,  0.2563,  0.0412,  0.3040,  0.5509, -0.1197, -0.1118,  0.1948,\n",
       "                      -0.3762,  0.9398, -0.0650,  0.4820,  0.4240,  0.5906,  0.8949, -0.3038,\n",
       "                       1.0009,  1.0421,  1.1160, -0.0086,  0.1943,  0.3431,  1.2314,  0.3642,\n",
       "                       0.6317,  1.0101,  0.0061,  0.6466, -0.4646,  0.6751,  0.9735, -0.0794,\n",
       "                      -0.0597,  0.7140,  1.0999,  1.2190,  0.3220, -0.6990,  0.6387, -0.1783,\n",
       "                       0.4299,  0.4730,  0.1985,  0.7929,  0.2475,  0.7869, -0.0863, -0.2204,\n",
       "                      -0.1540, -0.0599,  0.8633,  0.0588, -0.6164, -0.4665,  0.9108,  0.1148,\n",
       "                       0.9471,  0.8691,  1.1294,  0.9988,  0.0457,  0.1768,  1.0668,  0.0628,\n",
       "                       0.1351,  0.5865,  0.6045,  0.4858,  1.1593,  1.0673,  0.8023,  0.0111,\n",
       "                      -0.2500,  0.6749,  0.8988,  1.1299,  0.7960,  1.3128, -0.1445,  0.5875,\n",
       "                       1.0119,  0.4241, -0.0372,  1.1717,  0.3399, -0.3401, -0.5511,  0.0915,\n",
       "                      -0.2963, -0.1239,  0.6919,  0.1260,  1.0011,  1.0412, -0.1585,  1.0070,\n",
       "                      -0.2981,  1.0303,  0.2073,  0.6347,  0.4260, -0.5420,  0.2196, -0.4405,\n",
       "                       0.4159, -0.1407,  0.9010,  0.6877, -0.4212,  0.8932,  0.7635, -0.3647,\n",
       "                       0.2725,  0.5109,  1.0580,  0.4333,  0.0283,  0.8099, -0.4202, -0.2143,\n",
       "                       0.3318, -0.0890,  0.6247,  1.0576,  0.3869,  0.5933,  0.7832,  1.0337,\n",
       "                       1.0567,  1.1706,  0.9366, -0.6211, -0.6760,  1.0905,  0.4131, -0.4540,\n",
       "                       0.8530,  0.0606,  0.5941,  0.7295,  0.1745,  1.0213,  0.7388,  0.8568,\n",
       "                       0.0422, -0.0374, -0.0874,  0.7500,  0.8480,  0.6204,  0.5765, -0.0074,\n",
       "                      -0.2024,  0.8884,  0.4562, -0.0352,  0.6038,  1.1027, -0.2816,  0.9251,\n",
       "                       0.1088,  0.6935,  0.9856, -0.0673, -0.2795,  0.0948,  0.3139, -0.1487],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.14.1.running_mean',\n",
       "              tensor([ 8.4103e-01, -1.4786e+00, -1.0419e+00, -8.4637e-01, -2.0922e-01,\n",
       "                      -6.9158e-01, -1.4732e+00, -7.0341e-01,  3.7654e-01,  7.5749e-01,\n",
       "                      -1.3189e+00,  7.7319e-01, -7.6179e-02,  2.2633e+00,  7.7760e-02,\n",
       "                       2.9085e-02, -7.5102e-01, -8.2784e-01, -7.7201e-01, -1.1107e+00,\n",
       "                       4.9076e-01, -6.0997e-01, -9.7592e-01,  2.0693e-01,  1.2800e+00,\n",
       "                       5.8454e-01, -5.7084e-02, -5.1512e-01,  3.0469e-01,  2.7359e-01,\n",
       "                       9.9237e-01,  2.5314e-01, -6.4291e-01, -1.9162e-01, -9.8421e-01,\n",
       "                      -1.0149e+00, -3.8021e-01,  5.7260e-01,  1.2459e+00,  7.9983e-01,\n",
       "                      -9.8102e-01, -4.6611e-01,  6.7861e-01,  2.6221e-01, -3.4664e-01,\n",
       "                      -4.8615e-01,  2.7177e-01,  1.4813e+00, -2.3564e-01, -1.1886e+00,\n",
       "                       1.0629e-01, -3.9616e-01,  1.2495e+00,  1.7424e+00, -1.3051e+00,\n",
       "                      -8.0408e-01, -6.9334e-01,  6.9191e-01, -2.2176e+00,  4.2986e-01,\n",
       "                       3.4546e-01,  1.4144e+00,  9.8293e-01, -6.6521e-03, -4.0030e-01,\n",
       "                       6.7848e-01, -2.8285e-01,  8.2437e-01, -8.7420e-01, -2.3694e-01,\n",
       "                       4.2334e-01, -3.2875e-01,  6.8363e-01,  5.0614e-01, -1.5119e+00,\n",
       "                      -1.1535e+00, -3.6743e-01, -7.7903e-02, -4.4072e-01,  9.9717e-03,\n",
       "                      -6.5084e-01, -3.0309e+00, -4.9287e-01,  2.2509e-01, -8.2050e-01,\n",
       "                       1.0492e+00, -1.1457e+00,  1.9838e-01,  1.9437e+00, -2.8469e-01,\n",
       "                      -3.7554e-01, -7.2219e-01, -1.1629e+00,  6.1741e-01, -7.7331e-01,\n",
       "                      -4.9975e-01,  7.3797e-01, -2.0170e-01,  5.5833e-03, -8.4057e-01,\n",
       "                      -9.2057e-01, -3.8823e-01,  4.6341e-01, -9.2227e-02,  2.1362e+00,\n",
       "                      -3.2222e-01,  1.0568e+00, -5.4170e-01, -5.7465e-02, -9.5781e-01,\n",
       "                       2.3289e-01,  7.5515e-01, -6.5880e-01, -8.6662e-01,  1.2244e+00,\n",
       "                       2.6257e-01, -7.6373e-01, -7.2683e-01,  1.7529e+00, -4.5000e-02,\n",
       "                      -5.2061e-01, -1.7227e+00, -1.8997e+00,  3.3043e-01, -6.4494e-01,\n",
       "                       6.7887e-01,  4.0301e-02,  1.9577e-01, -1.9548e-01,  1.0972e-01,\n",
       "                      -5.2060e-01, -8.7994e-01,  6.4093e-01, -3.5025e+00, -1.8781e+00,\n",
       "                       2.4893e-01, -3.3349e-01, -2.3124e-01, -3.1114e-01, -1.2519e+00,\n",
       "                      -3.9327e-01, -4.0420e-01, -4.7491e-02,  2.3302e-01, -2.5495e-01,\n",
       "                      -8.8227e-01, -1.0689e+00, -4.0763e-01,  2.0737e+00,  1.6908e+00,\n",
       "                       1.5084e-01,  1.2041e+00,  7.1672e-01,  2.5095e-01, -5.2984e-01,\n",
       "                       1.3237e+00,  1.8263e-01,  5.7603e-01,  7.9757e-01, -2.1256e-01,\n",
       "                      -1.1280e+00, -5.3834e-01,  1.2265e+00,  2.8266e-04,  1.1708e+00,\n",
       "                       1.7727e+00,  3.6752e-02, -1.7733e+00,  3.3268e-01, -1.4242e+00,\n",
       "                      -2.1534e-02, -1.3987e-01,  1.6117e+00, -7.1874e-01,  5.0981e-01,\n",
       "                      -3.1620e-01, -9.5416e-01,  1.0045e+00, -2.4688e-02,  6.6476e-01,\n",
       "                      -1.5154e+00,  1.1607e+00,  6.2345e-02,  1.3163e-01,  6.7981e-02,\n",
       "                       1.2461e-01, -4.7698e-01, -1.8069e-01, -1.4061e-01, -7.0607e-01,\n",
       "                      -2.6815e-02,  6.7302e-01, -8.1754e-01,  1.4774e+00,  8.0922e-01,\n",
       "                       5.7458e-01,  1.8385e-01,  2.5744e-01, -1.2860e+00,  7.5089e-02,\n",
       "                       8.0605e-01, -2.5913e-01,  1.1798e+00,  9.1103e-01,  1.3559e-01,\n",
       "                      -2.4348e-01,  3.4897e-01, -1.8822e+00,  1.4277e-01,  6.7576e-01,\n",
       "                      -4.8288e-01, -2.5423e-01,  6.3777e-01, -9.8672e-01,  2.2689e-01,\n",
       "                       2.0302e-01,  9.3092e-01,  1.2807e+00, -1.9052e-02, -3.6405e-01,\n",
       "                      -1.6608e-01, -4.9590e-01,  8.6312e-02, -1.5075e+00,  8.2509e-01,\n",
       "                       1.1072e+00, -4.7816e-01, -1.6554e+00,  1.8536e+00, -1.9422e-01,\n",
       "                      -4.4340e-02,  9.0615e-01, -5.2586e-01, -1.0150e+00, -3.3419e-01,\n",
       "                       4.6990e-01,  5.2127e-01, -4.7927e-01,  8.0931e-01, -3.7925e-01,\n",
       "                      -9.5781e-01,  2.1581e-02, -5.8642e-01, -1.5960e+00,  2.8156e-02,\n",
       "                      -8.1411e-01, -5.4207e-01,  6.5872e-01, -7.0308e-01,  9.4796e-02,\n",
       "                      -4.0768e-01,  4.7325e-01, -2.5742e-01, -2.6805e-01, -3.1142e-01,\n",
       "                       3.2554e-01,  9.4747e-01,  1.5071e-01,  2.7704e-01, -1.4654e+00,\n",
       "                       1.1333e-01,  2.9684e-01,  6.8240e-01, -2.1583e-01, -1.2681e+00,\n",
       "                      -3.9087e-01, -2.6826e-01,  1.0836e+00,  8.9889e-01,  1.4475e-01,\n",
       "                       1.1896e-01, -5.7037e-01,  4.6877e-01,  9.2111e-02, -6.5839e-01,\n",
       "                       5.9525e-01,  1.2403e+00, -6.0192e-02,  6.9470e-03, -6.0914e-01,\n",
       "                      -1.3814e+00,  3.0942e-01, -7.9273e-01,  6.8595e-01,  4.8258e-01,\n",
       "                      -9.9553e-01, -2.4194e-01, -4.4265e-01,  4.3709e-01, -3.6703e-01,\n",
       "                      -8.7920e-01,  4.6578e-01, -2.2730e-01,  1.9403e-01, -2.6371e+00,\n",
       "                       7.5045e-02, -2.6985e+00, -1.5222e+00, -2.7173e-01, -6.0862e-01,\n",
       "                       7.4406e-01,  7.8295e-01, -2.7431e-01,  4.8661e-02, -1.9725e-01,\n",
       "                       1.5164e+00, -1.5625e-01, -7.1539e-01, -1.3154e-02, -3.7468e-01,\n",
       "                      -7.6493e-01, -6.3392e-01, -9.2399e-01, -6.0086e-01,  6.5984e-01,\n",
       "                      -1.8138e+00, -8.5654e-03, -9.0728e-01, -8.0792e-01,  1.7768e+00,\n",
       "                      -2.8089e-01, -3.2502e-01, -1.7031e-02,  5.9010e-01,  9.2245e-01,\n",
       "                      -7.9221e-01, -9.0633e-01, -1.6387e+00, -1.5918e+00, -1.1119e+00,\n",
       "                       3.6584e-01, -4.1268e-01, -2.5354e-02,  3.7858e+00, -9.0960e-01,\n",
       "                       3.8164e-02, -6.1841e-02,  1.2160e-01,  6.5256e-01, -1.2773e-01,\n",
       "                      -1.2937e+00, -3.1887e-01, -3.6225e-01, -3.1798e-01, -8.2968e-01,\n",
       "                       5.0601e-01,  1.7526e-01, -1.1932e-01,  6.1760e-01, -7.1727e-02,\n",
       "                      -4.9709e-01,  4.2614e-01, -3.8811e-01, -1.3335e+00,  5.8594e-01,\n",
       "                       1.6886e-02,  5.0132e-01,  2.7486e-01, -6.5214e-01, -1.1251e+00,\n",
       "                       6.7699e-01, -3.0946e-01,  5.6525e-01, -4.1995e-01, -2.6672e-01,\n",
       "                      -4.0643e-01, -4.4451e-01, -5.7491e-02,  3.7138e-01, -5.7658e-02,\n",
       "                      -1.2792e+00, -4.0578e-01,  6.5810e-01,  6.5849e-01, -2.3179e-02,\n",
       "                       5.0969e-01, -7.0048e-02,  8.7545e-01,  8.1406e-01, -1.1230e+00,\n",
       "                       3.1954e-01, -1.3139e+00,  3.3429e-01, -5.4375e-01, -6.8271e-01,\n",
       "                      -3.0108e-01,  1.2895e+00, -4.8773e-01,  5.3015e-01, -1.0406e+00,\n",
       "                      -4.0837e-01, -3.5651e-01, -5.9031e-01,  7.8356e-01,  4.1505e-02,\n",
       "                       5.3572e-01, -2.2652e+00, -1.1929e+00,  1.1394e+00, -1.6081e-01,\n",
       "                       4.4913e-01,  4.0666e-01, -1.1324e+00, -1.3781e-01,  3.7749e-01,\n",
       "                      -3.0629e-01, -1.3623e+00, -9.6449e-01, -1.9080e-01, -1.1660e+00,\n",
       "                      -4.9860e-02,  2.9098e-02, -1.0933e+00, -8.9468e-01, -7.3221e-01,\n",
       "                       6.8112e-01, -1.2603e+00,  9.5497e-01,  3.6741e-01, -2.5174e-01,\n",
       "                       1.7994e-01, -8.0290e-01, -5.2447e-01,  1.9096e-01,  9.8911e-02,\n",
       "                       8.2385e-01, -6.5323e-01,  2.0412e+00,  1.7536e-01, -2.1638e-03,\n",
       "                      -8.2852e-01, -1.2400e+00,  1.4969e+00, -4.3662e-01,  8.1542e-01,\n",
       "                       8.2248e-01, -9.2196e-02,  1.0780e+00, -1.4441e-01, -1.0008e+00,\n",
       "                      -2.6300e-01, -4.6023e-02, -6.3461e-01, -8.2770e-01, -1.2855e+00,\n",
       "                      -3.4604e-02,  5.6049e-01, -1.9174e-01,  4.0311e-01,  4.6502e-01,\n",
       "                       7.4804e-01,  3.5987e-01, -5.1203e-01,  1.2153e+00,  1.7695e+00,\n",
       "                       8.0147e-01,  2.3106e-01, -9.1876e-02,  1.7886e+00,  5.3834e-01,\n",
       "                      -1.0690e+00,  4.8769e-01, -2.4091e+00,  1.3721e-01,  7.8825e-01,\n",
       "                      -1.5306e+00,  1.7342e+00,  1.4772e+00,  1.2789e+00,  2.1807e+00,\n",
       "                       1.9865e-01,  1.0257e+00, -2.4894e-01,  1.2183e+00,  2.5697e-01,\n",
       "                      -9.7104e-01, -3.5620e+00, -7.3103e-01,  2.8265e-01, -5.2413e-01,\n",
       "                      -7.7191e-01,  4.9228e-01,  8.7750e-01, -2.8471e-01,  1.0175e-01,\n",
       "                       2.6696e-02, -3.5321e-01, -3.4421e-01, -5.2548e-01, -1.5865e+00,\n",
       "                       3.0414e-01,  8.1293e-01, -1.9075e-03, -1.6767e-01, -8.3477e-01,\n",
       "                       2.0823e-01, -1.1206e+00, -2.2772e-02, -1.1939e+00,  1.2045e+00,\n",
       "                      -4.5300e-01,  1.1346e+00, -1.7204e+00,  2.7628e-01,  2.6295e-01,\n",
       "                       2.6187e-01, -1.5433e+00, -4.8547e-01,  2.1104e-01, -5.2158e-01,\n",
       "                      -3.9074e-01,  1.0937e+00], device='cuda:0')),\n",
       "             ('module.model.14.1.running_var',\n",
       "              tensor([0.9192, 0.5712, 0.8745, 0.9440, 1.0086, 0.8318, 0.6115, 0.6788, 0.8859,\n",
       "                      0.7004, 0.6349, 0.6849, 0.7467, 1.2021, 0.6686, 0.5300, 0.7973, 0.5620,\n",
       "                      0.6888, 0.8704, 1.0427, 1.1893, 1.3574, 1.0024, 0.6531, 0.6968, 0.9999,\n",
       "                      1.2255, 0.8391, 1.1170, 0.6890, 1.7208, 0.7696, 0.7300, 0.9131, 0.6710,\n",
       "                      0.7042, 0.8535, 1.1434, 0.8906, 0.6643, 0.7818, 0.7039, 1.0309, 1.4560,\n",
       "                      0.7443, 0.9917, 0.6137, 0.7713, 0.6550, 0.6522, 0.5787, 1.1160, 0.7936,\n",
       "                      0.9744, 0.6237, 0.6047, 0.8926, 0.6238, 0.5402, 0.5530, 1.1532, 1.3284,\n",
       "                      1.2248, 1.0161, 0.7669, 0.6123, 1.0437, 1.5157, 1.3190, 0.5812, 0.7140,\n",
       "                      0.5106, 0.9056, 0.8172, 1.0531, 0.6185, 0.6752, 1.0865, 0.8659, 0.8771,\n",
       "                      1.2620, 0.6371, 1.0993, 1.3458, 0.6197, 1.5506, 0.8544, 0.7250, 0.5045,\n",
       "                      0.7885, 0.7968, 0.7297, 1.0855, 0.6710, 0.6726, 1.6840, 0.5362, 0.8435,\n",
       "                      0.5920, 0.5584, 0.7217, 0.6321, 0.7140, 1.7709, 0.4831, 1.0728, 0.8136,\n",
       "                      1.4599, 1.2173, 0.9511, 0.9498, 1.6155, 0.6991, 0.7163, 0.8241, 0.6299,\n",
       "                      1.6131, 0.7556, 0.6850, 0.5547, 0.9179, 0.8327, 1.2307, 1.0059, 0.7519,\n",
       "                      0.7429, 0.5918, 1.4370, 0.7096, 1.1860, 1.1121, 0.7109, 2.1044, 0.7364,\n",
       "                      0.8318, 0.7909, 0.9363, 1.0713, 1.5293, 0.6976, 0.6384, 0.4595, 0.5428,\n",
       "                      0.5565, 0.8420, 0.5973, 0.9414, 1.0829, 1.3417, 0.7420, 0.7661, 0.5661,\n",
       "                      0.6553, 0.7627, 0.9596, 0.9318, 0.6415, 0.9154, 1.5093, 0.6370, 0.6375,\n",
       "                      1.2692, 0.6173, 0.7237, 1.3943, 0.8085, 0.6176, 1.6586, 1.1166, 1.1801,\n",
       "                      0.5602, 0.7924, 1.3460, 0.9185, 0.8470, 0.8814, 0.8606, 0.5104, 0.9697,\n",
       "                      0.6003, 0.7700, 0.6451, 1.3868, 0.6720, 1.2570, 0.9187, 0.9364, 0.7594,\n",
       "                      0.7487, 0.7997, 1.3076, 0.6439, 0.5214, 0.5782, 0.8660, 0.9194, 0.7686,\n",
       "                      0.6111, 1.7409, 1.0373, 1.0154, 0.9553, 0.8566, 0.8562, 0.7182, 0.9381,\n",
       "                      0.6815, 1.0067, 1.0220, 0.8251, 0.8243, 1.9378, 0.5660, 0.9137, 1.0254,\n",
       "                      1.8486, 0.9581, 1.0000, 1.2581, 0.7635, 0.4400, 0.6196, 0.6314, 2.0326,\n",
       "                      1.0840, 0.4329, 1.1644, 0.9824, 0.5385, 1.0545, 2.1123, 1.2541, 0.8060,\n",
       "                      0.7311, 0.9590, 1.1849, 0.6078, 0.7281, 0.5969, 0.9155, 0.7060, 0.7499,\n",
       "                      0.7250, 0.9680, 0.6478, 0.9435, 0.7443, 0.7869, 1.0713, 1.0108, 0.8553,\n",
       "                      0.9547, 0.6898, 0.9294, 0.6525, 1.5287, 0.7303, 0.5762, 0.7182, 0.8639,\n",
       "                      0.6277, 0.6673, 0.6002, 0.7309, 0.7825, 0.4569, 1.1194, 0.9860, 1.0225,\n",
       "                      0.8802, 0.6337, 0.5229, 1.1664, 0.8049, 1.5062, 1.1454, 0.6259, 0.8448,\n",
       "                      1.2464, 0.7958, 1.1500, 1.0208, 0.9720, 1.0064, 0.6581, 0.8889, 0.6061,\n",
       "                      1.0355, 0.6119, 0.8181, 0.7985, 0.7972, 0.7953, 1.8920, 0.8077, 0.6262,\n",
       "                      0.6044, 1.0592, 0.6985, 1.0382, 0.8063, 0.8296, 0.9001, 0.7054, 0.6365,\n",
       "                      0.9021, 0.5652, 1.0456, 0.9478, 0.7729, 0.8563, 0.8885, 0.5620, 0.5186,\n",
       "                      0.9992, 0.7581, 0.9033, 0.8019, 1.6181, 0.5699, 0.7154, 0.7069, 0.6548,\n",
       "                      0.7119, 0.5773, 0.5267, 1.4507, 0.4835, 0.5829, 0.8044, 0.7258, 0.8014,\n",
       "                      1.1338, 0.5714, 0.8805, 0.5209, 0.6213, 0.4838, 0.6715, 0.5073, 1.4827,\n",
       "                      1.3781, 0.5111, 0.7739, 0.9634, 0.7663, 0.9454, 1.3714, 0.8424, 1.0558,\n",
       "                      0.7325, 1.3833, 0.5456, 1.0747, 0.7239, 0.9025, 1.2012, 0.4897, 1.5772,\n",
       "                      1.0410, 0.6984, 0.8367, 0.7826, 0.6193, 0.8220, 2.0275, 0.6497, 1.1305,\n",
       "                      0.9369, 0.8942, 0.6332, 0.8009, 0.7298, 0.5506, 0.7117, 0.7242, 1.1773,\n",
       "                      1.6664, 0.9601, 0.8370, 0.5192, 0.7148, 0.7920, 0.8431, 0.6583, 0.7984,\n",
       "                      0.7972, 0.7681, 0.6741, 0.8774, 1.0015, 1.3729, 0.7353, 1.2023, 0.7853,\n",
       "                      1.2871, 0.7541, 0.7791, 0.8941, 0.6780, 0.6121, 0.6236, 0.7522, 0.5263,\n",
       "                      1.2466, 1.5044, 0.5044, 0.8822, 1.1685, 1.2425, 0.5898, 1.2492, 0.8153,\n",
       "                      0.7520, 0.9093, 0.5160, 1.3842, 0.7428, 0.6728, 0.7890, 1.2077, 1.3474,\n",
       "                      0.9188, 0.4816, 0.8053, 0.6853, 0.8633, 1.5628, 1.3749, 1.0540, 0.9930,\n",
       "                      0.5205, 0.6051, 1.0699, 0.5952, 0.6125, 0.9610, 0.5997, 1.0784, 1.6069,\n",
       "                      1.1109, 0.9444, 0.9316, 0.7766, 1.4127, 0.5054, 0.8386, 0.6977, 0.7096,\n",
       "                      0.5401, 0.6127, 1.0439, 0.8242, 0.7233, 0.7432, 0.9889, 0.7331, 1.1428,\n",
       "                      0.9858, 0.5816, 1.1645, 3.2854, 2.0946, 0.7342, 0.8547, 0.8359, 1.7815,\n",
       "                      1.1342, 0.9737, 0.5835, 0.9161, 0.5714, 0.8908, 0.8501, 0.6655, 1.4536,\n",
       "                      0.4845, 0.7065, 1.0497, 0.7997, 1.4927, 0.5643, 0.7242, 0.6183, 1.2974,\n",
       "                      0.9136, 0.6356, 0.4320, 0.5898, 0.6656, 0.9613, 1.0233, 0.5494, 1.0301,\n",
       "                      0.5565, 0.5919, 1.1742, 0.6798, 0.9777, 0.7007, 1.2882, 1.2068, 0.8285,\n",
       "                      0.6840, 1.2175, 0.9338, 1.0782, 0.6941, 0.9547, 0.5929, 1.2197],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.14.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.14.2.clip_val', tensor([5.5735], device='cuda:0')),\n",
       "             ('module.model.15.0.weight',\n",
       "              tensor([[[[ 0.1842, -0.2279,  0.0069],\n",
       "                        [ 1.1500, -1.0433, -0.5101],\n",
       "                        [ 0.3842, -0.3644, -0.0360]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0978,  0.7579,  0.1962],\n",
       "                        [ 0.0327,  1.1959,  0.2189],\n",
       "                        [-0.2287, -0.4595, -0.4135]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0100, -0.2657, -0.0344],\n",
       "                        [-0.2644,  1.8884,  0.0224],\n",
       "                        [ 0.0058,  0.0421,  0.0487]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1430,  0.4041,  0.1104],\n",
       "                        [ 0.0855,  1.6886,  0.0549],\n",
       "                        [ 0.1095, -0.2143,  0.0847]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2964, -0.3950, -0.7965],\n",
       "                        [ 0.1820, -0.7635, -0.2713],\n",
       "                        [ 0.1005,  0.3585, -0.0922]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1629,  0.1775,  0.1141],\n",
       "                        [ 0.1111,  1.3390,  0.2901],\n",
       "                        [ 0.1909,  0.2632,  0.1533]]]], device='cuda:0')),\n",
       "             ('module.model.15.1.weight',\n",
       "              tensor([0.9267, 0.6539, 0.7465, 1.2066, 1.2438, 0.7950, 0.9256, 0.8765, 0.7294,\n",
       "                      0.5992, 1.0790, 0.8510, 0.5261, 0.9907, 0.6079, 0.6680, 0.5622, 1.1011,\n",
       "                      1.1356, 0.6724, 1.1810, 0.9666, 0.7927, 0.6846, 0.9237, 0.9677, 1.1053,\n",
       "                      1.6977, 1.1650, 0.6052, 0.6799, 0.9046, 0.8831, 1.2879, 1.4315, 0.7921,\n",
       "                      0.6416, 0.7160, 0.8148, 0.8675, 0.6487, 1.1988, 0.5360, 0.7798, 0.7657,\n",
       "                      0.7911, 0.8167, 0.5765, 0.6152, 0.7531, 0.6036, 0.6286, 0.8418, 0.6191,\n",
       "                      0.7187, 1.1620, 0.5603, 0.6181, 0.7417, 0.9373, 0.5720, 0.6611, 1.3331,\n",
       "                      1.3862, 0.7632, 0.9217, 0.7991, 0.4932, 0.9512, 0.8712, 1.1049, 0.9843,\n",
       "                      0.5130, 1.3253, 0.8896, 1.1546, 0.7290, 0.7335, 0.6193, 0.7356, 0.5809,\n",
       "                      0.9294, 0.7170, 0.4460, 0.7611, 0.6501, 0.9584, 0.5533, 0.8277, 0.5276,\n",
       "                      0.8154, 1.0594, 0.6315, 0.7665, 0.6646, 0.8362, 1.9919, 1.0267, 0.9803,\n",
       "                      0.7958, 0.6014, 0.4971, 0.7645, 0.5865, 1.5302, 2.1724, 1.4229, 0.5159,\n",
       "                      0.8875, 1.1341, 0.6785, 0.6960, 0.6974, 0.8340, 0.9200, 0.7762, 0.5565,\n",
       "                      0.9228, 0.6702, 0.6584, 0.6145, 0.7989, 1.0267, 0.9064, 0.7229, 0.7775,\n",
       "                      0.7486, 0.5797, 1.2992, 0.6313, 0.8367, 0.9669, 0.4806, 1.6371, 0.4801,\n",
       "                      0.9612, 0.6357, 1.4434, 0.6270, 0.9239, 0.4394, 0.6459, 0.5669, 1.0937,\n",
       "                      0.7016, 0.8382, 0.7854, 1.1028, 1.2326, 0.7778, 0.7717, 0.8993, 0.7190,\n",
       "                      0.6394, 0.5849, 0.8481, 0.8161, 0.7334, 0.5481, 0.5603, 1.0573, 0.6856,\n",
       "                      0.8083, 1.0746, 0.8443, 1.2947, 0.7495, 0.6356, 0.7959, 0.8658, 0.7684,\n",
       "                      0.5382, 0.7658, 0.7970, 0.8260, 0.6733, 0.8496, 0.7945, 1.2586, 0.7722,\n",
       "                      0.6336, 0.4606, 0.7027, 0.7897, 1.0929, 1.0929, 0.6275, 0.7308, 0.7091,\n",
       "                      0.7230, 0.7121, 1.0101, 0.5992, 1.6430, 1.0304, 0.6036, 1.1521, 0.7361,\n",
       "                      0.8660, 1.5251, 1.0031, 0.8402, 0.9196, 1.7334, 0.8588, 0.5730, 0.6397,\n",
       "                      0.6762, 0.9164, 0.9353, 1.3806, 0.6214, 1.5748, 0.6978, 0.7254, 0.8867,\n",
       "                      0.9563, 1.1772, 0.7142, 0.8266, 0.7833, 0.6145, 1.2938, 0.9156, 1.6592,\n",
       "                      0.6767, 0.5826, 0.9023, 0.5524, 0.7163, 0.7297, 0.7149, 0.7327, 0.8457,\n",
       "                      1.1626, 0.7647, 0.7113, 0.9782, 0.7868, 0.8128, 0.8161, 0.8538, 0.9808,\n",
       "                      1.4586, 0.9459, 0.5932, 0.7594, 0.8261, 1.1677, 1.0159, 0.6814, 1.4594,\n",
       "                      0.9734, 0.4949, 0.6936, 0.9081, 0.8453, 0.8082, 0.9253, 0.5430, 0.7882,\n",
       "                      0.7438, 0.5559, 1.0682, 0.9985, 1.1099, 0.9110, 0.7394, 0.4746, 1.0642,\n",
       "                      0.7473, 0.6155, 0.5460, 1.2528, 0.7896, 1.5709, 1.3565, 0.7322, 0.9253,\n",
       "                      0.5299, 0.7817, 0.7786, 0.6366, 1.0784, 0.8575, 0.5768, 0.4705, 0.5983,\n",
       "                      0.7858, 0.5303, 1.0984, 0.8257, 0.9837, 0.8840, 0.8160, 0.5697, 0.8182,\n",
       "                      0.6248, 0.9454, 0.6548, 0.7295, 0.9723, 0.8390, 0.6777, 1.1850, 0.5663,\n",
       "                      0.8080, 0.7914, 0.5859, 0.4939, 0.9292, 0.6159, 0.8429, 0.6021, 0.6182,\n",
       "                      1.2276, 1.0059, 0.9016, 1.1001, 1.4089, 0.7160, 0.7726, 1.0758, 0.7430,\n",
       "                      0.7852, 0.5712, 0.9205, 0.8122, 0.7535, 0.8216, 0.7233, 1.0291, 0.8754,\n",
       "                      1.4532, 0.9152, 0.6561, 0.5695, 0.5357, 0.9149, 0.5787, 0.5894, 1.6453,\n",
       "                      0.8006, 0.9852, 1.9211, 0.5760, 0.8646, 1.0589, 0.7639, 0.7430, 0.4420,\n",
       "                      0.4991, 0.7713, 1.0903, 0.7399, 1.1778, 0.6087, 1.0884, 1.5293, 0.8696,\n",
       "                      0.9544, 1.0311, 0.9071, 0.7415, 0.6478, 1.4036, 0.9387, 0.8367, 0.8680,\n",
       "                      0.8089, 0.9905, 0.9073, 0.8424, 0.7598, 1.3459, 0.7303, 0.6528, 0.7319,\n",
       "                      1.6015, 0.9150, 0.7333, 0.7701, 0.8594, 0.6757, 0.6513, 0.6195, 0.6868,\n",
       "                      0.8675, 1.0243, 0.6282, 0.8593, 0.7019, 0.8366, 0.5618, 1.3867, 0.7790,\n",
       "                      0.5675, 1.1909, 0.7250, 0.7413, 0.5913, 0.9910, 0.8968, 0.8174, 0.8097,\n",
       "                      0.9029, 0.9131, 0.5352, 0.7994, 0.7979, 0.8036, 0.9065, 0.9696, 0.8205,\n",
       "                      1.5270, 0.6327, 0.7955, 1.2008, 0.6551, 1.1800, 1.2056, 1.0080, 0.8871,\n",
       "                      0.6868, 0.6867, 0.5333, 0.5984, 0.9136, 0.7463, 0.7941, 1.1187, 0.8259,\n",
       "                      0.5165, 0.7041, 0.9415, 0.8336, 1.2371, 0.8463, 0.7028, 1.0631, 0.7926,\n",
       "                      0.8182, 0.9986, 1.2866, 0.9502, 0.7628, 0.6505, 0.5234, 1.0532, 0.5465,\n",
       "                      0.8461, 1.3775, 0.5746, 0.7252, 1.1785, 0.6156, 1.2094, 0.7636, 0.7010,\n",
       "                      0.7571, 0.7351, 0.7779, 0.6709, 0.9035, 1.1158, 0.7579, 0.8042, 1.4795,\n",
       "                      0.7419, 0.6469, 1.0818, 0.9233, 1.1661, 0.7100, 1.0859, 0.5816, 0.7097,\n",
       "                      1.4533, 0.5702, 0.8600, 0.8721, 0.7726, 0.6422, 0.6372, 0.6573, 1.1056,\n",
       "                      1.1670, 0.9249, 0.4969, 0.6230, 0.6856, 1.4678, 0.7679, 0.6153, 0.8325,\n",
       "                      0.5945, 0.8701, 1.0145, 0.8439, 0.8146, 0.5601, 0.8683, 0.9148, 1.3525,\n",
       "                      0.6184, 1.0202, 0.8275, 0.7071, 0.7104, 0.6337, 0.4912, 0.8405],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.15.1.bias',\n",
       "              tensor([ 2.9234e-01, -2.7225e-01,  1.1351e+00, -9.2735e-01, -3.4387e-01,\n",
       "                       5.7711e-01, -8.1123e-01, -4.3487e-01,  1.0410e+00, -3.0863e-02,\n",
       "                      -6.2283e-02,  6.0852e-02,  8.7639e-01,  7.8776e-02,  4.1099e-01,\n",
       "                      -3.0902e-01,  3.4940e-01, -3.7618e-02, -1.5481e-01,  1.4517e+00,\n",
       "                      -7.2843e-01, -6.1509e-01, -4.8116e-01, -3.4946e-01,  2.1893e+00,\n",
       "                       2.1720e-01, -4.2388e-02, -7.6786e-01, -2.6937e-01, -7.5006e-03,\n",
       "                       5.7056e-01, -2.4308e-01,  1.0325e-01, -3.6721e-01, -1.5210e-01,\n",
       "                      -7.6701e-01, -6.4237e-02,  6.3493e-01,  1.5814e+00,  7.0827e-03,\n",
       "                       1.2324e-01, -5.8548e-01,  1.7200e-01,  6.9614e-01,  5.2854e-01,\n",
       "                       4.7755e-01, -6.4436e-01,  6.8489e-01,  5.7015e-01, -1.4063e-01,\n",
       "                       2.8687e-01,  3.6455e-01, -3.9777e-01,  5.1975e-01,  2.0187e-01,\n",
       "                      -9.8453e-02,  2.7483e-01,  1.2697e+00,  2.0059e+00,  4.9370e-01,\n",
       "                       3.7066e-01,  2.4465e-01, -4.5117e-01, -4.1338e-01,  3.9888e-01,\n",
       "                       3.2190e-01, -8.7057e-01,  1.2590e+00, -7.4685e-01, -4.5012e-01,\n",
       "                      -3.7612e-01, -1.7619e-02,  1.2197e+00, -6.5326e-02, -5.1260e-01,\n",
       "                       2.3174e-01,  6.6933e-01,  5.0662e-01,  5.0927e-01,  5.1894e-01,\n",
       "                       9.8532e-01, -4.2662e-01,  9.7235e-01,  1.4466e+00,  1.2114e+00,\n",
       "                       2.6456e-01, -6.3923e-01,  7.8212e-02, -2.4845e-01,  7.3789e-02,\n",
       "                       1.9552e-01, -2.3756e-01,  6.2826e-01, -2.2213e-02, -5.5695e-02,\n",
       "                      -3.1101e-01, -1.0393e+00, -1.5744e-01, -3.0701e-01, -4.6410e-01,\n",
       "                       1.5155e+00,  2.1535e-01,  2.3515e-01,  1.5209e+00, -3.5991e-01,\n",
       "                      -2.0915e+00, -3.4439e-01,  8.7762e-01, -5.9442e-01, -1.2478e+00,\n",
       "                       1.7693e+00,  3.3238e-01,  3.2628e-02, -2.1280e-01, -5.5476e-01,\n",
       "                       9.7386e-02,  1.3232e+00,  2.1431e+00,  4.0901e-01,  5.4687e-01,\n",
       "                       1.6468e+00, -4.5634e-01, -2.2350e-01,  1.6045e+00,  3.8794e-02,\n",
       "                       2.8693e-02, -8.3986e-02,  5.8945e-01, -1.4299e-01, -1.0192e-01,\n",
       "                       1.6760e+00,  2.4921e-01,  1.1315e+00, -2.4479e-01,  2.8053e-01,\n",
       "                       5.2610e-02,  1.5861e+00, -1.5207e+00,  4.0471e-02,  7.2343e-01,\n",
       "                       1.1864e+00,  7.0646e-01,  1.1182e+00, -3.7078e-01, -3.6101e-01,\n",
       "                       5.2744e-01,  9.5751e-01,  4.5454e-02, -4.9461e-02,  4.9139e-02,\n",
       "                      -4.1103e-01,  1.3871e-01, -1.3051e-01,  3.8850e-01, -1.1957e-01,\n",
       "                      -3.0461e-01, -2.1440e-01, -4.8593e-02,  2.3612e-01,  6.1394e-01,\n",
       "                      -1.4756e-01,  1.9677e-01,  8.8300e-01, -1.0667e-01, -2.4776e-01,\n",
       "                      -3.7596e-01,  1.7748e+00, -1.2020e+00,  2.8141e-01,  1.5010e+00,\n",
       "                       2.4440e-01,  2.2037e-01,  4.4290e-02,  1.1370e+00,  4.3806e-01,\n",
       "                       9.9258e-01,  1.5660e+00, -5.1796e-01, -7.7003e-01,  1.7911e-01,\n",
       "                       1.7982e+00,  3.7100e-01,  3.6522e-01,  8.4527e-02,  1.5390e-01,\n",
       "                       4.6845e-01,  1.2684e-01,  2.5618e-02,  3.5560e-01,  1.0451e+00,\n",
       "                       1.4512e+00,  8.3879e-01,  4.7947e-02, -8.5810e-01, -3.2327e-01,\n",
       "                       8.4643e-01, -7.9856e-02,  1.5361e+00, -5.7708e-01, -4.9349e-01,\n",
       "                      -4.7003e-01, -3.2725e-01, -2.8921e-01, -1.0420e+00, -7.2681e-01,\n",
       "                       1.7904e-01,  1.4097e-01, -6.5634e-01, -2.6054e-01, -2.0827e-01,\n",
       "                      -4.6564e-01,  1.2935e+00, -3.7446e-01,  1.3621e+00,  8.0839e-01,\n",
       "                      -5.4714e-01, -6.4140e-01, -2.0292e-01,  1.3869e-01, -4.1517e-01,\n",
       "                      -3.6689e-01,  1.3570e+00, -6.9762e-01,  1.7160e+00, -5.7874e-01,\n",
       "                       1.6493e+00,  8.6640e-01,  2.0860e+00,  1.3257e+00,  5.5361e-01,\n",
       "                       9.6460e-02,  1.5270e-02,  1.1599e-01,  3.5233e-01, -1.2045e+00,\n",
       "                      -2.9922e-01,  3.9018e-01, -1.6239e-01,  9.3857e-02,  1.3669e-01,\n",
       "                       4.1418e-02,  2.7707e-01, -3.9901e-01, -9.7633e-02, -3.2524e-01,\n",
       "                       7.6701e-01, -1.4540e-01,  1.5581e-01, -1.8972e-01,  2.6717e+00,\n",
       "                       2.9930e-01, -5.3251e-01,  6.0295e-01,  3.8859e-02,  6.4608e-01,\n",
       "                      -4.7282e-02,  1.9786e+00,  7.2331e-01, -2.9836e-01,  1.3249e+00,\n",
       "                      -2.5515e-01,  5.7277e-01,  6.1137e-01, -2.7789e-01, -8.3693e-01,\n",
       "                      -1.9659e-01, -7.1860e-01,  2.6794e-01,  2.7684e-01, -6.1612e-01,\n",
       "                       4.6278e-01,  1.0206e+00,  8.7811e-01, -2.8587e-01, -2.7981e-01,\n",
       "                      -8.7234e-01, -6.8135e-01, -6.2178e-02, -3.3737e-01,  1.4354e+00,\n",
       "                      -6.3289e-02,  1.3166e+00,  2.4231e-01, -3.5527e-02, -4.8006e-01,\n",
       "                       1.0308e+00,  1.0514e+00,  1.4427e+00,  2.9899e-01,  1.9070e+00,\n",
       "                      -9.6065e-04,  3.9990e-01, -6.8228e-02, -5.8188e-01,  1.1980e+00,\n",
       "                       4.2435e-01,  1.3005e-01,  2.9402e-01,  4.1718e-01,  1.0976e+00,\n",
       "                       6.5087e-01,  2.7132e-01,  2.9244e-01, -4.4366e-01, -2.4240e-01,\n",
       "                       2.1939e-01,  8.0170e-01,  1.7114e+00,  8.5470e-02,  1.5991e+00,\n",
       "                       8.7059e-01,  1.5488e+00,  9.9243e-01,  7.9503e-01,  1.4934e+00,\n",
       "                       1.8781e-01,  1.6773e-01, -4.8673e-01, -6.5433e-01, -3.5480e-01,\n",
       "                      -8.1632e-02, -4.5101e-02,  2.6463e-01,  6.1122e-01,  7.8945e-01,\n",
       "                      -5.9919e-01, -2.8394e-01, -3.8795e-01,  6.5385e-01, -2.3066e-01,\n",
       "                      -2.6643e-02, -2.5986e-01,  1.3549e-01,  4.8628e-01, -7.4277e-01,\n",
       "                       5.3596e-01,  1.4613e+00,  1.1358e-01,  1.4141e-01,  2.1972e-01,\n",
       "                       1.6071e+00, -1.0386e+00, -3.7136e-01, -1.7488e-01, -1.2309e+00,\n",
       "                       5.6621e-01, -4.7244e-01, -3.6232e-01,  1.4380e+00, -3.4932e-01,\n",
       "                       1.0182e+00,  2.8244e-01, -2.8285e-01, -3.0661e-01,  6.2199e-01,\n",
       "                      -7.8057e-02,  5.8416e-01, -9.7502e-02, -1.5310e+00, -4.3124e-01,\n",
       "                       5.0326e-01, -3.9908e-01,  9.6625e-01,  6.5223e-01,  1.2092e+00,\n",
       "                      -4.2195e-01,  1.9692e+00,  3.0558e-01,  1.7344e-01,  8.3561e-02,\n",
       "                      -3.9110e-01,  5.3756e-01, -3.4189e-01,  3.8701e-01, -7.3167e-01,\n",
       "                      -9.0365e-02, -9.6610e-02,  7.2108e-01, -6.9089e-01,  2.4631e+00,\n",
       "                      -2.6973e-01,  1.0982e+00,  1.4338e-01,  1.1539e+00,  2.0253e-01,\n",
       "                       5.9412e-01,  6.1460e-02,  4.5173e-01,  3.3110e-01,  9.9933e-01,\n",
       "                      -2.1949e-01, -7.5438e-02, -2.3626e-01,  3.1240e-01, -4.8465e-01,\n",
       "                      -3.4438e-01,  8.2930e-02, -6.7993e-01,  6.9817e-01, -4.2172e-02,\n",
       "                       1.2095e+00, -1.8952e-01,  8.6388e-01,  3.1933e-01, -7.6179e-03,\n",
       "                      -2.5028e-01,  1.6595e+00,  1.2671e+00, -7.2697e-02,  1.8842e+00,\n",
       "                       2.8786e-01,  1.4864e-01,  2.1362e+00,  7.4336e-01, -7.6068e-01,\n",
       "                       3.5770e-01, -8.1787e-02, -1.4102e-01,  5.6390e-01, -1.6762e-01,\n",
       "                      -3.8698e-01,  1.6693e+00, -6.2616e-01,  1.3406e+00,  5.2613e-02,\n",
       "                       7.6951e-01,  5.0024e-01,  1.7063e+00,  3.1943e-01,  1.5337e-01,\n",
       "                      -1.2313e+00, -1.6030e-01,  1.1251e-01, -3.2135e-01, -2.4153e-01,\n",
       "                      -3.7175e-01,  1.8038e-01,  3.3177e-01,  1.8645e+00, -2.4757e-01,\n",
       "                      -3.3488e-01,  1.8865e+00, -5.1584e-01, -7.8415e-01,  3.1757e-01,\n",
       "                      -2.2638e-01,  2.3683e-01,  1.0238e-01, -1.0697e-01,  4.3133e-01,\n",
       "                       3.8273e-01, -5.5389e-01,  1.1356e-01,  4.2169e-01, -8.4617e-01,\n",
       "                       1.3144e-01, -1.1048e+00,  7.7190e-04,  6.5283e-01,  2.1108e-01,\n",
       "                       6.0256e-01,  3.6840e-01,  5.3249e-01, -1.4861e-01, -3.1446e-01,\n",
       "                      -6.7251e-02,  2.2069e-01, -5.9282e-01,  2.7551e-01,  5.2246e-01,\n",
       "                      -1.8538e-01,  1.0997e+00, -4.7280e-01,  1.1422e+00, -4.8851e-01,\n",
       "                       2.6437e-01, -4.3581e-02, -5.3176e-01,  1.5155e+00, -4.2691e-01,\n",
       "                       6.4509e-01,  7.8953e-01,  7.0716e-01,  9.2754e-01,  1.9052e-01,\n",
       "                       3.5469e-01, -3.0217e-01,  1.3567e-01,  1.2946e+00,  1.5701e+00,\n",
       "                      -1.1063e-01, -5.0415e-01,  5.8017e-01,  1.0242e+00, -1.2781e-01,\n",
       "                       1.1339e+00, -1.8896e-01,  1.5723e-02, -2.4782e-01, -1.9115e-01,\n",
       "                       5.1712e-01,  2.1859e+00, -7.3706e-01, -4.2120e-01,  1.0022e+00,\n",
       "                       3.8887e-01,  1.5493e+00,  2.0363e+00, -2.9773e-01,  2.9741e-01,\n",
       "                       1.3946e+00, -3.2019e-01], device='cuda:0')),\n",
       "             ('module.model.15.1.running_mean',\n",
       "              tensor([-2.0888e-01,  1.6492e-01,  1.1540e+00,  7.5258e-01, -2.4675e+00,\n",
       "                      -1.5812e+00,  6.9653e-01,  8.8294e-01, -1.9311e-01,  8.5459e-01,\n",
       "                       8.4083e-02, -2.4982e+00, -1.3868e+00, -2.1861e+00,  8.1965e-01,\n",
       "                       4.0123e-01,  8.5550e-01, -2.1234e-01, -1.0644e+00, -1.3434e+00,\n",
       "                       3.2119e+00,  1.4123e+00,  5.4689e-01,  4.7255e-01, -7.9005e-01,\n",
       "                      -1.0766e+00, -1.8577e+00, -2.8292e+00, -2.9393e-01,  8.4427e-01,\n",
       "                       2.6212e-01,  6.7121e-01,  1.3800e+00, -4.7860e-01,  3.9180e-01,\n",
       "                       5.0497e-01,  1.2979e+00, -1.9098e+00,  1.6212e+00,  6.1475e-01,\n",
       "                       9.6066e-01,  1.2726e-01,  5.4248e-01, -1.3439e+00,  1.0521e+00,\n",
       "                      -1.3919e-01,  9.2761e-01,  2.7983e-01, -1.3384e+00,  2.6579e-01,\n",
       "                       4.8277e-01, -1.7023e+00,  7.2247e-01,  4.4860e-01,  1.2299e+00,\n",
       "                      -9.4363e-01,  4.2669e-01, -6.4545e-01, -7.5679e-01, -3.0289e-01,\n",
       "                      -1.6523e+00,  1.4279e+00, -2.1272e+00, -3.0556e-01, -1.9314e+00,\n",
       "                      -1.8024e+00,  2.6534e-01, -1.3893e+00,  6.5157e-01,  8.6486e-01,\n",
       "                      -4.0066e-01,  9.4674e-01, -1.0022e+00, -1.5126e+00,  6.2986e-01,\n",
       "                      -1.2438e+00, -1.0909e+00, -7.6315e-02,  7.7777e-01, -2.6863e+00,\n",
       "                       5.2766e-01,  5.5454e-01, -1.9580e-01, -1.0398e+00,  1.4885e-01,\n",
       "                       8.7739e-01,  4.5161e-01,  4.8630e-01,  1.3449e+00,  6.0332e-01,\n",
       "                       3.2150e-01,  1.2284e+00, -8.6531e-01,  9.7257e-01,  5.7849e-01,\n",
       "                       1.3898e+00, -3.9557e+00,  9.0591e-01,  8.4426e-01,  6.0901e-01,\n",
       "                       4.3153e-01,  9.3173e-01, -1.6731e+00, -2.4129e-01, -2.7370e+00,\n",
       "                      -2.8604e+00, -6.5645e-01, -1.2097e+00,  9.7671e-01,  4.5037e-01,\n",
       "                      -6.3141e-02,  9.4188e-01,  4.3891e-01,  2.8661e-01,  1.3677e+00,\n",
       "                       1.1504e+00,  2.2253e-01,  1.6381e+00, -1.5955e+00, -1.4907e+00,\n",
       "                      -1.5860e-01,  6.7701e-01,  3.1902e-01,  1.0802e+00,  6.0744e-01,\n",
       "                       8.0113e-01,  6.2993e-01, -1.4811e+00, -2.5159e+00,  6.2736e-01,\n",
       "                       1.0344e+00, -1.4845e+00, -9.6199e-01,  1.3328e+00,  3.1838e-01,\n",
       "                      -1.9615e+00,  3.9946e-03,  2.5453e+00,  5.9785e-01,  4.2955e-01,\n",
       "                      -9.9885e-01, -1.1115e+00, -1.8629e-01,  8.8908e-01,  4.4105e-01,\n",
       "                      -2.3045e-01,  1.2216e-01, -1.2801e+00, -2.1687e+00,  7.3831e-01,\n",
       "                       1.3344e+00, -2.5141e-02,  4.8125e-01,  7.8943e-01,  4.5670e-01,\n",
       "                       7.7732e-01,  8.8464e-01,  8.5073e-01,  1.6578e+00,  3.7950e-01,\n",
       "                       5.7608e-03, -2.4217e+00,  4.9088e-01,  9.1364e-02,  1.3288e+00,\n",
       "                      -2.5930e+00,  6.0291e-01,  1.2171e-01,  1.1774e+00,  9.6524e-01,\n",
       "                       1.1525e+00,  5.1941e-01,  1.1211e+00,  2.2263e+00,  8.2911e-01,\n",
       "                       2.6040e-01,  8.4588e-01,  1.0300e+00, -2.4463e+00,  1.0226e+00,\n",
       "                       1.1227e-01,  1.4052e+00,  9.3789e-02,  1.2236e+00, -1.7373e+00,\n",
       "                       7.0161e-01,  3.8410e-01,  1.1782e+00,  6.3803e-01,  1.6164e-01,\n",
       "                       1.6643e-01, -7.5275e-03,  6.9644e-01, -1.4676e+00,  1.2710e-01,\n",
       "                       3.6002e-01, -1.6354e+00, -1.3200e+00,  5.2078e-01, -2.3207e+00,\n",
       "                       1.6497e+00,  9.1554e-01,  4.3908e-01, -2.5195e+00,  6.0642e-01,\n",
       "                       7.8950e-01,  1.0665e+00,  6.4571e-02,  1.0665e+00,  1.9946e+00,\n",
       "                      -1.6127e+00, -1.4718e+00, -2.9103e+00,  1.9322e-02,  7.9226e-01,\n",
       "                       1.0564e+00,  1.2955e+00, -2.3463e-01,  7.1516e-01,  8.6026e-01,\n",
       "                       8.4770e-01,  1.2121e-01,  1.6323e+00, -2.5446e-02, -2.2795e+00,\n",
       "                       2.6961e-01, -1.2613e+00,  1.2893e+00, -1.9751e+00, -9.9536e-02,\n",
       "                       9.6188e-01,  8.9641e-01,  8.7217e-01,  8.9612e-01,  9.5678e-01,\n",
       "                       1.0705e+00,  8.8134e-01,  5.9560e-01,  1.0032e-01,  1.4105e+00,\n",
       "                       1.1729e+00, -2.3635e-02,  1.1090e+00,  1.2742e+00,  1.7885e+00,\n",
       "                       2.9821e-01,  7.4962e-01, -1.9699e+00, -1.6800e-01,  4.4606e-01,\n",
       "                       6.9849e-01, -1.1311e+00, -1.3880e+00,  2.5444e-01, -1.2769e+00,\n",
       "                      -1.8499e+00,  1.5247e+00,  6.4624e-01,  9.0292e-02, -1.3656e+00,\n",
       "                       1.1050e+00, -7.3076e-02,  1.1859e+00, -6.9350e-02,  9.6368e-01,\n",
       "                       3.5812e-01,  6.1991e-01,  1.1839e+00,  7.3008e-01,  6.0615e+00,\n",
       "                      -1.3718e+00, -4.4197e-01, -1.2935e+00, -2.8649e+00,  8.4594e-01,\n",
       "                       3.7226e+00,  3.3223e+00,  1.1030e+00,  9.0553e-01, -1.4917e+00,\n",
       "                       3.0247e-01,  3.9215e-02,  7.2664e-01, -1.7148e+00,  5.8489e-01,\n",
       "                       2.4197e-01, -8.8290e-01, -2.9396e-01, -1.7542e+00, -7.6586e-01,\n",
       "                      -1.0223e-01, -9.8298e-01, -1.4168e+00,  1.3380e+00,  2.9580e-01,\n",
       "                       6.4957e-01, -9.4895e-01,  5.1519e-01, -2.1573e-01, -1.9964e-02,\n",
       "                      -8.9572e-04, -2.5226e-01, -1.8761e+00,  5.1880e-01, -1.1125e+00,\n",
       "                       6.5129e-01, -1.0142e+00, -1.1847e-02,  5.0701e-01, -1.1019e+00,\n",
       "                      -5.6906e-01,  2.3035e-01, -3.1480e-01,  7.9343e-02, -5.4285e-01,\n",
       "                      -2.3624e-02,  3.0947e-02,  8.0262e-01,  1.3799e+00, -2.8117e+00,\n",
       "                       3.3779e-01,  1.0745e+00,  6.4971e-01,  7.1298e-01, -1.3890e+00,\n",
       "                       2.2326e-01,  9.0291e-01,  5.3954e-01,  8.8707e-01,  9.0866e-01,\n",
       "                       2.1076e+00,  1.0096e+00, -2.1504e+00, -2.8334e-01,  1.0607e+00,\n",
       "                       1.2538e+00,  2.4215e-02,  3.2871e-01, -6.0470e-01,  8.0949e-01,\n",
       "                      -1.9459e-01,  3.9356e+00,  6.9292e-01, -4.3975e-01, -2.7181e+00,\n",
       "                       8.6337e-01,  9.7568e-01,  6.8352e-01,  8.4708e-01,  9.4309e-01,\n",
       "                      -8.3828e-01,  9.1809e-01,  6.6096e-01, -1.9789e+00, -8.5802e-01,\n",
       "                      -1.4281e+00,  8.8952e-01, -1.7593e+00,  8.2706e-01,  7.7551e-01,\n",
       "                      -2.3010e+00, -1.9797e+00,  1.2647e+00, -8.6054e-01,  1.4957e-01,\n",
       "                      -1.4386e+00,  2.1342e+00, -7.4832e-01,  1.3945e+00, -2.4390e+00,\n",
       "                       5.9428e-01, -1.1137e+00,  3.4391e-01,  3.7589e-01,  1.2529e+00,\n",
       "                       1.8653e-01,  8.3561e-01,  1.2283e+00, -3.3011e+00,  4.9151e-01,\n",
       "                       1.5648e+00,  5.0532e-02,  1.3553e+00,  3.1338e-01,  1.1374e+00,\n",
       "                       9.7318e-01,  3.9297e-01, -7.3738e-01, -1.2108e+00,  9.1446e-02,\n",
       "                       5.5823e-01,  6.9494e-01,  1.0106e+00,  8.5298e-01, -2.3221e+00,\n",
       "                       1.0333e+00,  2.2537e-01,  3.7064e-01, -1.4691e+00,  1.0733e+00,\n",
       "                      -1.1861e+00, -1.7518e+00,  8.8464e-02, -1.8983e+00,  2.0014e-01,\n",
       "                       8.8357e-01,  1.6328e+00, -5.5599e-02,  1.0827e+00,  5.5684e-01,\n",
       "                       1.4974e+00, -1.6183e+00,  8.7378e-01,  1.2876e+00,  4.1586e-01,\n",
       "                       5.4505e-01,  4.2805e-01, -1.5919e+00, -1.7323e+00,  8.2712e-01,\n",
       "                      -3.5230e-01,  3.3094e-01,  1.0409e+00,  7.7538e-01, -2.0777e+00,\n",
       "                      -1.4443e+00, -9.7055e-01, -1.0551e+00,  1.3725e+00,  9.0679e-01,\n",
       "                       6.1702e-01,  1.5184e+00,  4.2780e-01,  7.3286e-01,  1.6729e+00,\n",
       "                       8.1158e-01,  4.9605e-01, -2.0828e+00,  1.4865e-01, -1.8940e+00,\n",
       "                       8.6131e-01,  1.1200e+00,  1.2998e+00,  1.3335e+00, -9.7752e-02,\n",
       "                       5.0134e-01,  2.8212e-01,  5.6291e-01, -1.5607e+00,  3.6184e-01,\n",
       "                       5.2677e-01, -1.6622e+00,  6.5745e-01, -2.0218e+00,  2.2552e+00,\n",
       "                       3.5289e-01,  1.3186e+00,  1.3212e+00, -1.9352e+00,  1.1815e+00,\n",
       "                      -3.0221e-02,  2.0547e+00,  5.4223e-01,  1.1862e+00, -1.1432e+00,\n",
       "                       6.5071e-01, -1.7270e+00, -3.0016e+00,  1.4560e+00,  4.4449e-01,\n",
       "                      -4.0430e-01,  1.2808e+00,  1.0641e+00, -1.9417e+00,  2.7273e+00,\n",
       "                       3.0186e-01,  5.6980e-01, -1.9268e-01, -1.0163e+00,  5.7206e-01,\n",
       "                       5.6769e-01, -9.5736e-01,  6.0348e-01,  5.1698e-01,  8.5123e-01,\n",
       "                      -2.8167e+00,  1.1604e+00,  3.2252e-01,  3.2833e-01,  1.6883e-01,\n",
       "                       7.9819e-01, -1.7448e+00,  1.2946e+00, -2.9600e-01,  1.8268e+00,\n",
       "                      -9.7723e-02,  2.7902e-01, -2.0610e+00,  1.2465e+00,  8.1124e-01,\n",
       "                      -1.6258e+00,  1.3884e+00,  1.0049e+00, -1.6323e+00,  1.5122e-01,\n",
       "                      -2.2497e+00,  1.2291e+00, -4.2079e-01,  5.7468e-01,  1.0138e+00,\n",
       "                      -9.1194e-01,  9.7596e-01], device='cuda:0')),\n",
       "             ('module.model.15.1.running_var',\n",
       "              tensor([0.8838, 0.3008, 1.2004, 0.9045, 2.1429, 1.9192, 0.9187, 0.8545, 1.6429,\n",
       "                      1.2490, 0.1933, 1.4279, 0.9834, 1.7178, 1.6602, 0.3886, 1.5335, 1.6571,\n",
       "                      0.8708, 0.8808, 2.3351, 1.7939, 0.7577, 0.3293, 1.1510, 1.0128, 0.2370,\n",
       "                      2.5771, 1.7161, 0.8982, 0.9756, 0.5325, 1.4586, 1.8187, 1.6757, 0.6697,\n",
       "                      1.0814, 0.7993, 0.9875, 0.8377, 1.2196, 1.8416, 0.3897, 1.2252, 2.0675,\n",
       "                      1.4265, 1.1333, 0.3692, 0.8712, 0.4786, 0.7041, 0.3550, 0.5765, 0.8474,\n",
       "                      1.2108, 0.7323, 0.5843, 0.7138, 0.4491, 1.1318, 0.6387, 1.4223, 0.6903,\n",
       "                      2.4227, 0.3889, 1.5599, 0.2582, 1.7350, 0.7872, 0.8150, 1.4708, 0.7875,\n",
       "                      0.5430, 2.0476, 0.9692, 1.7312, 1.2329, 1.0490, 1.3139, 1.2421, 1.0282,\n",
       "                      0.6108, 0.8131, 1.0220, 0.9881, 1.0675, 0.3579, 0.3380, 2.1910, 0.6611,\n",
       "                      0.7224, 1.4356, 0.5818, 1.2897, 0.6828, 1.0942, 4.0825, 0.7430, 0.8434,\n",
       "                      0.7629, 0.9200, 1.3516, 0.9106, 1.2189, 1.0091, 2.3269, 1.5331, 0.7837,\n",
       "                      1.1077, 0.3881, 0.7607, 0.9565, 0.3361, 0.4901, 1.7552, 1.9930, 1.1182,\n",
       "                      1.7310, 1.2791, 1.2913, 0.9442, 0.7849, 1.1216, 1.9669, 0.8386, 1.5318,\n",
       "                      0.6950, 1.1322, 1.1915, 0.8031, 1.5036, 1.2011, 0.7739, 1.6540, 0.2515,\n",
       "                      1.3211, 0.8079, 3.9841, 0.6118, 1.5158, 0.6352, 0.7401, 1.0413, 0.9050,\n",
       "                      0.5039, 1.4764, 0.4164, 1.4520, 0.6730, 1.1112, 1.3671, 0.9132, 0.7473,\n",
       "                      1.1208, 0.4187, 0.8663, 0.9377, 1.1103, 1.4870, 0.3154, 0.6438, 0.4271,\n",
       "                      2.0189, 1.4327, 1.1985, 1.1562, 1.6546, 0.1048, 1.0212, 0.7117, 2.2092,\n",
       "                      0.8163, 1.5973, 8.9881, 1.4905, 0.7359, 0.8805, 0.9844, 0.7532, 1.3955,\n",
       "                      0.6544, 1.8331, 0.5269, 1.7763, 1.9701, 1.0497, 0.4123, 1.0832, 1.5981,\n",
       "                      1.3042, 2.9577, 0.6923, 0.7412, 0.9229, 0.9880, 1.0606, 1.8875, 0.8462,\n",
       "                      0.8504, 1.6749, 1.6604, 0.9633, 0.7232, 1.8599, 0.6636, 1.0193, 0.9455,\n",
       "                      0.0763, 1.0000, 1.7492, 1.7133, 0.2840, 3.4195, 0.7974, 1.9125, 1.0956,\n",
       "                      1.1878, 1.0655, 0.7708, 1.2759, 1.2988, 0.7945, 0.6036, 1.0769, 0.6127,\n",
       "                      1.1191, 0.6406, 0.7644, 0.7758, 0.6052, 1.1843, 0.7737, 1.2090, 1.2946,\n",
       "                      0.7518, 0.9869, 2.2939, 0.8537, 1.0057, 1.1838, 1.9884, 0.9489, 1.1922,\n",
       "                      0.4429, 1.6896, 2.9055, 0.9790, 0.2131, 0.9891, 1.5308, 0.9197, 0.9131,\n",
       "                      0.4355, 0.1926, 1.0196, 0.9983, 1.4487, 1.0141, 0.8916, 1.1472, 1.6277,\n",
       "                      1.0953, 1.0423, 1.8221, 1.3208, 0.9920, 0.9260, 1.5007, 0.7419, 4.5224,\n",
       "                      1.2373, 0.7785, 0.8763, 1.8506, 0.8752, 0.8848, 0.6867, 1.6206, 1.4490,\n",
       "                      1.0652, 0.6033, 1.1697, 0.9737, 1.2132, 0.6864, 4.8464, 0.6028, 1.4020,\n",
       "                      0.6450, 0.9801, 1.9151, 0.9923, 1.0612, 1.5137, 0.5660, 1.1013, 0.4988,\n",
       "                      1.7407, 1.8987, 0.6049, 0.4516, 0.5662, 0.7570, 0.4344, 0.9303, 1.4250,\n",
       "                      1.4427, 1.4408, 0.4024, 0.9609, 1.1265, 1.0175, 1.8419, 1.2315, 1.7359,\n",
       "                      0.7819, 2.1181, 0.8470, 2.0042, 1.1365, 0.4860, 1.7820, 0.5795, 0.9012,\n",
       "                      0.9853, 0.1871, 1.2111, 0.7019, 1.1462, 1.5472, 3.9479, 2.1780, 0.9477,\n",
       "                      3.1772, 1.1632, 1.7654, 1.1088, 0.3008, 0.7892, 1.1752, 1.5327, 1.0150,\n",
       "                      1.1152, 1.6232, 2.9454, 0.8019, 1.2256, 0.7263, 3.2277, 0.8247, 0.4346,\n",
       "                      1.0211, 1.0452, 0.6055, 0.7663, 1.9967, 1.2578, 1.2481, 0.5512, 0.9597,\n",
       "                      1.1834, 0.4037, 0.8327, 1.0137, 0.7835, 1.2224, 2.4918, 0.4724, 1.2985,\n",
       "                      0.7731, 1.0169, 1.5479, 0.4371, 1.3755, 0.9603, 0.3432, 0.7673, 1.5376,\n",
       "                      1.9744, 2.0440, 1.0472, 0.3977, 1.2900, 1.2925, 1.4573, 1.2919, 0.8297,\n",
       "                      1.0082, 1.3727, 1.6558, 0.9659, 0.6956, 1.1558, 1.1609, 1.2094, 0.9763,\n",
       "                      0.1581, 0.7981, 0.7027, 1.1396, 0.9230, 0.8784, 1.8458, 0.3639, 0.7003,\n",
       "                      0.7668, 0.9747, 1.1708, 1.5955, 5.2785, 2.3477, 1.3336, 1.6203, 1.3480,\n",
       "                      1.4118, 0.8483, 0.6699, 0.9398, 0.8271, 0.7226, 0.7565, 1.0519, 1.2113,\n",
       "                      1.8155, 0.3959, 1.0305, 0.8969, 0.7185, 3.1297, 1.6001, 0.6866, 2.1363,\n",
       "                      0.3895, 0.8303, 2.5366, 0.8308, 0.5056, 0.3174, 0.8539, 0.4772, 1.0374,\n",
       "                      0.6802, 2.0331, 2.2444, 1.0795, 0.7960, 1.0318, 0.4611, 1.2426, 0.4374,\n",
       "                      0.8311, 1.2344, 0.6165, 0.6710, 3.0550, 0.5527, 0.9812, 1.5501, 0.2038,\n",
       "                      1.4408, 0.6944, 2.1058, 0.5078, 1.8245, 0.8506, 0.9721, 1.6481, 1.5638,\n",
       "                      2.2414, 1.0286, 0.8472, 0.9941, 0.7813, 1.9675, 1.2610, 0.3666, 0.7080,\n",
       "                      0.2565, 0.9909, 0.4989, 1.7938, 1.0263, 1.3762, 1.4408, 0.8177, 1.7748,\n",
       "                      1.3785, 1.3096, 1.4712, 1.0393, 0.9660, 1.6180, 0.5088, 0.8259, 1.4418,\n",
       "                      0.6528, 1.2384, 0.9247, 1.0125, 0.6686, 1.1552, 0.9944, 1.5650, 2.6505,\n",
       "                      1.1094, 3.0115, 1.1688, 1.3315, 0.6257, 1.6894, 1.1378, 1.1227],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.15.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.15.2.clip_val', tensor([6.0943], device='cuda:0')),\n",
       "             ('module.model.16.0.weight', tensor([[[[ 0.0191]],\n",
       "              \n",
       "                       [[ 0.0472]],\n",
       "              \n",
       "                       [[ 0.0211]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0255]],\n",
       "              \n",
       "                       [[ 0.0113]],\n",
       "              \n",
       "                       [[-0.0023]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0518]],\n",
       "              \n",
       "                       [[ 0.0869]],\n",
       "              \n",
       "                       [[ 0.0239]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0211]],\n",
       "              \n",
       "                       [[ 0.0210]],\n",
       "              \n",
       "                       [[-0.1137]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0097]],\n",
       "              \n",
       "                       [[-0.0436]],\n",
       "              \n",
       "                       [[-0.0166]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0067]],\n",
       "              \n",
       "                       [[ 0.1095]],\n",
       "              \n",
       "                       [[ 0.0307]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0242]],\n",
       "              \n",
       "                       [[-0.0787]],\n",
       "              \n",
       "                       [[-0.0404]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0817]],\n",
       "              \n",
       "                       [[ 0.0486]],\n",
       "              \n",
       "                       [[-0.0250]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0122]],\n",
       "              \n",
       "                       [[ 0.0597]],\n",
       "              \n",
       "                       [[ 0.0038]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0358]],\n",
       "              \n",
       "                       [[-0.1123]],\n",
       "              \n",
       "                       [[ 0.0698]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0126]],\n",
       "              \n",
       "                       [[ 0.0511]],\n",
       "              \n",
       "                       [[-0.0851]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0151]],\n",
       "              \n",
       "                       [[-0.0896]],\n",
       "              \n",
       "                       [[-0.0859]]]], device='cuda:0')),\n",
       "             ('module.model.16.1.weight',\n",
       "              tensor([1.0302, 0.9900, 0.8828, 0.8872, 0.7720, 1.1528, 0.9240, 1.0964, 0.7841,\n",
       "                      0.9547, 0.6962, 1.0005, 0.8823, 0.8597, 1.0387, 0.4037, 1.1350, 0.9640,\n",
       "                      1.1065, 1.0111, 1.0666, 1.0467, 0.9454, 0.8680, 0.9675, 0.8020, 1.0140,\n",
       "                      0.5621, 1.0354, 0.9457, 1.0312, 1.0070, 1.0319, 0.6993, 1.0120, 1.0583,\n",
       "                      1.0470, 1.0437, 1.0562, 1.0535, 0.9006, 0.9862, 0.9789, 0.9698, 0.3762,\n",
       "                      0.9738, 1.0188, 0.8296, 1.0271, 1.0062, 0.9126, 1.0345, 0.9085, 0.6821,\n",
       "                      0.3584, 0.3237, 1.0956, 0.8445, 0.8615, 1.0097, 1.1147, 1.0593, 0.8639,\n",
       "                      0.7179, 1.0399, 0.4626, 1.0114, 0.9748, 1.0428, 0.9471, 1.0993, 0.6867,\n",
       "                      0.9909, 1.0451, 1.0666, 1.0266, 1.2719, 0.9266, 1.0790, 0.8448, 0.4835,\n",
       "                      0.9443, 0.9964, 0.7604, 0.9272, 0.6804, 0.8524, 0.7529, 0.5180, 0.7768,\n",
       "                      1.0094, 1.0011, 0.9502, 0.9925, 1.0885, 1.1227, 0.9792, 1.0110, 1.0695,\n",
       "                      1.0384, 0.3889, 1.0250, 0.9127, 0.7978, 0.9813, 0.9950, 1.0295, 0.8512,\n",
       "                      0.9955, 1.0321, 0.8452, 1.1063, 0.9981, 0.9882, 0.9574, 0.7033, 0.6011,\n",
       "                      0.6765, 0.9824, 0.9857, 0.8603, 1.0805, 1.0375, 0.9721, 0.9061, 0.6218,\n",
       "                      1.0722, 1.0672, 0.9058, 0.4655, 1.0134, 0.6619, 1.0574, 1.0703, 0.9041,\n",
       "                      1.0816, 1.0736, 1.0382, 0.9826, 1.0088, 1.0421, 0.9580, 0.5739, 0.9192,\n",
       "                      1.0244, 1.1042, 0.8412, 0.8862, 0.8195, 0.9713, 0.3958, 0.9068, 0.7619,\n",
       "                      1.0388, 0.9819, 0.9814, 1.1759, 1.0250, 0.9058, 0.9912, 1.0356, 0.3550,\n",
       "                      0.8023, 1.0485, 0.9469, 1.0149, 0.5590, 0.9941, 0.9175, 0.5452, 0.5182,\n",
       "                      0.8768, 1.0327, 1.0783, 1.0353, 1.0970, 1.0817, 1.0746, 0.7434, 1.0007,\n",
       "                      0.2921, 1.0165, 0.8459, 0.9549, 0.8731, 0.8940, 1.0138, 1.0774, 1.1250,\n",
       "                      1.1328, 0.9600, 1.0173, 1.0178, 0.5421, 1.0451, 1.1499, 0.9844, 1.1490,\n",
       "                      1.0026, 0.9327, 0.6671, 1.0135, 1.0726, 1.1071, 0.7182, 0.9942, 1.0290,\n",
       "                      0.9858, 0.3807, 1.0814, 0.5615, 0.6279, 1.0282, 1.0057, 0.9964, 0.9731,\n",
       "                      1.0004, 0.9606, 0.9389, 0.8813, 0.9527, 0.3944, 0.9822, 1.0798, 1.0838,\n",
       "                      0.7687, 0.5181, 0.6503, 1.0048, 1.0538, 0.9186, 0.9220, 0.8981, 1.0385,\n",
       "                      0.6834, 0.5789, 0.8597, 0.6641, 0.5949, 1.0664, 0.6609, 0.3408, 0.7307,\n",
       "                      1.0290, 0.8949, 1.0963, 0.9688, 1.0031, 0.6059, 0.9661, 1.0527, 1.1196,\n",
       "                      0.7205, 1.1108, 0.9687, 1.0350, 1.0106, 1.0604, 0.9868, 1.0410, 1.0038,\n",
       "                      1.0171, 1.0433, 1.1505, 0.9596, 0.6368, 0.9575, 0.9923, 1.1075, 0.7569,\n",
       "                      1.0476, 1.1024, 0.8258, 1.0213, 0.4021, 1.0586, 0.5219, 0.9285, 0.9984,\n",
       "                      0.5444, 1.0895, 0.8494, 1.0545, 0.9116, 1.0896, 0.9948, 1.0102, 1.0388,\n",
       "                      0.9843, 1.0855, 0.6665, 0.9196, 1.0401, 0.9583, 1.2539, 0.9187, 1.0192,\n",
       "                      0.5611, 1.0056, 0.7371, 0.9678, 0.9876, 1.0782, 0.7025, 0.9218, 1.1360,\n",
       "                      1.1824, 1.0031, 0.9336, 0.6545, 0.7909, 0.8447, 1.0652, 0.8161, 1.0210,\n",
       "                      1.1140, 1.1572, 0.9964, 0.3637, 1.0496, 1.0001, 1.0401, 0.9559, 1.2163,\n",
       "                      0.8445, 1.0527, 0.9506, 0.9004, 0.9485, 1.0756, 0.9725, 1.0730, 0.6291,\n",
       "                      0.4800, 1.1042, 0.7907, 0.5788, 1.0000, 1.0264, 0.9520, 1.0202, 1.0165,\n",
       "                      0.8695, 1.0752, 0.8466, 1.0795, 0.9958, 0.9839, 0.9113, 1.0427, 0.4844,\n",
       "                      0.5797, 1.0187, 1.0564, 0.8225, 1.0061, 1.0109, 0.9908, 0.9736, 0.9921,\n",
       "                      0.9645, 1.0295, 0.9474, 0.8011, 0.8485, 1.0996, 0.8972, 0.9831, 0.7160,\n",
       "                      0.9658, 1.0804, 1.0855, 1.0730, 0.7759, 1.0643, 0.9847, 1.0767, 1.0606,\n",
       "                      0.9555, 0.9855, 1.0156, 0.9553, 1.1566, 0.7019, 1.1071, 1.0530, 1.0260,\n",
       "                      0.5550, 0.9582, 0.9788, 1.0387, 0.9049, 0.8198, 0.9929, 1.0338, 1.0864,\n",
       "                      0.8444, 0.5453, 1.0052, 1.0574, 0.9836, 0.6112, 1.0033, 0.9601, 1.0944,\n",
       "                      1.0651, 0.9847, 0.6578, 1.0225, 1.0584, 1.0450, 1.0027, 1.0785, 0.9269,\n",
       "                      0.9978, 1.0657, 1.0654, 0.7443, 0.9060, 1.0402, 1.0703, 1.0066, 1.0114,\n",
       "                      0.8789, 0.9163, 0.8998, 0.9971, 0.5054, 0.9126, 0.9606, 1.0230, 0.9539,\n",
       "                      1.0423, 0.8906, 0.9396, 0.7941, 0.9150, 0.9325, 0.8032, 0.9416, 0.8563,\n",
       "                      0.6038, 0.7872, 1.0174, 1.0562, 0.9844, 0.9780, 1.0835, 0.9531, 0.8896,\n",
       "                      0.6025, 1.0762, 0.9222, 1.0548, 0.4912, 0.6793, 1.1358, 0.8546, 0.9805,\n",
       "                      1.0800, 0.8517, 0.5964, 1.0080, 0.9400, 1.0482, 0.6155, 1.0037, 1.0406,\n",
       "                      1.0667, 0.9114, 0.9857, 1.0819, 0.8198, 0.9730, 1.0342, 1.0543, 1.0395,\n",
       "                      1.0973, 0.6272, 1.0134, 0.7524, 0.7261, 0.5955, 1.0451, 0.8285, 1.0455,\n",
       "                      0.6785, 1.1354, 1.0435, 1.1314, 1.0761, 1.0407, 1.0199, 0.8279, 0.7845,\n",
       "                      1.0585, 1.0195, 0.4331, 0.8909, 1.0406, 1.0454, 0.9757, 1.0410, 0.7391,\n",
       "                      0.9028, 0.6860, 0.8104, 1.0441, 1.0227, 0.5114, 0.9183, 0.7529],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.16.1.bias',\n",
       "              tensor([-0.4279, -0.4100, -0.5283,  0.5849,  0.7228,  0.0816, -0.4792,  0.0870,\n",
       "                       0.7640, -0.6330,  0.9759, -0.0366, -0.5953, -0.7130,  0.0578,  1.0217,\n",
       "                       0.1576,  0.7080, -0.1500, -0.2344,  0.1634, -0.3370, -0.2186, -0.3904,\n",
       "                       0.5995,  0.6933, -0.4022,  0.9970, -0.3564, -0.5356,  0.0316,  0.2580,\n",
       "                       0.1462,  0.7767,  0.2659,  0.1310,  0.0708,  0.1089,  0.1416,  0.3530,\n",
       "                       0.5766,  0.0154,  0.1819,  0.5958,  1.2599, -0.5578,  0.0599,  0.6539,\n",
       "                      -0.0720, -0.0692,  0.5785, -0.3664, -0.7045,  0.8660,  1.0334,  1.0072,\n",
       "                      -0.1951, -0.6872,  0.5991,  0.1472,  0.2168, -0.2844, -0.7755,  0.8173,\n",
       "                      -0.2257,  0.9955,  0.2771, -0.4410,  0.0567,  0.5951,  0.3815,  0.8041,\n",
       "                      -0.3911, -0.0993, -0.0644, -0.1146,  0.5665,  0.8320,  0.2578,  0.8203,\n",
       "                       1.1494, -0.5898, -0.3140,  0.8187, -0.6175,  0.8406, -0.7739,  0.7467,\n",
       "                       1.1290,  0.7014, -0.2969, -0.4539,  0.5495,  0.2930,  0.1550,  0.0813,\n",
       "                      -0.5131, -0.2757, -0.1704, -0.0263,  1.1809, -0.2783,  0.7078,  0.6954,\n",
       "                      -0.2907, -0.3353, -0.3841, -0.2337, -0.6130, -0.1244,  0.6703, -0.0197,\n",
       "                       0.3153, -0.2378, -0.3377,  0.8173,  1.0853,  0.8422,  0.2031,  0.4296,\n",
       "                      -0.8736,  0.0346,  0.1218,  0.4493,  0.5486,  0.9220, -0.0319, -0.3606,\n",
       "                       0.5490,  1.0506, -0.3502,  0.9389,  0.0485, -0.1853, -0.7204,  0.1460,\n",
       "                       0.0040, -0.4705,  0.0758, -0.2123, -0.5630,  0.4539,  0.9696,  0.4990,\n",
       "                       0.2345,  0.1421, -0.8913,  0.6396,  0.7094,  0.3974,  0.9797,  0.2675,\n",
       "                       0.6791, -0.2352, -0.7000, -0.2577, -0.1745, -0.3850, -0.7473,  0.0546,\n",
       "                      -0.0180,  1.0323, -0.6648, -0.1935,  0.1023, -0.1908,  0.9041, -0.3260,\n",
       "                       0.6399,  1.0627,  0.9733, -0.8737,  0.0996,  0.0507,  0.1370, -0.5682,\n",
       "                      -0.3050, -0.3233,  0.7780,  0.2937,  1.0537, -0.2224,  0.6021, -0.6316,\n",
       "                       0.7012,  0.5779, -0.0215,  0.3494,  0.0788,  0.5410,  0.4502,  0.1427,\n",
       "                      -0.1617,  0.9453,  0.2364,  0.4322, -0.3132,  0.2422, -0.3199, -0.5552,\n",
       "                       0.9625, -0.3322, -0.0464, -0.4272,  0.8151,  0.2696,  0.2427, -0.4786,\n",
       "                       1.0210, -0.0259,  0.9264,  0.8807,  0.0564, -0.5661, -0.3585,  0.1814,\n",
       "                      -0.4421, -0.3864, -0.4916, -0.6434, -0.3267,  1.0380, -0.5111,  0.0158,\n",
       "                      -0.1117,  0.8067,  0.9523,  0.8711,  0.0889, -0.4536, -0.4827,  0.7057,\n",
       "                      -0.5086,  0.1515,  0.7733,  0.8923,  0.6082,  0.8340,  0.9031,  0.0449,\n",
       "                       1.3097,  1.0328,  0.7687,  0.0845, -0.7327,  0.2562,  0.4229, -0.3074,\n",
       "                       1.1549,  0.3687, -0.1931, -0.0828,  0.8985, -0.1523,  0.5519,  0.4291,\n",
       "                       0.2247, -0.1183, -0.3931, -0.2220,  0.1181, -0.4254,  0.2134,  0.0390,\n",
       "                      -0.2311,  0.9218, -0.6981,  0.7415, -0.4483,  0.7720,  0.2827,  0.4155,\n",
       "                      -0.6122,  0.0539,  1.0191, -0.2882,  0.9529,  0.5721, -0.5604,  1.0185,\n",
       "                      -0.0224,  0.7481, -0.1692,  0.4984,  0.6482, -0.3658, -0.5870, -0.3103,\n",
       "                      -0.3833, -0.0196,  0.9732, -0.5725, -0.3551,  0.5355,  0.0422,  0.0796,\n",
       "                      -0.4400,  0.9011,  0.2773,  0.8232,  0.3320, -0.0568, -0.2292,  0.8680,\n",
       "                      -0.6666,  0.1220,  0.0219,  0.0514,  0.4405,  0.8570,  0.7341,  0.6074,\n",
       "                      -0.3252,  0.6565,  0.1708,  0.2107, -0.0988,  0.5002,  0.9946, -0.1175,\n",
       "                       0.7649,  0.1691,  0.4559,  0.6554,  0.6568, -0.4341,  0.4347,  0.6575,\n",
       "                      -0.2017,  0.3502,  0.4888, -0.1146,  0.9880,  1.0512, -0.1286,  0.6879,\n",
       "                       0.8775, -0.2139, -0.3362,  0.4590,  0.1619,  0.2534,  0.9107,  0.1013,\n",
       "                       0.6039, -0.3867, -0.2865, -0.3918, -0.0196, -0.1570,  1.1532,  0.8849,\n",
       "                       0.4490, -0.0524, -0.4590, -0.4072, -0.4749, -0.3495, -0.0525,  0.4907,\n",
       "                       0.2396,  0.0788,  0.6806, -0.7562, -0.7186, -0.1097, -0.5443,  0.4141,\n",
       "                       0.9391,  0.4721,  0.3905, -0.4355,  0.0175,  0.7899, -0.1390, -0.5575,\n",
       "                       0.2129,  0.0851, -0.5093,  0.3817, -0.1606,  0.4692, -0.0965,  0.8178,\n",
       "                      -0.3741,  0.0662,  0.1199,  0.9942,  0.4096, -0.0327, -0.1938,  0.2851,\n",
       "                       0.6992, -0.2238,  0.2618, -0.5307,  0.6519,  0.9848, -0.6044, -0.1062,\n",
       "                      -0.4483,  0.8942, -0.5812,  0.3951,  0.0360, -0.3737,  0.1278,  0.7996,\n",
       "                       0.2343,  0.1303, -0.4372, -0.0902, -0.0587, -0.5223,  0.4156, -0.0325,\n",
       "                      -0.2410,  0.7454,  0.5808, -0.2867, -0.0041, -0.1730, -0.3096,  0.3236,\n",
       "                      -0.7822,  0.5160, -0.4483,  1.0432,  0.5034, -0.4176,  0.3481,  0.4596,\n",
       "                       0.5297,  0.6098,  0.4823,  0.7822,  0.5832, -0.4623,  0.7232,  0.5643,\n",
       "                       0.6212,  0.9252,  0.9615, -0.0337,  0.0920,  0.4592,  0.3661, -0.1246,\n",
       "                      -0.5952,  0.3360,  0.9281, -0.0130,  0.5434, -0.2857,  1.0625,  0.7774,\n",
       "                      -0.5041, -0.7615, -0.6681,  0.0441, -0.6505,  1.1218, -0.1981,  0.5159,\n",
       "                       0.2917,  0.9042,  0.3570,  0.2090,  0.1127,  0.5775,  0.3628, -0.0214,\n",
       "                       0.6775, -0.4480, -0.0519, -0.1426, -0.2806,  0.4548,  0.8740, -0.1960,\n",
       "                       0.9439, -0.5151,  0.8929, -0.2784,  0.7769, -0.1419,  0.7975, -0.3439,\n",
       "                      -0.1872, -0.0274,  0.0228, -0.0896, -0.1592,  0.7050, -0.7308, -0.1817,\n",
       "                       0.3383,  0.9894,  0.5782, -0.0905,  0.1584, -0.3948,  0.1033,  0.8184,\n",
       "                       0.5861,  0.9283,  0.6108,  0.0917, -0.2301,  1.0436,  0.5087, -0.2454],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.16.1.running_mean',\n",
       "              tensor([-5.8909e-01,  9.3991e-01, -6.9753e-01, -2.4099e-01, -5.5888e-01,\n",
       "                       2.3486e-01,  2.4033e-01,  1.2992e-01,  5.4241e-01, -9.8314e-01,\n",
       "                       2.3089e+00,  7.6360e-01, -1.3103e-01,  5.4432e-01, -1.1059e+00,\n",
       "                      -6.9519e-01,  2.3468e-01,  7.9843e-01,  1.1428e+00,  8.7526e-01,\n",
       "                       1.6625e-01,  3.8840e-01,  5.0432e-01, -9.5672e-01, -9.8959e-02,\n",
       "                      -3.0883e-01,  1.4052e+00, -2.3880e-03,  5.3886e-01,  1.8429e-01,\n",
       "                       1.0704e-01,  2.0488e+00,  1.2251e+00,  2.2399e-01, -5.6338e-01,\n",
       "                      -7.0443e-01,  1.5703e+00,  8.0336e-01,  1.1565e-01, -3.1211e-01,\n",
       "                       1.3906e+00, -7.3663e-01,  1.9907e+00,  1.6202e+00, -1.1890e+00,\n",
       "                      -4.0807e-01,  1.1425e+00,  4.0681e-01,  9.5736e-01, -8.5509e-01,\n",
       "                       7.5429e-01,  2.6135e-01, -4.7979e-01, -1.7478e+00,  2.6866e+00,\n",
       "                      -6.3108e-01,  9.4383e-01,  5.3030e-02,  3.0762e-01,  1.7100e-01,\n",
       "                       1.5369e+00,  1.5668e+00,  1.0493e+00,  1.0112e+00,  1.0086e+00,\n",
       "                       2.2802e+00,  3.0009e-01, -1.2734e+00,  2.0566e+00,  5.7947e-01,\n",
       "                       1.1673e+00,  5.0727e-02, -5.5084e-02,  4.5146e-01,  6.1535e-01,\n",
       "                       7.4317e-01, -1.5646e-01,  5.8417e-02,  3.2677e-01,  9.4051e-01,\n",
       "                       1.1408e+00,  4.2795e-01, -1.8878e-01,  1.0788e+00,  2.0542e-01,\n",
       "                       4.6560e-01, -1.1166e+00, -5.0846e-01,  6.7958e-01, -3.1672e-01,\n",
       "                      -8.1388e-01, -4.8978e-01, -5.5795e-02,  7.3919e-01, -4.9028e-01,\n",
       "                       1.9359e+00,  1.2489e+00, -5.1705e-01,  8.3429e-01,  8.2647e-01,\n",
       "                       2.3468e-01, -4.6669e-01,  6.3069e-01,  3.5430e-01, -2.5656e-01,\n",
       "                       5.1234e-02,  4.2904e-01,  4.1190e-02, -9.7349e-01,  9.5813e-01,\n",
       "                      -2.6269e-01,  2.1026e-02,  6.5997e-01, -1.3626e+00, -6.4901e-01,\n",
       "                       1.4316e+00,  1.4462e+00, -4.1193e-01,  1.8266e-01,  9.9181e-01,\n",
       "                      -7.5982e-01,  1.5774e+00,  1.3486e+00,  8.5403e-01,  1.3871e+00,\n",
       "                       2.1103e-01, -1.0197e-01, -8.3965e-01,  3.4471e-01, -1.3181e+00,\n",
       "                      -4.2602e-01,  1.4381e+00,  1.2676e-01, -2.2339e-01, -7.6625e-01,\n",
       "                      -2.2277e-01,  6.3805e-01,  2.9789e-01, -3.9887e-01, -3.5756e-01,\n",
       "                      -4.1893e-01,  1.2882e+00,  1.1488e+00,  4.6340e-01, -7.9931e-01,\n",
       "                       2.0518e-01,  2.0151e+00, -7.4551e-01,  8.8898e-02, -6.4066e-01,\n",
       "                      -3.4010e-01, -3.5612e-01, -1.7360e+00, -3.6809e-02, -6.3172e-01,\n",
       "                      -2.6419e-01, -2.1569e-04,  5.0641e-01,  1.4133e+00,  6.9506e-01,\n",
       "                      -1.7186e-01,  1.0870e+00,  1.3339e-01,  1.5097e+00,  8.8178e-01,\n",
       "                      -1.1083e+00, -1.0773e+00,  1.2015e-01,  2.4805e+00,  1.8500e+00,\n",
       "                      -8.7230e-01, -1.8878e+00,  8.6661e-01,  1.5517e+00,  6.4181e-01,\n",
       "                       2.4481e-01, -5.3534e-01, -7.5364e-01,  4.4545e-01,  1.4883e-01,\n",
       "                      -1.9559e+00,  6.2154e-01,  9.4821e-02, -2.4005e-02,  8.5917e-01,\n",
       "                      -2.3111e-01,  6.0739e-01,  1.5409e+00,  3.6385e-01,  6.2603e-01,\n",
       "                      -2.8628e-01,  2.3104e+00,  1.2732e+00,  3.6184e-01, -7.7559e-02,\n",
       "                      -7.6663e-01, -1.4436e+00,  1.2976e-01,  1.9154e-01,  3.4765e-01,\n",
       "                      -1.8745e+00,  5.2964e-01,  3.3602e-01, -3.1863e-01,  1.7129e-01,\n",
       "                      -5.2044e-01, -4.1571e-01, -8.1423e-02, -1.3014e+00,  1.3258e+00,\n",
       "                      -6.4451e-01, -2.9222e-01,  9.7518e-01, -3.9711e-01, -2.4280e-01,\n",
       "                       3.5329e-01,  8.5921e-01, -1.0811e+00, -1.7068e-01,  1.2397e+00,\n",
       "                      -6.2621e-01, -4.8359e-01, -4.6716e-01,  4.2397e-01, -1.1438e+00,\n",
       "                       1.6112e+00,  5.8943e-01, -1.0914e+00, -1.1303e+00,  1.3690e-01,\n",
       "                      -1.7364e-01,  8.8698e-01,  4.9448e-02, -3.1466e-01,  7.8992e-01,\n",
       "                      -3.5437e-01, -7.2907e-01, -1.2629e-01, -5.7146e-01,  1.3161e+00,\n",
       "                       6.6250e-01, -7.7758e-01,  7.2898e-01,  1.4767e+00, -1.1599e+00,\n",
       "                      -2.0508e+00,  4.4520e-01,  5.0868e-02,  1.6068e-01, -1.5714e+00,\n",
       "                       2.5515e-01, -1.0100e+00,  8.3952e-01,  4.2132e-01, -5.5073e-02,\n",
       "                      -1.0028e+00, -5.4392e-01,  3.8140e-02, -3.4266e-01,  6.3525e-01,\n",
       "                       1.7906e+00, -6.0928e-01, -6.9671e-01,  2.7396e-01, -8.4288e-01,\n",
       "                       6.5707e-01, -8.3832e-01,  3.2738e+00,  3.5204e-01,  1.1793e+00,\n",
       "                       2.1821e+00,  2.2141e+00, -1.2442e-01,  7.8602e-01,  2.9445e-01,\n",
       "                      -1.9069e-01,  1.0822e+00,  8.5561e-01,  1.9610e-01,  9.9571e-01,\n",
       "                       1.0393e+00, -3.1602e-01, -1.6921e-01,  4.5008e-01,  9.1146e-01,\n",
       "                       1.1895e+00,  1.0280e+00,  8.8049e-01,  3.0355e-01,  1.0671e-01,\n",
       "                       4.2519e-01, -1.1460e+00, -7.3731e-02,  5.9981e-01, -1.0626e-01,\n",
       "                      -3.5275e-01,  2.2311e-01, -6.9017e-01, -5.0255e-01,  3.9279e-01,\n",
       "                       1.5430e+00,  3.7649e-01, -7.1758e-01, -5.9238e-01, -4.6102e-01,\n",
       "                      -2.8857e-01, -2.4130e-01,  2.2445e+00, -6.0785e-01, -4.8406e-01,\n",
       "                       1.3731e+00, -4.8287e-01,  3.5873e-01,  1.2686e+00, -6.2768e-01,\n",
       "                      -2.9729e-01,  5.4968e-02,  6.3814e-01,  1.0567e+00, -5.0597e-01,\n",
       "                       1.1432e+00, -6.0931e-01,  7.9663e-01,  3.1936e-01,  1.0921e+00,\n",
       "                       1.4975e-01, -9.2314e-01,  2.3766e-01,  1.5233e+00, -7.3137e-01,\n",
       "                       3.5165e-01, -1.2673e+00,  1.4669e+00,  4.7768e-01, -3.6049e-01,\n",
       "                       1.9813e-01,  6.2006e-01,  7.3851e-01,  4.2388e-01,  7.7023e-01,\n",
       "                       1.0037e+00, -8.7081e-03, -4.7290e-01,  1.6895e-01,  6.6655e-01,\n",
       "                       1.1721e-01,  1.4977e+00, -2.6183e-01,  1.6534e-01,  1.5367e+00,\n",
       "                      -3.2490e-01, -4.0348e-01,  1.3267e-01, -5.8892e-01, -1.4569e-01,\n",
       "                       6.4104e-01, -7.6895e-01, -2.4935e-01, -6.3044e-01, -1.2368e-01,\n",
       "                      -1.2521e+00,  3.9791e-02,  1.2308e+00, -1.1684e+00, -1.5160e+00,\n",
       "                       3.0120e-01, -4.0668e-01,  5.0051e-01,  9.3373e-02, -2.8038e-01,\n",
       "                       7.8546e-01,  5.6048e-01,  8.7725e-01, -1.8694e-01,  1.6028e+00,\n",
       "                       1.0354e+00,  3.7885e-01,  2.8008e-03, -2.2817e-01,  4.8356e-01,\n",
       "                       9.9273e-01,  9.3001e-01,  1.2049e-01,  1.2957e+00, -1.2691e+00,\n",
       "                       2.0860e-01,  1.8851e+00,  1.7018e-01, -8.9553e-01,  3.6931e-01,\n",
       "                       6.4345e-01, -1.0487e+00,  9.6638e-01,  1.1662e+00,  6.1732e-01,\n",
       "                       1.3591e-01,  9.6775e-01, -1.5315e+00,  7.9016e-01,  2.4743e+00,\n",
       "                      -1.8157e-01,  4.4877e-01, -1.7762e-01,  3.7214e-01,  3.3972e-01,\n",
       "                      -8.1332e-02,  2.0830e+00,  6.3058e-01, -4.6576e-01, -2.6419e-01,\n",
       "                       6.1814e-01,  8.6205e-01, -1.0743e-01, -1.5544e+00,  8.7530e-01,\n",
       "                      -5.6656e-01,  4.6135e-01, -5.7422e-01,  1.0454e+00, -3.9474e-01,\n",
       "                      -2.5881e-01, -7.3302e-01, -1.2338e+00, -9.9524e-01,  3.0528e-01,\n",
       "                       2.5686e+00,  2.8778e-01, -1.4788e+00,  5.1200e-01, -4.2949e-01,\n",
       "                       3.3103e-01,  7.6156e-01, -1.0378e-01,  2.4111e-01,  1.0625e-01,\n",
       "                       8.5535e-01,  9.7862e-01, -4.2198e-01, -2.2984e-01,  1.6601e-01,\n",
       "                       1.1730e+00,  1.5189e+00,  1.1822e+00, -3.1320e-01, -2.4355e-02,\n",
       "                       1.5378e+00, -1.4497e-01,  2.7574e-01, -1.4263e+00,  1.3805e-01,\n",
       "                       9.8665e-01,  8.5807e-01,  6.9303e-01, -1.2299e+00,  9.9502e-02,\n",
       "                       8.1477e-01, -5.1616e-01,  2.9107e-01,  8.9566e-01,  1.0425e+00,\n",
       "                      -1.9250e+00,  7.9893e-01, -2.8727e-01, -1.1617e+00,  9.1640e-02,\n",
       "                       1.5770e-01,  2.6350e-01,  3.4570e-01,  4.4780e-01,  1.4877e+00,\n",
       "                       4.3357e-01, -1.0008e+00,  2.4576e-01, -9.2949e-01, -1.1901e-01,\n",
       "                       8.6132e-01, -6.2785e-02, -9.3309e-01,  9.1474e-01,  1.1019e+00,\n",
       "                      -3.7002e-01, -9.6885e-01,  2.8463e-01, -4.0343e-01,  3.2429e-01,\n",
       "                      -7.4691e-01,  9.6413e-02,  5.3596e-01, -8.3891e-02,  1.3407e+00,\n",
       "                      -9.4360e-01,  2.3179e-01,  2.1518e-01,  7.7252e-01, -2.3189e-01,\n",
       "                      -6.2211e-01,  8.7629e-01,  2.9127e-01,  8.5533e-01, -1.4085e-01,\n",
       "                       2.9016e-01, -9.7184e-01, -5.1596e-02,  1.9128e+00, -1.3916e+00,\n",
       "                       4.8889e-01, -2.7690e-01, -8.6214e-01,  1.0489e-01, -6.7656e-01,\n",
       "                       7.8489e-01, -1.2338e+00], device='cuda:0')),\n",
       "             ('module.model.16.1.running_var',\n",
       "              tensor([0.5594, 0.5118, 0.3692, 0.4657, 0.3524, 0.9605, 0.6696, 0.7493, 0.6877,\n",
       "                      0.5846, 0.6458, 0.5490, 0.4570, 0.5379, 0.4510, 0.6556, 1.1759, 0.6336,\n",
       "                      0.8598, 0.3955, 0.6561, 0.7161, 0.5610, 0.4873, 0.7244, 0.3738, 0.7200,\n",
       "                      0.5473, 0.6761, 0.6450, 0.5992, 0.3744, 0.3727, 0.3142, 0.5434, 0.3580,\n",
       "                      0.5851, 0.6384, 0.7810, 0.5776, 0.4888, 0.5462, 0.6876, 0.9207, 0.7233,\n",
       "                      0.5170, 0.5949, 0.4013, 0.6795, 0.3952, 0.5716, 0.6490, 0.7797, 0.4031,\n",
       "                      0.6264, 0.5681, 0.7433, 0.5060, 0.3970, 0.7842, 0.8729, 0.6129, 0.5399,\n",
       "                      0.5167, 0.6390, 0.4896, 0.4660, 0.4396, 0.6320, 0.5670, 0.7201, 0.4787,\n",
       "                      0.6408, 0.6398, 0.8343, 0.7463, 1.1991, 0.4693, 0.8437, 0.5903, 0.4420,\n",
       "                      0.7695, 0.5318, 0.4423, 0.4205, 0.5737, 0.5459, 0.6252, 0.7516, 0.4110,\n",
       "                      0.6771, 0.6850, 0.5128, 0.6260, 0.5937, 0.7339, 0.5662, 0.5094, 0.4462,\n",
       "                      0.7983, 0.7648, 0.5352, 0.6734, 0.6824, 0.4375, 0.6478, 0.5493, 0.5249,\n",
       "                      0.6271, 0.6684, 0.5945, 0.4504, 1.2884, 0.6068, 0.5245, 0.5898, 1.9443,\n",
       "                      0.4314, 0.9648, 0.7052, 0.4696, 0.9042, 0.8082, 0.4829, 0.6320, 0.7797,\n",
       "                      0.6733, 0.9610, 0.7348, 0.8296, 0.4581, 0.5004, 0.7629, 0.8331, 0.5857,\n",
       "                      0.8067, 0.7049, 0.5574, 0.6661, 0.4359, 0.6680, 0.5119, 1.0320, 0.4042,\n",
       "                      0.5396, 0.6567, 0.6550, 1.1600, 0.3339, 0.7951, 0.6812, 0.7406, 0.6801,\n",
       "                      0.5494, 0.3960, 0.5576, 1.0200, 0.8727, 0.6096, 0.5710, 0.5875, 0.4054,\n",
       "                      0.4765, 0.8204, 0.9937, 0.3925, 0.6167, 0.6815, 0.9275, 0.5264, 0.8070,\n",
       "                      0.6424, 0.6922, 0.4530, 0.5382, 0.5785, 1.1032, 0.5369, 0.5273, 0.7395,\n",
       "                      0.8281, 0.5740, 0.5003, 0.7435, 0.8929, 0.6634, 0.6161, 0.5311, 0.4629,\n",
       "                      0.8373, 0.8494, 0.5449, 0.6062, 0.4446, 0.5668, 0.5052, 0.6043, 0.6086,\n",
       "                      0.6213, 0.5318, 0.5337, 0.7079, 0.8525, 0.6576, 0.4837, 0.6190, 0.5700,\n",
       "                      0.5332, 1.0581, 0.8171, 0.4011, 0.4786, 0.5958, 0.6612, 0.4206, 0.6446,\n",
       "                      0.5705, 0.4349, 0.4857, 0.6911, 0.5455, 0.7304, 0.5138, 1.1358, 0.4970,\n",
       "                      0.5086, 0.4384, 0.4010, 0.6516, 0.6993, 0.5156, 0.8636, 0.6548, 0.5867,\n",
       "                      0.3202, 0.6787, 0.4527, 0.5866, 0.5270, 0.6780, 0.2406, 0.5628, 1.0092,\n",
       "                      0.6846, 0.5198, 1.4299, 0.7127, 0.4517, 0.4088, 0.3493, 0.5924, 0.5926,\n",
       "                      1.6047, 1.0577, 0.7690, 0.7631, 0.3624, 0.4532, 0.8573, 0.7239, 0.7058,\n",
       "                      0.7190, 0.9418, 0.6758, 0.4434, 0.2875, 0.5707, 1.1152, 0.6055, 0.7112,\n",
       "                      1.5441, 0.8875, 0.4270, 0.3880, 0.2925, 0.5413, 1.0058, 0.5930, 0.8922,\n",
       "                      0.9602, 0.7507, 0.9356, 0.5164, 0.4908, 1.0792, 0.6340, 0.5648, 0.7716,\n",
       "                      0.5514, 0.4734, 0.3248, 0.5319, 0.5215, 1.0240, 0.7021, 0.6256, 0.5088,\n",
       "                      0.4889, 0.4274, 0.3823, 0.9076, 0.5640, 0.7730, 0.6120, 0.5480, 0.7874,\n",
       "                      0.8227, 0.7104, 0.4123, 0.4096, 0.4512, 0.4585, 0.4396, 0.4365, 0.4735,\n",
       "                      0.5478, 0.7221, 0.4907, 0.5392, 0.5250, 0.6628, 0.7023, 0.4257, 0.8845,\n",
       "                      0.5760, 0.5047, 0.3220, 0.5339, 0.7142, 0.7286, 1.3791, 0.5736, 0.7868,\n",
       "                      0.6764, 0.3823, 0.6422, 0.3552, 0.4917, 0.6501, 0.4960, 0.6508, 0.4882,\n",
       "                      1.3289, 0.7830, 0.3581, 0.7586, 0.5638, 0.5672, 1.0247, 0.5883, 0.3891,\n",
       "                      0.2940, 0.7381, 0.6145, 0.4126, 0.7329, 0.4767, 0.4336, 0.3373, 0.9817,\n",
       "                      0.5241, 0.4273, 0.9758, 0.4008, 0.4776, 0.8073, 0.3861, 1.0520, 1.2191,\n",
       "                      0.8541, 0.5994, 0.7138, 0.3999, 0.6722, 0.6711, 0.4875, 0.7097, 0.6870,\n",
       "                      0.6081, 0.6017, 0.5265, 0.9173, 0.6257, 1.0940, 0.6301, 0.7763, 0.7534,\n",
       "                      0.6180, 0.4934, 0.4110, 0.7200, 0.4448, 0.8938, 0.5236, 0.5667, 0.6820,\n",
       "                      0.6915, 1.1643, 0.6289, 0.8326, 0.5066, 0.4769, 0.5705, 0.5009, 0.6414,\n",
       "                      0.5795, 0.7187, 0.4663, 0.3384, 0.9704, 0.7261, 0.4566, 0.6389, 0.5028,\n",
       "                      0.4895, 0.9803, 0.5886, 0.4165, 0.5844, 0.6028, 0.4402, 0.5774, 0.7301,\n",
       "                      0.6569, 0.6606, 0.7849, 0.7631, 0.5288, 1.2018, 0.6577, 0.6452, 0.5044,\n",
       "                      0.7378, 1.3272, 0.5065, 0.8219, 0.5151, 0.6313, 0.7242, 0.5666, 0.6503,\n",
       "                      0.9174, 0.5228, 0.6295, 0.7477, 0.8058, 0.3878, 0.8275, 0.7042, 0.6615,\n",
       "                      0.4122, 0.5041, 0.4733, 0.7181, 0.7021, 0.4037, 1.0706, 0.4308, 0.5209,\n",
       "                      1.2221, 0.3990, 0.5879, 0.3788, 0.5791, 0.8843, 0.8282, 0.7967, 0.6916,\n",
       "                      0.5250, 0.4623, 0.5495, 0.6234, 0.4695, 0.6619, 0.3792, 0.6181, 1.5387,\n",
       "                      0.6696, 0.6410, 0.6260, 0.8915, 0.5618, 0.5263, 0.4778, 0.8711, 0.4584,\n",
       "                      0.3220, 0.7109, 0.4873, 0.8930, 0.5841, 0.5396, 0.7221, 0.4997, 0.3951,\n",
       "                      0.8614, 0.7727, 0.4477, 0.8019, 0.6866, 0.6746, 0.6541, 0.6003, 0.7821,\n",
       "                      0.7868, 0.4132, 0.5221, 0.3825, 0.5664, 0.4269, 0.3791, 0.5340],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.16.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.16.2.clip_val', tensor([5.8174], device='cuda:0')),\n",
       "             ('module.model.17.0.weight',\n",
       "              tensor([[[[-0.0791, -0.4033,  0.3749],\n",
       "                        [-0.5336, -0.0615,  1.3926],\n",
       "                        [-0.0083, -0.5224,  0.4690]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0656,  0.2334,  0.0499],\n",
       "                        [ 0.3043,  1.0500,  0.2165],\n",
       "                        [ 0.1324,  0.3729,  0.1174]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3598,  1.2715,  0.2366],\n",
       "                        [-0.1801, -0.7597, -0.0962],\n",
       "                        [-0.0223,  0.3835, -0.0307]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.8379, -0.5746, -0.6592],\n",
       "                        [ 0.0816,  0.6097, -0.0422],\n",
       "                        [-0.0475,  0.5549, -0.0389]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.6866,  0.4999,  0.3813],\n",
       "                        [-1.0431,  0.7206,  0.6214],\n",
       "                        [-0.5950, -0.4998, -0.0855]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0891,  0.2634,  0.0515],\n",
       "                        [ 0.3254,  1.5478,  0.1434],\n",
       "                        [-0.0328,  0.0981, -0.0844]]]], device='cuda:0')),\n",
       "             ('module.model.17.1.weight',\n",
       "              tensor([0.5682, 0.5662, 0.6191, 0.9790, 0.6249, 0.5941, 0.7576, 1.0101, 1.4455,\n",
       "                      0.6587, 1.3040, 0.8068, 0.5298, 0.4718, 0.9383, 1.2447, 0.8895, 1.2211,\n",
       "                      0.7830, 0.5964, 0.8224, 0.8199, 0.6283, 0.7016, 1.0691, 0.7323, 0.5750,\n",
       "                      1.1095, 0.5906, 0.5025, 0.9132, 0.5261, 0.5503, 0.5362, 0.6341, 1.1267,\n",
       "                      0.4522, 0.4455, 0.6702, 0.8297, 0.6170, 0.7581, 0.6570, 1.2335, 1.5457,\n",
       "                      0.5183, 0.6904, 0.6371, 0.6171, 0.6934, 1.0574, 0.4359, 0.5691, 0.6110,\n",
       "                      0.9251, 0.6429, 1.0024, 0.4897, 0.6107, 0.8149, 0.9692, 0.4535, 0.5692,\n",
       "                      1.1822, 0.6482, 0.7379, 1.0548, 0.7796, 0.4873, 0.8467, 0.8218, 1.0859,\n",
       "                      0.6256, 0.7913, 0.4956, 0.7312, 0.7899, 1.2194, 1.0551, 1.4558, 0.7941,\n",
       "                      0.5801, 0.5907, 1.1835, 0.4627, 1.0067, 0.5024, 0.9269, 0.8653, 0.6835,\n",
       "                      1.0494, 0.6429, 1.2289, 0.8933, 1.3997, 0.9504, 0.7040, 0.5789, 0.6638,\n",
       "                      0.7370, 1.0570, 0.7334, 1.2312, 0.7520, 0.6542, 0.7497, 0.6829, 0.6000,\n",
       "                      0.8744, 0.8429, 0.6575, 0.6820, 0.7663, 0.6488, 0.8980, 1.1876, 0.9157,\n",
       "                      0.8052, 0.9094, 0.8714, 0.7202, 0.7342, 0.7188, 0.5797, 1.0609, 1.2172,\n",
       "                      0.7545, 0.5781, 0.5658, 0.8317, 0.5465, 0.6623, 0.7338, 0.5777, 0.5492,\n",
       "                      0.8463, 1.3729, 0.6265, 0.8719, 0.5800, 0.5306, 0.7217, 1.0026, 0.7228,\n",
       "                      0.6997, 1.3653, 1.3526, 0.7880, 0.5785, 1.0813, 1.3404, 0.8755, 0.7737,\n",
       "                      0.4954, 0.8773, 0.7624, 0.6824, 1.0542, 0.7211, 0.7392, 0.7310, 0.8392,\n",
       "                      0.7253, 0.4322, 0.6955, 0.4757, 0.8714, 0.6447, 0.7715, 0.9684, 0.6909,\n",
       "                      0.6599, 0.7987, 0.5357, 0.6341, 1.2263, 0.9388, 0.8724, 1.0803, 0.6972,\n",
       "                      0.8746, 1.0389, 0.6115, 0.5368, 1.6489, 0.9415, 0.8109, 1.0698, 1.1691,\n",
       "                      1.0763, 1.0102, 0.4312, 0.6833, 0.6732, 0.8292, 0.7224, 0.7580, 0.6037,\n",
       "                      0.4106, 0.6497, 0.7267, 0.7019, 0.8156, 0.9076, 0.9589, 1.0256, 0.6552,\n",
       "                      0.4798, 0.7437, 0.7309, 0.9366, 0.6616, 0.7438, 0.7921, 0.4516, 0.6543,\n",
       "                      0.6941, 0.5205, 0.6662, 0.6571, 0.5223, 0.7036, 0.5468, 0.7712, 0.5380,\n",
       "                      1.6026, 0.9442, 0.8923, 0.7098, 0.5982, 0.5789, 1.0589, 1.0550, 0.7854,\n",
       "                      0.6411, 1.0955, 0.6618, 0.9819, 0.8052, 0.8197, 2.0211, 1.1619, 1.2325,\n",
       "                      0.7244, 0.7071, 0.7923, 0.8468, 0.5846, 0.6066, 0.5129, 0.6261, 1.0839,\n",
       "                      0.8915, 0.8619, 0.9661, 1.3643, 0.5539, 0.5890, 0.8959, 0.7112, 0.6618,\n",
       "                      0.6741, 0.8523, 1.1534, 0.7679, 0.6220, 0.8152, 1.2642, 0.8736, 0.8295,\n",
       "                      0.9110, 0.9372, 0.7630, 0.5551, 0.8185, 0.7880, 1.3305, 1.3172, 0.5445,\n",
       "                      0.8358, 0.6842, 0.7222, 1.0082, 0.5204, 2.0664, 1.0405, 0.6145, 0.6080,\n",
       "                      0.9342, 0.9528, 0.5458, 0.7056, 0.9104, 1.0557, 1.2219, 0.8482, 0.7414,\n",
       "                      1.2821, 0.5765, 0.6926, 0.9278, 0.7842, 0.8340, 1.2853, 0.8364, 1.4677,\n",
       "                      1.3003, 0.7248, 0.6063, 1.0671, 0.9810, 0.6884, 0.6220, 0.5758, 0.6102,\n",
       "                      0.8507, 0.8065, 0.6399, 0.6111, 0.5632, 1.4791, 1.0645, 0.5870, 0.7502,\n",
       "                      0.6825, 0.7761, 0.5035, 1.3642, 0.6906, 1.3381, 0.8676, 0.7499, 1.4009,\n",
       "                      1.7185, 0.6293, 0.9518, 0.8525, 0.6618, 0.6302, 0.5710, 0.6460, 0.9114,\n",
       "                      1.4375, 0.9601, 0.5563, 0.9539, 0.5516, 0.7783, 1.9126, 1.0382, 0.9508,\n",
       "                      0.6058, 0.7641, 0.9104, 0.7895, 0.6134, 0.8209, 0.8172, 0.5569, 0.9540,\n",
       "                      0.6693, 0.6392, 1.0474, 0.6558, 0.6711, 0.8682, 0.5507, 0.5870, 0.8677,\n",
       "                      0.8931, 1.5661, 0.9192, 1.0465, 0.5224, 0.7938, 0.5909, 0.5794, 0.6968,\n",
       "                      0.3844, 0.5201, 0.6729, 0.9976, 0.9024, 0.8149, 0.6527, 0.6970, 0.8761,\n",
       "                      1.6883, 0.5594, 0.8286, 0.8119, 0.6887, 0.7706, 0.7263, 0.9331, 1.1492,\n",
       "                      0.8608, 0.8890, 0.7116, 0.5886, 0.5954, 1.0643, 0.5425, 0.5811, 1.0480,\n",
       "                      0.6033, 0.5823, 0.6252, 0.5456, 0.7161, 0.6095, 0.5545, 0.9652, 0.7410,\n",
       "                      0.7440, 0.8633, 0.9397, 0.8890, 0.6559, 0.6677, 0.6905, 0.8433, 0.7043,\n",
       "                      0.7687, 0.4804, 1.4761, 0.4083, 0.8883, 0.8999, 0.7486, 1.0051, 0.8909,\n",
       "                      1.3466, 0.9454, 0.6101, 1.1650, 1.2055, 0.5828, 0.7584, 1.2663, 0.7051,\n",
       "                      0.9172, 1.1213, 0.7088, 1.0964, 0.7638, 0.5068, 0.8410, 0.9072, 0.8161,\n",
       "                      0.5631, 1.2402, 0.8687, 0.8184, 0.8856, 0.5565, 0.6255, 0.5761, 0.7472,\n",
       "                      0.7412, 0.5457, 1.4468, 0.6696, 0.6580, 0.9249, 1.1935, 0.7997, 0.8713,\n",
       "                      0.8206, 0.5753, 0.6293, 0.7898, 0.4710, 0.6092, 0.6100, 0.7136, 0.8016,\n",
       "                      0.8256, 1.3540, 0.6278, 0.7466, 0.5859, 0.6764, 1.0572, 1.0516, 0.5121,\n",
       "                      0.6098, 0.7436, 0.5245, 0.9474, 0.8811, 0.7445, 0.5901, 0.6200, 0.6121,\n",
       "                      0.6675, 0.7843, 0.6939, 1.4340, 0.9358, 0.7352, 0.7204, 1.0457, 0.8782,\n",
       "                      0.7337, 0.8156, 0.8380, 0.5803, 0.7086, 0.6780, 1.0001, 0.6862],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.17.1.bias',\n",
       "              tensor([ 1.1260e+00,  8.6449e-02,  1.0772e+00, -2.4569e-01,  2.0354e-01,\n",
       "                       5.3115e-01, -4.0851e-01,  2.6972e-01, -7.0428e-01, -8.1274e-02,\n",
       "                      -7.1366e-01, -2.4375e-01,  2.0282e-01,  1.0234e-01, -7.0416e-01,\n",
       "                      -1.5763e-01, -4.1741e-02, -1.2425e+00, -2.4909e-01,  1.3254e+00,\n",
       "                       4.2714e-01, -4.7941e-01,  1.8933e-01, -3.3393e-01, -1.5054e-01,\n",
       "                       4.6517e-01,  3.2731e-02, -2.2881e-01,  3.3094e-02,  2.8450e-01,\n",
       "                      -6.5583e-01,  1.1280e+00,  6.0138e-01,  9.6081e-01,  5.5045e-01,\n",
       "                      -1.2500e+00,  1.1646e+00,  1.1284e+00,  2.5768e-01,  3.1390e-01,\n",
       "                       7.6325e-01, -3.4141e-01,  3.0423e-01, -2.8705e-01, -8.4139e-01,\n",
       "                       6.7826e-01,  1.7225e-02,  2.1568e+00,  9.2877e-02,  2.9554e-01,\n",
       "                      -1.7953e-01,  1.3861e+00,  2.8343e-02,  1.4282e+00, -5.5186e-02,\n",
       "                       7.0105e-01, -6.6942e-01,  7.1550e-02,  1.2673e+00, -2.3157e-01,\n",
       "                      -3.6422e-03,  1.0382e+00, -7.9336e-02, -2.5527e-01,  4.7550e-01,\n",
       "                       1.4303e+00,  1.2499e-01, -5.5561e-01,  9.3388e-01, -6.0705e-02,\n",
       "                       1.8387e-01, -3.1455e-01,  5.8878e-02, -3.4050e-01,  7.5969e-01,\n",
       "                      -2.1352e-01,  1.4060e+00, -3.6068e-01, -2.1477e-01, -7.0820e-01,\n",
       "                       2.9689e-01,  1.1409e-01, -1.4149e-01, -3.5924e-01,  2.4043e-01,\n",
       "                      -4.4267e-01,  1.9325e-01, -2.5359e-01,  1.8792e+00,  1.2713e+00,\n",
       "                      -6.3104e-01, -1.7634e-01, -1.3039e-01, -3.7965e-01, -3.7417e-01,\n",
       "                       8.7971e-01, -1.1906e-01,  7.1771e-01,  8.1568e-01,  7.6751e-02,\n",
       "                       4.3744e-01, -4.8475e-01, -1.2847e+00,  5.8737e-01,  3.4591e-02,\n",
       "                      -5.2325e-01,  4.4762e-02,  2.8660e-01, -3.2064e-01, -3.3082e-01,\n",
       "                       2.4316e-01, -6.2094e-02,  2.7515e-01,  1.6041e-01, -6.2882e-01,\n",
       "                      -4.1995e-01,  1.8543e+00, -2.4017e-01, -3.4518e-01,  1.8390e-01,\n",
       "                      -3.2762e-01, -4.6413e-02, -1.8835e-01,  9.3302e-01, -3.6773e-02,\n",
       "                      -4.3768e-01, -7.6738e-02,  3.3666e-01,  5.7828e-01,  2.0039e+00,\n",
       "                       1.4443e+00,  1.7807e+00,  9.2370e-02,  7.2739e-01,  2.4944e-02,\n",
       "                      -9.8453e-02, -1.2277e+00, -1.9010e-01, -4.4469e-01,  1.2127e+00,\n",
       "                      -1.8851e-02,  7.5464e-01,  2.1378e-01,  3.9844e-01,  1.4500e+00,\n",
       "                      -3.8633e-01, -1.2254e+00,  5.3406e-01,  1.5342e+00, -5.5838e-01,\n",
       "                      -6.8042e-01, -2.3375e-01,  8.3061e-01,  8.3733e-01, -5.6576e-01,\n",
       "                      -4.0990e-01, -2.6463e-02, -6.6321e-01, -4.7133e-01, -9.3019e-02,\n",
       "                      -9.9629e-02,  3.4564e-02, -6.9275e-01,  1.3583e+00,  1.4354e-01,\n",
       "                       9.8362e-01,  8.3506e-01, -1.8411e-01,  6.2013e-01, -5.4089e-02,\n",
       "                       1.3761e+00, -2.7582e-01, -2.7712e-02,  6.7760e-01,  5.0715e-01,\n",
       "                      -5.1291e-01, -1.8787e-01, -4.2376e-01, -4.3230e-01,  2.6824e-01,\n",
       "                       1.3981e+00, -9.1402e-01,  1.3022e+00,  2.8152e-01, -6.1435e-01,\n",
       "                      -6.2708e-02, -4.2191e-01, -1.5294e-01, -6.4702e-01,  1.4994e-01,\n",
       "                      -3.7880e-01,  1.3571e+00, -2.3241e-01,  9.7596e-01,  3.5271e-01,\n",
       "                       9.1262e-01, -4.3175e-01,  7.5921e-01,  1.1283e+00, -2.9322e-01,\n",
       "                       1.2871e-01, -1.4881e-01, -6.1685e-02, -1.5353e+00,  1.1370e+00,\n",
       "                      -2.6987e-01,  3.6111e-01,  1.5677e-01,  1.8568e+00, -1.0495e-01,\n",
       "                      -1.0858e-01,  1.6413e+00, -7.8778e-02, -8.1791e-01,  5.8927e-01,\n",
       "                       1.1632e-01, -2.1377e-01,  5.1421e-01,  1.5253e+00, -3.9174e-01,\n",
       "                       9.3626e-02,  2.8158e-01,  7.5944e-01,  1.6869e-01,  2.7291e+00,\n",
       "                      -1.0348e+00, -1.0890e-02,  5.1583e-01, -5.7812e-02, -1.6411e-02,\n",
       "                       1.0950e-01, -9.5902e-02, -1.2601e+00,  3.7806e-01,  1.1104e+00,\n",
       "                      -1.9945e-01,  3.7397e-01, -2.4253e-01,  1.6486e+00, -3.2267e-01,\n",
       "                      -2.5524e+00,  4.4001e-01, -2.8459e-02,  1.4378e-01, -6.5274e-01,\n",
       "                       1.0971e+00, -2.6880e-01,  1.3847e+00,  1.4022e+00,  1.1622e+00,\n",
       "                       5.1297e-02,  1.5729e-01,  1.5032e+00, -4.7326e-01,  2.5438e-03,\n",
       "                      -2.9566e-01,  1.7465e+00,  1.5891e+00, -4.8747e-01,  1.2512e-01,\n",
       "                       4.0383e-01, -1.9523e-01, -5.6679e-02, -1.8169e-01,  4.0456e-01,\n",
       "                       8.9502e-01, -1.9647e-01, -7.4304e-01, -1.0693e+00,  2.2496e-01,\n",
       "                       1.1166e-01,  1.3004e-01, -4.7923e-01,  9.7749e-01,  2.2995e-01,\n",
       "                      -5.8974e-01, -7.2302e-01, -2.7126e-01,  3.1212e-01,  1.8571e+00,\n",
       "                       8.0605e-02,  9.6508e-01, -4.6307e-01,  5.2704e-01, -1.2623e+00,\n",
       "                      -1.5943e+00, -1.2677e-02, -1.0909e-02, -2.2976e-01,  2.8056e-01,\n",
       "                       1.1474e+00, -3.7568e-01, -4.7399e-01, -1.7776e-01, -3.2364e-01,\n",
       "                      -3.7420e-01, -4.6003e-01, -4.4785e-01,  1.3847e+00,  1.5123e+00,\n",
       "                      -3.8516e-01, -1.5485e-01, -6.8198e-01, -2.7817e-02, -4.5168e-01,\n",
       "                      -1.4336e+00, -2.0912e-01, -3.6571e-02,  1.4275e+00, -4.2964e-01,\n",
       "                      -1.7635e-01,  5.5155e-01,  1.8268e+00,  1.1127e+00,  1.4075e+00,\n",
       "                       3.5832e-01,  3.9552e-01,  6.1912e-01,  4.6722e-01,  1.2268e+00,\n",
       "                      -5.4588e-01, -7.1377e-01,  1.1332e+00,  1.3835e+00,  4.6794e-01,\n",
       "                       8.2470e-01,  1.5180e+00, -7.7388e-01,  1.4322e-02, -4.5449e-01,\n",
       "                       8.6236e-02,  3.8305e-01, -6.3217e-01, -7.6921e-01,  6.4309e-01,\n",
       "                       2.4712e-01,  2.0870e-01,  4.3091e-02,  2.2956e-01,  1.0493e+00,\n",
       "                      -1.3171e-01,  1.2264e-01, -5.3370e-01, -6.4081e-01,  1.1220e+00,\n",
       "                       3.6289e-01,  2.1822e-01, -4.6299e-01, -3.0832e+00, -3.8283e-01,\n",
       "                       2.7470e-01,  7.7025e-01,  1.1725e+00, -6.3648e-01, -5.7107e-01,\n",
       "                       1.5583e-02, -2.4067e-01, -6.6794e-01,  1.2184e+00, -3.2290e-01,\n",
       "                      -1.9458e-01, -1.6267e-01, -6.0598e-01, -4.7138e-01, -5.3243e-01,\n",
       "                      -4.9835e-01,  1.4721e+00,  2.4339e+00,  1.5811e+00, -2.0710e-01,\n",
       "                      -5.5555e-01, -3.0906e-01, -4.2834e-01,  5.7727e-01, -5.8197e-01,\n",
       "                      -1.1968e-01,  9.6548e-01,  7.6906e-01,  1.1829e+00,  9.3797e-01,\n",
       "                      -2.0502e-01, -4.2153e-01,  3.8871e-01,  8.9501e-01,  6.4065e-01,\n",
       "                       2.0394e-01, -3.6217e-01, -8.5327e-01,  9.6421e-01, -3.8186e-01,\n",
       "                      -4.3908e-01,  1.5493e+00,  5.7388e-01,  5.2477e-02,  1.7291e-01,\n",
       "                      -3.6223e-01,  3.8398e-02,  1.6170e+00, -2.1475e-01,  2.3560e-01,\n",
       "                       9.3002e-01, -4.2683e-01,  2.0017e+00,  1.0749e+00,  8.3906e-01,\n",
       "                       5.5570e-02,  1.8036e-01,  1.6567e+00,  1.6791e+00,  8.6834e-02,\n",
       "                       5.5260e-01,  7.9482e-02,  1.0769e-01, -1.8431e-01,  4.9143e-01,\n",
       "                      -1.4727e-01, -7.2190e-01,  3.0614e-01,  1.2969e+00, -1.3413e-01,\n",
       "                       8.4092e-01, -4.1564e-01,  9.0862e-02,  2.3222e-01,  3.1833e-01,\n",
       "                      -1.2343e+00,  1.7519e+00,  9.5351e-01, -6.0593e-02, -5.9286e-02,\n",
       "                       9.9558e-02, -2.6817e-03, -5.1157e-01,  1.5748e-01,  7.1054e-01,\n",
       "                      -2.8210e-01, -5.4909e-01,  1.0559e-01,  1.3120e+00, -3.6557e-01,\n",
       "                       3.1628e-01,  1.2690e+00, -5.3936e-02,  1.6030e-02, -1.2210e-01,\n",
       "                       4.1158e-01,  1.0199e+00, -5.3886e-01, -2.6767e-01,  4.5503e-02,\n",
       "                       1.4773e+00, -1.3406e+00,  9.4151e-02, -4.5236e-01,  2.2436e+00,\n",
       "                       1.1356e+00,  1.2819e-01, -6.9855e-01, -6.3738e-01,  2.1992e-02,\n",
       "                       9.9250e-01, -4.6467e-01, -3.2698e-01,  1.8795e+00, -2.7437e-03,\n",
       "                      -4.7763e-01,  8.3938e-02, -4.4854e-01, -4.3135e-01,  1.3760e+00,\n",
       "                       7.7171e-01,  3.4663e-01,  1.4043e+00,  5.6989e-01, -1.3921e-01,\n",
       "                      -1.2546e-02, -4.9480e-02,  4.2709e-01, -3.9833e-01,  2.0890e-01,\n",
       "                       1.1266e+00, -1.4565e-01,  1.5203e+00, -5.8427e-01, -5.5225e-01,\n",
       "                       1.2684e+00,  1.2563e+00, -2.8646e-01,  1.0313e+00, -7.2995e-01,\n",
       "                      -3.2869e-01, -3.5609e-01,  2.8735e-01,  6.6068e-01, -9.5811e-01,\n",
       "                       1.8536e-02,  3.2444e-01,  8.3977e-01, -1.1313e+00,  3.4681e-01,\n",
       "                       3.1801e-01, -1.1818e-01, -9.3986e-01,  1.9312e+00,  5.6958e-02,\n",
       "                       3.3632e-01,  1.4243e+00,  1.2389e+00, -1.7081e-01,  1.1445e+00,\n",
       "                      -1.9944e-01, -5.0133e-01], device='cuda:0')),\n",
       "             ('module.model.17.1.running_mean',\n",
       "              tensor([ 1.9824e-01,  4.5948e-01,  1.8934e-01,  1.1989e+00, -1.5566e+00,\n",
       "                      -1.4469e+00,  4.5124e-01, -1.1301e+00, -1.8311e+00,  3.5236e-01,\n",
       "                      -2.4695e+00,  8.6761e-01,  2.2568e-01,  2.1325e-01,  8.2513e-01,\n",
       "                       8.9409e-01,  1.1547e+00,  2.5284e+00,  8.6063e-01, -4.6788e-02,\n",
       "                      -1.0333e+00,  4.6503e-01,  3.1452e-01,  4.2399e-01, -1.7855e+00,\n",
       "                      -2.3566e-01,  4.7221e-01, -2.1088e+00,  5.8069e-01,  3.4647e-01,\n",
       "                       1.0224e+00,  4.4525e-01,  4.3373e-01, -9.4424e-03, -1.0521e+00,\n",
       "                       6.4651e-01, -9.0112e-01, -9.1079e-01,  7.0101e-01, -1.4808e+00,\n",
       "                       5.3793e-01,  7.3825e-01,  8.8491e-01, -1.5693e+00, -3.5691e+00,\n",
       "                       1.9718e-01,  8.1096e-01,  1.9597e-01,  6.6792e-01,  3.8365e-02,\n",
       "                      -1.4617e+00, -5.4400e-01,  3.5607e-01, -8.7402e-01, -1.8756e+00,\n",
       "                      -1.6342e+00,  7.5222e-01,  2.3987e-01, -2.0467e-01,  1.1561e+00,\n",
       "                      -1.3522e+00, -6.4549e-01,  2.2717e-01, -1.7445e+00, -6.7533e-01,\n",
       "                      -1.1618e+00, -1.0836e+00,  4.5326e-01, -1.0849e+00, -1.7970e+00,\n",
       "                      -1.6701e+00, -1.5904e-01,  6.2559e-01,  7.9589e-01, -8.0056e-01,\n",
       "                       6.2179e-01,  8.5759e-01, -4.9500e-01, -1.4754e-01, -2.0404e+00,\n",
       "                      -2.4121e+00,  4.2132e-01,  5.4432e-01, -4.2716e-01,  2.8251e-01,\n",
       "                       2.4315e+00,  2.2441e-01,  1.8523e+00,  9.4238e-01, -3.5445e-01,\n",
       "                       5.3993e-01,  4.2943e-01, -1.6186e+00,  1.2032e+00, -1.0415e+00,\n",
       "                      -3.0328e-01,  2.4282e-01,  2.7258e-01, -1.5619e-01,  1.0030e+00,\n",
       "                       1.4278e+00,  6.2392e-01,  2.3809e+00,  1.1531e+00,  3.1437e-01,\n",
       "                       6.8916e-01,  5.1403e-01,  3.2341e-01,  2.6719e-01,  7.0228e-01,\n",
       "                      -1.6884e+00,  3.9040e-01,  1.0879e+00,  5.3109e-01,  3.7579e-01,\n",
       "                      -1.8516e+00,  2.0592e+00,  2.5018e+00,  1.1174e+00, -1.4910e+00,\n",
       "                       1.4460e-01,  9.1961e-01,  1.0522e+00,  7.5235e-01, -1.3971e+00,\n",
       "                      -2.0190e+00,  7.6998e-01,  8.3383e-01, -1.5540e+00,  1.2714e+00,\n",
       "                       3.3857e-01, -9.8076e-01,  9.0078e-01, -8.0912e-01,  2.9961e-01,\n",
       "                       1.0459e+00,  9.0000e-01,  4.8467e-01,  9.0512e-01,  6.6777e-02,\n",
       "                       5.5257e-01, -7.3121e-01,  1.9517e+00, -1.1074e+00,  5.0818e-01,\n",
       "                      -1.1656e+00,  1.8771e-01,  1.2115e+00, -6.7185e-02,  1.1799e+00,\n",
       "                       2.3072e+00,  1.3031e+00,  6.7789e-01,  4.3888e-01,  3.3107e-01,\n",
       "                       5.4880e-01,  1.0166e+00,  5.8126e-01,  2.6001e-01,  7.3499e-01,\n",
       "                       6.2883e-01, -2.2452e+00,  2.1438e-01, -7.5734e-01,  9.8551e-01,\n",
       "                       3.3809e-01,  1.0864e+00,  5.5435e-01, -1.6424e+00, -2.3388e+00,\n",
       "                      -1.3944e+00,  1.1626e-01,  9.4993e-01,  5.4644e-01,  7.4744e-01,\n",
       "                       2.6112e-01,  5.2801e-01,  5.0546e-01, -1.8918e+00,  1.0262e+00,\n",
       "                      -1.0518e+00,  6.3424e-01,  6.4088e-01,  4.1967e-01, -1.6960e+00,\n",
       "                      -1.6970e+00,  8.7750e-01, -1.4090e+00,  7.8156e-01, -1.8464e+00,\n",
       "                       1.6945e+00, -9.8287e-01,  7.7980e-01, -1.4101e+00, -1.1876e+00,\n",
       "                      -2.1580e-01,  5.4406e-01, -1.5321e+00, -5.6871e-01,  2.9137e-01,\n",
       "                      -1.5623e+00,  5.5214e-01,  8.1816e-01,  5.6411e-01,  4.4346e-02,\n",
       "                       1.1675e+00, -1.1608e+00,  4.3333e-01,  1.7558e+00,  1.0797e+00,\n",
       "                      -2.1975e+00, -1.7187e-01,  8.6635e-01,  4.5897e-01,  4.4147e-01,\n",
       "                       9.1463e-01,  4.1087e-01,  2.6935e-01,  2.0421e-02,  3.6033e-01,\n",
       "                       5.0218e-01, -2.0744e+00,  2.4803e-01,  9.1936e-01, -4.4704e-01,\n",
       "                      -1.9267e+00, -2.0557e+00,  1.5446e-01,  8.5213e-01,  6.1580e-01,\n",
       "                       3.2355e-01, -2.1348e+00,  4.1210e-01, -1.0302e+00, -4.3551e-02,\n",
       "                       1.6392e+00, -1.4641e+00, -2.1372e+00, -1.4405e-01,  1.0773e+00,\n",
       "                      -3.2972e+00,  7.7548e-01, -1.6587e+00,  8.3239e-01,  2.7045e-01,\n",
       "                       7.5820e-01,  1.5976e+00,  1.6405e-01,  8.4769e-02,  3.7261e-01,\n",
       "                       6.1829e-01, -8.9555e-01,  1.5802e+00,  8.9106e-01, -1.7028e+00,\n",
       "                      -5.6850e-01,  5.2187e-02,  1.4893e-01,  5.9365e-01,  6.2288e-01,\n",
       "                       6.7988e-01,  5.8842e-01,  1.1578e+00, -1.0698e+00, -1.0198e-01,\n",
       "                      -1.8308e-01,  2.7469e-01,  2.6187e+00,  5.9416e-01, -1.6019e+00,\n",
       "                       1.1849e+00, -1.8249e+00,  2.1261e-01,  2.3958e-01, -1.1287e+00,\n",
       "                       6.5362e-01,  2.5030e+00, -7.1074e-01,  5.2458e-01,  1.3738e+00,\n",
       "                       9.9279e-01,  1.4965e+00,  5.7995e-01,  1.1280e+00, -2.1144e+00,\n",
       "                       5.3624e-01,  3.9041e-01,  6.7824e-01,  3.7309e-01, -1.3879e-01,\n",
       "                      -2.5223e-01,  3.4367e-01,  3.0890e-01,  1.7540e+00, -1.2144e+00,\n",
       "                       9.1311e-01,  3.9863e-01, -6.9259e-04, -8.6898e-02,  2.1306e-01,\n",
       "                       1.1284e+00,  7.2344e-01,  8.1993e-01, -9.4053e-01,  2.2317e-01,\n",
       "                       1.3385e+00, -1.0831e+00,  8.7261e-01, -6.8724e-02, -1.8580e+00,\n",
       "                      -1.5677e+00,  7.4321e-01,  2.2714e-01,  2.3054e-01,  1.1222e-01,\n",
       "                      -1.0456e+00, -8.3772e-01, -1.2564e+00, -1.4974e+00,  6.6030e-02,\n",
       "                      -2.1797e+00,  8.1400e-01, -1.5722e-01,  7.5838e-01, -1.1560e+00,\n",
       "                      -5.4366e-01,  2.4392e-01, -1.8599e+00,  7.3769e-01, -1.6848e+00,\n",
       "                       1.5205e+00, -7.7734e-01, -2.2845e+00, -2.3311e-01,  3.4316e-01,\n",
       "                      -1.6425e+00,  3.8300e-01,  6.1384e-01,  5.9730e-01,  4.9056e-01,\n",
       "                       7.3794e-01, -1.2167e+00, -2.3184e+00,  9.7803e-01,  4.9476e-02,\n",
       "                      -6.4739e-01,  5.6025e-01,  5.1711e-01,  1.0721e+00,  5.9842e-01,\n",
       "                      -1.7445e+00, -8.0422e-01,  5.6913e-01,  7.1736e-01,  2.2083e-01,\n",
       "                       5.1342e-01,  3.0757e-01,  3.7887e-01,  4.2959e-01,  2.1729e+00,\n",
       "                       1.0811e+00,  8.8244e-01,  2.7423e+00,  1.0530e-01,  1.9530e-01,\n",
       "                       8.4869e-01,  1.6710e-01, -1.0356e+00,  1.2826e+00,  1.7032e+00,\n",
       "                      -1.3575e+00,  6.5264e-01,  3.6876e-02, -1.9855e+00,  1.0525e+00,\n",
       "                       3.2265e-01,  1.1471e+00,  3.4957e-01, -4.1381e-01, -1.4355e+00,\n",
       "                       6.6002e-01,  1.7117e+00, -1.0153e+00,  1.3275e+00, -7.0714e-01,\n",
       "                       9.3654e-01,  7.9631e-01, -1.5985e-01,  6.4098e-01,  5.7776e-01,\n",
       "                       6.9824e-01, -2.0795e-01,  1.3952e+00,  5.5180e-01, -1.1615e+00,\n",
       "                       3.3803e-01, -1.7595e+00,  1.3098e+00,  3.3000e-01,  9.3081e-01,\n",
       "                      -4.1503e-02, -1.8511e+00,  3.0178e-01, -5.4837e-01,  3.2783e-02,\n",
       "                       6.0296e-01,  7.6518e-01, -6.7819e-01,  1.5252e-02,  1.0237e+00,\n",
       "                       2.1383e-01,  7.3661e-01, -8.1988e-01,  1.8193e-01, -3.7820e-01,\n",
       "                       9.6426e-01,  7.3299e-01, -3.0349e-02,  6.3380e-02,  6.8865e-01,\n",
       "                      -1.9975e-01,  7.5922e-01,  6.7389e-01,  7.6567e-01,  2.8665e-01,\n",
       "                       9.8561e-01, -5.2636e-01,  8.0515e-01,  1.5630e+00,  4.2086e-01,\n",
       "                      -1.2956e+00, -1.4684e+00, -1.6716e+00,  1.6949e+00, -9.5253e-01,\n",
       "                      -2.0517e+00, -1.6101e+00,  3.7800e-01,  7.5610e-01, -1.3813e+00,\n",
       "                      -1.5960e+00,  1.0861e+00,  5.7155e-02,  7.2267e-01, -1.4954e-01,\n",
       "                      -1.4522e+00,  2.7682e-01,  8.5069e-01,  2.8376e-01,  8.6453e-01,\n",
       "                       9.7614e-02,  8.5411e-01, -1.3536e+00,  6.5959e-01,  8.3393e-01,\n",
       "                      -1.1680e+00,  6.6383e-01,  2.3861e-01,  3.9104e-01,  1.1018e+00,\n",
       "                       1.9601e-01, -2.7868e+00,  5.5558e-01, -3.9302e-01,  8.9979e-01,\n",
       "                       2.5123e+00,  1.1630e+00,  1.2997e+00,  1.2474e+00, -2.6892e-01,\n",
       "                      -1.8955e-01, -2.5055e-02, -9.5082e-01,  3.6275e-01,  6.8750e-01,\n",
       "                       4.5911e-01,  6.7819e-01,  5.6371e-01, -1.8882e+00,  5.6535e-01,\n",
       "                       2.0430e+00,  3.5865e-01, -5.9534e-01,  6.4344e-01,  2.9647e+00,\n",
       "                      -3.2568e-01,  4.9768e-01,  6.4590e-01, -6.7147e-01,  1.0259e+00,\n",
       "                       1.0705e+00,  6.1299e-01,  5.2515e-01,  6.8892e-01,  1.1654e-01,\n",
       "                       7.5315e-01, -1.3366e+00, -1.1594e+00,  1.6713e+00, -8.0344e-01,\n",
       "                      -1.2824e+00,  4.6363e-01,  1.3026e+00,  6.6233e-01,  2.1209e+00,\n",
       "                       4.7602e-01,  8.0943e-02,  2.3508e-01,  6.4887e-01, -7.4585e-01,\n",
       "                      -2.0714e-01,  4.4662e-01], device='cuda:0')),\n",
       "             ('module.model.17.1.running_var',\n",
       "              tensor([0.5721, 0.4192, 0.3423, 2.2856, 0.8946, 1.4487, 0.3929, 1.2901, 0.9400,\n",
       "                      0.4176, 1.1774, 1.0210, 0.2772, 0.1366, 1.0663, 0.4108, 1.8649, 2.6012,\n",
       "                      0.7625, 0.4281, 0.8729, 0.3481, 0.9264, 0.5677, 1.4938, 1.4054, 0.4180,\n",
       "                      0.8587, 0.6147, 0.3051, 1.4519, 1.0709, 0.9267, 0.9216, 0.8576, 0.4649,\n",
       "                      0.5448, 0.4603, 1.0311, 1.6853, 1.1774, 0.7916, 1.4240, 1.3161, 1.1404,\n",
       "                      0.3770, 0.8449, 1.3363, 0.7196, 0.4587, 0.9934, 0.2713, 0.3576, 1.1503,\n",
       "                      0.1921, 0.1976, 0.8315, 0.2641, 1.1221, 1.4875, 1.1284, 0.3419, 0.2083,\n",
       "                      0.9924, 0.4252, 0.6626, 1.2375, 0.5578, 0.7524, 1.2032, 1.5464, 0.7754,\n",
       "                      0.6878, 0.9647, 0.3754, 0.6996, 4.6000, 1.9525, 1.8912, 1.4499, 1.2576,\n",
       "                      0.4677, 0.3995, 1.3203, 0.2715, 1.5754, 0.2847, 1.3681, 1.3645, 0.8969,\n",
       "                      0.7005, 0.4352, 1.5051, 1.1661, 0.8599, 2.0386, 0.3410, 0.7464, 0.9489,\n",
       "                      1.3362, 0.5546, 0.7325, 2.1872, 1.2552, 0.4153, 0.9724, 0.7511, 0.5595,\n",
       "                      0.5027, 0.7238, 0.9580, 1.0397, 1.3333, 0.6554, 0.3754, 1.0432, 2.6159,\n",
       "                      1.8975, 1.5196, 1.4090, 0.2234, 0.9219, 0.9266, 1.8801, 1.0015, 0.7478,\n",
       "                      1.3442, 1.0555, 0.8875, 0.8387, 0.6694, 1.0190, 1.4981, 0.5854, 0.2624,\n",
       "                      2.2889, 0.9295, 0.5126, 0.9652, 0.8846, 0.5194, 1.1575, 1.4216, 0.7640,\n",
       "                      1.7625, 0.8854, 0.2715, 2.0315, 1.8723, 1.5469, 0.6414, 1.9716, 0.8673,\n",
       "                      0.8033, 0.4368, 0.6755, 1.0003, 0.8018, 0.1860, 0.8436, 0.8635, 0.3639,\n",
       "                      0.1779, 0.3498, 1.5148, 0.8197, 0.7380, 0.4851, 1.8582, 0.8741, 0.5814,\n",
       "                      0.0630, 0.8674, 1.6638, 1.3049, 0.7372, 0.6627, 1.0568, 0.8655, 1.1915,\n",
       "                      0.2407, 0.6652, 1.1513, 0.4862, 1.4106, 1.0152, 0.9886, 1.2248, 1.6065,\n",
       "                      2.4669, 2.0793, 0.7730, 0.7585, 0.6284, 1.0933, 2.0417, 0.5028, 2.2089,\n",
       "                      0.2875, 0.2058, 0.6850, 1.0496, 1.1733, 0.7648, 1.0377, 1.2288, 0.8710,\n",
       "                      0.4010, 0.6791, 1.2627, 0.8909, 0.8354, 0.8892, 0.5074, 0.7209, 1.0208,\n",
       "                      0.6428, 0.3555, 0.4675, 0.3426, 0.5035, 0.3069, 0.5327, 0.9235, 1.0414,\n",
       "                      1.0412, 0.6930, 0.7101, 0.8491, 0.5503, 0.2786, 1.5909, 0.4679, 0.8921,\n",
       "                      0.9905, 1.0789, 0.8788, 0.8927, 0.6909, 1.4544, 1.6271, 0.3252, 0.8818,\n",
       "                      1.3704, 0.4870, 2.1484, 2.1875, 0.6107, 2.1255, 1.1345, 0.9268, 0.8176,\n",
       "                      2.2019, 1.5115, 1.3257, 1.3731, 1.2434, 0.9955, 0.7588, 1.1885, 1.0025,\n",
       "                      0.5804, 1.7329, 0.7985, 0.7506, 1.1968, 0.5852, 3.4791, 0.6400, 0.7308,\n",
       "                      2.0195, 1.8314, 0.1996, 0.7283, 0.5154, 0.6204, 1.1653, 1.4275, 0.5607,\n",
       "                      1.3251, 1.2743, 1.9473, 0.8774, 1.0703, 2.1330, 0.4884, 0.4992, 0.6000,\n",
       "                      0.4367, 1.0575, 1.5733, 0.4069, 0.5121, 2.6338, 1.1120, 1.3861, 0.3897,\n",
       "                      0.6811, 0.8834, 1.3554, 1.3131, 1.0248, 0.7430, 0.7727, 0.3097, 2.0547,\n",
       "                      1.0617, 0.6502, 0.7117, 0.6964, 0.9312, 1.1109, 1.0220, 1.1585, 0.7458,\n",
       "                      1.3073, 0.8773, 1.3331, 0.1764, 0.6150, 2.0634, 0.9525, 1.3545, 2.6488,\n",
       "                      1.0565, 0.6896, 1.3639, 1.1048, 1.0782, 1.3388, 3.3769, 0.6634, 1.0134,\n",
       "                      0.8346, 1.0510, 1.0870, 0.6772, 0.6098, 0.8674, 1.3455, 0.9072, 1.0506,\n",
       "                      1.8005, 1.4786, 1.1570, 0.8349, 0.5696, 0.6872, 1.4142, 0.6178, 0.7253,\n",
       "                      0.6464, 1.3669, 1.1936, 0.3803, 0.4337, 0.6583, 0.7332, 0.6509, 3.1839,\n",
       "                      1.0532, 0.8696, 3.9091, 0.1318, 0.1826, 0.8217, 0.4865, 1.2360, 2.4934,\n",
       "                      2.2886, 1.5381, 0.7421, 1.3492, 1.0271, 1.0596, 0.4676, 1.9640, 1.2055,\n",
       "                      0.2052, 0.9921, 0.5158, 1.9689, 1.3510, 1.5670, 0.5538, 1.2166, 0.7395,\n",
       "                      1.1694, 1.1818, 0.8079, 0.6642, 0.6589, 1.8975, 0.8015, 1.1337, 1.0296,\n",
       "                      1.1012, 1.3282, 0.4127, 1.0233, 0.5694, 0.7086, 0.7778, 0.8187, 0.7941,\n",
       "                      0.8583, 0.6555, 0.5480, 1.6028, 0.9124, 0.7321, 0.6880, 0.5817, 0.3251,\n",
       "                      1.4025, 1.6036, 0.6798, 0.9186, 1.0947, 0.6881, 1.2625, 1.2021, 1.1023,\n",
       "                      1.3660, 0.1853, 1.6334, 0.2413, 0.8198, 1.7849, 0.6185, 1.0925, 0.9272,\n",
       "                      1.4790, 2.1783, 1.0497, 1.2585, 0.8806, 0.3533, 1.3890, 1.0183, 1.0425,\n",
       "                      1.2659, 1.5922, 0.9456, 1.0897, 1.0566, 1.4413, 1.2153, 0.5451, 1.4488,\n",
       "                      1.4421, 0.9674, 0.9022, 0.9835, 1.0566, 0.6456, 0.6937, 0.1618, 0.4013,\n",
       "                      1.8944, 0.3755, 1.3131, 0.6485, 1.5760, 1.6005, 1.5671, 1.8862, 1.3919,\n",
       "                      1.4369, 1.3160, 1.0318, 1.0670, 0.6253, 0.7150, 0.8690, 0.6641, 0.6653,\n",
       "                      2.0327, 0.8185, 0.6759, 3.0761, 0.2951, 0.6554, 1.0904, 3.4357, 0.5127,\n",
       "                      1.0483, 0.5900, 0.7000, 1.2747, 1.3333, 0.9308, 0.4876, 1.1923, 0.1160,\n",
       "                      1.0869, 0.9943, 0.5436, 1.5249, 0.8563, 1.1556, 0.5108, 1.5530, 1.6393,\n",
       "                      2.3736, 1.1983, 0.9224, 1.1145, 0.6424, 0.8179, 1.8095, 0.6609],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.17.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.17.2.clip_val', tensor([6.3144], device='cuda:0')),\n",
       "             ('module.model.18.0.weight', tensor([[[[ 0.0201]],\n",
       "              \n",
       "                       [[-0.0088]],\n",
       "              \n",
       "                       [[ 0.0181]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0296]],\n",
       "              \n",
       "                       [[ 0.0318]],\n",
       "              \n",
       "                       [[ 0.0168]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0198]],\n",
       "              \n",
       "                       [[-0.0015]],\n",
       "              \n",
       "                       [[ 0.0238]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0520]],\n",
       "              \n",
       "                       [[-0.0127]],\n",
       "              \n",
       "                       [[ 0.1032]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0004]],\n",
       "              \n",
       "                       [[-0.0269]],\n",
       "              \n",
       "                       [[-0.0244]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0605]],\n",
       "              \n",
       "                       [[-0.0258]],\n",
       "              \n",
       "                       [[-0.0714]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0270]],\n",
       "              \n",
       "                       [[-0.0164]],\n",
       "              \n",
       "                       [[-0.0098]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0104]],\n",
       "              \n",
       "                       [[ 0.0125]],\n",
       "              \n",
       "                       [[ 0.0884]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0232]],\n",
       "              \n",
       "                       [[-0.0778]],\n",
       "              \n",
       "                       [[ 0.0337]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0163]],\n",
       "              \n",
       "                       [[-0.0617]],\n",
       "              \n",
       "                       [[ 0.0612]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0053]],\n",
       "              \n",
       "                       [[ 0.0284]],\n",
       "              \n",
       "                       [[ 0.0817]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1471]],\n",
       "              \n",
       "                       [[-0.0125]],\n",
       "              \n",
       "                       [[-0.0072]]]], device='cuda:0')),\n",
       "             ('module.model.18.1.weight',\n",
       "              tensor([0.6534, 0.6976, 1.1102, 0.9892, 1.0830, 1.0055, 1.0547, 1.0389, 1.0408,\n",
       "                      1.0524, 1.0172, 0.9731, 1.0498, 0.9994, 0.8817, 1.0648, 0.9602, 1.0308,\n",
       "                      1.0165, 1.0611, 0.7755, 0.9947, 1.0841, 0.9466, 0.9730, 0.4788, 0.9019,\n",
       "                      0.9996, 0.9905, 1.1074, 0.8899, 1.0495, 0.9195, 1.1388, 1.0268, 1.0205,\n",
       "                      1.0447, 0.9532, 1.0535, 1.1246, 0.9584, 0.9799, 0.9068, 1.0839, 1.0378,\n",
       "                      1.1232, 0.9215, 0.8824, 1.0521, 0.9568, 1.0617, 0.8485, 0.9566, 0.9135,\n",
       "                      0.5859, 0.6440, 0.5820, 0.8983, 0.8558, 1.0744, 0.7946, 1.0205, 0.4722,\n",
       "                      1.2137, 1.0195, 1.0233, 0.9906, 1.0072, 0.8790, 1.0436, 1.0384, 0.4865,\n",
       "                      1.1068, 0.9249, 1.0887, 0.9428, 0.6927, 0.6439, 1.0249, 0.8152, 0.7013,\n",
       "                      0.9227, 0.9107, 0.9920, 0.6695, 1.0365, 0.9471, 0.8985, 1.0172, 1.0298,\n",
       "                      0.8632, 1.0157, 1.0160, 1.0489, 0.6900, 1.1614, 1.1100, 1.0704, 0.7410,\n",
       "                      1.0382, 1.1081, 0.9092, 0.9322, 0.2158, 1.0138, 1.0423, 0.9861, 0.6086,\n",
       "                      0.9631, 1.0606, 1.0384, 0.5511, 1.0170, 0.7174, 0.9558, 1.0547, 1.0509,\n",
       "                      0.5569, 0.9477, 0.8377, 0.9936, 0.6852, 1.0186, 1.1338, 1.0654, 1.0495,\n",
       "                      0.9187, 0.8482, 0.8145, 1.0760, 0.6879, 1.0501, 1.0339, 1.0177, 0.9624,\n",
       "                      0.6798, 0.9160, 0.5671, 0.9651, 1.1011, 1.0912, 1.0346, 1.0391, 0.3837,\n",
       "                      1.0582, 0.5863, 0.5378, 0.9995, 0.8385, 1.0960, 1.0169, 0.3137, 1.1195,\n",
       "                      0.9942, 1.0219, 1.0658, 0.6059, 1.0259, 1.0372, 0.9916, 1.0301, 0.9353,\n",
       "                      1.0162, 1.0915, 1.1198, 0.8410, 0.9462, 0.7593, 0.3529, 0.4251, 1.0637,\n",
       "                      0.9975, 1.0308, 0.9692, 0.9231, 1.0442, 1.0939, 0.9541, 1.0858, 0.6714,\n",
       "                      1.0301, 1.0879, 0.4542, 1.0271, 1.0392, 1.0408, 0.9551, 0.9865, 1.0257,\n",
       "                      1.0366, 1.0487, 0.9536, 1.0379, 0.5506, 1.2143, 0.3738, 1.0905, 0.9070,\n",
       "                      0.8046, 0.5945, 0.8822, 1.0346, 1.0645, 1.0233, 1.0332, 0.9747, 1.0982,\n",
       "                      0.5911, 0.9516, 0.7345, 0.3702, 0.8746, 1.0374, 1.0041, 0.9710, 0.7843,\n",
       "                      0.6524, 1.1031, 1.0469, 0.9753, 0.9737, 0.9853, 0.8786, 0.8768, 1.1354,\n",
       "                      1.0151, 0.9234, 1.7335, 0.9862, 1.0442, 1.0728, 0.9749, 1.1599, 1.0831,\n",
       "                      0.6232, 0.4508, 0.9967, 1.0767, 1.0401, 1.0546, 0.9144, 0.9348, 1.0412,\n",
       "                      0.3890, 1.0640, 0.6255, 1.0935, 0.6218, 0.8600, 1.0881, 1.0020, 0.7599,\n",
       "                      0.8449, 1.0358, 0.7548, 0.8072, 0.4982, 0.9635, 1.0821, 0.9600, 0.9423,\n",
       "                      0.9914, 0.4734, 1.1465, 1.1155, 0.9579, 1.0359, 0.7931, 1.0239, 1.0330,\n",
       "                      0.9924, 0.4071, 0.6514, 0.5323, 0.9376, 0.7210, 0.6094, 0.8589, 0.9084,\n",
       "                      0.9391, 1.0727, 0.8962, 1.0920, 0.9618, 0.8540, 0.9967, 0.7233, 1.0732,\n",
       "                      0.9820, 0.5310, 0.8641, 1.0871, 1.0574, 1.0144, 1.0334, 0.9821, 0.8351,\n",
       "                      1.0849, 1.0072, 1.0414, 1.0475, 1.0366, 0.9886, 0.8841, 1.1440, 0.8327,\n",
       "                      1.0728, 0.7314, 0.8282, 1.0900, 1.0542, 0.9914, 0.9968, 1.0946, 1.0380,\n",
       "                      1.0380, 1.0712, 1.0605, 1.1261, 0.7832, 1.0226, 0.9230, 0.6475, 0.7745,\n",
       "                      1.0287, 0.5717, 1.0347, 0.6996, 1.0416, 1.0707, 1.0418, 0.5872, 0.9004,\n",
       "                      1.0493, 0.9356, 0.9303, 0.8234, 1.0822, 0.8412, 0.9362, 1.1021, 1.1156,\n",
       "                      0.8189, 1.0300, 1.0033, 0.4061, 0.7077, 1.0373, 0.9095, 0.9898, 1.0360,\n",
       "                      0.9949, 0.8486, 0.6125, 0.6761, 1.4144, 1.0106, 0.9116, 1.0132, 1.0087,\n",
       "                      1.0012, 1.0176, 0.9057, 1.0346, 0.9401, 0.7737, 0.8879, 1.0387, 0.9196,\n",
       "                      0.9561, 1.0429, 0.8652, 0.8797, 0.9128, 1.1136, 1.0338, 0.5391, 0.9470,\n",
       "                      0.7111, 1.0271, 0.9633, 0.9820, 0.4634, 1.0435, 1.0125, 1.0394, 0.8974,\n",
       "                      1.0027, 0.6801, 1.0041, 1.2343, 0.9857, 1.0081, 1.1274, 0.9684, 0.7829,\n",
       "                      0.4295, 0.9410, 0.6766, 1.0187, 1.0452, 1.0438, 0.9232, 0.9886, 1.0273,\n",
       "                      0.9790, 1.0177, 1.1897, 1.0784, 0.8039, 1.0454, 1.0084, 1.1128, 0.6382,\n",
       "                      0.8728, 1.1124, 1.0442, 1.0299, 1.1663, 0.6076, 1.0406, 1.0220, 0.5380,\n",
       "                      0.7100, 1.0442, 0.9455, 0.6139, 0.9707, 1.0384, 0.9840, 1.1092, 1.0212,\n",
       "                      1.0049, 0.5875, 1.0486, 0.9397, 0.8464, 1.1000, 0.8183, 0.6932, 0.6809,\n",
       "                      1.0944, 0.9299, 1.1014, 1.0768, 0.9263, 1.0374, 1.1637, 0.9981, 1.1234,\n",
       "                      0.9035, 0.7655, 1.0200, 1.0652, 0.9615, 1.0811, 0.9659, 0.7839, 0.7174,\n",
       "                      1.0474, 0.4099, 1.0028, 0.5928, 0.8051, 0.6290, 1.0279, 1.0203, 1.0002,\n",
       "                      1.1375, 1.0769, 0.5834, 1.1662, 1.1785, 1.0702, 1.0061, 1.0127, 1.0611,\n",
       "                      1.0796, 1.0605, 0.7493, 0.7687, 0.8197, 0.8296, 1.1793, 0.9503, 0.2987,\n",
       "                      1.0886, 1.0310, 1.0607, 0.9730, 1.0953, 1.0563, 0.9511, 0.9068, 1.1236,\n",
       "                      1.0476, 1.1193, 0.9967, 0.9545, 0.5780, 1.1252, 0.8661, 0.9492, 0.6528,\n",
       "                      1.0584, 1.1030, 1.0361, 0.7927, 1.1713, 1.0384, 0.9524, 0.9207],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.18.1.bias',\n",
       "              tensor([ 8.7505e-01,  8.0065e-01, -2.8518e-02, -4.8562e-01,  2.1699e-01,\n",
       "                       6.9024e-01,  3.5030e-02, -5.3026e-02, -2.7317e-01, -1.0304e-01,\n",
       "                       4.8387e-01,  3.8651e-01, -2.8665e-01,  3.4334e-01, -6.6965e-01,\n",
       "                       4.1591e-01, -3.3386e-01,  1.1580e-02, -2.8804e-01, -3.9076e-01,\n",
       "                       6.9384e-01, -2.0761e-01, -5.1423e-02,  4.7030e-01,  1.0125e+00,\n",
       "                       9.8687e-01,  1.6278e-02, -5.3106e-01,  3.6804e-01, -2.0589e-01,\n",
       "                       1.8255e-01,  2.7670e-01,  5.6950e-01, -1.7901e-01, -4.5054e-01,\n",
       "                      -9.4667e-01,  1.3165e-01, -5.4203e-01, -2.8825e-02,  1.2189e-01,\n",
       "                      -4.5175e-01, -5.7818e-01, -6.1109e-01,  3.5911e-01, -3.7274e-01,\n",
       "                      -6.2713e-02,  5.5470e-01,  6.3488e-01,  5.7804e-02, -6.0528e-01,\n",
       "                       1.3070e-01, -6.8976e-01,  3.6314e-02,  5.4934e-01,  1.0159e+00,\n",
       "                       8.7639e-01,  1.1008e+00,  6.7418e-01,  7.6355e-01, -1.1405e-01,\n",
       "                       7.6557e-01, -4.2801e-01,  1.0318e+00,  3.1927e-01, -2.7897e-02,\n",
       "                       1.0039e+00, -1.4413e-01, -5.6150e-01,  6.6406e-01, -2.6957e-03,\n",
       "                      -2.7079e-01,  9.5308e-01,  5.4769e-02, -1.2996e+00, -2.7253e-01,\n",
       "                      -7.9622e-01,  1.1840e+00, -6.0792e-01,  1.9494e-01, -7.7067e-01,\n",
       "                       7.2553e-01, -5.5594e-01,  5.6285e-01, -4.3872e-01,  7.2490e-01,\n",
       "                       2.9753e-01, -4.7262e-01,  5.5131e-01, -2.6884e-01, -8.4479e-02,\n",
       "                      -7.1551e-01, -9.9919e-02, -4.2758e-01, -3.0834e-01,  8.0673e-01,\n",
       "                       4.2401e-01,  1.4809e-03,  9.0198e-02, -8.2516e-01, -1.9911e-01,\n",
       "                      -2.2913e-01,  5.0655e-01, -6.1152e-01,  1.0737e+00,  1.5080e-02,\n",
       "                      -1.8385e-01,  3.5326e-01,  9.4633e-01,  4.4276e-01, -7.1644e-02,\n",
       "                       2.5282e-02,  9.4926e-01, -2.1029e-01, -9.9107e-01,  5.3592e-02,\n",
       "                       2.4161e-01,  1.9416e-01,  1.0551e+00, -5.3358e-01,  6.4903e-01,\n",
       "                      -8.6286e-03,  8.5691e-01,  1.9846e-01, -2.3011e-01, -5.0150e-01,\n",
       "                      -2.8300e-01, -5.1831e-01, -6.8239e-01, -9.1587e-01,  4.6657e-02,\n",
       "                       1.0860e+00, -1.2432e-01, -1.0744e-01, -3.5935e-01,  4.2092e-01,\n",
       "                       9.3366e-01,  5.2360e-01,  9.4271e-01, -3.4506e-01,  3.3796e-01,\n",
       "                      -4.0803e-01, -2.1474e-01,  4.9535e-01,  1.0743e+00,  3.4625e-01,\n",
       "                       1.0380e+00,  1.2489e+00, -3.9348e-01,  6.3468e-01, -1.4394e-01,\n",
       "                      -3.5839e-01,  1.0884e+00, -4.3672e-01,  1.9614e-01, -5.1997e-02,\n",
       "                       1.1411e-01,  8.6299e-01,  9.2019e-02, -1.5017e-01,  4.5047e-02,\n",
       "                       4.2293e-01,  4.8119e-01,  6.3612e-02, -1.3834e-01,  2.2362e-01,\n",
       "                      -7.8936e-01,  1.4135e-02,  8.6679e-01,  1.0087e+00,  1.1462e+00,\n",
       "                       4.1159e-02,  3.4447e-01, -3.5932e-01, -4.0956e-01,  5.9177e-01,\n",
       "                      -1.3942e-01, -2.5700e-01,  5.7894e-01,  9.0962e-02,  8.8830e-01,\n",
       "                      -3.9549e-01, -5.1056e-01,  9.6617e-01,  2.3069e-01,  2.1030e-01,\n",
       "                       1.2990e-01,  4.9511e-01, -2.5109e-01, -1.5151e-02, -9.3650e-02,\n",
       "                      -2.6298e-01, -1.5875e-01,  3.1879e-01,  9.3142e-01, -7.5409e-01,\n",
       "                       1.0165e+00,  4.2539e-01,  5.1584e-01,  7.2305e-01,  1.0339e+00,\n",
       "                       6.0266e-01, -1.7754e-01, -1.7053e-01, -1.8439e-01, -1.5228e-01,\n",
       "                      -3.4188e-01, -1.0120e-01,  9.3300e-01,  6.0992e-01,  8.2447e-01,\n",
       "                       1.0735e+00,  6.9915e-01,  3.0147e-01, -2.9981e-01,  1.8690e-01,\n",
       "                      -7.6406e-01,  8.4426e-01,  3.6061e-02,  1.1034e-01,  5.2248e-01,\n",
       "                       4.9860e-01, -4.4730e-01, -7.6885e-01, -4.0965e-01,  1.6202e-01,\n",
       "                      -3.4174e-01, -3.7408e-01,  9.0257e-01, -3.4178e-01,  9.1864e-02,\n",
       "                      -7.1935e-02, -4.0134e-01,  4.6101e-02, -1.8849e-01,  1.0055e+00,\n",
       "                       1.0132e+00, -4.7205e-01,  1.7996e-01, -2.4770e-01, -2.8175e-01,\n",
       "                       3.6802e-02, -6.5697e-01, -4.9305e-02,  1.0467e+00, -7.4560e-02,\n",
       "                       1.0578e+00, -5.2676e-01,  8.7225e-01, -5.4890e-01, -6.9609e-02,\n",
       "                       7.3050e-01, -4.7320e-01, -6.6161e-01, -4.7634e-01,  7.2814e-01,\n",
       "                       7.0632e-01,  9.5271e-01, -7.9587e-01,  1.7363e-03, -5.6207e-01,\n",
       "                      -6.2310e-01, -3.5983e-01,  9.4264e-01, -5.0854e-01, -7.9107e-02,\n",
       "                      -8.4963e-01, -3.4785e-01,  8.1901e-01, -4.2726e-01,  5.0311e-02,\n",
       "                      -2.0609e-01,  1.0529e+00,  9.0063e-01,  9.4633e-01,  4.9509e-01,\n",
       "                       8.6077e-01,  1.0473e+00,  5.9354e-01, -6.3538e-01,  5.7404e-01,\n",
       "                       1.0740e-01,  9.5608e-01,  2.0442e-01, -1.6277e-01,  6.6343e-01,\n",
       "                      -3.5738e-01, -7.5934e-01,  2.6071e-01, -3.3449e-01,  9.2723e-01,\n",
       "                       6.5604e-01, -1.6777e-01, -3.0039e-01,  2.4860e-01,  2.0322e-01,\n",
       "                       5.2213e-01,  1.7663e-01,  1.2718e-01, -6.7680e-02, -2.9143e-01,\n",
       "                      -2.6409e-01, -3.8806e-01,  2.5188e-01,  7.1259e-01, -2.2481e-01,\n",
       "                      -9.0269e-01, -9.5346e-02,  7.4081e-01,  7.5948e-01, -2.2886e-01,\n",
       "                       1.6328e-01, -5.3614e-01, -3.0062e-01,  2.2027e-01, -1.6908e-01,\n",
       "                      -1.9479e-01,  5.8928e-01,  1.4912e-01,  1.4229e-01,  7.9640e-01,\n",
       "                      -4.0703e-01,  4.9143e-01,  9.9378e-01,  6.7793e-01,  4.8992e-01,\n",
       "                       1.6843e+00,  3.2034e-01,  8.6609e-01, -3.2863e-01,  4.4229e-03,\n",
       "                       5.4627e-02,  9.1694e-01,  5.5858e-01, -2.4714e-02,  5.1977e-01,\n",
       "                       5.3301e-01, -3.1824e-01, -1.0896e-02,  3.3625e-02,  6.6800e-01,\n",
       "                       2.1847e-03, -2.4435e-01,  6.6075e-01,  1.1727e-01, -3.2659e-01,\n",
       "                       1.1315e+00,  7.9690e-01,  2.7513e-01, -9.3138e-01, -4.5737e-01,\n",
       "                       3.9852e-01,  2.9108e-01,  4.3722e-01,  8.6687e-01,  8.2974e-01,\n",
       "                      -4.7817e-01, -1.1185e-01,  5.4026e-01, -2.2098e-01,  3.5468e-01,\n",
       "                       3.4327e-01, -2.2375e-02,  6.2601e-01, -2.5415e-02, -4.0541e-01,\n",
       "                       9.5576e-01,  2.1237e-01, -1.3739e-01, -3.2362e-02,  6.5256e-01,\n",
       "                       3.1160e-01, -9.8881e-01,  2.5953e-01,  5.8785e-01, -2.3480e-01,\n",
       "                      -4.3929e-01,  9.8516e-01,  5.2364e-01,  8.0686e-01,  2.2185e-01,\n",
       "                       6.3804e-01,  4.6222e-01,  9.5140e-01,  2.4101e-01, -4.2979e-01,\n",
       "                      -2.7391e-01,  5.4546e-01,  2.3036e-01,  9.0249e-01, -2.7037e-01,\n",
       "                       3.8467e-01, -4.8908e-01,  1.6460e-01, -4.0494e-01,  1.6788e-01,\n",
       "                       7.2095e-01,  1.0084e+00,  6.3712e-01,  9.2268e-01, -1.7788e-01,\n",
       "                      -3.1547e-01, -7.5112e-02, -5.4992e-01, -1.5703e-01,  1.2938e-01,\n",
       "                       3.9488e-01,  9.3586e-02, -7.2156e-01,  6.7710e-02, -8.1200e-01,\n",
       "                      -8.9188e-02,  1.7094e-01, -9.1488e-02,  8.7625e-01,  6.0831e-01,\n",
       "                       5.0767e-02, -7.4050e-02,  9.0811e-02, -3.2879e-02,  8.4472e-01,\n",
       "                       1.1480e+00,  3.4810e-01,  9.1381e-01,  9.5488e-01, -1.7254e-01,\n",
       "                       5.6177e-01,  8.7326e-01, -4.5032e-01, -2.3222e-01,  4.1950e-01,\n",
       "                       3.4986e-02,  2.8363e-01,  2.9122e-01,  1.0370e+00,  2.7140e-01,\n",
       "                      -6.1403e-01, -4.9457e-01, -9.2958e-02,  7.8711e-01,  9.1582e-01,\n",
       "                       8.1901e-01, -3.7357e-01, -5.7993e-01, -7.0385e-02,  2.8156e-01,\n",
       "                       5.3848e-01, -1.2010e-01, -4.7140e-01, -2.8072e-02,  5.8409e-02,\n",
       "                      -4.5843e-01,  7.9509e-01, -3.4101e-01, -2.2754e-01, -4.6655e-01,\n",
       "                      -2.4890e-01, -4.8409e-01, -8.3653e-01,  1.0960e+00,  4.6262e-01,\n",
       "                       1.0360e+00,  1.3011e-01,  8.8761e-01,  8.5045e-01,  1.0011e+00,\n",
       "                      -1.9090e-01,  9.4271e-02,  3.8596e-01,  2.6442e-01, -1.2667e-01,\n",
       "                      -7.6853e-01,  2.2064e-01, -6.9386e-01, -1.5740e-02, -4.2958e-01,\n",
       "                      -1.2861e-01, -3.4651e-01, -2.3002e-02, -4.1676e-01,  8.1904e-01,\n",
       "                      -6.8155e-01, -7.2642e-01,  7.5149e-01,  3.8497e-01, -5.8451e-01,\n",
       "                       1.0619e+00,  2.8207e-01, -2.5827e-01, -1.3611e-01,  4.4698e-01,\n",
       "                      -1.5503e-01, -1.1740e-01,  1.9852e-01,  5.3430e-01, -3.9197e-01,\n",
       "                       3.1120e-01, -4.7161e-01,  4.2943e-01,  4.1917e-01,  9.0111e-01,\n",
       "                      -2.2103e-01,  3.6945e-01,  5.0027e-01,  9.4053e-01,  2.3952e-01,\n",
       "                       4.6962e-01, -4.3150e-02,  7.0573e-01,  2.5816e-01,  2.7374e-01,\n",
       "                       4.0718e-01, -5.1073e-01], device='cuda:0')),\n",
       "             ('module.model.18.1.running_mean',\n",
       "              tensor([ 1.0298e+00,  2.3205e-01,  3.3601e-01, -6.5549e-02, -4.1425e-01,\n",
       "                       2.0510e-01, -2.3208e-01, -9.8767e-01,  3.1640e-01,  8.5739e-01,\n",
       "                       6.2560e-01,  8.0160e-01,  2.0921e+00,  1.3948e-01,  5.7939e-01,\n",
       "                      -7.4367e-01, -7.0648e-01,  5.0185e-01, -1.2937e-01,  5.3305e-01,\n",
       "                       4.1516e-01,  5.1094e-02,  4.8730e-01, -2.7116e-02,  3.5609e-01,\n",
       "                       6.9286e-02,  6.9925e-01, -8.3877e-01,  8.8658e-01,  8.6177e-01,\n",
       "                       9.8869e-01,  5.4035e-01, -3.9848e-01, -2.7946e-01,  4.2385e-02,\n",
       "                      -1.0859e+00, -3.5221e-01, -6.6898e-01, -8.5008e-01,  1.0479e-01,\n",
       "                      -6.2139e-01, -4.1557e-01,  1.4230e-01,  4.5456e-01,  4.0888e-01,\n",
       "                      -8.1458e-02,  6.5630e-01,  4.0514e-01, -2.7557e-01,  1.3117e+00,\n",
       "                      -6.2013e-01,  6.0187e-01, -1.0893e-01, -9.8228e-01, -4.4238e-01,\n",
       "                       2.8495e-01, -2.4451e-01,  2.2783e-01, -7.1360e-01, -5.3686e-01,\n",
       "                       4.9262e-01,  1.8425e-01,  1.1175e+00,  1.7492e-01,  9.9982e-01,\n",
       "                       6.8527e-01, -1.1758e+00, -8.6194e-01, -8.2539e-01, -3.0632e-01,\n",
       "                      -1.0549e+00,  4.0775e-01, -3.0704e-01,  2.1418e-01,  5.1469e-01,\n",
       "                      -5.6987e-01, -1.6106e+00,  3.9939e-01, -4.9788e-01, -2.6621e-01,\n",
       "                       4.4345e-01, -1.0366e+00, -1.1693e+00, -6.2971e-02,  5.2017e-01,\n",
       "                      -9.3866e-01,  5.3343e-02,  1.5069e+00, -1.0862e+00,  6.1979e-01,\n",
       "                      -2.0131e-01,  5.6115e-01,  2.8846e-01,  2.1294e+00,  3.5583e-01,\n",
       "                      -4.1346e-02,  1.2108e+00,  3.5664e-01, -3.9021e-01,  3.6652e-01,\n",
       "                      -7.1760e-02,  1.2163e+00, -5.0107e-01,  1.9959e+00,  5.5244e-01,\n",
       "                       6.7482e-01, -1.0386e+00, -2.9523e-02,  2.0140e-02, -4.2771e-01,\n",
       "                      -7.6921e-01,  3.9206e-01,  2.4437e+00, -1.4148e-01,  5.0105e-02,\n",
       "                      -2.5670e-02, -2.7536e-01,  3.5820e-01,  1.3545e-02, -4.2477e-01,\n",
       "                       9.7238e-01,  8.7295e-01, -3.9066e-01, -2.3406e-01, -1.0330e+00,\n",
       "                       3.3386e-01, -9.5225e-01,  4.7087e-01, -1.3010e+00, -3.7010e-01,\n",
       "                      -8.4896e-01,  6.9857e-01,  5.8785e-01,  9.3394e-01, -9.5472e-02,\n",
       "                       6.1766e-01, -1.7258e-01,  3.4848e-01, -8.6716e-01, -1.3934e-01,\n",
       "                       3.5748e-01,  4.9698e-01,  7.6921e-01,  9.0201e-01, -1.7929e+00,\n",
       "                       2.1164e-01, -1.2982e+00,  5.0159e-01, -6.3418e-01,  2.5684e-02,\n",
       "                       8.4657e-01,  4.4685e-01, -1.5261e+00, -3.4057e-01, -4.1343e-01,\n",
       "                       4.5187e-01,  6.4762e-01, -1.2187e-01, -6.9206e-01,  4.4698e-01,\n",
       "                      -1.0234e+00,  1.7033e+00,  6.1074e-01,  5.8829e-01, -1.1633e+00,\n",
       "                       8.7603e-02, -5.1798e-01,  3.1336e-01, -3.7204e-01, -6.0497e-01,\n",
       "                      -1.2580e-01,  7.8029e-01, -4.4988e-01,  1.0268e+00, -4.2082e-01,\n",
       "                       9.8576e-02, -1.8526e-01, -4.4324e-04, -8.1937e-01,  3.6776e-01,\n",
       "                      -9.2151e-01,  1.4996e+00, -9.0870e-01,  1.1633e+00, -1.7973e-01,\n",
       "                      -4.1426e-02,  1.0380e+00,  8.3933e-01,  7.8937e-01, -1.2210e+00,\n",
       "                       1.4305e+00, -5.7725e-01, -4.2454e-02,  6.8935e-01, -8.0447e-01,\n",
       "                       1.2580e+00,  8.2556e-01,  4.0926e-01, -5.5582e-01,  6.4912e-01,\n",
       "                      -1.5509e+00,  3.3626e-01,  4.5030e-01,  9.3026e-01, -3.0541e-01,\n",
       "                       2.7574e-03,  1.6691e-01,  2.7671e+00,  5.5791e-01, -6.4048e-01,\n",
       "                       2.4846e-01, -5.5900e-01,  2.7172e-01,  7.0929e-01, -1.0542e-01,\n",
       "                       1.5732e-01,  1.2858e+00,  8.7343e-01, -9.8153e-01,  1.0702e+00,\n",
       "                       6.6126e-01,  3.8426e-01, -1.0343e+00,  2.2894e-01, -1.7851e-01,\n",
       "                       1.4014e+00, -6.3792e-01,  1.7887e-01,  4.0471e-01,  1.7100e-01,\n",
       "                      -4.1547e-01,  6.3081e-02,  9.3926e-01,  2.2040e-01,  2.3652e+00,\n",
       "                       2.0410e+00, -2.6374e-01,  5.9546e-01, -3.3196e-01,  2.5950e-01,\n",
       "                       5.8248e-02,  8.1991e-02,  9.9546e-01, -6.8379e-01, -5.3761e-01,\n",
       "                      -2.2063e-02,  5.4114e-01,  4.7461e-01, -4.9976e-01,  1.2347e+00,\n",
       "                       1.3321e-01, -5.8366e-01, -1.4121e-02, -6.5383e-01, -2.5101e-01,\n",
       "                      -2.8631e-01, -1.1443e-01, -1.9618e-01, -2.4117e-01, -9.5337e-02,\n",
       "                       7.8773e-02,  6.4871e-02,  1.7843e+00,  2.4281e-02,  6.9539e-02,\n",
       "                      -5.5137e-01, -2.0986e-01, -1.0953e-01,  6.3942e-01, -8.5561e-02,\n",
       "                       5.7646e-01, -1.3831e+00,  1.3998e+00, -9.6732e-03,  6.0030e-01,\n",
       "                       1.5882e+00,  1.6429e+00, -5.5045e-01,  4.5383e-01,  1.1434e+00,\n",
       "                       2.7376e-01,  1.2118e+00, -2.9352e-01, -5.1494e-01,  4.0968e-01,\n",
       "                      -2.0251e-02,  8.0036e-02,  6.1845e-01, -2.1613e-01, -7.8513e-02,\n",
       "                       1.2663e+00,  7.7019e-01,  2.5516e-01,  9.3372e-01, -8.0411e-01,\n",
       "                       3.9982e-01, -2.1822e-01, -2.5782e-01,  7.8153e-02, -2.4201e-01,\n",
       "                      -7.5536e-01, -1.1807e+00,  9.4532e-01,  1.2325e+00, -5.6857e-01,\n",
       "                      -9.8654e-01,  6.2162e-01, -8.3504e-01, -1.8265e-02,  6.3922e-01,\n",
       "                       3.2428e-01,  7.6436e-01,  1.5452e+00, -5.5700e-01, -9.9264e-02,\n",
       "                      -1.2320e+00,  1.5927e-02,  1.8669e-01,  1.2105e+00,  7.2294e-01,\n",
       "                       7.1264e-01,  2.3999e-02, -3.3314e-01,  1.6464e+00,  3.7697e-01,\n",
       "                      -5.5049e-01, -1.4595e-01,  4.2291e-01,  1.8368e-01, -1.2867e-01,\n",
       "                       7.7902e-02,  1.4152e+00, -7.2315e-01,  1.4791e-01, -1.4343e+00,\n",
       "                      -1.0330e+00, -1.2835e+00,  1.0125e+00,  6.4415e-01, -3.1879e-01,\n",
       "                      -2.7975e-01, -8.5906e-01, -1.0295e+00, -3.0507e-01, -5.0858e-01,\n",
       "                       5.8662e-01, -1.1164e-02,  3.1296e-01, -1.3587e+00, -1.4032e+00,\n",
       "                       2.5120e-01,  7.5751e-03, -5.9138e-01, -8.3348e-01,  1.5279e-01,\n",
       "                      -7.2518e-01,  2.6210e-01,  2.2030e-01, -9.4505e-01, -2.9517e-01,\n",
       "                       9.1592e-01,  1.1193e+00,  5.0823e-01,  5.0239e-01,  2.5585e-01,\n",
       "                       1.5837e+00,  1.0520e+00, -1.6391e-02,  7.7561e-01,  1.9790e+00,\n",
       "                       5.1108e-02, -3.1459e-01, -1.0548e+00, -6.3452e-01,  3.8310e-01,\n",
       "                      -2.2303e-01,  7.3887e-01,  9.4488e-01,  3.1570e-01,  6.9532e-01,\n",
       "                      -1.4445e+00,  3.6738e-01,  1.6129e+00, -1.5249e+00,  3.6927e-01,\n",
       "                      -1.6178e-01, -5.7473e-01,  1.8487e-01, -7.3383e-01,  5.7483e-01,\n",
       "                      -4.6209e-01, -2.7845e-01,  4.3966e-01, -8.7492e-01, -1.1089e+00,\n",
       "                      -3.2810e-01,  2.5335e-01, -6.4414e-01, -1.0027e+00,  9.1511e-01,\n",
       "                      -9.9279e-02,  1.0443e-01, -4.5591e-03, -1.8396e+00,  8.6918e-02,\n",
       "                       8.1997e-01, -8.7748e-01, -4.8019e-01, -1.6463e-01,  1.7855e-01,\n",
       "                       4.9751e-02, -4.8547e-01, -3.9995e-01, -6.4769e-01,  1.0971e+00,\n",
       "                      -1.1519e+00, -2.2713e-01, -6.3835e-01,  1.4754e-01,  3.9708e-01,\n",
       "                       1.3883e+00,  9.3437e-02, -1.3936e-01, -1.0012e+00,  3.7536e-01,\n",
       "                      -5.5904e-01, -1.7916e+00, -3.5517e-01, -1.4300e-01,  3.4523e-01,\n",
       "                       9.5512e-01,  3.7415e-01,  1.0665e+00, -3.6954e-02, -5.4654e-01,\n",
       "                       1.0139e+00, -3.9878e-01,  1.6006e+00,  3.2277e-01,  2.5235e-01,\n",
       "                      -8.1757e-01,  2.2506e-01,  3.8645e-01,  1.4261e-01, -3.6256e-01,\n",
       "                      -5.3246e-01,  1.1655e+00, -7.5998e-01,  1.0355e+00, -1.3996e-01,\n",
       "                       4.5551e-01,  6.0136e-01,  5.5771e-03,  2.7073e-01, -7.2854e-01,\n",
       "                      -6.4450e-01, -8.2700e-01, -1.0742e+00,  6.8270e-02,  7.7861e-01,\n",
       "                       8.9444e-01,  3.8439e-01, -7.3437e-01, -1.4087e+00,  1.0477e+00,\n",
       "                      -4.2503e-01, -2.6772e-01,  1.3968e+00,  5.3630e-01,  7.3010e-01,\n",
       "                      -1.1714e+00,  1.0730e+00, -4.9265e-01, -4.9036e-01, -8.7855e-01,\n",
       "                       3.7402e-01, -7.5226e-01, -5.6213e-01,  1.1875e+00,  5.4019e-01,\n",
       "                      -8.3420e-01,  2.0065e-01,  9.2245e-01,  1.2132e+00,  4.7772e-01,\n",
       "                       7.4073e-01, -1.4477e-01, -1.3091e-02, -1.2124e+00, -3.4787e-01,\n",
       "                      -2.0185e-01, -4.3555e-01,  8.9710e-01, -9.5604e-01, -7.0107e-01,\n",
       "                      -2.5889e-01, -9.2365e-02, -1.0864e-02,  4.7738e-01,  4.7776e-01,\n",
       "                       5.9505e-01, -1.2667e-01, -1.0962e+00,  3.2290e-01, -1.4880e-01,\n",
       "                       1.7006e-01,  2.0402e-01, -7.1925e-02,  1.2370e-01, -1.3209e+00,\n",
       "                       6.3772e-02,  7.8112e-01], device='cuda:0')),\n",
       "             ('module.model.18.1.running_var',\n",
       "              tensor([0.3904, 0.3783, 0.9805, 0.4747, 0.7324, 0.5505, 0.7573, 0.5574, 0.9771,\n",
       "                      0.8082, 0.4064, 0.4994, 0.8367, 0.4762, 0.6548, 0.5701, 0.5727, 0.8162,\n",
       "                      0.7134, 0.5373, 0.6035, 0.3709, 0.4695, 0.5750, 0.6577, 0.4325, 0.6252,\n",
       "                      0.5690, 0.5274, 1.0079, 0.8515, 0.3712, 0.5565, 1.0776, 0.6537, 0.5730,\n",
       "                      0.8992, 0.4788, 0.5508, 1.0648, 0.5099, 0.7257, 0.4363, 0.6047, 0.6523,\n",
       "                      0.5721, 0.9615, 0.3278, 0.4832, 0.5414, 0.6796, 0.4564, 0.8004, 0.4244,\n",
       "                      0.4793, 0.4786, 0.5377, 0.5579, 0.5864, 0.5370, 0.4596, 0.5104, 0.5152,\n",
       "                      0.7741, 0.5813, 0.5476, 0.4185, 0.6841, 0.4600, 0.3921, 0.5819, 0.3684,\n",
       "                      0.6016, 0.5383, 0.8024, 0.6243, 0.5793, 0.4175, 0.6728, 0.4209, 0.4314,\n",
       "                      0.4284, 0.5214, 0.4289, 0.4253, 0.4403, 0.5735, 0.5312, 0.5427, 0.7526,\n",
       "                      0.4858, 0.5731, 0.6747, 0.9725, 0.4014, 0.6698, 0.6954, 0.4568, 0.4050,\n",
       "                      0.6251, 0.7928, 0.4387, 0.4390, 0.7570, 0.4781, 0.5616, 0.4643, 0.6260,\n",
       "                      0.6840, 0.5072, 0.4543, 0.4899, 0.5726, 0.3375, 0.5659, 0.3634, 0.4927,\n",
       "                      0.8040, 0.6476, 0.4831, 0.7133, 0.3522, 0.4155, 0.7196, 0.5139, 0.6501,\n",
       "                      0.4579, 0.4357, 0.5148, 0.8853, 0.5487, 0.4880, 0.8748, 0.8089, 0.5734,\n",
       "                      0.4992, 0.3487, 0.3064, 0.4296, 0.8586, 0.6580, 0.6695, 0.6499, 0.4798,\n",
       "                      0.6496, 0.3674, 0.4752, 0.6548, 0.7950, 0.6855, 0.6661, 0.4098, 1.0729,\n",
       "                      0.7888, 0.9597, 1.0648, 0.3968, 0.7091, 0.6687, 0.4350, 0.7635, 0.4545,\n",
       "                      0.6793, 0.8504, 0.5553, 0.4039, 0.4577, 0.5382, 0.5033, 0.5909, 0.6804,\n",
       "                      0.4381, 0.7249, 0.5065, 0.6375, 0.3878, 0.6481, 0.6631, 0.6093, 0.5472,\n",
       "                      1.0199, 0.6114, 0.7559, 0.6250, 0.3668, 0.6040, 0.4731, 0.5321, 0.6239,\n",
       "                      0.5758, 0.3676, 1.0410, 0.5273, 0.6228, 0.6754, 0.5180, 0.8360, 0.3707,\n",
       "                      0.5346, 0.5836, 0.5862, 0.3442, 0.8199, 0.6093, 0.6578, 0.5661, 0.6447,\n",
       "                      0.5321, 0.9402, 0.5229, 0.8066, 0.5188, 0.6082, 0.5687, 1.2144, 0.6508,\n",
       "                      0.3349, 0.7110, 0.3006, 0.7151, 0.4232, 0.3446, 0.6152, 0.4189, 0.4838,\n",
       "                      0.5464, 0.4342, 0.4176, 0.3878, 0.6740, 0.6670, 0.6417, 0.6436, 0.5843,\n",
       "                      0.7184, 0.8419, 0.5465, 0.4255, 0.5738, 0.6687, 0.4616, 0.5706, 0.6639,\n",
       "                      0.3735, 0.6093, 0.3851, 0.7809, 0.5962, 0.4597, 0.6998, 0.7685, 0.3376,\n",
       "                      0.4356, 0.7140, 0.5702, 0.6578, 0.5014, 0.6201, 0.4270, 0.5493, 0.5213,\n",
       "                      0.5033, 0.3492, 0.5652, 1.1348, 0.6966, 0.8924, 0.4374, 0.8307, 0.3890,\n",
       "                      0.7428, 0.8541, 0.5759, 0.6221, 0.5032, 0.4186, 0.6796, 0.4088, 0.5068,\n",
       "                      0.4719, 0.4476, 0.6162, 0.5865, 0.6159, 0.4812, 0.5762, 0.5696, 0.5284,\n",
       "                      0.4857, 0.4689, 0.6249, 0.8397, 0.5910, 0.3902, 0.3308, 0.4630, 0.7447,\n",
       "                      0.6001, 0.5075, 0.5070, 0.5894, 0.5688, 0.3092, 0.7225, 0.9573, 0.6965,\n",
       "                      0.5179, 0.3756, 0.8474, 0.5343, 0.3813, 0.7353, 0.5928, 0.6777, 0.5990,\n",
       "                      0.7846, 0.5812, 0.4188, 0.6650, 0.4226, 0.7733, 0.5295, 0.5745, 0.5220,\n",
       "                      0.8633, 0.3961, 0.5206, 0.9840, 0.9946, 0.5847, 0.4135, 0.5082, 0.7170,\n",
       "                      0.6624, 0.4014, 0.6603, 0.5583, 0.6369, 0.6832, 0.5909, 0.6055, 0.6114,\n",
       "                      0.6176, 0.8672, 0.4918, 0.3583, 0.6862, 0.5623, 0.5663, 0.5147, 0.7809,\n",
       "                      0.5048, 0.4343, 0.4743, 0.5081, 0.4226, 0.3149, 0.5578, 0.4588, 0.6592,\n",
       "                      0.4226, 0.6099, 0.8806, 0.5065, 0.5153, 0.8626, 0.8134, 0.7972, 0.4843,\n",
       "                      1.0172, 0.8546, 0.4261, 0.8371, 0.6498, 0.9218, 0.5913, 0.4545, 0.5429,\n",
       "                      0.5209, 0.4276, 0.7407, 0.3382, 0.3735, 0.4060, 0.6887, 0.7203, 0.4272,\n",
       "                      0.4445, 0.6090, 0.6925, 0.6910, 0.5721, 0.4151, 1.1284, 0.9199, 0.4072,\n",
       "                      0.6908, 0.6010, 1.1555, 0.6208, 0.5710, 0.7104, 0.5441, 0.5487, 0.6206,\n",
       "                      0.5826, 0.5046, 0.5486, 0.5764, 0.3956, 0.5478, 0.7161, 1.0851, 0.4252,\n",
       "                      0.5750, 0.5593, 0.7427, 0.4754, 0.6688, 0.3620, 1.3256, 0.6133, 0.4295,\n",
       "                      0.9566, 0.5987, 0.5983, 0.6418, 0.3930, 0.5543, 0.5993, 0.6651, 0.6973,\n",
       "                      0.4235, 0.4256, 0.5316, 0.5659, 0.5210, 0.6445, 0.6689, 0.7777, 0.6213,\n",
       "                      0.6330, 0.6776, 0.5666, 1.0311, 0.5904, 0.9055, 1.0568, 0.6858, 0.5005,\n",
       "                      0.4498, 0.3715, 0.4756, 0.3944, 0.5520, 0.6756, 0.7095, 0.2788, 0.7490,\n",
       "                      0.6820, 0.3995, 0.6025, 0.5901, 0.5441, 0.4549, 0.6796, 0.6524, 0.5077,\n",
       "                      0.6118, 0.6721, 0.3410, 0.8730, 0.6056, 0.6190, 0.6720, 0.6278, 0.6125,\n",
       "                      0.3544, 0.7648, 0.4547, 0.2873, 0.5938, 0.6844, 0.7732, 0.6045, 0.4784,\n",
       "                      0.6602, 0.5156, 0.4676, 0.4607, 0.6236, 0.4910, 0.5273, 0.6484, 0.7413,\n",
       "                      0.5780, 0.5954, 0.4150, 0.4804, 0.4257, 0.8620, 0.5534, 0.5251, 0.7197,\n",
       "                      0.5036, 0.2535, 0.7126, 0.4910, 0.6771, 0.5076, 0.5496, 0.5326],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.18.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.18.2.clip_val', tensor([6.0241], device='cuda:0')),\n",
       "             ('module.model.19.0.weight',\n",
       "              tensor([[[[-0.4733, -0.1685,  0.3974],\n",
       "                        [-0.6356, -0.5735,  0.5773],\n",
       "                        [-0.5586,  0.0238,  0.2106]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1340, -0.4567, -0.0437],\n",
       "                        [ 0.2020, -1.2437,  0.2578],\n",
       "                        [ 0.3004,  0.6436,  0.2534]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.5170,  0.0723,  0.4234],\n",
       "                        [ 0.4575,  1.0010,  0.7296],\n",
       "                        [ 0.7114,  0.0900, -0.1605]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.1997, -0.2977, -0.2142],\n",
       "                        [-0.2593, -0.2392, -0.2148],\n",
       "                        [-0.3504, -0.3219, -0.3295]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3044, -0.3167, -0.2858],\n",
       "                        [-0.2139, -0.4748, -0.1807],\n",
       "                        [-0.2903, -0.0825, -0.2838]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3434,  0.9528,  0.2565],\n",
       "                        [ 0.2335,  0.6749,  0.0929],\n",
       "                        [ 0.0208, -0.4484,  0.0573]]]], device='cuda:0')),\n",
       "             ('module.model.19.1.weight',\n",
       "              tensor([1.1166, 0.6533, 1.0222, 0.5388, 0.6130, 0.5807, 0.6231, 0.6595, 0.7990,\n",
       "                      0.6720, 0.5872, 0.4512, 0.8379, 0.4853, 0.4675, 1.0491, 0.6048, 0.6618,\n",
       "                      0.6593, 0.6416, 0.6888, 0.5358, 0.8414, 0.6013, 1.3062, 0.6686, 0.7893,\n",
       "                      0.6862, 0.5094, 0.7846, 0.8011, 0.5515, 0.5758, 0.6941, 0.8606, 1.2119,\n",
       "                      0.6690, 0.4541, 0.6590, 0.6055, 0.8368, 0.5557, 0.6860, 1.1418, 0.4528,\n",
       "                      0.9599, 0.6238, 1.0823, 0.5438, 0.8596, 0.4600, 0.7074, 0.7187, 0.6969,\n",
       "                      0.8932, 0.9630, 0.9891, 1.1102, 1.2018, 0.6422, 1.6343, 0.7333, 0.6568,\n",
       "                      0.8144, 0.7909, 0.9798, 0.8705, 0.7233, 0.5878, 0.6509, 1.0099, 0.7509,\n",
       "                      1.0262, 0.6127, 1.0270, 0.8685, 0.8330, 0.6372, 0.8779, 0.4917, 0.9410,\n",
       "                      0.4353, 0.9314, 0.7720, 0.8284, 0.5056, 0.7348, 0.7451, 0.5349, 0.8290,\n",
       "                      0.4845, 0.8658, 0.7524, 0.5939, 0.7792, 0.7148, 0.8485, 0.6247, 0.6136,\n",
       "                      0.6792, 1.1741, 0.7824, 0.6223, 0.7408, 0.6830, 1.0248, 0.6436, 0.8559,\n",
       "                      0.5414, 0.8591, 0.4296, 1.0200, 0.6074, 0.5102, 0.6682, 0.5158, 0.5446,\n",
       "                      0.8865, 0.5089, 0.5995, 0.6894, 0.6139, 1.0972, 0.9305, 1.1355, 0.5611,\n",
       "                      0.9876, 0.4602, 0.5136, 0.7712, 0.8595, 0.7785, 0.7011, 0.6911, 0.6091,\n",
       "                      1.2341, 0.4802, 0.9742, 0.6273, 0.7557, 0.6371, 0.7269, 0.9077, 0.7779,\n",
       "                      0.9243, 0.7826, 1.5870, 0.5771, 0.8526, 0.5676, 0.7290, 0.6989, 0.7953,\n",
       "                      0.7724, 0.7911, 0.6963, 0.8548, 0.8311, 0.7489, 0.7902, 0.6906, 1.0566,\n",
       "                      0.6035, 0.7615, 0.7882, 0.5833, 0.5238, 1.0932, 0.8268, 0.7690, 0.6463,\n",
       "                      0.5943, 0.6604, 0.5936, 1.0369, 0.5452, 0.8416, 1.2640, 0.5774, 1.0629,\n",
       "                      0.5050, 0.5912, 0.9426, 0.6339, 0.4858, 0.5176, 0.9534, 0.7958, 0.7315,\n",
       "                      0.6463, 0.6218, 0.9334, 0.8355, 1.6100, 1.3854, 0.5475, 0.7365, 0.5719,\n",
       "                      1.2146, 0.7942, 0.7138, 0.5298, 0.4751, 0.4971, 0.7137, 0.6056, 0.8996,\n",
       "                      0.6274, 0.5365, 1.2566, 0.9613, 1.3180, 1.0646, 0.6382, 0.6961, 0.5032,\n",
       "                      0.5969, 1.0247, 0.5354, 0.7135, 0.9873, 0.9739, 0.5203, 0.8505, 1.0057,\n",
       "                      0.4360, 0.7065, 1.0970, 0.5217, 0.5122, 0.5635, 0.5559, 0.8074, 0.6219,\n",
       "                      0.8133, 0.9274, 0.9102, 0.8946, 0.8383, 0.5479, 0.9583, 0.7003, 1.0356,\n",
       "                      0.7137, 0.6713, 0.9101, 0.4532, 0.8160, 0.5794, 0.4209, 0.5732, 0.6751,\n",
       "                      0.5683, 0.7005, 0.5429, 0.8199, 0.5701, 0.5090, 0.7297, 0.6613, 0.7201,\n",
       "                      0.6306, 0.6199, 0.7651, 0.8479, 1.0070, 1.2865, 0.6345, 0.6503, 0.5851,\n",
       "                      0.6524, 0.9261, 1.0843, 0.7802, 0.6263, 1.3422, 0.8318, 0.6499, 0.5949,\n",
       "                      0.7677, 0.5298, 1.7617, 0.6158, 0.5974, 0.9794, 0.6465, 0.4544, 0.5641,\n",
       "                      0.9578, 0.6824, 0.6938, 0.6525, 0.7104, 0.5597, 0.7940, 1.1568, 0.9009,\n",
       "                      0.9789, 0.8095, 0.7317, 0.4166, 0.6957, 0.6210, 1.5272, 0.7244, 0.9670,\n",
       "                      0.8315, 0.4823, 1.0785, 0.4216, 0.5232, 0.4542, 0.5131, 1.2996, 0.4969,\n",
       "                      0.8901, 0.8476, 0.8476, 1.4064, 1.4188, 0.6662, 0.4234, 0.9822, 0.6728,\n",
       "                      0.7602, 0.8360, 0.8222, 0.8347, 0.6119, 0.8707, 1.0535, 1.0681, 0.5792,\n",
       "                      0.9200, 0.9835, 0.5759, 0.5799, 0.6762, 0.7323, 1.4434, 0.8761, 0.7581,\n",
       "                      0.8581, 0.8189, 0.6935, 1.5066, 1.0606, 1.3166, 0.4748, 0.7256, 1.2129,\n",
       "                      0.7905, 0.8930, 0.7032, 1.2956, 1.0594, 0.9562, 0.6036, 0.5432, 0.8126,\n",
       "                      0.7319, 0.6986, 0.7549, 0.5120, 0.6146, 1.2864, 0.8744, 0.7016, 0.9056,\n",
       "                      0.7412, 0.7011, 0.5355, 0.7379, 0.6549, 1.0373, 0.7614, 1.1146, 0.5923,\n",
       "                      0.6737, 0.5138, 0.8029, 0.6909, 0.5734, 0.8887, 0.6565, 0.7588, 0.7572,\n",
       "                      1.0690, 0.9560, 0.7005, 0.7256, 0.5952, 0.9325, 0.7079, 0.7551, 0.5798,\n",
       "                      0.8434, 1.5111, 0.8839, 0.9051, 0.4163, 0.7742, 0.5855, 0.7746, 0.4179,\n",
       "                      0.7849, 0.8235, 1.1845, 0.7047, 0.5063, 0.4288, 0.7702, 0.7379, 0.5690,\n",
       "                      0.7782, 0.5571, 0.7084, 0.5487, 1.1694, 0.6358, 0.9173, 0.6955, 0.8304,\n",
       "                      0.8216, 1.0375, 1.1416, 0.9461, 0.5232, 0.7166, 0.7263, 0.4476, 0.5305,\n",
       "                      0.4934, 1.3827, 0.4190, 0.6359, 0.5898, 0.9714, 1.2255, 0.7311, 0.5703,\n",
       "                      0.6718, 0.6509, 0.6858, 1.4644, 0.8841, 0.6122, 1.3396, 0.7019, 0.5812,\n",
       "                      0.5825, 0.6586, 0.6764, 0.6200, 0.6757, 0.9614, 0.6817, 0.7444, 1.1091,\n",
       "                      0.8390, 0.6342, 0.7098, 0.8793, 1.3345, 1.5261, 0.7604, 1.0021, 0.9087,\n",
       "                      1.2369, 0.8825, 0.4992, 1.0206, 1.1325, 0.7272, 0.5643, 0.7830, 0.8554,\n",
       "                      0.4872, 0.6962, 0.8524, 0.5932, 0.9385, 1.1061, 1.6881, 0.6647, 0.6633,\n",
       "                      1.1055, 0.9062, 0.9575, 1.0669, 0.6570, 0.6347, 0.7091, 0.5965, 0.7844,\n",
       "                      0.8958, 0.4411, 0.7610, 0.6089, 1.0664, 0.6898, 0.9995, 0.7100, 0.7089,\n",
       "                      0.8397, 1.2024, 0.6781, 0.6745, 1.3032, 0.7943, 0.5425, 0.6449],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.19.1.bias',\n",
       "              tensor([-1.0744e-01,  1.5157e+00, -4.9698e-01,  1.5690e+00,  4.1302e-01,\n",
       "                       1.4753e+00,  1.9457e-02,  1.6183e-01, -3.9643e-02,  1.8746e-01,\n",
       "                       1.3786e+00,  1.1991e+00,  7.7850e-02,  7.8232e-01,  2.6565e-01,\n",
       "                      -1.8109e-01,  1.5404e-02,  3.3692e-01, -7.5382e-02, -1.4598e-02,\n",
       "                       5.1057e-01,  6.3055e-03,  3.7549e-01,  1.3258e+00, -3.7906e-01,\n",
       "                       5.2833e-01, -1.9603e-01,  6.9227e-01,  1.5546e+00, -1.8784e-01,\n",
       "                      -1.7800e-02,  1.4242e+00,  2.0771e+00,  1.2042e+00, -6.6025e-01,\n",
       "                      -2.9236e-01,  1.6746e-01,  9.3169e-01,  9.1960e-01,  7.2166e-01,\n",
       "                      -3.5591e-01,  1.1214e-02, -5.3969e-01, -9.2186e-01,  1.0057e+00,\n",
       "                       9.7166e-02,  8.3715e-01, -3.9390e-01,  4.7592e-01, -4.8589e-01,\n",
       "                       1.5090e+00, -2.7565e-01,  9.9689e-02,  6.1805e-01,  1.2533e+00,\n",
       "                      -1.6691e-01,  2.2185e+00, -2.7948e-01, -3.9446e-01,  4.3649e-01,\n",
       "                      -8.2045e-01, -4.8509e-01,  3.7087e-01,  2.3115e-01, -2.8807e-01,\n",
       "                       1.3514e-01, -6.4294e-01, -3.6851e-01,  1.7923e+00, -2.6373e-01,\n",
       "                      -1.0372e+00,  6.2012e-01,  4.3587e-02,  2.1315e-01,  1.0642e+00,\n",
       "                      -1.3481e+00,  2.0652e+00, -2.2523e-01, -3.0161e-01,  4.7099e-01,\n",
       "                      -1.9072e-01,  1.5644e-01, -2.1364e-01,  2.5078e-01,  4.2836e-01,\n",
       "                       4.5789e-01, -4.9256e-01,  1.6779e-01,  6.0572e-02,  1.8264e-02,\n",
       "                       8.7841e-01, -6.8923e-01, -3.7978e-01,  3.5450e-01,  5.0382e-01,\n",
       "                       2.2563e-01,  4.2620e-01,  1.7417e+00, -1.2217e+00,  2.4143e+00,\n",
       "                      -7.9093e-01, -5.1758e-01,  6.8706e-01,  5.1610e-01, -1.8936e-01,\n",
       "                      -4.3608e-01,  3.1335e-01,  9.5974e-01,  4.8297e-01, -7.9543e-01,\n",
       "                       6.9767e-01, -6.0367e-02,  1.5354e-01, -3.4333e-01,  2.9203e-01,\n",
       "                       1.7909e+00,  3.9577e-01,  8.5873e-01,  1.8770e-01,  1.3546e+00,\n",
       "                      -1.0274e-02,  1.0666e+00, -1.3238e+00, -6.3481e-01, -1.6818e+00,\n",
       "                       8.6444e-02, -9.9003e-01,  1.8892e-02,  6.4491e-02, -2.0856e-01,\n",
       "                       2.1781e+00, -3.8017e-01,  2.0082e-01, -1.2451e-01,  1.7674e+00,\n",
       "                      -2.5219e-01,  1.2455e+00,  1.2294e-01, -1.8554e-01,  3.3172e-01,\n",
       "                      -2.9228e-02, -2.5125e-01,  9.5490e-01,  8.7278e-02,  5.4659e-02,\n",
       "                       1.9337e+00, -9.5260e-01,  2.3227e-01, -4.7412e-02,  1.3050e-03,\n",
       "                      -2.9156e-01,  2.1233e-01, -2.0573e-01, -7.7482e-02, -9.4244e-02,\n",
       "                       2.8669e-01,  7.1060e-02, -4.9799e-01, -3.6395e-01, -1.3298e-01,\n",
       "                       1.8035e+00, -1.3838e-01,  3.0541e-01, -1.9684e-01,  2.6373e-01,\n",
       "                      -3.1261e-01,  8.5761e-01, -1.5148e-01, -2.4145e-03,  5.3222e-01,\n",
       "                       2.5624e-01,  7.2479e-02, -9.8556e-02, -3.1175e-01,  4.4438e-02,\n",
       "                       1.4184e+00,  3.5951e-01, -4.9224e-01,  1.1124e+00, -5.8351e-01,\n",
       "                       1.2704e+00, -1.1934e-01,  9.7318e-01,  2.1318e-01,  1.5471e+00,\n",
       "                       6.9154e-01,  1.7414e-01, -5.2236e-01, -1.6028e-01,  1.1874e-01,\n",
       "                      -2.9720e-01, -2.1407e-01,  7.9707e-02, -1.5129e+00, -5.3229e-01,\n",
       "                       6.1515e-01,  9.3637e-01,  1.4270e+00, -2.2793e-01,  1.4595e+00,\n",
       "                       1.9054e+00,  1.1789e+00,  1.1268e+00,  7.6830e-01, -3.3934e-01,\n",
       "                      -1.3064e-01,  1.8620e-01,  1.4047e+00,  1.6721e+00, -2.0485e-01,\n",
       "                       4.7347e-01, -6.5009e-01, -1.4686e-01,  1.9778e-01,  4.0082e-01,\n",
       "                       4.1683e-02,  1.1100e+00,  2.9064e-01,  1.7533e+00,  2.1656e-01,\n",
       "                      -3.7277e-01, -6.3193e-01, -1.0553e-01, -1.0089e+00, -1.1699e+00,\n",
       "                       9.6909e-01, -6.8105e-02, -3.7321e-01,  1.4707e+00,  2.8465e-01,\n",
       "                       6.4600e-01,  1.7298e-01,  2.3975e-01,  1.2546e+00,  1.0440e+00,\n",
       "                       6.5802e-01, -4.3017e-01, -2.9995e-01, -6.9520e-01,  8.3733e-01,\n",
       "                      -8.7679e-01, -3.5060e-01, -3.1908e-01,  3.5830e-01, -2.4029e-02,\n",
       "                       3.7309e-01,  1.2099e-01, -2.0434e-02,  1.9339e+00,  9.8443e-01,\n",
       "                       1.5209e+00,  1.9870e-01, -1.8854e-01, -1.7768e-01,  5.0431e-01,\n",
       "                       4.9156e-01,  9.0285e-01,  7.9304e-02,  2.0612e+00, -3.4881e-01,\n",
       "                      -4.9515e-01, -1.6905e-01,  1.3611e+00, -4.0089e-01,  7.5961e-01,\n",
       "                       1.0607e+00, -1.2177e+00,  8.9286e-01, -6.0233e-03,  1.3503e+00,\n",
       "                       7.1799e-02,  1.2481e+00, -2.4272e-01,  2.8960e-03,  2.2493e+00,\n",
       "                      -3.3852e-01,  9.7009e-01,  1.8060e+00, -7.5816e-02,  1.4837e-01,\n",
       "                       4.9797e-01, -9.0394e-01,  1.7677e+00,  2.4263e+00, -1.3195e-01,\n",
       "                      -1.3134e-01, -6.9993e-02,  1.4892e+00, -5.1674e-01,  2.1120e-01,\n",
       "                       1.6059e+00,  5.5710e-01,  8.6229e-01,  8.0248e-01, -1.7285e-01,\n",
       "                      -2.1374e-01, -5.7347e-01,  1.0503e-01, -2.8743e-01,  8.1986e-02,\n",
       "                       9.1946e-01, -7.4771e-02,  4.4363e-01, -5.9511e-01, -1.1995e-01,\n",
       "                      -6.6507e-01,  7.3868e-01,  9.8930e-01, -6.1800e-02,  6.9050e-01,\n",
       "                       1.3580e+00,  2.1949e-01,  1.4277e-01, -1.3132e-01,  8.7011e-01,\n",
       "                      -1.7778e-01,  2.0432e+00, -7.0389e-01, -3.3394e-01, -5.7658e-01,\n",
       "                      -1.1192e-01,  1.2795e+00,  2.0711e-01,  1.5890e-01,  1.9150e+00,\n",
       "                       2.2810e-01,  2.3129e-01,  1.5505e+00,  1.3013e-01, -3.3926e-02,\n",
       "                       6.1508e-02, -1.0961e-01,  1.0974e+00, -5.6606e-01, -4.5938e-01,\n",
       "                       7.6283e-01,  7.9997e-02,  3.6108e-01, -1.3957e-01, -5.8240e-01,\n",
       "                      -6.0247e-01,  8.6871e-01,  1.8774e-01, -1.4239e-01, -1.9688e-01,\n",
       "                      -1.1289e+00, -1.9477e-01, -3.9517e-01,  3.8836e-02,  1.7879e+00,\n",
       "                      -1.8429e-01, -4.5845e-01, -4.2604e-01,  1.8705e-01, -5.4095e-01,\n",
       "                      -5.3121e-01, -1.1769e+00,  3.6680e-01,  9.7310e-01,  1.9195e-01,\n",
       "                       3.9977e-01,  9.8823e-02,  2.3263e-01,  8.1910e-01, -2.8101e-01,\n",
       "                      -2.6688e-01, -2.4527e-01,  1.1614e-01, -7.7369e-01,  2.9819e-01,\n",
       "                       3.3449e-01, -3.0317e-01,  1.0573e-02,  9.9048e-01, -5.7273e-01,\n",
       "                      -4.7392e-01, -4.1375e-01,  9.5536e-01,  4.4501e-01,  6.8168e-01,\n",
       "                       1.4348e-01,  1.6615e-01,  8.9334e-01, -5.0528e-01, -1.5824e-01,\n",
       "                      -4.2793e-01,  8.0631e-02, -7.0981e-01,  2.1842e+00,  4.2266e-02,\n",
       "                       6.3816e-01,  1.1090e-01, -4.4712e-01,  4.1972e-02,  1.1772e-01,\n",
       "                       1.2586e+00,  5.7233e-01, -1.5206e+00,  1.5676e+00, -7.7633e-01,\n",
       "                       1.0854e+00,  6.7770e-02, -1.9598e-01, -6.3665e-01,  1.8239e+00,\n",
       "                       2.4212e-01, -5.2591e-02, -1.6494e+00, -2.1969e-01, -9.2071e-02,\n",
       "                       1.9499e+00, -1.5219e-01, -4.3079e-02,  1.0304e+00,  2.0822e+00,\n",
       "                       8.2893e-01, -1.5301e-01,  1.2703e+00, -5.8037e-02,  9.3830e-01,\n",
       "                      -3.7375e-02,  3.4187e-01,  8.9851e-02,  1.7924e+00, -8.1561e-01,\n",
       "                      -1.2466e-01, -1.3422e-01,  1.7635e+00, -2.1845e-01,  3.5962e-01,\n",
       "                       7.4680e-01,  6.5165e-01,  1.2756e+00, -6.1234e-01,  6.1402e-01,\n",
       "                      -3.0737e-01, -3.4481e-02,  3.9366e-02, -1.8001e-01,  5.6078e-01,\n",
       "                       9.9962e-01,  9.1239e-01, -7.9737e-02, -1.5069e-01, -7.0443e-02,\n",
       "                       2.0626e-01,  3.4078e-01, -7.4973e-01, -1.7914e-01,  9.2075e-01,\n",
       "                      -1.5513e-01,  1.8717e+00, -1.7530e-01, -3.6443e-01, -1.7071e-01,\n",
       "                      -7.3994e-01, -1.0962e-01, -5.8553e-01,  2.9549e-01,  1.5604e-01,\n",
       "                       4.2017e-01,  1.7796e-02,  7.4634e-01, -4.2565e-01, -9.6317e-01,\n",
       "                      -3.6009e-01, -2.1629e-01,  5.6770e-02, -2.0132e-01, -6.6314e-01,\n",
       "                      -4.3001e-01,  2.0041e-02, -1.5358e+00,  3.9673e-01,  4.2696e-02,\n",
       "                      -3.5060e-01, -8.2215e-01,  1.6511e+00, -1.9658e-01,  6.3627e-01,\n",
       "                      -7.9632e-01, -1.1549e+00, -3.5576e-01, -3.8269e-01, -5.3797e-02,\n",
       "                       2.5673e-01,  1.7031e-01, -8.7520e-01, -1.2542e+00, -2.0057e-01,\n",
       "                      -2.5033e-01,  1.1967e+00,  8.3470e-01,  5.9966e-01, -6.0289e-01,\n",
       "                      -3.8353e-01,  9.4528e-01,  1.6525e-01,  3.3730e-01, -4.0604e-01,\n",
       "                       1.0338e-02,  2.7171e-02,  3.6461e-01,  2.2012e-01,  1.0074e-01,\n",
       "                      -8.5751e-01,  5.1249e-02,  4.1914e-01, -1.8050e-01,  3.0672e-01,\n",
       "                       4.6432e-01, -1.0233e-01], device='cuda:0')),\n",
       "             ('module.model.19.1.running_mean',\n",
       "              tensor([-9.9390e-01, -6.5823e-02,  1.2536e+00,  2.1093e-01, -1.0874e+00,\n",
       "                      -1.4049e+00,  8.5531e-01,  5.6006e-01,  5.5722e-01,  7.2097e-01,\n",
       "                       2.3069e-01, -1.2068e+00,  9.2060e-01, -1.1514e+00,  2.9857e-01,\n",
       "                      -1.2696e+00,  3.6582e-01,  5.9995e-01,  6.2409e-01,  4.9249e-01,\n",
       "                       6.7425e-01,  4.3503e-01, -7.7863e-01,  6.0805e-01, -4.1738e-01,\n",
       "                      -1.6760e+00,  6.6512e-01, -3.5035e-01, -9.9296e-01,  6.6431e-01,\n",
       "                       1.0163e+00, -4.5715e-01, -9.7765e-01,  5.2266e-01,  5.2517e-01,\n",
       "                       7.2877e-02,  9.4937e-01, -3.5653e-01,  3.6361e-02, -1.3447e+00,\n",
       "                       3.2457e-01,  4.9321e-01,  2.4217e-01,  1.2038e+00, -5.3185e-01,\n",
       "                      -2.4267e-01, -1.4245e+00, -2.6302e-01, -8.5339e-01,  2.7046e-01,\n",
       "                      -9.3027e-01,  1.2398e-01,  7.7387e-01,  3.7534e-01,  1.8434e-01,\n",
       "                      -1.5991e+00, -6.5819e-01, -1.6752e+00, -2.0588e-01, -7.4526e-01,\n",
       "                      -1.5116e+00,  3.5672e-01, -2.0118e+00, -1.2552e+00,  7.2709e-01,\n",
       "                      -2.5614e+00,  7.1814e-01,  3.9102e-01, -9.5420e-01,  7.8182e-01,\n",
       "                       5.4115e-01, -4.2663e-01, -9.2025e-01,  5.9636e-02,  2.8123e-01,\n",
       "                       2.3371e-01,  1.0582e-01,  1.1455e-01,  1.1406e+00,  1.0428e-01,\n",
       "                       4.6028e-01,  3.5928e-01, -1.4710e+00, -3.8213e-02,  2.0596e-01,\n",
       "                      -1.0908e+00,  3.5384e-01, -1.2190e+00,  5.4984e-01,  6.8861e-01,\n",
       "                       1.8780e-01,  7.3977e-01,  5.0898e-01,  7.1448e-01, -4.4709e-01,\n",
       "                       7.3167e-02, -9.0296e-01,  8.3816e-02,  1.2405e-01,  1.3666e-01,\n",
       "                       6.4498e-01,  1.8584e+00,  1.0463e-01, -1.4926e+00,  7.0330e-01,\n",
       "                       7.4447e-01, -1.1674e+00,  8.8553e-01, -1.2497e+00,  6.6900e-01,\n",
       "                      -7.2394e-01,  1.0663e+00,  5.8168e-01,  1.2652e-01,  6.0758e-01,\n",
       "                       1.7066e-01, -1.0103e+00,  8.6331e-01,  4.4348e-01, -5.3106e-01,\n",
       "                       7.1828e-01,  1.3461e-01,  1.1636e+00,  7.0813e-01,  4.9386e-01,\n",
       "                       7.0918e-01,  2.8814e-01,  2.4733e-01,  1.5755e-01,  9.1419e-01,\n",
       "                       2.6197e-01,  8.5107e-01,  6.9319e-01,  6.0883e-01, -4.3668e-01,\n",
       "                      -1.4695e+00, -8.0133e-01, -3.5649e-01,  4.3237e-01, -1.4631e+00,\n",
       "                       5.4747e-01,  5.9280e-01, -6.1416e-01, -2.3661e+00, -1.4297e+00,\n",
       "                      -6.0199e-02, -3.0033e+00,  3.4635e-01,  1.8527e+00,  8.2914e-01,\n",
       "                       4.1899e-01, -1.9044e+00,  5.8103e-01,  1.0786e+00,  8.7411e-01,\n",
       "                       9.0533e-01, -1.6986e+00,  8.8108e-01,  7.5336e-01,  5.4011e-01,\n",
       "                      -3.5158e-01, -1.2367e+00,  7.1262e-01,  6.4190e-01, -9.8762e-01,\n",
       "                       1.9827e-01, -6.8763e-01, -1.5353e+00, -1.6457e+00, -2.8433e+00,\n",
       "                       6.3644e-01,  1.2467e+00,  5.5639e-01,  4.4495e-01, -1.6196e+00,\n",
       "                       1.2910e-01, -6.8053e-01, -1.5047e+00, -4.0470e-03,  1.1826e+00,\n",
       "                      -5.8820e-01,  4.7442e-01,  7.7561e-01,  1.1767e+00,  3.8075e-01,\n",
       "                      -1.0167e+00, -1.3207e+00,  6.9332e-01,  7.6717e-01,  6.3392e-01,\n",
       "                       5.9138e-01,  6.2790e-01, -1.2677e+00,  1.9831e+00,  2.5909e-01,\n",
       "                      -1.8599e+00,  7.8966e-01,  1.8874e-01, -1.6035e+00,  9.8204e-01,\n",
       "                      -2.1951e-01, -1.7150e-01, -7.4996e-01, -6.3136e-01,  6.3199e-01,\n",
       "                       4.7350e-01, -8.3092e-02, -1.2467e+00, -1.1626e+00, -1.7235e+00,\n",
       "                       1.4510e+00, -1.7241e+00, -1.8300e-01,  4.2737e-01,  9.2149e-01,\n",
       "                       1.8441e-01,  3.2080e-01, -9.9237e-01,  7.5082e-02, -1.6547e+00,\n",
       "                      -2.5517e-02,  2.9092e-01,  2.5934e-01,  1.1537e-01,  1.3215e+00,\n",
       "                      -5.7279e-01,  3.2866e-01, -2.2268e-01,  9.3126e-02,  5.1896e-01,\n",
       "                      -8.4207e-01,  3.9333e-01, -1.0045e+00, -1.0297e-01,  7.2842e-01,\n",
       "                       1.6004e+00,  4.4664e-01,  2.2228e-01,  5.5603e-01,  3.5033e-01,\n",
       "                       4.4467e-01,  1.8049e-01,  6.3744e-01, -2.1710e+00,  8.6719e-01,\n",
       "                       5.7777e-01,  5.4871e-01, -1.7052e+00,  1.7890e-01, -8.0436e-01,\n",
       "                      -1.2815e+00,  1.8533e-02,  2.4960e-01,  4.7338e-01, -1.6356e+00,\n",
       "                       8.3342e-01, -1.3737e+00,  3.7592e-01,  2.8337e-03,  3.5035e-01,\n",
       "                       3.7480e-01,  5.1464e-01, -9.6887e-01,  6.0576e-01, -9.6273e-01,\n",
       "                       1.6097e-01,  4.4508e-01, -3.7247e-01,  4.0170e-01,  9.7424e-02,\n",
       "                       5.1934e-01, -1.0760e+00, -1.9031e+00, -1.9374e+00, -5.0956e-01,\n",
       "                      -1.6543e+00,  9.5588e-01, -1.8200e-01,  3.2422e-01,  1.7392e-01,\n",
       "                      -1.0288e+00, -1.8323e+00, -2.4976e-01,  2.2682e-01,  2.6263e-01,\n",
       "                       6.0679e-01,  1.8972e-01,  2.3085e-01,  5.8651e-01, -1.8540e+00,\n",
       "                      -3.6110e-01, -6.8640e-01,  7.4736e-02,  1.4861e-01,  5.2778e-02,\n",
       "                      -1.3082e+00,  8.6846e-01, -1.0522e+00,  6.6410e-01, -3.2308e-02,\n",
       "                      -5.8148e-01,  5.1491e-01,  3.2408e-02, -1.7887e+00,  9.5896e-01,\n",
       "                       1.0632e-01, -5.9707e-01, -1.1969e+00, -1.7423e+00, -6.5932e-01,\n",
       "                       4.6144e-01,  4.5603e-01,  5.3703e-01, -1.0730e+00, -7.1033e-01,\n",
       "                       6.6714e-01,  2.5062e-01,  1.0775e+00, -1.2008e+00, -1.4993e+00,\n",
       "                       5.2571e-01, -1.3648e+00,  1.1774e+00,  1.4008e+00,  5.8346e-01,\n",
       "                      -3.5786e+00, -9.6580e-01,  1.0737e+00,  5.5395e-01,  5.8283e-01,\n",
       "                      -1.0046e+00, -1.8765e+00, -1.3748e+00,  7.6999e-01,  1.7881e+00,\n",
       "                      -1.2176e+00,  4.0344e-01, -9.2826e-01,  8.2691e-01, -1.6751e+00,\n",
       "                       9.3410e-01, -5.3430e-01, -1.6477e+00,  8.1816e-01,  5.1498e-01,\n",
       "                      -2.4931e-01, -6.9412e-02, -1.2726e+00,  1.8589e-01, -1.2917e-01,\n",
       "                      -1.3040e+00,  1.3533e+00,  8.6819e-01, -1.7774e+00, -1.6247e-01,\n",
       "                       2.0364e-01,  6.5293e-01, -1.5299e+00,  1.3800e-01, -1.3719e+00,\n",
       "                      -1.1755e+00,  7.2994e-01, -1.6011e+00,  4.5390e-01,  3.5067e-01,\n",
       "                      -2.4267e+00,  1.0345e+00,  5.9916e-01,  7.3470e-01, -2.1117e+00,\n",
       "                       1.1216e+00,  1.5317e-01,  8.5071e-01,  5.6104e-01,  6.4809e-01,\n",
       "                       4.8553e-01, -2.0000e+00,  5.2950e-01, -1.4603e+00, -9.2859e-01,\n",
       "                      -1.6751e+00, -9.8727e-01, -1.3888e+00,  7.7604e-01,  4.2603e-01,\n",
       "                       6.6616e-01,  1.0925e+00,  2.9965e-01, -2.0182e-01,  5.0733e-01,\n",
       "                      -1.5249e+00,  2.9988e-01,  5.7325e-01,  6.8804e-01,  9.0595e-01,\n",
       "                      -3.6120e-01, -1.2752e+00,  1.2012e+00, -1.1779e+00,  6.5199e-01,\n",
       "                      -5.3260e-01,  7.2697e-01,  3.2063e-01,  5.8911e-01, -9.1962e-01,\n",
       "                      -1.1981e+00,  7.1139e-01,  4.9889e-01,  1.0367e+00,  1.4984e-01,\n",
       "                      -7.6495e-01,  9.8828e-01,  8.8832e-01, -8.0285e-01,  2.2724e-01,\n",
       "                      -4.1536e-01,  7.0643e-01,  5.9425e-01, -9.5475e-01,  3.6693e-01,\n",
       "                      -4.3201e+00, -1.1549e+00, -1.8306e+00,  1.0365e+00,  5.6655e-01,\n",
       "                      -1.5681e+00, -1.7116e+00,  1.5579e-02,  5.4571e-01, -1.2848e+00,\n",
       "                      -9.3087e-01, -1.2074e+00, -9.1624e-01, -2.0481e+00, -1.0272e+00,\n",
       "                       2.3057e-01,  2.7797e-01, -8.4782e-01, -1.6645e+00, -1.3283e+00,\n",
       "                      -1.2618e+00, -6.0527e-01,  4.1512e-01,  8.7527e-01, -1.0623e+00,\n",
       "                      -3.6541e-01,  7.5515e-01,  5.4546e-01,  7.3726e-01,  3.8025e-01,\n",
       "                       3.4918e-01, -2.3839e-01,  4.5472e-01,  4.7920e-01,  3.5619e-01,\n",
       "                       6.9286e-01,  4.0106e-01,  4.3302e-02,  2.4973e-01, -1.5915e+00,\n",
       "                      -2.2079e+00,  6.0170e-01,  8.7019e-01, -1.5894e+00, -1.9720e+00,\n",
       "                       5.9353e-01,  6.7852e-01, -1.2122e+00, -1.2527e+00,  8.9442e-01,\n",
       "                       3.2323e-02, -1.1501e+00,  4.8670e-01, -9.9395e-01,  4.7678e-01,\n",
       "                       8.0439e-01,  4.6489e-01, -7.0815e-02,  5.7778e-01, -3.2358e-01,\n",
       "                       8.8803e-02,  1.8608e-01, -1.7976e+00, -1.6478e+00,  3.5782e-01,\n",
       "                      -1.6091e+00, -1.1142e+00,  6.4333e-01,  7.5472e-01, -1.2294e+00,\n",
       "                       7.6404e-01,  2.7345e-03, -1.6488e-01, -1.5389e+00,  6.8596e-01,\n",
       "                       1.0994e+00, -5.3998e-01, -1.1633e+00, -1.1614e+00, -1.7743e+00,\n",
       "                       8.3334e-01, -3.2547e-01, -8.2538e-01, -2.0679e+00, -1.0430e+00,\n",
       "                      -3.0486e-01,  7.0777e-01, -1.6502e+00, -1.1328e+00, -1.1189e+00,\n",
       "                      -1.3026e+00,  2.9935e-01], device='cuda:0')),\n",
       "             ('module.model.19.1.running_var',\n",
       "              tensor([1.1241, 0.6250, 1.8991, 0.6789, 1.0099, 2.5359, 0.7249, 1.2162, 0.7053,\n",
       "                      1.0458, 1.7673, 0.8695, 1.5980, 0.8319, 0.2320, 1.1174, 0.4934, 1.0828,\n",
       "                      0.6413, 0.7128, 0.7760, 0.5692, 0.9723, 1.3224, 4.6500, 0.6902, 1.3730,\n",
       "                      0.3080, 1.0634, 0.8139, 1.9155, 1.4620, 1.0841, 1.6018, 0.9550, 0.4243,\n",
       "                      1.7613, 0.2134, 1.2947, 1.7285, 0.3953, 0.4481, 0.3255, 2.4460, 0.3225,\n",
       "                      1.5008, 1.1691, 1.6379, 0.5625, 0.2414, 0.9407, 0.2026, 1.3668, 1.4782,\n",
       "                      0.9007, 0.5980, 1.2295, 1.1508, 2.2205, 0.5620, 0.9233, 0.5424, 0.6207,\n",
       "                      1.6657, 0.6996, 4.7382, 0.8580, 0.3763, 1.7893, 0.9250, 0.4508, 0.6735,\n",
       "                      0.9286, 0.1315, 3.0503, 0.1984, 2.1119, 0.2120, 2.2682, 0.2001, 1.0101,\n",
       "                      0.3664, 0.6546, 0.6337, 1.0595, 0.6987, 0.4247, 0.6624, 0.5037, 1.2209,\n",
       "                      0.3208, 0.9967, 0.6120, 1.1150, 1.0601, 3.2202, 0.9546, 1.0427, 0.1304,\n",
       "                      1.3016, 0.6890, 1.7112, 0.4933, 0.0988, 0.7017, 1.1003, 0.7090, 0.8158,\n",
       "                      0.5660, 0.8758, 0.4736, 0.7884, 0.8469, 0.0926, 1.2648, 2.1062, 0.6770,\n",
       "                      1.1520, 0.4954, 1.0326, 1.0706, 1.1456, 0.9378, 1.2293, 0.6635, 0.8602,\n",
       "                      0.3051, 0.2270, 0.1297, 1.0888, 1.9679, 1.4574, 1.0809, 0.6782, 0.9227,\n",
       "                      1.2004, 0.7954, 1.0199, 0.5183, 1.5503, 0.8474, 0.7700, 3.0287, 0.5135,\n",
       "                      1.2476, 1.3886, 1.3521, 0.6818, 1.7659, 0.9945, 0.3259, 0.1955, 0.5775,\n",
       "                      1.5164, 1.3041, 1.5179, 0.6432, 0.7304, 0.9013, 1.1210, 1.5237, 0.8587,\n",
       "                      0.9874, 0.7209, 1.0426, 0.2259, 0.7170, 1.0502, 0.1910, 1.1239, 1.2767,\n",
       "                      1.5926, 0.7327, 0.4615, 1.7023, 0.8179, 0.6581, 1.0341, 1.0693, 1.0852,\n",
       "                      0.3573, 0.4983, 0.5538, 2.0389, 1.1825, 0.5965, 1.4146, 0.9734, 0.7972,\n",
       "                      0.9688, 0.5455, 1.5337, 1.1425, 1.0094, 1.3391, 0.2771, 2.0476, 1.2951,\n",
       "                      1.2388, 1.3376, 1.0002, 0.5653, 0.5681, 0.3988, 0.7434, 0.6470, 1.0932,\n",
       "                      1.0004, 0.9688, 1.2101, 0.4717, 1.1454, 2.0933, 0.6414, 2.0835, 0.1489,\n",
       "                      1.3059, 1.4798, 1.5092, 1.2385, 1.6539, 0.5864, 0.2662, 0.1526, 1.4711,\n",
       "                      0.4163, 0.5180, 3.2816, 0.7184, 0.6068, 0.5429, 0.5107, 0.8796, 1.1047,\n",
       "                      1.2790, 0.9437, 0.5391, 1.3126, 0.9461, 0.6544, 0.6287, 0.2787, 0.6206,\n",
       "                      0.6805, 1.1764, 1.2774, 0.5243, 0.6707, 0.4083, 0.4602, 2.2882, 0.2905,\n",
       "                      0.1901, 0.5765, 0.6704, 1.3977, 0.4176, 0.3737, 1.6021, 0.2648, 0.3354,\n",
       "                      0.6224, 0.6691, 0.8321, 1.4989, 0.5661, 0.4456, 1.5333, 0.3421, 1.4757,\n",
       "                      0.6815, 0.5102, 1.0134, 0.4912, 1.1984, 1.1065, 1.5076, 0.8399, 0.4041,\n",
       "                      1.2550, 0.7744, 1.3347, 2.0164, 1.2059, 1.2312, 0.8787, 0.1477, 1.7711,\n",
       "                      0.7848, 0.5726, 1.0890, 0.4787, 1.0199, 0.8451, 1.4352, 1.2041, 1.0475,\n",
       "                      0.9565, 1.0927, 0.4831, 0.3036, 0.8628, 0.8671, 1.3491, 1.0766, 0.1462,\n",
       "                      1.1236, 0.7748, 1.5398, 0.3772, 1.9130, 0.4751, 0.4374, 1.0448, 0.5269,\n",
       "                      1.3326, 3.3628, 1.5874, 1.1494, 0.9987, 0.5792, 0.7954, 1.4436, 0.9727,\n",
       "                      2.2316, 1.7228, 0.8185, 1.3000, 0.4806, 0.8715, 1.1686, 0.8005, 1.0952,\n",
       "                      0.7650, 2.2847, 1.1492, 0.5887, 0.6167, 1.5638, 1.3472, 1.1727, 0.8006,\n",
       "                      1.0772, 0.8631, 0.7328, 0.5728, 0.8198, 1.1243, 0.1190, 0.3354, 1.0555,\n",
       "                      1.1769, 1.7424, 0.7813, 0.5431, 2.1093, 0.9106, 0.8250, 0.6070, 0.9934,\n",
       "                      1.2472, 0.9056, 1.0368, 0.7958, 0.3259, 1.9458, 1.4317, 0.6202, 0.9501,\n",
       "                      2.0115, 1.7253, 0.1542, 1.7313, 2.2004, 1.0858, 0.5664, 0.7372, 1.4765,\n",
       "                      0.9349, 0.8038, 1.2905, 0.6076, 0.5894, 1.0858, 0.5420, 1.0966, 1.6330,\n",
       "                      1.0558, 1.3299, 0.4072, 4.0582, 0.2858, 0.9969, 1.5969, 2.1068, 1.1113,\n",
       "                      0.4289, 1.5549, 1.5535, 0.4511, 0.3403, 0.8903, 0.3415, 0.6355, 0.6242,\n",
       "                      0.7878, 0.7765, 0.6902, 1.1870, 0.1564, 0.7471, 1.5246, 1.0132, 0.5989,\n",
       "                      1.5811, 0.4433, 0.8635, 1.2735, 0.9078, 1.0269, 4.4908, 0.9099, 0.6371,\n",
       "                      2.4740, 0.5101, 1.4120, 0.5837, 0.6990, 0.9226, 1.1203, 0.6422, 0.8481,\n",
       "                      0.7727, 0.8857, 0.6316, 0.2731, 0.3673, 0.6179, 1.3017, 0.9042, 0.8129,\n",
       "                      0.7193, 0.5045, 1.4575, 1.1706, 0.9023, 1.1934, 0.8358, 1.0689, 1.7210,\n",
       "                      0.3107, 1.2243, 0.7160, 0.5173, 0.4125, 1.1433, 0.7273, 0.2507, 1.4861,\n",
       "                      1.7020, 0.5493, 1.1126, 0.9045, 1.0709, 0.8319, 0.7878, 1.1842, 0.8513,\n",
       "                      1.6917, 1.0368, 0.0301, 1.0386, 0.7699, 0.9981, 0.6330, 1.4994, 0.4756,\n",
       "                      1.0922, 0.7537, 0.9317, 0.1606, 0.1601, 1.0292, 2.1968, 0.4817, 0.1423,\n",
       "                      1.4006, 0.7020, 0.9999, 0.7945, 0.5864, 0.6793, 1.1019, 1.0499, 0.9792,\n",
       "                      1.3278, 0.4114, 0.9054, 0.6223, 0.5506, 1.1103, 1.4026, 0.5890, 1.0165,\n",
       "                      0.7590, 1.9500, 0.9106, 1.0356, 1.4149, 0.9462, 0.7634, 0.3370],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.19.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.19.2.clip_val', tensor([6.4401], device='cuda:0')),\n",
       "             ('module.model.20.0.weight', tensor([[[[ 0.0059]],\n",
       "              \n",
       "                       [[-0.0430]],\n",
       "              \n",
       "                       [[ 0.0571]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0342]],\n",
       "              \n",
       "                       [[-0.0264]],\n",
       "              \n",
       "                       [[-0.0043]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0155]],\n",
       "              \n",
       "                       [[ 0.0583]],\n",
       "              \n",
       "                       [[-0.0595]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0337]],\n",
       "              \n",
       "                       [[-0.0059]],\n",
       "              \n",
       "                       [[ 0.0726]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0552]],\n",
       "              \n",
       "                       [[-0.0381]],\n",
       "              \n",
       "                       [[ 0.0668]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0315]],\n",
       "              \n",
       "                       [[-0.0342]],\n",
       "              \n",
       "                       [[ 0.0388]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0776]],\n",
       "              \n",
       "                       [[-0.0082]],\n",
       "              \n",
       "                       [[ 0.0126]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0189]],\n",
       "              \n",
       "                       [[-0.0362]],\n",
       "              \n",
       "                       [[-0.0879]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0128]],\n",
       "              \n",
       "                       [[-0.0155]],\n",
       "              \n",
       "                       [[-0.0099]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0226]],\n",
       "              \n",
       "                       [[-0.0499]],\n",
       "              \n",
       "                       [[-0.0007]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0007]],\n",
       "              \n",
       "                       [[ 0.0005]],\n",
       "              \n",
       "                       [[-0.0219]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0357]],\n",
       "              \n",
       "                       [[ 0.0500]],\n",
       "              \n",
       "                       [[-0.0660]]]], device='cuda:0')),\n",
       "             ('module.model.20.1.weight',\n",
       "              tensor([1.0011, 0.6110, 0.8498, 1.0611, 0.9713, 0.9483, 0.4678, 1.0240, 1.0554,\n",
       "                      0.8512, 0.8555, 1.0179, 1.0723, 1.1049, 1.0762, 0.9392, 0.9428, 0.9791,\n",
       "                      1.0240, 1.0276, 1.0580, 0.9977, 0.9932, 0.8608, 1.1007, 0.4574, 1.1624,\n",
       "                      0.9161, 0.9490, 0.9938, 0.7151, 1.0320, 0.7049, 1.0685, 1.1220, 0.9084,\n",
       "                      0.8803, 0.9804, 1.1204, 1.9604, 1.1411, 0.9183, 0.7947, 0.6425, 0.8658,\n",
       "                      1.0393, 1.0817, 0.5279, 0.8821, 1.0296, 1.0692, 1.0780, 1.0421, 1.0858,\n",
       "                      1.0748, 1.1534, 1.0638, 1.0157, 1.0173, 0.9820, 0.9914, 1.0364, 0.9310,\n",
       "                      1.0330, 1.0660, 0.9482, 1.0760, 1.1193, 1.0634, 1.1022, 0.9744, 0.8361,\n",
       "                      0.7600, 1.0378, 1.0864, 1.0656, 1.0642, 0.9559, 1.1568, 0.6477, 0.5429,\n",
       "                      1.0046, 0.8087, 0.9180, 1.1026, 0.8357, 0.9620, 0.8402, 0.9038, 0.9254,\n",
       "                      0.9071, 1.0517, 0.8960, 0.9167, 1.0147, 1.1398, 1.0262, 0.9040, 0.5095,\n",
       "                      0.9448, 0.8373, 1.0110, 0.4842, 0.8623, 0.8227, 0.7869, 0.9285, 1.0299,\n",
       "                      1.0170, 0.9511, 0.8681, 0.9730, 1.2560, 0.8483, 0.8287, 0.9502, 0.5807,\n",
       "                      0.5667, 0.9989, 0.6920, 0.8671, 0.8637, 0.8039, 0.8360, 0.8726, 1.0604,\n",
       "                      0.9671, 1.0169, 0.9481, 1.0361, 1.0378, 0.9033, 0.9558, 1.0652, 1.0503,\n",
       "                      0.9647, 0.3451, 1.0303, 0.6827, 0.9758, 0.9500, 1.0615, 0.9798, 1.0212,\n",
       "                      0.9537, 1.4570, 1.0435, 1.0008, 0.7558, 0.9916, 1.0653, 1.0070, 0.9841,\n",
       "                      1.0365, 1.0524, 1.0842, 0.6094, 0.7474, 0.9972, 1.0994, 1.0137, 0.9835,\n",
       "                      0.5246, 1.0533, 0.9985, 0.9351, 1.0253, 0.9856, 0.6045, 0.9520, 1.0848,\n",
       "                      1.1690, 1.0387, 1.1072, 1.0733, 1.0344, 1.0497, 0.9395, 0.9145, 1.2134,\n",
       "                      0.8605, 1.1940, 0.9389, 0.7024, 0.3784, 0.8373, 0.4738, 0.9495, 0.4964,\n",
       "                      1.1338, 0.9686, 0.7691, 0.9238, 1.1477, 1.0306, 1.1170, 1.0825, 1.0270,\n",
       "                      0.8480, 1.0571, 0.9303, 1.0074, 1.0670, 1.0710, 0.6854, 1.1152, 1.0553,\n",
       "                      0.9832, 1.0781, 1.0672, 0.8232, 0.9264, 1.0898, 1.0001, 1.1415, 1.0784,\n",
       "                      0.5042, 0.8233, 0.8141, 0.6382, 0.9347, 0.9344, 1.0424, 0.7666, 1.0037,\n",
       "                      0.9491, 0.9876, 0.9786, 1.0356, 0.9288, 1.0612, 1.0253, 1.1621, 1.0668,\n",
       "                      1.0015, 0.8517, 0.7902, 1.0956, 1.0009, 0.4839, 1.0496, 1.0442, 0.9476,\n",
       "                      0.9247, 1.1068, 1.0202, 1.1841, 0.8481, 1.2441, 0.9553, 0.9939, 1.0828,\n",
       "                      1.0936, 1.0686, 0.7134, 0.8447, 0.9539, 0.9629, 0.9875, 0.5816, 1.0402,\n",
       "                      0.4103, 0.8443, 1.0314, 1.0155, 1.0018, 0.7135, 1.0617, 1.0272, 1.0629,\n",
       "                      1.0769, 0.9685, 1.0381, 1.1146, 1.0158, 1.0459, 1.0501, 1.0794, 0.3929,\n",
       "                      0.9185, 1.0202, 0.6214, 0.8433, 1.0517, 0.9641, 0.8766, 1.0575, 1.0828,\n",
       "                      1.0421, 0.9622, 0.9921, 1.0565, 0.9662, 0.9140, 1.0526, 1.0691, 0.8961,\n",
       "                      0.9921, 1.0549, 0.7936, 0.4846, 1.0338, 0.8850, 1.0423, 1.0281, 0.9816,\n",
       "                      1.0022, 1.0394, 0.7599, 1.0215, 0.9174, 1.0856, 0.8943, 1.0963, 1.1593,\n",
       "                      1.0868, 0.6398, 0.6479, 0.6923, 1.1197, 0.5879, 1.1631, 0.8943, 0.8489,\n",
       "                      0.8450, 0.6855, 1.0748, 1.1014, 0.9928, 0.8046, 1.0058, 1.0062, 0.8540,\n",
       "                      0.6220, 0.8119, 0.6358, 1.1505, 0.6262, 1.0132, 1.1525, 1.0268, 0.8982,\n",
       "                      1.1241, 0.9926, 0.7628, 1.1690, 0.9148, 0.9592, 1.0169, 0.9199, 0.9297,\n",
       "                      1.0540, 1.0006, 0.9320, 0.9875, 0.8233, 0.6434, 0.8600, 1.0742, 0.8783,\n",
       "                      0.8777, 1.0716, 1.1517, 1.1137, 0.9882, 1.0517, 1.0816, 0.7476, 0.9607,\n",
       "                      0.8894, 0.8925, 1.0370, 1.0353, 0.6944, 1.0124, 0.7307, 0.7845, 1.0115,\n",
       "                      1.1021, 0.5363, 0.9139, 0.9477, 1.0724, 0.9491, 0.9308, 1.0668, 1.1742,\n",
       "                      0.5376, 1.0818, 0.6910, 0.8076, 0.9505, 0.9537, 0.9869, 1.0327, 0.5716,\n",
       "                      1.0571, 1.0354, 1.1466, 1.0768, 0.9762, 1.0489, 0.5102, 0.8433, 0.9168,\n",
       "                      1.0596, 0.9244, 1.0413, 1.1080, 0.9696, 1.0720, 0.9791, 1.0833, 0.8703,\n",
       "                      1.0839, 0.9871, 0.9662, 0.4730, 1.0598, 1.1304, 1.2402, 0.6082, 0.7327,\n",
       "                      1.0251, 1.0163, 1.0224, 1.0397, 0.9369, 0.7995, 1.0117, 1.0672, 0.7186,\n",
       "                      1.1114, 0.5709, 0.8666, 1.0621, 1.1725, 1.0652, 0.8919, 0.9703, 0.9369,\n",
       "                      1.0764, 0.7229, 1.0092, 1.1256, 0.9210, 0.5652, 1.0344, 1.2737, 0.9667,\n",
       "                      0.7499, 1.0273, 0.9358, 1.0420, 1.0013, 0.3204, 0.6865, 0.9518, 1.0566,\n",
       "                      0.9897, 0.6753, 1.0071, 0.7901, 0.9050, 1.0583, 0.8161, 1.0091, 0.8844,\n",
       "                      0.8142, 0.9972, 0.9643, 0.5927, 0.9469, 1.0611, 1.0482, 1.0104, 1.0694,\n",
       "                      1.0622, 1.0750, 0.9783, 1.1088, 0.8407, 0.9343, 1.0452, 0.9686, 0.9499,\n",
       "                      0.6701, 1.1991, 1.0001, 1.1587, 0.9848, 1.0900, 0.8818, 0.9689, 1.0625,\n",
       "                      0.8140, 0.8712, 0.8629, 1.1668, 0.9486, 0.5590, 1.0899, 0.9872, 1.0261,\n",
       "                      0.9439, 0.9959, 0.8214, 1.0129, 1.0757, 1.0035, 0.8442, 0.9527],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.20.1.bias',\n",
       "              tensor([-4.8207e-01,  8.8792e-01, -6.2443e-01, -5.8663e-01, -5.0069e-01,\n",
       "                      -5.3462e-01,  9.6543e-01,  5.1731e-01, -1.5290e-01,  7.2285e-01,\n",
       "                      -5.7730e-01, -2.9013e-01,  4.4772e-03,  3.1759e-02,  1.6134e-02,\n",
       "                       4.9490e-01, -7.1251e-01,  4.5589e-01, -3.7698e-01,  2.3918e-01,\n",
       "                      -2.1686e-01, -5.4941e-01, -3.4480e-01,  8.5108e-01, -1.7066e-01,\n",
       "                       9.9940e-01,  5.6013e-01, -3.7986e-01, -5.3701e-01, -4.7041e-01,\n",
       "                       7.7873e-01, -7.1701e-01,  2.8368e-01, -2.9655e-01,  2.7054e-01,\n",
       "                      -4.3180e-01,  6.4455e-01, -2.2944e-01,  4.4382e-02,  1.2006e-01,\n",
       "                      -8.6748e-02,  7.7473e-01, -5.6781e-01,  1.0815e+00, -7.0717e-01,\n",
       "                       7.8514e-02,  2.0436e-01,  1.0596e+00, -7.9420e-01,  4.7959e-01,\n",
       "                       3.1757e-01, -4.2964e-02, -3.0233e-01, -2.3886e-01,  2.1647e-01,\n",
       "                      -2.6465e-01, -1.0929e-01, -1.5576e-01,  3.9706e-01, -3.5867e-01,\n",
       "                       3.1372e-01, -3.9870e-01, -5.4457e-01,  2.1125e-01,  3.5016e-01,\n",
       "                      -4.3234e-01, -3.8973e-01, -2.3945e-01,  8.9371e-02, -4.9919e-02,\n",
       "                      -4.0413e-01, -5.9407e-01,  9.1960e-01,  3.7985e-01,  1.1730e-01,\n",
       "                      -2.3991e-01,  2.3770e-02, -7.3046e-01, -4.5118e-01, -8.8209e-01,\n",
       "                       1.0356e+00,  5.6917e-01, -8.5411e-01, -5.0724e-01, -2.3252e-01,\n",
       "                       6.7972e-01, -6.0494e-01, -6.2486e-01,  5.1502e-01, -2.6286e-01,\n",
       "                      -1.6150e-01,  2.5336e-02,  6.6363e-01, -6.0163e-01,  2.3121e-01,\n",
       "                      -2.5661e-01, -1.6088e-01, -3.2254e-02,  9.3553e-01, -7.9647e-01,\n",
       "                       6.9383e-01, -4.1034e-01,  1.0108e+00,  6.7443e-01, -1.4421e-01,\n",
       "                       7.6048e-01, -6.8752e-01,  2.1967e-01,  2.1465e-01, -3.4097e-01,\n",
       "                      -4.3016e-01, -7.2718e-01, -6.1437e-02,  6.9103e-01,  6.8874e-01,\n",
       "                      -6.6783e-01,  9.3858e-01,  9.0557e-01, -6.0187e-01,  8.6462e-01,\n",
       "                       4.6515e-02, -5.6630e-01,  7.7397e-01, -4.7280e-01, -8.0448e-01,\n",
       "                      -1.2446e-01, -6.9546e-01, -7.4374e-01,  2.8972e-01,  8.0055e-02,\n",
       "                       1.6988e-01, -5.7697e-01, -2.3032e-01, -1.0547e-01, -2.5810e-01,\n",
       "                       6.7214e-01,  1.1073e+00, -1.3353e-01, -8.5556e-01, -4.9643e-01,\n",
       "                      -6.0686e-01, -1.9027e-01, -3.0884e-01, -4.4780e-01, -4.4828e-01,\n",
       "                      -4.0515e-01, -4.9822e-01, -2.1001e-01,  7.7412e-01, -3.1931e-01,\n",
       "                      -6.9477e-01,  3.3295e-01, -5.8795e-01,  1.8183e-01,  2.8174e-01,\n",
       "                      -1.6888e-01,  8.6850e-01,  9.0430e-01,  5.2464e-01, -1.4583e-01,\n",
       "                      -7.1058e-01, -4.3608e-01,  9.3923e-01, -9.0202e-02, -5.6620e-01,\n",
       "                      -4.8605e-01,  3.1757e-01, -6.8437e-01,  9.1164e-01, -5.7445e-01,\n",
       "                      -8.9092e-04, -3.2464e-01,  2.2847e-01,  1.5726e-01,  7.0349e-02,\n",
       "                      -2.4505e-01, -1.9975e-01, -3.4311e-01, -3.1700e-01,  4.7354e-02,\n",
       "                      -5.8875e-01,  1.6322e-01, -5.3946e-01, -1.1628e+00,  1.0117e+00,\n",
       "                       6.6668e-01,  1.0067e+00, -4.7252e-01,  9.6519e-01,  6.9948e-02,\n",
       "                       4.8396e-01,  1.0538e+00, -4.4022e-01, -3.0486e-02,  2.7309e-01,\n",
       "                       7.1709e-01,  5.6482e-02, -6.7053e-01, -7.1104e-01,  1.3566e-01,\n",
       "                      -5.6600e-01, -3.3624e-01, -1.0587e-02, -1.1723e-01,  1.0482e+00,\n",
       "                      -3.2102e-01, -4.2880e-01,  3.4406e-01, -4.3638e-01, -3.0436e-01,\n",
       "                       6.8280e-01, -1.0308e+00, -4.2981e-01, -4.2187e-01,  3.0064e-01,\n",
       "                      -3.8840e-02,  9.6974e-01, -8.4418e-01,  6.8972e-01,  8.9995e-01,\n",
       "                      -5.4403e-01,  6.1617e-01,  3.5681e-02, -6.1481e-01, -2.2070e-01,\n",
       "                       3.4121e-01, -4.3827e-01,  4.5730e-01, -4.4462e-01, -5.8685e-01,\n",
       "                      -1.1601e-01,  1.3344e-01,  4.0595e-01, -4.4537e-01, -4.4111e-01,\n",
       "                       6.9760e-01,  1.0628e+00, -8.4542e-02,  4.6854e-01,  9.5852e-01,\n",
       "                       2.5110e-01, -4.4665e-01,  5.8844e-01,  9.4988e-01,  8.4248e-02,\n",
       "                       3.8787e-01,  8.3536e-02, -6.4881e-01,  3.1165e-01,  5.5843e-01,\n",
       "                      -5.7226e-01,  1.1148e-01, -1.0221e-01,  8.5018e-02,  7.8629e-01,\n",
       "                      -7.2138e-01, -5.2026e-01, -1.8504e-01,  5.4238e-01,  9.2279e-01,\n",
       "                       3.1841e-01,  1.0679e+00, -6.2215e-01,  1.0780e-02, -3.1985e-01,\n",
       "                      -3.9045e-01,  8.1044e-01, -1.4675e-01, -6.7485e-02, -1.0460e-01,\n",
       "                       2.9729e-01,  4.7460e-01, -1.9795e-01,  3.5143e-02, -3.1040e-01,\n",
       "                      -4.5020e-01, -3.3196e-01,  1.4430e-01,  1.0064e+00, -2.7725e-01,\n",
       "                       6.5418e-01,  8.6217e-01, -6.3576e-01,  2.5610e-01,  4.2765e-01,\n",
       "                      -1.9433e-01,  3.0867e-01, -1.4011e-01, -3.8884e-01, -2.9734e-01,\n",
       "                      -2.7134e-01,  1.0675e-01, -8.1402e-01,  5.6648e-01,  1.7795e-04,\n",
       "                      -1.7718e-02, -4.8374e-01,  1.6007e-01,  2.8679e-01, -4.2678e-01,\n",
       "                       9.7714e-01, -2.6964e-01, -7.1768e-01,  2.4442e-01,  5.5847e-01,\n",
       "                      -1.5763e-01,  3.1673e-01, -3.7382e-02, -1.2364e+00, -6.2959e-01,\n",
       "                      -6.3146e-01, -6.5362e-02, -7.7595e-01,  5.2483e-02, -1.2635e-01,\n",
       "                       1.0535e-01,  8.1893e-01, -8.5250e-01, -2.9564e-01,  4.9732e-01,\n",
       "                       1.0482e+00,  1.1668e-02, -6.1042e-01,  6.4066e-01, -8.5266e-01,\n",
       "                       7.9662e-01, -3.4156e-01, -8.8768e-02, -2.0563e-01, -3.9957e-01,\n",
       "                      -1.1361e-01, -2.1563e-01,  8.3019e-01,  1.0263e+00, -2.3329e-01,\n",
       "                       8.6234e-01, -1.3136e-01,  8.7845e-01, -9.0991e-02,  1.6137e-01,\n",
       "                      -2.0476e-01, -9.4725e-01, -9.1261e-02, -3.4984e-01, -9.1991e-01,\n",
       "                      -3.7624e-01, -6.1719e-01,  4.9553e-01,  4.1395e-01, -8.8593e-01,\n",
       "                       2.3691e-01, -1.9564e-01, -1.0918e+00, -6.0114e-01,  1.0332e+00,\n",
       "                       6.5307e-01,  8.6369e-01, -5.8083e-01,  5.6962e-01, -8.6241e-01,\n",
       "                      -7.5767e-01, -2.3796e-02,  5.2685e-01, -5.1907e-02,  3.7948e-01,\n",
       "                       1.4953e-01, -3.9957e-01, -1.2505e-01, -5.8713e-01, -6.8966e-01,\n",
       "                       5.0562e-02, -2.7319e-01, -2.2512e-01,  1.0072e+00, -4.1508e-01,\n",
       "                       7.9227e-01, -3.1121e-01, -4.9804e-01,  7.9602e-01,  9.2955e-01,\n",
       "                      -6.1311e-01,  2.8641e-01,  3.0760e-01, -8.6371e-01, -4.2850e-01,\n",
       "                      -5.4278e-02, -1.6794e-01,  9.1308e-01,  1.7232e-01,  8.2384e-01,\n",
       "                       6.9607e-01, -3.8300e-01, -5.7103e-01, -1.5432e-01, -4.4583e-01,\n",
       "                       8.9992e-01, -1.8070e-01, -2.3678e-01,  8.3450e-02, -4.7862e-01,\n",
       "                      -5.0855e-01, -2.6091e-01,  1.4711e+00, -5.8341e-01,  6.8984e-01,\n",
       "                      -2.0447e-01, -6.0921e-01, -5.2636e-01,  8.9973e-02, -3.5644e-01,\n",
       "                      -1.6391e-01,  4.6888e-01, -2.1802e-01,  5.9412e-01,  1.0611e-01,\n",
       "                      -3.2361e-01, -6.0100e-01,  9.9081e-01, -1.4843e-01,  6.3557e-02,\n",
       "                       3.7508e-01,  9.4646e-01, -8.5475e-01, -4.5963e-01,  3.6289e-01,\n",
       "                      -3.2001e-01,  2.6633e-01, -5.3115e-01,  6.0831e-01,  3.0858e-01,\n",
       "                      -2.5960e-01,  9.1512e-01,  1.9044e-01,  9.3772e-01, -2.4169e-01,\n",
       "                       2.7788e-01, -1.2535e-01, -2.8740e-02,  1.7400e-01, -1.2573e-01,\n",
       "                      -5.5777e-01,  7.3041e-02,  7.6760e-01,  1.5949e-01,  2.8164e-01,\n",
       "                      -7.1291e-01,  9.2761e-01, -1.9164e-02,  4.1338e-02, -6.3438e-01,\n",
       "                       7.6451e-01, -5.8854e-01, -6.4575e-01,  1.1205e-01,  3.6929e-01,\n",
       "                       1.0635e+00,  8.1716e-01, -2.6239e-01,  1.3962e-01,  3.9004e-01,\n",
       "                       1.0406e+00, -4.7431e-01, -1.0495e+00, -8.2513e-01,  1.4029e-01,\n",
       "                       7.4216e-01,  3.1401e-01, -6.2743e-01, -5.5771e-01, -4.2610e-01,\n",
       "                      -3.9771e-01,  9.3450e-01, -6.9796e-01,  2.8030e-02, -4.7761e-01,\n",
       "                       5.9548e-02, -1.2505e-01,  2.9024e-01, -1.9673e-01,  4.4169e-01,\n",
       "                       1.8776e-01, -7.2863e-01,  5.0752e-01, -3.7395e-01,  5.4956e-01,\n",
       "                       4.3459e-01,  8.5708e-01, -6.0844e-01, -2.9349e-01, -2.1332e-01,\n",
       "                       4.0708e-01,  8.6611e-02,  5.9071e-01, -5.3352e-01, -4.4547e-01,\n",
       "                      -1.1726e+00, -8.4648e-01,  6.9590e-01, -3.0932e-01, -3.6606e-01,\n",
       "                       9.7371e-01,  4.3272e-01, -3.2389e-01,  2.6037e-01, -2.7409e-03,\n",
       "                       3.3320e-02,  8.1193e-01, -2.6190e-01, -2.0335e-01,  4.8800e-01,\n",
       "                      -8.3138e-01, -5.6629e-01], device='cuda:0')),\n",
       "             ('module.model.20.1.running_mean',\n",
       "              tensor([ 7.5984e-01,  1.3102e+00,  5.9875e-01, -6.4809e-01, -3.0419e-01,\n",
       "                      -2.8103e-01,  1.4365e+00,  5.0273e-01, -2.2829e-01,  5.1179e-02,\n",
       "                      -8.8112e-01,  4.9792e-01,  4.4694e-01, -6.2908e-01, -1.1209e-01,\n",
       "                      -1.0478e+00,  4.3437e-01,  3.9807e-02,  4.4875e-01,  1.1778e-01,\n",
       "                       1.4228e-01, -6.6689e-02, -8.3130e-01, -1.6497e+00,  1.4592e-01,\n",
       "                       7.5332e-01,  1.0551e+00,  4.5055e-02,  8.7492e-01, -3.1053e-01,\n",
       "                       3.6677e-01,  1.6355e+00, -1.4580e-01, -1.4213e+00, -5.4525e-01,\n",
       "                       2.7396e-01,  3.2055e-02,  4.3719e-01, -1.6332e-01, -7.7012e-02,\n",
       "                       5.1141e-01, -2.2376e+00,  1.8879e+00, -9.9402e-01, -6.3292e-01,\n",
       "                      -3.8583e-01, -2.1832e-01,  1.0697e+00, -1.5986e-01, -3.5838e-01,\n",
       "                       5.7296e-01,  6.1682e-01,  7.8302e-01,  4.1050e-02,  9.4134e-03,\n",
       "                       1.1830e+00,  3.7148e-01,  1.2891e+00, -6.9380e-02,  2.8478e-01,\n",
       "                      -5.8917e-01,  3.5702e-01, -1.8489e-01,  1.7859e+00,  2.2975e-01,\n",
       "                       1.5302e-01, -4.5587e-02, -7.3553e-01,  1.9879e-01, -1.0690e+00,\n",
       "                       3.5537e-01, -1.2755e-02, -2.8174e-01, -1.9379e+00, -5.6567e-01,\n",
       "                      -2.3480e-01, -1.8647e-01,  1.7335e+00, -9.1360e-02, -8.6052e-01,\n",
       "                       6.2148e-01,  4.0986e-01,  6.0792e-01, -5.1966e-01,  6.9316e-01,\n",
       "                       8.3838e-01,  2.0423e-01,  8.9020e-01, -8.6328e-01,  1.3719e-01,\n",
       "                       5.0185e-01,  2.1537e+00, -8.7454e-01,  9.3933e-01,  4.3137e-02,\n",
       "                       6.9379e-01,  1.3438e+00,  8.0973e-01,  1.4199e-01, -6.6359e-01,\n",
       "                      -5.8953e-01,  1.1624e+00,  5.0932e-01,  6.0099e-01, -2.3588e-01,\n",
       "                       4.7438e-02, -2.4395e-01, -6.3074e-01,  9.9539e-02,  6.0751e-01,\n",
       "                       5.9496e-01,  4.3122e-01,  7.8260e-02,  1.4941e+00,  1.2072e+00,\n",
       "                      -2.4797e-01,  7.0458e-01,  1.1799e+00, -6.7351e-01, -1.0748e+00,\n",
       "                       6.7056e-01,  3.3072e-01,  6.1479e-03,  1.1442e-01,  3.0833e-01,\n",
       "                      -6.8348e-01,  4.7345e-01, -8.7493e-01, -6.6877e-01,  2.7636e-01,\n",
       "                      -6.7280e-02,  5.5791e-01,  1.5411e+00,  2.5944e-01, -1.1338e+00,\n",
       "                      -2.9091e-01,  3.8841e-01, -3.6989e-01, -3.6011e-01,  1.0951e+00,\n",
       "                       1.4951e+00, -4.7074e-01,  3.6170e-01,  2.6006e-01,  5.9492e-01,\n",
       "                      -8.1066e-01,  1.1004e+00,  1.5833e+00,  3.9947e-01,  7.3372e-01,\n",
       "                       1.3096e+00, -2.9414e-01, -1.5341e-01, -4.1335e-01,  4.6622e-01,\n",
       "                      -2.4071e-01, -5.5290e-01,  1.7467e-01, -3.8457e-01,  9.3816e-03,\n",
       "                       4.7057e-01, -2.8075e-01,  3.3183e-01, -4.8211e-01,  7.6767e-01,\n",
       "                      -3.7006e-01,  5.1537e-01, -7.6177e-01,  1.6203e+00,  1.2773e+00,\n",
       "                      -6.3368e-01,  1.1643e+00, -1.1312e+00,  1.0900e+00,  1.9854e-02,\n",
       "                      -1.3971e+00, -6.8213e-01,  2.0142e-01,  8.7818e-01,  9.3436e-01,\n",
       "                       1.1697e+00, -4.9128e-01,  6.4079e-02, -7.1300e-01,  1.6527e+00,\n",
       "                      -1.3018e-01,  1.4242e+00,  3.5736e-01,  2.1584e-01,  5.9974e-01,\n",
       "                       1.1426e+00,  2.3876e+00,  2.9413e-01,  2.5468e-01,  3.3790e-01,\n",
       "                      -9.7628e-01,  5.8459e-01,  9.3946e-01,  1.2609e+00,  9.9677e-01,\n",
       "                      -6.1619e-01, -1.0750e+00, -3.8626e-01,  4.0173e-01,  1.2066e+00,\n",
       "                      -8.7007e-01,  3.0148e-01,  1.6128e+00, -1.0194e+00, -3.3883e-02,\n",
       "                       9.0881e-01,  7.8953e-02,  1.4671e+00, -3.2204e-01,  5.4022e-01,\n",
       "                      -5.4518e-01, -2.3340e-03,  1.7412e-01,  6.9181e-01, -5.7697e-01,\n",
       "                       5.1654e-01, -1.0821e+00,  9.3419e-01,  1.0285e+00,  1.3157e+00,\n",
       "                      -1.9116e+00,  9.3009e-02,  5.6888e-01,  9.9554e-01, -4.0274e-01,\n",
       "                       1.3208e+00,  1.0575e+00, -2.6795e-02, -4.3230e-01,  7.2722e-01,\n",
       "                       9.3257e-01,  2.7607e-01,  1.6746e-01,  9.8026e-01,  8.9434e-01,\n",
       "                       2.2161e+00, -3.4220e-01,  1.1567e+00,  2.3887e-01,  1.0262e-01,\n",
       "                       3.8367e-01,  1.0668e+00,  1.2752e+00, -8.6888e-01,  1.1392e+00,\n",
       "                       1.3670e+00, -1.1302e+00,  9.2678e-01, -1.1546e+00,  1.0340e+00,\n",
       "                       1.1605e-02,  5.1172e-01,  3.4076e-01, -8.3165e-02,  3.1407e-01,\n",
       "                       9.4636e-01,  1.2955e+00,  1.3621e+00,  5.2891e-01,  5.0161e-01,\n",
       "                       3.1457e-01,  2.5847e+00,  9.4710e-03,  1.4538e+00, -1.0371e+00,\n",
       "                       5.4876e-02,  9.8703e-01,  2.4651e-01, -8.2149e-01, -2.8569e-01,\n",
       "                      -9.8497e-01, -5.3275e-01, -5.5794e-01,  1.0769e+00,  1.1768e+00,\n",
       "                       1.2878e+00,  4.1037e-01,  2.0401e+00, -1.3114e-01, -3.8808e-01,\n",
       "                       7.4513e-01,  9.4156e-01, -5.5913e-01, -5.5393e-01, -9.4027e-01,\n",
       "                       7.3373e-01, -1.8943e+00, -3.8612e-01, -5.3168e-01, -2.2467e-01,\n",
       "                      -5.7511e-01,  9.0443e-01, -5.9623e-01,  5.9442e-01,  8.5297e-01,\n",
       "                       1.1461e+00, -1.7005e+00,  1.5196e+00,  2.7770e-01, -1.2909e+00,\n",
       "                       3.2360e-01,  1.7440e+00,  1.3362e+00, -1.2231e+00,  4.9338e-01,\n",
       "                       3.3219e-01,  1.1240e-01, -1.0433e+00, -1.2650e+00,  1.2981e+00,\n",
       "                       6.0890e-01,  8.3942e-01, -8.0454e-01, -3.6619e-01,  4.1712e-01,\n",
       "                      -1.9792e-01, -5.8482e-01,  5.7951e-01,  4.3330e-01, -1.4302e-01,\n",
       "                       2.3282e+00,  7.8602e-01,  3.1875e-01,  1.0848e+00,  9.4711e-02,\n",
       "                      -2.5375e-01, -1.3615e+00, -9.4086e-01,  1.0939e+00,  5.8015e-01,\n",
       "                      -9.7800e-01,  2.9948e-01, -9.4507e-02, -1.5475e-01,  1.5144e+00,\n",
       "                       5.6316e-01,  7.8991e-01,  8.0209e-01,  7.4125e-01, -5.8452e-01,\n",
       "                       7.7037e-01,  1.1017e+00,  5.7613e-01,  3.2640e-01, -4.3001e-01,\n",
       "                      -8.0051e-02, -1.9199e-01, -7.7368e-01, -2.1022e-01,  1.3302e+00,\n",
       "                       1.3874e+00,  9.4607e-01,  3.6499e-01, -1.0206e+00,  1.7236e-01,\n",
       "                       7.1711e-01, -3.5603e-01,  1.3649e+00, -9.3105e-01, -6.9921e-01,\n",
       "                       2.1327e-01, -1.4275e-01, -3.7168e-01,  3.6410e-02,  1.9645e-01,\n",
       "                       3.7997e-01, -1.5625e-01,  1.5182e+00,  1.3606e+00, -1.2201e-01,\n",
       "                       1.5039e-01,  5.4038e-01,  8.4018e-01,  3.4319e-01,  6.7654e-01,\n",
       "                       6.9462e-01,  1.6921e+00,  1.1500e+00,  1.1732e+00,  5.6222e-01,\n",
       "                       2.7482e-01,  5.7886e-01,  5.9729e-01,  1.3125e+00,  2.7118e-01,\n",
       "                       1.1860e+00,  1.8254e-01,  1.4206e+00,  1.1394e+00, -4.0071e-01,\n",
       "                      -1.2272e-01,  8.3574e-01,  5.2532e-01, -8.3761e-01,  9.7619e-01,\n",
       "                       3.2253e-01,  1.4900e+00,  2.3469e+00,  7.1525e-01, -2.2511e-01,\n",
       "                       3.5541e-01,  1.0509e+00, -7.3057e-01,  8.6886e-01,  7.0331e-01,\n",
       "                      -5.7293e-01,  1.0380e+00, -9.0534e-01, -9.5594e-01,  6.3874e-01,\n",
       "                      -1.1474e+00,  2.4210e-01, -1.1739e+00, -1.9894e-01,  4.4851e-01,\n",
       "                       9.3605e-01, -2.0576e-01, -7.2683e-02,  2.0244e-01, -1.6204e-01,\n",
       "                       6.2013e-01,  5.1484e-01,  5.0232e-01,  5.1366e-01, -2.1642e-01,\n",
       "                       1.0662e+00,  6.3565e-01, -1.0187e+00,  3.2215e-01,  1.1979e+00,\n",
       "                       6.4879e-01,  6.5168e-01,  1.4305e+00, -2.8461e-01,  4.0125e-01,\n",
       "                       2.1865e-01,  1.3074e-01, -5.1306e-01, -1.0141e+00, -2.2727e-01,\n",
       "                       1.0773e+00,  2.6589e-01,  6.2404e-03,  2.0044e-01,  3.9292e-01,\n",
       "                      -4.5857e-01, -7.4654e-02,  1.5802e+00,  1.5275e-01,  9.7853e-01,\n",
       "                       3.0074e-01,  3.1022e-01,  2.3285e+00, -3.6343e-01, -1.3360e+00,\n",
       "                      -3.6124e-01,  1.5007e+00, -2.0550e-01,  4.3628e-01,  7.5037e-01,\n",
       "                       9.8514e-01, -2.2250e-01, -5.5375e-02,  3.7378e-01,  1.3252e+00,\n",
       "                       2.6607e-01,  1.5787e+00,  5.5466e-01, -2.2663e+00,  1.1127e-01,\n",
       "                       7.9179e-01, -3.3610e-01,  3.7898e-01,  5.0974e-01, -3.1889e-02,\n",
       "                      -5.6079e-01,  2.3241e-01, -1.6231e-01,  4.7123e-01, -6.1532e-01,\n",
       "                       5.7827e-01,  4.5932e-01, -5.5054e-01, -7.6753e-01, -1.9399e+00,\n",
       "                       4.2897e-01, -4.4592e-01,  2.1193e-01,  1.2061e+00,  3.6854e-01,\n",
       "                      -8.6387e-01,  1.6100e+00, -7.3533e-01, -2.1593e-01, -2.1587e-01,\n",
       "                      -1.7322e-02,  1.3392e+00,  1.5719e+00,  1.0732e+00,  1.4461e-01,\n",
       "                       1.2513e+00,  1.0830e-01,  1.1511e+00, -3.0441e-01,  6.8779e-01,\n",
       "                       5.8290e-01,  4.2652e-01], device='cuda:0')),\n",
       "             ('module.model.20.1.running_var',\n",
       "              tensor([0.6726, 0.4193, 0.4306, 0.7899, 0.7602, 0.6371, 0.5811, 0.5343, 0.3942,\n",
       "                      0.4397, 0.2768, 0.6400, 0.6463, 0.6717, 0.4534, 0.4406, 0.5763, 0.7145,\n",
       "                      0.6767, 0.4035, 0.6922, 0.6423, 0.3796, 0.6387, 0.8961, 0.4631, 0.6659,\n",
       "                      0.6828, 0.6350, 0.6619, 0.4985, 0.8109, 0.3353, 0.3785, 0.4298, 0.7593,\n",
       "                      0.5996, 0.5870, 0.6989, 0.4174, 0.9009, 0.7645, 0.6650, 0.3819, 0.5854,\n",
       "                      0.4285, 0.5288, 0.4292, 0.6666, 0.6798, 0.5591, 0.4601, 0.7024, 0.5991,\n",
       "                      0.4607, 0.9641, 0.4336, 0.7674, 0.4620, 0.5268, 0.6081, 0.7946, 0.4001,\n",
       "                      0.7757, 0.4466, 0.6768, 0.8797, 0.6265, 0.4735, 0.4018, 0.5728, 0.5764,\n",
       "                      0.5358, 0.5483, 0.5723, 0.4978, 0.4416, 0.7712, 0.6969, 0.3307, 0.3703,\n",
       "                      0.4588, 0.4655, 0.4465, 0.6627, 0.4709, 0.4753, 0.4706, 0.3992, 0.5173,\n",
       "                      0.6846, 0.9521, 0.7641, 0.5358, 0.2901, 1.4376, 0.5841, 0.7050, 0.5970,\n",
       "                      0.6631, 0.3307, 0.5502, 0.3856, 0.4690, 1.0949, 0.4157, 0.6944, 0.7461,\n",
       "                      0.6512, 0.4650, 0.5030, 0.6099, 0.9230, 0.7531, 0.5995, 0.6210, 0.2960,\n",
       "                      0.4611, 0.6293, 0.5107, 0.8896, 0.4698, 0.6029, 0.4687, 0.4519, 0.7554,\n",
       "                      0.7311, 0.7770, 0.4907, 0.3341, 0.4677, 0.7592, 0.7028, 0.7277, 0.4408,\n",
       "                      0.5686, 0.5004, 0.5298, 0.2843, 0.6321, 0.6345, 0.3703, 0.8265, 0.8824,\n",
       "                      0.5453, 0.3186, 0.6601, 0.6059, 0.4227, 0.5281, 0.8092, 0.7803, 0.7163,\n",
       "                      0.5519, 0.4904, 0.5842, 0.4399, 0.4211, 0.6356, 0.4054, 0.6209, 0.4016,\n",
       "                      0.5755, 0.8219, 0.7435, 0.6928, 0.4835, 0.7925, 0.5026, 0.5600, 0.5093,\n",
       "                      0.7262, 0.4819, 0.7387, 0.8187, 0.5085, 0.3918, 0.4704, 0.5702, 0.8950,\n",
       "                      0.5311, 0.7383, 0.6445, 0.3906, 0.4542, 0.3819, 0.4327, 0.4922, 0.3878,\n",
       "                      0.5700, 0.4487, 1.0219, 0.5278, 0.5688, 0.4988, 0.4735, 0.6207, 0.7231,\n",
       "                      0.5692, 0.4869, 0.5598, 0.5565, 0.6714, 0.5555, 0.5881, 0.5718, 0.7776,\n",
       "                      0.5257, 0.5576, 0.7601, 0.5328, 0.7567, 1.4365, 0.5650, 0.9123, 0.4735,\n",
       "                      0.3146, 0.5301, 0.6830, 0.3986, 0.3889, 1.4173, 0.4819, 0.4758, 1.0752,\n",
       "                      0.7066, 0.6247, 0.3798, 0.6461, 0.6287, 0.5256, 0.4298, 0.7557, 0.9017,\n",
       "                      0.5976, 0.5925, 0.4926, 0.6015, 0.6442, 0.4409, 0.7319, 0.6090, 0.6157,\n",
       "                      0.5441, 0.5286, 0.6424, 0.9415, 0.4378, 0.5205, 0.5001, 0.7631, 0.5835,\n",
       "                      0.7503, 0.4696, 0.4702, 0.5420, 0.6404, 0.7069, 0.4437, 0.3224, 0.4276,\n",
       "                      0.5856, 0.4556, 0.4813, 0.9412, 0.7391, 0.4027, 0.7321, 0.8062, 0.5138,\n",
       "                      0.7796, 0.4776, 0.8302, 0.5977, 0.9735, 0.7740, 0.7449, 0.5498, 0.4105,\n",
       "                      0.6277, 0.6439, 0.4354, 0.5372, 0.4671, 0.4282, 0.3802, 0.7102, 0.4699,\n",
       "                      0.4779, 0.5308, 0.3892, 0.6337, 0.4965, 0.4919, 0.7576, 0.5653, 0.5180,\n",
       "                      0.3899, 0.6156, 0.7308, 0.4748, 0.6633, 0.5931, 0.5299, 0.4298, 0.5450,\n",
       "                      0.5389, 0.6762, 0.5934, 0.6611, 0.4427, 0.5939, 0.4032, 0.5429, 0.6460,\n",
       "                      0.4719, 0.5107, 0.4084, 0.7155, 0.4968, 0.6025, 0.6930, 0.5479, 0.7089,\n",
       "                      0.5412, 0.5159, 1.1333, 0.7210, 0.5277, 0.6173, 0.2477, 0.5073, 0.6308,\n",
       "                      0.6642, 0.8253, 0.5048, 0.8259, 0.6313, 0.4740, 0.5361, 0.4667, 0.6031,\n",
       "                      0.6861, 0.4145, 0.4338, 0.8855, 0.5106, 0.3573, 0.5170, 0.5777, 0.4834,\n",
       "                      0.6560, 0.5494, 0.5879, 0.6316, 0.4456, 0.3673, 0.3795, 0.3824, 0.5155,\n",
       "                      0.5345, 0.5020, 0.8468, 0.5839, 0.4701, 0.5719, 1.0179, 0.8449, 0.5653,\n",
       "                      0.5224, 0.2703, 0.6120, 0.7305, 0.3603, 0.2943, 0.6228, 0.5697, 0.7276,\n",
       "                      0.3156, 0.3382, 0.4502, 0.3905, 0.4258, 0.4000, 0.5997, 0.7443, 0.6237,\n",
       "                      0.5734, 0.6121, 0.4807, 0.3725, 0.4433, 0.5643, 0.6575, 0.5453, 0.5149,\n",
       "                      1.0995, 0.5043, 0.6208, 0.7540, 0.4121, 0.6833, 0.4285, 0.6454, 0.4023,\n",
       "                      0.5181, 0.4493, 0.6965, 0.7754, 0.4275, 0.7381, 0.7608, 0.7598, 0.6744,\n",
       "                      0.4461, 0.3800, 0.5097, 0.5514, 0.7732, 0.6147, 0.7910, 0.4787, 0.5318,\n",
       "                      1.0430, 0.5001, 0.5229, 0.6839, 0.5530, 0.3503, 0.4603, 0.4824, 0.5180,\n",
       "                      0.4124, 0.2039, 0.3579, 0.4940, 0.5945, 0.6222, 0.3802, 0.5231, 0.4717,\n",
       "                      0.4838, 0.3935, 0.3876, 0.7132, 0.7050, 0.3188, 0.7059, 0.6469, 0.5199,\n",
       "                      0.3611, 0.8582, 0.8331, 0.4848, 0.4438, 0.5388, 0.4923, 0.3492, 0.6430,\n",
       "                      0.5854, 0.4362, 0.5285, 0.3612, 0.5775, 0.6516, 0.5055, 0.3801, 0.7073,\n",
       "                      0.4502, 0.6655, 0.4184, 0.5289, 0.6822, 0.4945, 1.0072, 0.6161, 0.7743,\n",
       "                      0.5212, 0.5858, 0.5386, 0.4233, 0.4854, 0.5934, 0.4915, 0.7639, 0.6172,\n",
       "                      0.4879, 0.5928, 0.4379, 0.7237, 0.5450, 0.7358, 0.3818, 0.9940, 1.0031,\n",
       "                      0.5810, 0.6933, 0.3396, 0.6775, 0.3936, 0.5647, 0.6374, 0.7225, 0.5644,\n",
       "                      0.4770, 0.7121, 0.5207, 0.6072, 0.4954, 0.6741, 0.4773, 0.4629],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.20.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.20.2.clip_val', tensor([5.9523], device='cuda:0')),\n",
       "             ('module.model.21.0.weight',\n",
       "              tensor([[[[ 0.2181,  0.3889,  0.1170],\n",
       "                        [ 0.3743,  0.8911,  0.0487],\n",
       "                        [ 0.0907,  0.1246,  0.1151]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2500,  0.8355,  0.0484],\n",
       "                        [-0.1065,  0.3190, -0.1861],\n",
       "                        [-0.8445, -0.7740, -0.5032]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0197, -0.0844,  0.1940],\n",
       "                        [ 0.5880,  1.2157,  0.1072],\n",
       "                        [ 0.1839,  0.4095,  0.1563]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.1898, -0.4463, -0.1207],\n",
       "                        [-0.4368, -0.7201, -0.1929],\n",
       "                        [-0.1292, -0.2125, -0.0852]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3035,  0.6109,  0.1206],\n",
       "                        [ 0.4069,  0.3803,  0.2118],\n",
       "                        [ 0.3009,  0.1446,  0.3027]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0349,  0.4258, -0.0809],\n",
       "                        [ 0.5041,  1.1956,  0.1562],\n",
       "                        [-0.1246,  0.1298, -0.0711]]]], device='cuda:0')),\n",
       "             ('module.model.21.1.weight',\n",
       "              tensor([0.5102, 0.6269, 0.6169, 0.6844, 0.6543, 0.5256, 0.7100, 1.4307, 0.4533,\n",
       "                      1.3033, 0.5418, 0.6941, 0.6695, 0.7508, 0.6380, 1.0557, 0.5934, 0.7082,\n",
       "                      0.5059, 0.7198, 0.4564, 0.6078, 0.5866, 1.0544, 0.5550, 1.0709, 0.8437,\n",
       "                      0.5199, 0.5485, 1.0819, 0.7755, 0.4924, 1.3382, 0.8294, 0.7448, 0.8286,\n",
       "                      1.0261, 0.6260, 0.7318, 0.9050, 0.5608, 0.8247, 1.0230, 0.7957, 0.6241,\n",
       "                      0.5682, 0.8127, 1.1398, 0.4942, 1.4483, 1.5958, 0.7963, 0.4540, 0.8958,\n",
       "                      1.1075, 1.1144, 0.8430, 0.7319, 1.2273, 0.6502, 0.7463, 0.4790, 0.5936,\n",
       "                      0.7304, 1.4294, 0.5366, 1.7472, 0.5395, 0.4560, 1.0299, 0.7861, 0.5452,\n",
       "                      1.4068, 1.4631, 1.0488, 0.5969, 0.5300, 0.6731, 0.9519, 0.4986, 1.0960,\n",
       "                      1.7967, 0.5088, 0.7622, 0.4773, 1.0205, 0.6327, 0.5798, 0.6953, 0.7495,\n",
       "                      0.9419, 0.7580, 1.0420, 0.5850, 0.6578, 0.9322, 0.4509, 0.5742, 0.6973,\n",
       "                      0.4908, 0.7536, 0.5314, 0.7296, 1.3017, 0.7797, 0.8666, 0.5397, 1.6811,\n",
       "                      0.9615, 0.5857, 0.5794, 0.4831, 0.8656, 0.9325, 0.7094, 0.6215, 0.7399,\n",
       "                      0.8671, 0.5810, 1.2608, 0.7039, 0.5483, 1.4066, 0.5940, 0.6260, 0.5025,\n",
       "                      0.5379, 0.5866, 1.1522, 0.6787, 0.9920, 0.5713, 0.5716, 0.4870, 0.6787,\n",
       "                      0.8007, 1.1275, 0.4358, 0.6552, 0.8612, 0.6711, 0.7200, 0.5999, 0.6586,\n",
       "                      0.4901, 0.9601, 0.7254, 0.6789, 0.8857, 0.8407, 0.5837, 0.5022, 0.6019,\n",
       "                      0.6928, 0.6591, 0.5128, 1.0684, 0.7970, 0.9287, 0.9101, 0.6975, 0.6106,\n",
       "                      1.2792, 0.4916, 0.7434, 0.6664, 0.9369, 1.0419, 1.0094, 0.4641, 0.6327,\n",
       "                      0.6804, 0.6797, 0.7787, 1.0086, 0.7501, 0.6134, 0.6838, 0.6303, 0.8038,\n",
       "                      0.5398, 0.7757, 0.4069, 0.9972, 0.7170, 0.6471, 1.1129, 0.6063, 0.7572,\n",
       "                      0.8777, 0.9231, 1.0952, 0.5736, 0.6532, 0.7228, 0.6503, 0.7899, 1.2131,\n",
       "                      0.4243, 0.5630, 0.5863, 0.6525, 0.6122, 0.6234, 1.7733, 1.1331, 0.9140,\n",
       "                      0.4661, 0.8946, 0.5730, 1.0832, 0.5361, 0.9250, 0.9121, 1.1365, 0.9940,\n",
       "                      0.7083, 0.6421, 0.5840, 0.8404, 0.5492, 0.8555, 0.6262, 0.5033, 0.6824,\n",
       "                      0.8695, 0.6492, 0.8051, 0.5789, 0.5130, 0.4363, 0.6419, 1.4339, 0.6177,\n",
       "                      0.6574, 0.9515, 1.3687, 0.7634, 0.9215, 0.5897, 0.4881, 0.4445, 1.3865,\n",
       "                      1.0100, 1.0204, 1.1740, 0.7565, 0.5438, 0.6565, 1.5840, 0.5726, 0.5999,\n",
       "                      1.2248, 0.4761, 0.7767, 0.5518, 0.5428, 0.7263, 0.6478, 1.1572, 0.7572,\n",
       "                      0.8029, 0.7922, 0.6791, 0.7546, 0.6647, 1.0054, 0.7981, 1.1082, 0.5048,\n",
       "                      0.7721, 0.8420, 0.4729, 0.7321, 0.6218, 0.4607, 0.4862, 1.0597, 0.9006,\n",
       "                      0.6934, 0.6865, 1.0531, 0.5650, 0.6502, 0.7177, 0.8361, 0.6047, 0.6783,\n",
       "                      0.7275, 0.7350, 0.6020, 0.4934, 0.8000, 0.7857, 0.8025, 0.6356, 0.5413,\n",
       "                      0.7190, 0.6736, 0.5230, 0.9080, 0.5654, 0.5622, 0.5637, 0.6470, 0.7223,\n",
       "                      0.7748, 1.1286, 0.5054, 0.6869, 0.6382, 0.5618, 0.6401, 0.8806, 0.7682,\n",
       "                      1.1137, 1.0000, 0.5454, 0.8313, 0.8675, 0.8575, 0.7822, 0.5298, 0.5489,\n",
       "                      0.4736, 0.5910, 0.8399, 0.8000, 0.7291, 0.8010, 0.8161, 0.6797, 0.6315,\n",
       "                      1.3740, 0.6535, 0.6619, 0.5486, 0.7128, 0.9108, 0.6319, 0.5840, 0.5072,\n",
       "                      0.6413, 0.3963, 0.4470, 0.6448, 0.7397, 0.9640, 0.8168, 0.6540, 0.6770,\n",
       "                      0.7827, 0.6008, 0.6767, 0.5611, 0.6517, 1.0046, 0.5390, 0.7234, 0.5079,\n",
       "                      0.4911, 0.5265, 2.1643, 1.0619, 0.4824, 0.5072, 0.5697, 0.7638, 0.8628,\n",
       "                      0.5629, 1.0819, 0.4089, 0.7298, 1.2284, 0.5274, 0.7678, 0.6957, 0.5492,\n",
       "                      1.2007, 1.0266, 0.4830, 1.1923, 1.5243, 0.6011, 0.5413, 0.6367, 1.4433,\n",
       "                      0.8224, 0.7120, 1.0568, 1.1013, 0.6486, 0.6124, 0.7103, 0.8875, 0.7797,\n",
       "                      0.6359, 0.6843, 0.6263, 0.8787, 0.6287, 0.5042, 2.0527, 0.5491, 1.2964,\n",
       "                      0.5299, 0.6253, 0.5895, 0.8172, 0.7109, 0.6031, 1.1657, 0.8096, 0.7324,\n",
       "                      0.9074, 0.7983, 0.4888, 1.1889, 1.1444, 0.8056, 1.2937, 0.9610, 1.3923,\n",
       "                      0.5997, 0.6301, 0.4074, 0.6386, 0.6614, 0.6916, 0.9229, 0.4794, 1.1722,\n",
       "                      1.0453, 1.4529, 0.8150, 1.4130, 1.0852, 0.8433, 0.9717, 0.6241, 0.6089,\n",
       "                      0.4909, 0.6076, 0.7375, 0.7567, 0.5230, 1.4096, 0.8719, 0.9167, 0.5531,\n",
       "                      1.2103, 0.5844, 0.6052, 0.5861, 0.4379, 0.8684, 0.6680, 0.7489, 0.8962,\n",
       "                      1.0396, 0.8243, 0.6607, 0.5423, 0.5060, 0.4594, 1.1587, 0.6717, 1.3014,\n",
       "                      0.5362, 0.7399, 0.5895, 1.1596, 0.5385, 0.9631, 0.6703, 0.8686, 0.8194,\n",
       "                      0.8313, 0.4872, 1.3216, 1.0414, 0.5566, 0.5387, 0.5083, 0.9743, 0.5345,\n",
       "                      1.2538, 0.6616, 0.6524, 1.4600, 0.8966, 0.6975, 0.5377, 0.6144, 0.6442,\n",
       "                      0.4590, 0.6821, 0.6836, 0.7039, 0.6740, 0.6615, 1.7255, 0.5311, 0.5272,\n",
       "                      0.7924, 0.7589, 0.8487, 0.5806, 0.6807, 0.6035, 0.4425, 0.5616],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.21.1.bias',\n",
       "              tensor([ 2.8314e-01,  1.5423e+00, -2.1931e-01, -8.9036e-02, -2.2523e-02,\n",
       "                       3.1859e-01,  3.8298e-01, -9.7107e-01,  1.1162e+00, -2.8884e-01,\n",
       "                       1.3455e+00,  1.9193e-02,  7.0681e-01, -1.6003e-01,  8.1659e-01,\n",
       "                      -6.0475e-02,  7.3351e-02,  4.5435e-01,  3.5401e-01,  4.1051e-01,\n",
       "                       1.3461e+00, -1.1605e-01,  1.8380e+00,  2.3085e+00,  9.6313e-01,\n",
       "                      -2.8138e-01,  4.0917e-01,  3.1530e-01,  5.5275e-02, -6.7451e-01,\n",
       "                       2.9204e-01,  2.3765e-01, -6.1482e-01, -8.8477e-01,  7.0579e-01,\n",
       "                      -1.7578e-01,  1.9261e-03,  2.1647e-01,  3.8092e-01, -2.5086e-01,\n",
       "                       9.7338e-01,  8.4789e-01, -2.3406e-02,  1.1689e+00,  6.8103e-02,\n",
       "                       5.9519e-01,  3.9752e-01,  1.5464e-02,  1.7176e-01, -3.6699e-01,\n",
       "                      -4.2878e-01,  5.1048e-01,  1.1647e+00,  3.7855e-01, -4.6805e-01,\n",
       "                      -4.8043e-01,  5.0178e-02,  2.2331e-02, -1.9150e-01, -1.5759e-01,\n",
       "                       6.6785e-02,  1.5676e+00, -3.3902e-01,  3.9885e-01, -2.6838e-01,\n",
       "                       3.2994e-01, -1.0214e+00,  7.9689e-01,  9.9681e-01, -1.1939e+00,\n",
       "                      -1.8656e-01,  1.3115e-01, -5.6507e-01, -2.9697e-01,  1.8703e-01,\n",
       "                       7.0131e-01,  7.9934e-01,  9.2913e-02,  4.7836e-01, -4.0762e-02,\n",
       "                       7.0146e-04, -6.0820e-01,  2.0086e-01, -4.8438e-01,  1.8531e+00,\n",
       "                       5.3429e-02, -1.1372e-01, -2.9849e-01,  2.1388e+00, -5.2539e-01,\n",
       "                      -2.7267e-01,  1.5576e-01, -2.9083e-01,  1.7847e-01,  1.4895e+00,\n",
       "                       6.5398e-02,  1.2725e+00,  4.0174e-01,  4.7236e-01,  2.7312e-01,\n",
       "                       2.1031e+00,  5.3445e-02,  1.5499e+00, -2.8504e-01, -9.1119e-02,\n",
       "                       2.2063e-01,  2.9311e-01, -3.1791e-01, -4.4299e-01,  6.8467e-01,\n",
       "                      -4.7005e-02,  1.6523e-01,  3.4006e-01,  1.0764e-01,  8.4461e-01,\n",
       "                       2.1912e-02,  1.3193e+00,  1.9013e-01,  2.1693e-01, -2.9445e-01,\n",
       "                       3.1662e-01,  4.9643e-02, -4.5162e-01, -7.2001e-02, -6.0040e-01,\n",
       "                       1.0882e+00,  1.4640e-01,  4.2032e-02,  1.1597e+00,  1.4455e+00,\n",
       "                      -4.6492e-01,  2.9383e-01,  4.1199e-01,  1.3528e+00,  1.7125e+00,\n",
       "                       1.8484e+00, -2.3547e-01,  2.0460e+00, -2.6899e-01, -5.8370e-01,\n",
       "                      -6.2861e-03,  1.9631e+00,  3.2263e-01,  9.1014e-02,  1.6101e-01,\n",
       "                      -7.4317e-01, -1.2493e-01,  1.0201e-01,  3.6361e-02, -2.4918e-01,\n",
       "                       1.4709e-01,  8.3179e-01,  1.5089e-01,  9.7131e-01,  5.6465e-01,\n",
       "                       1.0747e+00, -2.7877e-01,  1.1309e+00,  1.5253e+00, -5.6512e-01,\n",
       "                      -3.0798e-01,  5.0443e-01, -2.8469e-01,  1.0672e+00, -1.6544e-01,\n",
       "                      -1.1215e-01,  2.4681e-01,  1.4813e+00, -1.9881e-02,  2.1902e+00,\n",
       "                       5.6024e-01,  3.2045e+00,  7.4165e-01,  4.3138e-01,  1.5546e-01,\n",
       "                       1.5735e-01,  1.6131e+00,  1.5259e+00, -1.6656e-01,  4.9852e-01,\n",
       "                       3.1805e-01,  3.5483e-01,  1.8249e+00,  1.6536e-01,  4.6151e-01,\n",
       "                       1.2145e+00, -3.2986e-01,  2.3358e-02,  1.8312e-01,  3.7117e-01,\n",
       "                       1.4707e-01,  2.2460e-01,  1.7947e-01,  6.8226e-01,  1.6838e+00,\n",
       "                       1.7650e+00,  5.8169e-01, -4.4922e-01,  2.1224e-01,  6.6029e-01,\n",
       "                       1.1731e-01,  1.5913e+00,  6.8226e-01,  5.9255e-01, -1.1154e+00,\n",
       "                      -1.8245e-01, -3.2900e-01,  1.3542e+00, -1.1840e+00,  9.6103e-01,\n",
       "                      -2.1756e-01,  1.3491e-01, -1.1343e-01, -5.8741e-01,  1.3645e-02,\n",
       "                       3.9522e-01,  1.7777e+00, -4.5657e-01,  7.9211e-01,  9.5812e-01,\n",
       "                       1.3368e-01, -1.2239e-01,  1.0637e+00,  1.8446e-01,  1.9128e-01,\n",
       "                       2.1699e+00,  1.0932e-01,  2.8899e-01,  3.3311e-03,  2.8043e-01,\n",
       "                       1.4150e+00,  9.6404e-01, -1.4606e-01, -2.8041e-03, -1.2571e-01,\n",
       "                      -6.1124e-01, -3.1358e-01,  4.4908e-01, -5.5549e-01,  6.3609e-01,\n",
       "                       1.9006e+00,  1.5320e+00, -7.3924e-01,  1.6953e+00,  1.3377e-01,\n",
       "                      -1.0066e-01,  5.9092e-01,  7.4949e-02,  2.2256e+00, -4.7566e-01,\n",
       "                       2.3232e-01,  7.5216e-01, -7.4098e-01,  1.4855e+00,  2.2102e-01,\n",
       "                       1.0266e-01,  2.8109e-01, -2.5828e-02,  2.1669e+00, -4.5260e-01,\n",
       "                       5.0141e-01,  1.3132e+00,  7.0492e-02,  1.0130e+00,  7.8927e-03,\n",
       "                       1.7596e-01, -1.3530e-01, -1.0094e-01,  4.2871e-01,  1.1532e+00,\n",
       "                       3.3750e-01,  1.8472e-01,  1.7067e+00,  6.4408e-01,  4.7743e-01,\n",
       "                       1.4437e+00,  1.1978e+00,  3.3372e-01,  1.1741e-01, -1.9924e-03,\n",
       "                       2.5759e-01, -2.9039e-02,  1.8364e-01,  5.3089e-01,  1.8262e+00,\n",
       "                      -1.1164e+00,  6.1918e-01,  8.7758e-01,  8.6134e-01, -5.0903e-01,\n",
       "                      -6.1637e-02,  1.1716e+00, -1.0049e+00,  3.5698e-01,  1.0546e-01,\n",
       "                       1.0764e+00,  2.6277e-01,  1.3981e+00,  4.7933e-01,  1.8522e-01,\n",
       "                       6.5054e-02,  2.3772e+00,  1.0376e-01,  1.6236e+00,  2.0352e+00,\n",
       "                      -1.7424e-01,  3.1129e-01, -5.6238e-01,  1.6294e-01, -3.9617e-01,\n",
       "                      -2.7837e-01,  5.9794e-01,  1.2574e+00,  4.6244e-01,  6.1089e-01,\n",
       "                      -3.5972e-02,  2.4127e-01,  1.1775e-01, -2.9254e-01,  5.5859e-01,\n",
       "                       2.1007e+00,  5.0246e-01,  1.3372e-01,  1.5954e+00,  1.8232e-01,\n",
       "                       6.7704e-01,  7.0188e-02,  5.9717e-01, -4.7013e-02, -9.2588e-02,\n",
       "                      -1.1324e+00, -3.2789e-01,  2.1413e+00, -4.4888e-01,  1.4854e-01,\n",
       "                       4.6638e-01,  8.0337e-01,  3.5828e-01, -6.4429e-01,  2.0246e+00,\n",
       "                       1.7496e+00,  2.1081e-01,  7.3594e-01,  1.9168e+00,  1.5620e-01,\n",
       "                       8.3570e-01,  1.0930e+00,  9.9725e-02,  2.7194e-01, -3.6576e-01,\n",
       "                       1.2261e+00, -4.1437e-01,  4.1541e-01, -1.0045e-01,  3.0394e-01,\n",
       "                       1.7071e+00, -1.2268e-01, -7.8539e-02,  1.9030e+00,  5.8447e-02,\n",
       "                       2.3676e-01,  7.0303e-01, -7.3658e-01,  1.8866e-01,  8.9762e-01,\n",
       "                       1.1635e+00,  2.9263e-01,  1.4285e-02, -1.0640e+00,  2.1000e-01,\n",
       "                      -5.4608e-01,  1.1975e+00,  1.5157e-01, -2.4278e-01,  1.2927e+00,\n",
       "                       1.1836e+00, -1.1837e-01,  1.7446e-01, -4.7899e-01,  3.6957e-03,\n",
       "                       4.9117e-01, -1.5194e+00, -4.5988e-01, -2.5581e-01,  2.6501e-01,\n",
       "                       7.8245e-01, -1.0049e+00,  9.7214e-01,  5.4782e-01,  5.3132e-02,\n",
       "                      -1.6875e-01, -2.5484e-02,  3.7881e-02,  3.5981e-02, -8.6625e-01,\n",
       "                       2.4319e-01,  6.3834e-01,  6.6336e-01,  7.7151e-01, -3.3635e-01,\n",
       "                      -2.2188e-01,  2.2791e-01, -1.9454e+00,  1.5472e-01, -8.6505e-01,\n",
       "                       1.1902e+00, -1.0405e-01,  1.1129e-03,  5.1004e-01, -2.9689e-01,\n",
       "                       8.3263e-01, -1.9900e-01, -3.3437e-01,  9.0242e-01,  2.1412e-01,\n",
       "                      -5.8606e-01,  1.6500e-01,  1.6228e-01, -7.9264e-01,  9.3676e-01,\n",
       "                       6.1106e-02,  2.7332e-01, -9.7673e-01,  4.8975e-01,  5.6441e-01,\n",
       "                       1.9304e+00,  7.2528e-01, -6.0285e-03,  1.4198e+00, -5.5013e-01,\n",
       "                       1.7576e+00, -1.8942e-01, -1.3195e+00, -9.6419e-01, -5.9704e-01,\n",
       "                      -1.9630e-01,  5.2984e-01, -4.1848e-01, -6.2373e-01,  2.2876e-01,\n",
       "                      -1.6493e-01,  1.0090e+00,  6.9563e-01, -3.8744e-03,  8.0039e-01,\n",
       "                       1.1357e-01, -6.4398e-01, -3.2789e-01,  1.9653e-01,  4.3586e-02,\n",
       "                      -3.7654e-01,  1.6177e-01,  2.8923e-01,  6.0000e-01,  1.0774e+00,\n",
       "                       1.2452e-01,  3.6496e-01, -5.5060e-01,  4.8784e-01,  8.5687e-02,\n",
       "                       1.1252e+00, -3.9252e-01, -1.0172e-01,  2.3540e-01,  1.4341e+00,\n",
       "                      -1.7298e-01,  1.7909e+00, -5.2924e-01,  6.4919e-02,  5.3115e-02,\n",
       "                      -2.8303e-01, -3.4630e-01,  9.0281e-02, -6.7482e-01,  2.2019e-01,\n",
       "                      -5.5282e-01, -2.3188e-01,  2.1081e-01,  7.8178e-01, -3.5561e-01,\n",
       "                      -1.2070e+00,  6.9860e-02,  9.8681e-01,  3.9025e-02, -1.5419e-01,\n",
       "                       1.5363e+00, -3.5020e-01, -1.8443e-01, -2.1308e-01, -6.5377e-01,\n",
       "                       2.9660e-01,  4.6588e-01,  1.3798e+00,  8.8159e-02,  3.0009e-02,\n",
       "                       1.2970e-01,  7.4103e-02,  1.9571e+00, -1.3244e-01,  2.4819e-02,\n",
       "                       2.2874e-01, -5.3185e-01,  3.4955e-01,  1.5216e+00, -5.4210e-01,\n",
       "                       1.4114e-01,  1.0960e+00,  6.5282e-02,  7.9944e-01,  8.8177e-01,\n",
       "                       1.8233e-01,  8.0105e-02], device='cuda:0')),\n",
       "             ('module.model.21.1.running_mean',\n",
       "              tensor([ 4.2613e-01, -5.1652e-01,  2.9737e-01,  5.1386e-01,  3.9250e-01,\n",
       "                       3.5835e-01, -1.7430e+00, -6.0016e-02, -6.3192e-01, -1.5254e+00,\n",
       "                       1.1747e-01,  6.4196e-01, -8.7503e-01,  1.0470e+00,  4.6308e-01,\n",
       "                      -1.1433e+00,  3.1057e-01, -1.3861e+00,  6.2699e-01, -8.6185e-01,\n",
       "                      -7.2318e-01,  3.8578e-01,  1.6116e-01, -8.1499e-01, -7.1088e-01,\n",
       "                      -1.9328e+00,  9.4129e-01,  4.9545e-01,  3.9904e-01,  4.0757e-01,\n",
       "                      -1.5666e+00,  4.7030e-01, -2.8689e-01,  5.1657e-01,  6.0465e-01,\n",
       "                       4.5292e-01, -1.4962e+00,  7.2924e-01, -9.4062e-01,  2.3764e-01,\n",
       "                      -8.2768e-01, -1.7023e+00,  2.4179e-01,  3.8749e-01,  3.2107e-01,\n",
       "                      -7.6962e-01, -9.4582e-01,  2.6193e-01,  2.1100e-01, -1.2788e+00,\n",
       "                      -1.0164e+00, -7.6101e-01, -5.0812e-01, -6.7460e-01,  9.4868e-01,\n",
       "                       8.3249e-01,  1.3731e-01,  6.6583e-01, -1.0283e-01,  5.5332e-01,\n",
       "                       1.0202e+00, -4.3339e-01,  3.1104e-01,  1.1425e+00, -1.1484e+00,\n",
       "                       5.7191e-01,  5.5643e-01, -6.4434e-01, -8.5948e-01,  8.3114e-01,\n",
       "                       4.9473e-01,  3.6876e-01, -2.0671e+00, -1.2963e+00, -9.2464e-01,\n",
       "                      -5.9685e-01, -7.4376e-01,  3.0234e-01, -6.4653e-01,  6.5564e-02,\n",
       "                       4.3112e-01, -1.2202e+00,  1.8771e-01,  3.2487e-01, -6.4275e-01,\n",
       "                      -1.3848e+00,  3.5752e-01,  2.8208e-01, -2.9583e-01,  5.0552e-01,\n",
       "                       6.0074e-01,  1.1168e+00,  2.2868e+00,  3.0481e-01, -5.6649e-04,\n",
       "                       7.3269e-01, -6.3039e-01,  7.4985e-01, -1.7187e+00,  3.0029e-01,\n",
       "                      -2.0161e-01,  5.7875e-01, -7.7954e-01, -1.3245e+00,  8.2704e-01,\n",
       "                      -1.4290e+00,  2.6626e-01,  8.0890e-01,  1.1401e+00,  3.3134e-01,\n",
       "                       4.0704e-01,  3.2693e-01, -1.2181e+00, -1.5979e+00,  6.0085e-01,\n",
       "                       3.0876e-01,  1.1054e-01, -1.7128e+00,  4.4774e-01, -1.6895e+00,\n",
       "                       8.3243e-01,  3.5255e-01, -1.5291e+00,  3.5314e-01,  1.9896e-01,\n",
       "                      -7.4329e-01,  3.8277e-01,  3.9310e-01, -2.4535e-01, -9.8442e-03,\n",
       "                       1.0032e+00,  3.5570e-01,  5.6356e-01, -7.5307e-01, -8.2691e-03,\n",
       "                       3.9245e-01, -2.1899e+00, -6.3383e-01,  1.3634e-02,  4.3182e-01,\n",
       "                       2.9705e-01, -1.0887e-01,  5.8291e-01,  5.1678e-01,  4.8875e-01,\n",
       "                      -1.7062e-02,  4.7970e-01,  6.7730e-01, -1.5640e+00,  5.6887e-01,\n",
       "                       3.5957e-01, -1.1825e+00,  3.0784e-01, -9.2280e-01, -1.0906e+00,\n",
       "                      -6.8189e-01, -1.5766e+00,  4.3217e-01, -1.2887e-01,  8.5724e-02,\n",
       "                       3.2888e-01,  2.6621e-01,  1.4811e+00, -7.7575e-01,  4.1652e-01,\n",
       "                       4.4244e-01, -1.0915e+00,  1.1952e-01, -1.6728e+00, -3.3993e-01,\n",
       "                      -8.1154e-01,  9.5726e-01,  5.1565e-01, -1.1468e+00,  9.7436e-01,\n",
       "                       5.8320e-01,  2.0808e-01,  1.1659e-01,  5.7806e-01, -1.2797e+00,\n",
       "                       2.8799e-01, -1.4217e+00, -2.8915e-01,  2.9307e-02, -1.7889e+00,\n",
       "                      -3.9225e-02, -1.8662e+00,  4.4170e-01, -1.8963e+00, -9.4927e-01,\n",
       "                      -1.3346e+00, -2.4283e+00,  3.6254e-01, -8.3045e-01, -2.8767e-01,\n",
       "                      -5.4998e-01, -8.9854e-01,  3.2081e-01,  2.5272e-01, -1.0040e+00,\n",
       "                       3.1830e-01,  8.0266e-02, -7.7417e-01, -6.1150e-01, -2.3593e+00,\n",
       "                      -1.4487e-01,  4.5456e-01, -1.2569e+00,  4.1657e-01, -5.5308e-01,\n",
       "                      -1.4246e+00,  3.0624e-01,  4.9998e-01,  4.7161e-01, -1.3809e+00,\n",
       "                      -3.6331e-01, -2.2929e-01,  1.7190e-01, -1.8310e+00, -1.3253e-01,\n",
       "                       2.4211e-01,  2.1089e+00,  7.0800e-01,  2.1931e-01,  6.6464e-01,\n",
       "                       5.7114e-02,  6.4419e-01, -1.1405e+00,  5.3791e-01,  3.6585e-01,\n",
       "                      -6.7226e-01,  2.8289e-01, -1.4010e+00,  5.7293e-01,  4.6042e-01,\n",
       "                       2.4776e+00, -3.1650e-01, -7.3439e-01,  2.0335e+00, -1.6670e+00,\n",
       "                      -1.0846e+00, -4.5950e-01, -1.8646e-01, -1.0467e+00, -8.7710e-01,\n",
       "                      -1.1687e+00, -1.0604e+00,  2.1822e-01, -3.1135e-01, -1.2898e+00,\n",
       "                       3.8398e-01, -8.8400e-01,  8.8358e-01, -8.6661e-01, -1.6004e+00,\n",
       "                       2.3545e-01,  3.8688e-01,  5.2792e-01, -1.9341e-01,  8.6083e-01,\n",
       "                      -9.5752e-01, -9.5227e-01,  2.3625e-01,  1.5642e-01,  5.5199e-01,\n",
       "                       5.5926e-01, -1.5614e+00,  7.6281e-01, -5.5631e-01, -8.5494e-01,\n",
       "                      -1.1477e+00, -1.1912e+00, -7.2635e-01, -9.0019e-01,  5.8369e-01,\n",
       "                      -4.9271e-01, -5.3436e-01, -1.0066e+00, -2.0199e+00,  5.2860e-01,\n",
       "                      -2.1159e+00,  1.0084e+00,  2.3642e-01, -9.3533e-01, -2.2809e-02,\n",
       "                       6.4806e-01, -1.2351e+00,  2.7965e-01, -5.1251e-01,  5.5870e-01,\n",
       "                       5.3804e-01, -9.6502e-01,  2.7720e-01, -1.4684e+00,  1.0524e+00,\n",
       "                       4.4321e-01,  3.7958e-01, -1.6692e-01, -1.2002e+00,  3.6022e-01,\n",
       "                      -1.9670e+00, -2.5038e-01,  2.5436e-01, -6.8994e-01, -4.2360e-01,\n",
       "                       8.1830e-01, -1.0663e+00,  3.8657e-01,  1.0476e-01,  3.6694e-01,\n",
       "                       2.5166e-01, -7.8869e-01,  1.1807e-01, -8.6962e-01, -8.0961e-01,\n",
       "                      -8.5778e-01,  9.9184e-01,  4.5842e-02,  4.3539e-01, -3.6642e-01,\n",
       "                       9.8648e-01, -9.0880e-01,  3.4114e-01, -1.4170e+00,  1.3964e-01,\n",
       "                      -1.6314e+00,  6.4813e-01, -7.6460e-01,  4.9337e-01,  3.8552e-01,\n",
       "                       5.3950e-01,  6.9287e-01, -1.0667e+00, -2.4258e+00,  6.1826e-01,\n",
       "                      -1.8082e+00, -8.4031e-01, -1.8104e+00,  8.4912e-01, -4.6658e-01,\n",
       "                      -3.4475e-01,  1.8988e-01, -9.5380e-01, -5.0550e-01,  1.3303e-01,\n",
       "                      -6.7080e-01, -2.9673e-01, -1.0096e+00, -1.1678e+00,  2.6147e-01,\n",
       "                       3.9730e-01,  9.5314e-01,  1.0972e-01,  2.6258e-01, -2.0474e+00,\n",
       "                      -8.1950e-01, -1.5750e+00,  2.5722e-01, -1.6301e-01,  2.5032e-01,\n",
       "                       2.6030e-01, -8.0460e-01, -1.8966e+00, -8.7636e-01, -1.0904e+00,\n",
       "                      -1.0021e+00,  6.6712e-01,  5.3150e-01,  3.5648e-01,  2.4556e-01,\n",
       "                      -2.0190e-01, -4.6894e-01,  5.8979e-01, -2.3914e-01,  9.0149e-02,\n",
       "                       1.1672e+00,  4.4669e-01,  4.7457e-01, -5.6333e-01,  6.6702e-01,\n",
       "                       2.9559e-01,  6.2183e-01, -1.0069e+00,  2.6257e-01,  4.7063e-01,\n",
       "                      -7.6785e-01,  9.4147e-01,  1.1543e+00, -9.2883e-01, -1.7162e+00,\n",
       "                      -1.3713e+00,  4.6332e-01,  3.6514e-01,  7.2282e-01,  5.9835e-01,\n",
       "                      -1.8726e+00,  8.0109e-01, -5.7583e-01, -1.0749e+00,  5.0900e-01,\n",
       "                       3.3403e-01,  4.6714e-01, -4.3007e+00,  2.9111e-01,  8.5731e-02,\n",
       "                      -5.9827e-01,  2.7779e-01,  5.4952e-01, -1.0814e+00,  5.7644e-01,\n",
       "                      -6.8779e-01, -1.3852e+00,  6.6737e-01, -1.1996e+00, -9.1211e-01,\n",
       "                       1.9361e-01,  3.1086e-01,  1.0246e+00,  8.7520e-01,  7.4744e-01,\n",
       "                      -1.6718e+00,  9.0708e-01,  1.1254e-01,  5.8338e-01, -1.3063e+00,\n",
       "                      -5.1667e-01, -1.1767e+00,  5.1251e-01,  2.7354e-01,  1.1487e+00,\n",
       "                      -5.4053e-01, -1.9758e+00,  1.1925e+00, -3.2610e-01,  4.8418e-01,\n",
       "                      -9.8171e-01, -8.8047e-01,  1.2261e+00,  5.7972e-01,  2.8104e-01,\n",
       "                       3.4937e-01, -9.2761e-01, -1.3533e+00,  7.9905e-01, -1.1255e+00,\n",
       "                       3.1328e-01, -1.9773e-01,  9.0635e-01, -1.1489e+00,  3.5695e-01,\n",
       "                      -1.4824e+00,  4.1141e-01,  2.7599e-01, -8.1499e-01, -1.0768e+00,\n",
       "                      -1.7365e+00, -1.6515e+00,  6.0489e-01, -1.0556e+00, -1.1793e+00,\n",
       "                      -1.0163e+00,  3.8235e-01,  7.8073e-02,  2.0131e-01, -9.0124e-01,\n",
       "                      -1.5411e+00,  8.4764e-02,  2.6566e-01,  3.2459e-01,  5.3789e-01,\n",
       "                       5.0593e-01, -1.7190e+00,  3.2170e-01,  8.1442e-01,  5.4335e-01,\n",
       "                       9.8165e-01,  8.2654e-01, -1.0449e+00, -6.7521e-01, -1.3053e+00,\n",
       "                       1.0881e+00,  2.0966e-01, -1.3111e+00,  6.1582e-01,  2.8127e+00,\n",
       "                      -1.1245e+00, -1.5630e+00,  6.3236e-01,  5.7594e-01,  9.0191e-01,\n",
       "                       7.7178e-01, -9.3009e-01, -8.3231e-01,  4.9115e-01,  6.1801e-01,\n",
       "                       8.8321e-02,  1.7924e-01, -1.8662e-01,  7.1910e-01,  3.6786e-01,\n",
       "                      -1.7997e+00, -1.4372e+00,  5.6193e-01, -1.0773e+00,  7.5593e-01,\n",
       "                       8.8909e-01,  4.4504e-01,  6.3589e-01, -6.4433e-01, -1.4605e+00,\n",
       "                       1.9888e-01,  3.3919e-01], device='cuda:0')),\n",
       "             ('module.model.21.1.running_var',\n",
       "              tensor([0.4095, 0.8750, 0.3888, 0.7160, 0.3164, 0.3121, 0.4085, 1.6627, 0.5343,\n",
       "                      1.1850, 0.2814, 1.1387, 0.6688, 1.7859, 2.4676, 0.8817, 0.4164, 0.8493,\n",
       "                      0.6934, 0.5759, 0.4626, 0.3789, 0.4627, 1.8481, 0.4230, 0.4679, 2.5757,\n",
       "                      0.5643, 0.4562, 0.4680, 0.7331, 0.4418, 1.2151, 0.8536, 2.5604, 0.6658,\n",
       "                      1.0580, 0.8101, 0.6102, 3.3062, 0.6079, 2.0261, 0.5046, 1.3415, 0.2545,\n",
       "                      0.4531, 0.9863, 0.8474, 0.1498, 1.2780, 0.8716, 0.7440, 0.2811, 0.6582,\n",
       "                      1.6498, 1.4590, 0.8431, 0.7430, 0.9898, 0.5409, 1.1785, 0.1799, 0.3111,\n",
       "                      1.4806, 1.1950, 0.6473, 0.9657, 0.4776, 0.5549, 0.7933, 0.5485, 0.4755,\n",
       "                      1.3309, 1.2807, 0.9084, 0.3815, 0.5567, 0.3721, 0.7245, 0.0397, 1.3584,\n",
       "                      1.2658, 0.2189, 0.3919, 0.4897, 0.9671, 0.6806, 0.2441, 0.8825, 0.7094,\n",
       "                      1.1030, 1.8569, 3.3828, 0.3044, 1.2151, 1.1734, 0.3651, 0.9059, 0.4355,\n",
       "                      0.2468, 1.5307, 0.6168, 0.4534, 0.9940, 1.3957, 1.1556, 0.2113, 1.1025,\n",
       "                      1.2800, 0.5169, 0.5158, 0.3019, 1.5938, 1.0378, 1.2737, 0.4045, 1.2137,\n",
       "                      0.6044, 0.4668, 0.9232, 1.7471, 0.3811, 1.0051, 0.4391, 0.2136, 0.4466,\n",
       "                      0.4567, 0.4416, 0.8437, 0.8248, 1.8111, 0.4377, 0.6579, 0.4588, 0.6533,\n",
       "                      2.6089, 0.2501, 0.4554, 0.1590, 0.3607, 0.3069, 0.7686, 0.7506, 0.6102,\n",
       "                      0.4609, 2.4758, 0.5854, 0.9768, 0.9313, 0.5748, 0.3446, 0.5991, 0.3167,\n",
       "                      1.2834, 1.1200, 0.5517, 0.5739, 1.5945, 2.1871, 0.7217, 0.3429, 0.7839,\n",
       "                      0.7403, 0.3445, 0.6582, 0.4889, 0.8298, 0.5384, 0.6950, 0.1882, 0.7343,\n",
       "                      3.0631, 1.1649, 1.1658, 1.2346, 1.3266, 0.9801, 0.7157, 0.6814, 1.5549,\n",
       "                      0.2210, 1.5363, 0.1286, 0.0713, 0.3028, 1.1726, 0.4750, 0.6570, 0.5232,\n",
       "                      1.0520, 1.0446, 2.5971, 0.2668, 0.8857, 1.0329, 2.6342, 1.0046, 0.4906,\n",
       "                      0.2108, 0.7932, 0.3563, 0.5445, 0.5988, 0.4545, 1.5212, 0.9271, 0.5162,\n",
       "                      1.0192, 0.5192, 0.3623, 0.6496, 0.3743, 0.5494, 0.4430, 1.4618, 1.8066,\n",
       "                      0.7792, 0.1425, 1.0624, 1.0412, 0.2730, 2.1109, 1.2079, 0.2925, 0.9559,\n",
       "                      0.9546, 0.8763, 1.0258, 0.7130, 0.3712, 0.4242, 1.2223, 2.4154, 0.5402,\n",
       "                      0.5004, 2.9284, 1.9104, 0.5879, 2.9507, 0.4353, 0.8015, 0.2751, 1.4326,\n",
       "                      4.4397, 0.7003, 0.9236, 1.3251, 0.1991, 3.5695, 1.1248, 0.3150, 0.6378,\n",
       "                      0.9643, 0.7036, 0.7928, 0.2166, 0.3419, 0.9746, 2.0948, 0.8728, 0.9503,\n",
       "                      0.4587, 0.5222, 0.9743, 0.6537, 0.7557, 0.8049, 1.1010, 0.7473, 0.7765,\n",
       "                      0.8918, 0.8107, 0.5683, 1.0219, 0.9521, 0.3038, 0.3151, 1.1167, 0.4117,\n",
       "                      0.5976, 1.7412, 0.8087, 0.1808, 0.7256, 1.2098, 0.9292, 1.0776, 1.5089,\n",
       "                      0.7805, 0.5621, 0.6204, 0.7306, 0.3499, 1.3186, 1.6719, 1.6524, 0.4159,\n",
       "                      0.9180, 0.8921, 0.4723, 0.5365, 0.8685, 0.2385, 0.9306, 2.5571, 1.3352,\n",
       "                      0.6182, 0.8715, 0.1000, 0.4527, 0.4271, 0.5314, 0.2236, 0.8671, 1.1110,\n",
       "                      0.6817, 0.8653, 0.0509, 0.7469, 2.0596, 1.5451, 0.8414, 0.2912, 0.7166,\n",
       "                      0.0970, 0.7365, 0.9536, 0.7688, 0.5169, 0.6995, 0.7600, 0.9662, 1.3260,\n",
       "                      1.2067, 0.9595, 0.7102, 0.7692, 0.7798, 1.2151, 2.3567, 0.9422, 0.1908,\n",
       "                      1.1860, 0.3947, 0.1133, 0.6155, 0.3752, 0.7652, 0.9801, 0.2398, 1.4384,\n",
       "                      1.4203, 0.2204, 0.2946, 1.7176, 0.9886, 0.6479, 0.2088, 2.8364, 0.2373,\n",
       "                      0.2705, 0.5756, 2.8755, 0.8214, 0.7637, 0.7592, 0.7112, 0.7377, 0.3714,\n",
       "                      0.2499, 0.6895, 0.2455, 0.5894, 1.8518, 0.4916, 1.4188, 0.4916, 0.4382,\n",
       "                      4.1638, 0.9256, 0.3857, 2.0170, 0.7684, 0.4678, 0.5981, 0.6122, 1.4503,\n",
       "                      0.9100, 0.8470, 0.8859, 0.9267, 0.5756, 0.4351, 1.1664, 0.7369, 0.6566,\n",
       "                      1.4157, 0.5245, 1.5396, 0.5309, 0.2945, 0.7857, 1.4334, 0.2572, 1.3160,\n",
       "                      0.6199, 0.2312, 0.7117, 0.9399, 0.5502, 0.6524, 0.8113, 0.9019, 1.1895,\n",
       "                      0.7720, 0.5144, 0.3135, 0.7078, 0.9254, 2.6788, 2.8474, 1.3504, 0.1999,\n",
       "                      0.9301, 1.1091, 0.3580, 0.9238, 0.6444, 1.1871, 1.8246, 0.4954, 1.2724,\n",
       "                      1.3884, 0.9610, 0.7282, 0.8589, 1.5127, 2.0456, 1.3719, 1.0282, 0.5118,\n",
       "                      0.6583, 0.8593, 1.5451, 1.4011, 0.2691, 1.0030, 1.0403, 1.5158, 0.4158,\n",
       "                      0.7883, 0.4059, 0.2259, 0.5759, 0.7211, 0.1999, 0.8083, 0.6557, 1.0507,\n",
       "                      0.9581, 2.2218, 0.3746, 0.1172, 0.1637, 0.6262, 1.0148, 1.2365, 0.7034,\n",
       "                      0.3271, 0.7769, 0.4057, 0.6750, 0.3556, 0.7924, 0.5977, 1.0693, 0.9887,\n",
       "                      0.8129, 0.4299, 0.9616, 1.4232, 0.2818, 0.8820, 0.6603, 5.4135, 1.0318,\n",
       "                      0.7514, 1.3753, 0.5301, 1.6294, 1.5324, 0.5320, 0.9153, 0.5801, 0.9621,\n",
       "                      0.1029, 0.1891, 1.6160, 0.9346, 0.7871, 0.5626, 1.3979, 0.5358, 0.9251,\n",
       "                      0.8302, 1.3412, 1.7990, 0.8020, 0.5847, 1.2113, 0.1378, 0.4480],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.21.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.21.2.clip_val', tensor([6.4917], device='cuda:0')),\n",
       "             ('module.model.22.0.weight', tensor([[[[ 0.0137]],\n",
       "              \n",
       "                       [[ 0.0854]],\n",
       "              \n",
       "                       [[ 0.0234]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0061]],\n",
       "              \n",
       "                       [[-0.1012]],\n",
       "              \n",
       "                       [[ 0.0266]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0450]],\n",
       "              \n",
       "                       [[ 0.0509]],\n",
       "              \n",
       "                       [[ 0.0072]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0602]],\n",
       "              \n",
       "                       [[-0.0500]],\n",
       "              \n",
       "                       [[ 0.1147]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0213]],\n",
       "              \n",
       "                       [[-0.0090]],\n",
       "              \n",
       "                       [[ 0.0116]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0159]],\n",
       "              \n",
       "                       [[ 0.0322]],\n",
       "              \n",
       "                       [[-0.0022]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0208]],\n",
       "              \n",
       "                       [[ 0.0324]],\n",
       "              \n",
       "                       [[ 0.1149]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0074]],\n",
       "              \n",
       "                       [[-0.0299]],\n",
       "              \n",
       "                       [[-0.0123]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0573]],\n",
       "              \n",
       "                       [[ 0.0215]],\n",
       "              \n",
       "                       [[ 0.0974]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0163]],\n",
       "              \n",
       "                       [[-0.0305]],\n",
       "              \n",
       "                       [[-0.0205]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0194]],\n",
       "              \n",
       "                       [[-0.0169]],\n",
       "              \n",
       "                       [[-0.0350]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0151]],\n",
       "              \n",
       "                       [[-0.0307]],\n",
       "              \n",
       "                       [[-0.1080]]]], device='cuda:0')),\n",
       "             ('module.model.22.1.weight',\n",
       "              tensor([1.0321, 1.0346, 0.7246, 0.8902, 0.8374, 0.7151, 0.9442, 0.9457, 0.8531,\n",
       "                      0.4253, 1.0182, 1.0720, 0.9511, 0.7588, 1.0604, 0.9284, 0.9261, 1.0018,\n",
       "                      0.9376, 0.4873, 0.8102, 1.0363, 0.3151, 0.9751, 0.8783, 0.9855, 0.9236,\n",
       "                      1.0191, 0.9762, 0.9970, 0.3374, 0.9966, 0.3383, 1.0408, 0.8567, 1.0388,\n",
       "                      1.0904, 0.6542, 0.8772, 0.8380, 0.9276, 1.0116, 0.7943, 0.4907, 1.0256,\n",
       "                      1.0539, 1.0667, 0.5223, 1.0394, 1.0441, 0.7841, 0.7860, 1.1121, 0.8425,\n",
       "                      0.9268, 0.8600, 0.3955, 1.0320, 0.9791, 0.9083, 0.8696, 0.8426, 0.9687,\n",
       "                      1.0685, 1.1059, 0.9756, 0.9513, 1.0011, 0.9447, 1.1428, 1.0990, 1.0351,\n",
       "                      1.0742, 0.9809, 0.9820, 0.9630, 0.7710, 0.8532, 0.7133, 0.8970, 1.0459,\n",
       "                      1.0521, 0.5173, 0.8490, 0.9509, 1.0356, 1.0033, 1.1459, 1.0252, 0.9512,\n",
       "                      0.3333, 0.6525, 0.9861, 1.0131, 0.6617, 1.0845, 1.0133, 0.9073, 0.8020,\n",
       "                      0.8654, 0.9506, 1.0806, 0.3429, 0.9456, 1.0034, 0.9141, 1.0298, 0.8373,\n",
       "                      0.9534, 0.9586, 0.9997, 0.9106, 0.8250, 0.9432, 0.8978, 1.0805, 1.0788,\n",
       "                      1.0531, 0.8553, 0.9069, 0.5025, 0.9481, 1.0499, 0.6081, 0.8669, 0.9593,\n",
       "                      1.0859, 0.9415, 1.0550, 0.8107, 0.9999, 1.0192, 0.9994, 0.9848, 1.0194,\n",
       "                      1.0726, 0.9365, 0.9064, 0.7590, 1.0334, 0.5309, 0.9469, 1.1194, 1.0564,\n",
       "                      1.0392, 0.8956, 1.0431, 0.8042, 1.0484, 0.9126, 0.9100, 0.9547, 1.0081,\n",
       "                      0.8951, 1.0675, 0.9515, 0.8546, 0.8688, 0.9935, 0.9086, 1.0813, 0.2809,\n",
       "                      0.9344, 1.0594, 1.1013, 1.2795, 0.7973, 0.4410, 1.0452, 0.9933, 0.8851,\n",
       "                      1.0672, 0.9782, 0.5842, 1.0780, 1.0191, 0.9500, 0.5937, 1.0438, 0.9612,\n",
       "                      0.3754, 1.0722, 0.9962, 0.8864, 0.8731, 1.0486, 0.8574, 0.9654, 1.1046,\n",
       "                      1.0583, 1.0506, 0.9413, 0.8995, 0.4662, 0.9299, 0.7931, 0.7862, 0.8350,\n",
       "                      1.0325, 0.7843, 1.0150, 0.8843, 0.9581, 1.0005, 0.9127, 1.1228, 0.6905,\n",
       "                      0.9020, 1.0566, 1.0566, 0.8604, 1.0923, 0.8652, 1.0198, 1.0653, 0.9136,\n",
       "                      1.1548, 0.7863, 1.0680, 0.8707, 0.8335, 1.0241, 0.9589, 1.0347, 0.8462,\n",
       "                      0.8506, 1.0883, 1.0492, 1.0660, 1.0464, 1.0925, 0.8932, 0.8414, 1.1421,\n",
       "                      0.6299, 1.0836, 1.1272, 0.9579, 0.8569, 0.9171, 1.0376, 0.8072, 1.0023,\n",
       "                      1.0410, 1.0753, 0.9973, 1.0608, 1.0456, 0.8919, 0.9807, 0.8620, 0.4918,\n",
       "                      0.9060, 1.0134, 1.1511, 1.0063, 1.1125, 0.9426, 1.0426, 0.8576, 0.9604,\n",
       "                      0.8198, 0.8980, 1.0622, 1.0617, 0.9880, 0.9829, 1.0929, 0.9629, 0.7827,\n",
       "                      0.7407, 0.8287, 1.0624, 0.9074, 0.8263, 1.0525, 1.0804, 1.1228, 0.9874,\n",
       "                      1.0593, 0.9661, 1.0506, 0.9172, 0.9005, 0.3104, 0.5432, 0.9865, 0.3241,\n",
       "                      0.9654, 0.9375, 0.9168, 1.0166, 0.3099, 0.9983, 1.0144, 1.0110, 0.8911,\n",
       "                      0.5573, 0.8876, 0.9859, 1.0700, 0.8735, 0.9496, 1.0367, 1.0181, 0.9181,\n",
       "                      0.8284, 1.0234, 0.9515, 0.9358, 0.9076, 1.0658, 0.8822, 0.9478, 1.0680,\n",
       "                      0.3782, 0.9099, 0.8180, 0.9312, 1.0670, 0.9202, 1.0670, 1.0062, 0.3145,\n",
       "                      0.3874, 0.7846, 1.0964, 1.0223, 0.8652, 1.1061, 0.8704, 0.8775, 0.9972,\n",
       "                      1.0177, 1.0571, 0.9319, 0.7641, 0.8392, 0.4231, 1.0646, 0.8098, 0.9480,\n",
       "                      0.9436, 1.0082, 1.1302, 0.9568, 1.0040, 0.9701, 0.8163, 1.0432, 0.9730,\n",
       "                      0.3603, 0.9100, 0.9771, 0.4177, 0.8952, 0.8811, 1.0550, 0.9894, 0.3771,\n",
       "                      0.8403, 0.9060, 0.8682, 0.4366, 0.9302, 1.0849, 0.3976, 1.0415, 0.9422,\n",
       "                      0.3496, 0.3301, 0.3813, 1.0720, 0.5797, 0.9268, 0.9734, 0.8103, 0.4654,\n",
       "                      0.9391, 0.9300, 1.0018, 1.0644, 1.0010, 0.9545, 0.4766, 0.9880, 0.9339,\n",
       "                      0.9658, 1.0335, 1.0830, 0.9588, 0.9467, 0.4921, 1.0720, 0.9748, 0.8908,\n",
       "                      0.8231, 0.7809, 1.0294, 0.8963, 1.0602, 1.1159, 1.0902, 0.7582, 0.8655,\n",
       "                      0.9799, 0.7515, 1.0815, 0.8246, 0.9355, 0.8777, 0.9630, 0.8516, 0.9643,\n",
       "                      0.9975, 0.9206, 1.0268, 0.8898, 1.1189, 0.9514, 1.0301, 0.8815, 0.9733,\n",
       "                      1.0143, 1.0355, 1.0524, 1.0389, 1.0143, 1.0300, 1.1581, 0.8926, 1.0885,\n",
       "                      1.1589, 0.8023, 0.9023, 0.8764, 0.9951, 0.9699, 0.9343, 0.9774, 0.7125,\n",
       "                      0.9035, 0.7785, 1.0455, 0.8712, 0.9354, 0.9908, 0.7682, 1.0377, 0.5720,\n",
       "                      0.8210, 1.0101, 0.9630, 0.8786, 0.8436, 0.9935, 0.8773, 0.8748, 1.0160,\n",
       "                      0.9564, 0.9056, 0.9400, 1.1409, 0.9814, 1.0480, 1.0049, 0.9969, 1.0947,\n",
       "                      0.7914, 1.0575, 0.8557, 0.7530, 1.0244, 0.7878, 0.9818, 0.9288, 1.0017,\n",
       "                      0.9091, 0.9619, 0.8646, 0.8822, 0.4758, 1.0188, 0.9132, 1.0118, 0.7451,\n",
       "                      1.1244, 1.0198, 0.4813, 0.9289, 1.0155, 0.6312, 0.7534, 0.6551, 0.9488,\n",
       "                      1.0203, 0.9365, 1.1174, 0.8047, 0.8368, 1.0432, 0.8800, 0.9306, 1.0446,\n",
       "                      0.9589, 1.0742, 0.6651, 1.0583, 1.0553, 1.0399, 1.0428, 0.9521],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.22.1.bias',\n",
       "              tensor([-3.9709e-01, -6.1777e-01,  9.0489e-01, -6.3710e-01, -8.8599e-01,\n",
       "                       9.1958e-01, -9.1600e-01, -5.4773e-01, -7.3779e-01,  1.0658e+00,\n",
       "                       3.4320e-01, -2.9239e-02, -9.5446e-01, -8.9606e-01, -1.3872e-01,\n",
       "                      -5.1298e-01, -1.0660e+00, -6.3179e-01,  4.7189e-01,  9.7982e-01,\n",
       "                      -9.2340e-01, -1.3532e-01,  1.0630e+00, -1.9244e-01, -8.2876e-01,\n",
       "                       1.1539e+00, -1.1936e+00, -2.7558e-01,  3.8919e-01, -8.6510e-01,\n",
       "                       1.0527e+00, -3.1451e-01,  1.0172e+00,  3.9949e-02, -1.0246e+00,\n",
       "                       1.4526e-01, -1.9067e-01,  8.2905e-01, -1.0451e+00, -1.2452e+00,\n",
       "                      -1.2467e+00, -9.0822e-01, -1.0551e+00,  9.3329e-01, -3.2824e-01,\n",
       "                       3.7475e-01, -3.1328e-02,  1.0355e+00,  1.5240e-01,  2.0045e-01,\n",
       "                      -1.1368e+00, -9.4630e-01,  1.2624e-01, -8.3112e-01, -8.6467e-01,\n",
       "                      -9.6107e-01,  1.0259e+00, -3.4298e-01,  4.9537e-01, -6.0313e-01,\n",
       "                      -7.9221e-01,  6.7351e-01, -7.8036e-01, -6.6699e-01,  2.6849e-01,\n",
       "                      -1.0239e+00, -8.5674e-01, -1.1260e-01, -8.8752e-01, -1.9711e-01,\n",
       "                       9.4558e-01, -1.0177e+00, -9.6966e-01, -1.0447e+00, -8.2923e-01,\n",
       "                      -7.7307e-01, -1.1048e+00, -9.6898e-01,  8.4393e-01, -8.7754e-01,\n",
       "                       6.6817e-02, -1.1546e+00,  9.7438e-01,  6.3060e-01, -7.0250e-01,\n",
       "                       3.1808e-01,  5.2202e-01, -6.4604e-01, -9.5514e-01, -1.0391e+00,\n",
       "                       1.0534e+00,  8.2282e-01, -3.7890e-01,  2.9314e-01,  8.4848e-01,\n",
       "                       1.3350e-01, -1.3334e+00, -8.4912e-01, -1.0333e+00, -6.7407e-01,\n",
       "                      -6.1275e-01, -2.4702e-01,  1.0740e+00,  4.6031e-01, -3.9628e-01,\n",
       "                      -1.2240e+00, -2.9060e-01,  5.8291e-01, -9.5384e-01, -8.1264e-01,\n",
       "                      -7.2627e-01, -5.5235e-01, -8.4417e-01, -9.0364e-01,  5.6659e-01,\n",
       "                      -1.4255e-02, -1.8180e-01,  1.3665e-01, -8.6271e-01, -1.0367e+00,\n",
       "                       9.1937e-01, -1.1265e+00,  9.4311e-02,  8.5887e-01, -8.4542e-01,\n",
       "                      -7.3038e-01, -2.2992e-01, -5.3430e-01,  1.8832e-02, -6.8219e-01,\n",
       "                      -7.7084e-01, -9.0209e-01, -1.0450e+00, -4.7839e-01,  4.2370e-01,\n",
       "                       4.8191e-01,  4.5705e-01, -1.1199e+00,  6.8524e-01, -4.7337e-01,\n",
       "                       9.1600e-01, -8.4887e-01,  5.7653e-02, -4.8162e-01, -3.3600e-01,\n",
       "                       5.8850e-01,  1.9336e-01, -8.7073e-01,  1.0762e-01, -6.8821e-01,\n",
       "                      -8.5523e-01,  3.6750e-01, -1.0324e+00, -8.1258e-01, -3.2116e-02,\n",
       "                      -7.7335e-01, -1.1536e+00, -7.4962e-01, -9.1071e-01, -9.1587e-01,\n",
       "                       2.3308e-01,  1.0659e+00, -8.8386e-01, -6.4430e-02, -1.2947e-01,\n",
       "                       1.0839e+00,  8.9118e-01,  1.0129e+00, -1.8817e-01, -1.4724e-01,\n",
       "                      -5.4997e-01, -3.7327e-01, -5.3513e-01,  9.4323e-01,  2.0798e-01,\n",
       "                      -3.2076e-01, -1.3854e+00,  8.7822e-01,  5.8524e-02, -8.7962e-01,\n",
       "                       1.0814e+00, -1.1010e-01,  2.9249e-01, -1.2111e+00, -8.6303e-01,\n",
       "                       3.3930e-01, -7.3769e-01, -8.8182e-01, -1.6740e-01,  3.1607e-02,\n",
       "                      -6.8964e-01,  4.9902e-01, -8.5501e-01,  1.0198e+00, -1.0074e+00,\n",
       "                      -8.1059e-01, -1.1182e+00, -9.0351e-01,  2.0300e-01, -9.5719e-01,\n",
       "                      -4.2403e-01, -8.7792e-01, -7.0391e-01, -8.7254e-01, -1.1445e+00,\n",
       "                       3.4432e-03,  7.9509e-01, -7.6701e-01, -1.8837e-01,  4.1814e-03,\n",
       "                      -9.9433e-01, -6.5731e-02,  5.8866e-01, -1.1067e+00, -2.4744e-01,\n",
       "                      -1.0085e+00,  1.9223e-02, -1.0262e+00, -4.3237e-01, -8.4782e-01,\n",
       "                       6.5175e-01, -4.5594e-01, -1.1994e+00, -3.7954e-02, -1.1359e+00,\n",
       "                      -9.8448e-01, -1.0350e+00,  8.4117e-02, -1.2375e-02, -3.5078e-01,\n",
       "                       4.5951e-02, -1.1285e+00, -9.3225e-01, -2.8286e-01,  8.6084e-01,\n",
       "                      -3.9239e-01,  5.6446e-02, -5.9421e-01, -8.4299e-01, -8.4840e-01,\n",
       "                      -2.4342e-01,  6.7402e-01, -1.6155e-01,  1.3218e-01,  2.3392e-02,\n",
       "                      -6.0845e-01, -1.8808e-01,  3.0056e-01, -1.2145e+00, -1.0139e+00,\n",
       "                      -6.1255e-01,  9.5788e-01, -6.7551e-01, -5.2315e-01,  1.9477e-01,\n",
       "                      -4.5411e-01,  1.5986e-01, -7.9599e-01, -1.8999e-02, -1.0146e+00,\n",
       "                      -5.7195e-01,  6.7967e-01, -5.7740e-01, -8.8970e-02, -3.6262e-01,\n",
       "                       3.6017e-01, -8.0082e-01, -8.2377e-01, -6.2322e-01,  7.2582e-01,\n",
       "                       8.0464e-01, -1.1190e+00, -2.3252e-01, -9.7108e-01, -1.3637e+00,\n",
       "                      -1.9258e-01, -9.6144e-01,  1.1113e-01,  3.6614e-01,  1.3003e-01,\n",
       "                      -8.6049e-01, -3.0526e-01, -8.6187e-01, -8.3410e-01,  1.1104e+00,\n",
       "                       9.4633e-01,  6.0333e-01,  1.0091e+00, -6.5023e-01, -1.1092e+00,\n",
       "                       4.4579e-01, -7.8479e-01,  1.0369e+00, -8.2366e-01, -3.8046e-01,\n",
       "                      -3.4369e-01, -9.1959e-01,  1.1436e+00,  6.3684e-01, -4.6244e-01,\n",
       "                       1.1618e-01, -8.1076e-01, -7.1033e-01,  5.4856e-03,  3.4662e-01,\n",
       "                      -8.7371e-01, -7.8492e-01, -8.4303e-02,  4.8212e-01, -8.9493e-01,\n",
       "                      -6.7515e-01,  1.1577e-01, -8.3775e-01, -4.7933e-01, -3.3264e-01,\n",
       "                       1.0243e+00, -7.2926e-01, -6.1290e-01, -8.7016e-01, -1.2491e-01,\n",
       "                      -1.0818e+00,  5.2607e-01,  4.3427e-01,  1.1279e+00,  1.0224e+00,\n",
       "                      -1.0445e+00,  2.4683e-01, -1.6264e-01, -7.7085e-01, -1.4969e-01,\n",
       "                       6.2517e-01, -8.8109e-01, -7.7446e-01, -7.0265e-01, -8.6847e-02,\n",
       "                      -6.0092e-01, -8.5407e-01, -9.1526e-01,  9.9404e-01,  6.8968e-02,\n",
       "                      -1.2024e+00, -9.2987e-01, -8.6453e-01, -3.6862e-01, -2.0631e+00,\n",
       "                      -7.0351e-01, -7.5874e-01, -6.5534e-01, -1.0449e+00, -1.1831e-01,\n",
       "                       4.9251e-01,  1.0462e+00, -8.9248e-01, -7.1686e-01,  9.5401e-01,\n",
       "                      -9.4615e-01,  5.5448e-01, -7.8579e-02, -7.1829e-01,  1.0296e+00,\n",
       "                      -6.3431e-01, -1.3750e+00, -9.7243e-01,  1.0286e+00, -8.6413e-01,\n",
       "                      -8.0287e-02,  1.0021e+00, -9.1482e-01, -7.1407e-01,  1.0177e+00,\n",
       "                       1.0828e+00,  9.8680e-01,  8.8566e-02,  8.9562e-01,  4.6673e-01,\n",
       "                      -8.9287e-01, -7.9586e-01,  9.7032e-01, -7.5627e-01, -2.0821e-01,\n",
       "                      -1.1744e-01, -9.3534e-02, -3.4314e-01, -4.6817e-01,  9.4429e-01,\n",
       "                      -1.0708e+00, -8.6154e-01, -8.1593e-01, -4.1535e-01, -8.8398e-02,\n",
       "                      -9.2371e-01, -6.4829e-01,  9.4779e-01,  3.0558e-01,  4.4007e-01,\n",
       "                      -8.3449e-01, -8.5501e-01, -9.8361e-01, -2.8731e-01,  5.8464e-01,\n",
       "                      -9.7604e-01,  1.2163e-01, -3.9570e-01, -9.4833e-01, -1.0738e+00,\n",
       "                      -4.6312e-01, -1.0896e+00,  2.0854e-01,  7.0442e-01,  5.2630e-01,\n",
       "                      -9.3713e-01, -1.0545e+00,  5.6632e-01, -6.6749e-01, -8.5735e-01,\n",
       "                      -7.0277e-01, -6.9279e-01,  5.2995e-01, -3.1187e-01, -8.6702e-01,\n",
       "                      -5.1089e-02, -9.7706e-01, -8.8311e-01, -2.0490e-01,  2.1587e-01,\n",
       "                       5.1082e-01, -1.4381e-01,  4.3770e-01,  2.2543e-01, -1.1089e-03,\n",
       "                      -7.7967e-01, -3.0185e-01, -1.2463e-01,  6.6385e-01, -7.6519e-01,\n",
       "                       6.0768e-01,  3.3250e-01, -1.1767e+00, -8.0450e-01, -6.1690e-01,\n",
       "                       7.7720e-01, -6.2195e-01, -7.5703e-01, -1.2045e-01, -7.3027e-01,\n",
       "                      -5.8640e-01, -3.5901e-01,  7.5568e-01, -1.5186e-01,  8.9956e-01,\n",
       "                       7.5677e-01, -3.3498e-01, -7.0747e-01, -7.2104e-01, -1.2657e+00,\n",
       "                      -7.6066e-01, -9.1331e-01, -1.0082e+00, -9.7751e-01, -9.9268e-01,\n",
       "                      -7.1865e-01, -8.7206e-01, -1.5258e-01, -9.4379e-01, -8.1564e-01,\n",
       "                       3.2935e-01, -9.5331e-01, -5.3908e-01, -8.5874e-01, -9.7935e-01,\n",
       "                      -1.0388e+00,  7.4295e-01, -3.4068e-01, -8.3796e-01, -4.5451e-01,\n",
       "                      -8.5111e-01, -1.9813e-01, -8.0978e-01, -7.9804e-01, -1.0539e+00,\n",
       "                      -8.7145e-01,  1.0055e+00,  2.3675e-01, -1.0849e+00, -1.0083e+00,\n",
       "                      -8.2121e-01,  1.5647e-03, -5.4531e-01,  1.0573e+00, -9.8506e-01,\n",
       "                      -3.5429e-01,  8.9402e-01, -9.1654e-01,  8.2757e-01, -9.9455e-01,\n",
       "                      -9.1539e-01,  5.1355e-01, -1.3201e-01, -9.9890e-01, -1.2667e+00,\n",
       "                      -1.3362e-01, -8.0652e-01, -1.1260e+00, -2.3469e-01, -8.9110e-01,\n",
       "                      -2.2123e-01,  8.5817e-01,  1.3773e-01, -1.2814e-01, -3.5401e-01,\n",
       "                      -7.0355e-01, -6.9859e-01], device='cuda:0')),\n",
       "             ('module.model.22.1.running_mean',\n",
       "              tensor([ 4.2265e-01,  2.3082e+00,  1.0111e+00,  4.0701e-01,  6.4633e-04,\n",
       "                      -9.8284e-02,  1.5556e-01,  1.0811e+00, -6.2137e-02, -6.6863e-02,\n",
       "                      -2.6892e-01,  6.0067e-01,  1.1608e+00, -1.1727e+00,  4.7349e-01,\n",
       "                       1.0936e+00, -2.1314e-01, -1.8749e-01,  5.8837e-01,  3.9361e-01,\n",
       "                       1.8636e-01,  1.5843e+00, -3.5971e-02,  1.3213e+00,  4.2934e-01,\n",
       "                       2.2058e-01, -6.4431e-01, -1.6951e-01, -7.3145e-01,  2.9151e-01,\n",
       "                       2.6711e-01,  4.5670e-01, -4.7777e-01,  1.8604e-01,  1.4070e+00,\n",
       "                       1.7856e-01, -4.2841e-01,  3.8776e-01, -1.4822e-02,  1.4791e+00,\n",
       "                       1.0162e+00, -1.2360e-01,  9.3080e-01, -5.5247e-03,  7.6426e-01,\n",
       "                       5.1904e-01, -1.6801e-01,  4.4679e-03, -7.6377e-02, -9.4441e-01,\n",
       "                      -1.3514e+00,  5.2429e-01, -1.4591e+00,  1.1545e-01,  1.2680e+00,\n",
       "                      -9.4172e-02, -8.6086e-01,  1.0902e+00,  5.5539e-01,  1.4703e-01,\n",
       "                       1.2676e-01, -1.0931e+00,  1.0536e-01, -3.9848e-01, -6.8808e-01,\n",
       "                       1.1586e+00, -1.0646e+00, -6.3086e-01,  1.1692e+00, -2.0535e-01,\n",
       "                      -6.7821e-01,  3.0504e-01,  4.4434e-01, -1.0449e+00,  6.7834e-02,\n",
       "                      -2.1106e-01, -6.7736e-01,  1.4943e+00,  1.4121e+00,  1.8082e-01,\n",
       "                      -7.9773e-01, -1.5223e-01,  1.9423e-01,  5.4678e-01,  2.2035e+00,\n",
       "                       2.4785e-01,  7.7164e-01, -2.3208e-01,  8.3931e-03,  1.0465e+00,\n",
       "                       4.4705e-01, -6.3872e-01, -6.5064e-01,  1.4020e-01,  1.6089e+00,\n",
       "                       5.3288e-01,  9.9890e-01,  4.6581e-01,  3.9434e-01,  5.3056e-01,\n",
       "                       7.6666e-01,  1.1545e+00, -1.0515e+00,  9.0393e-01,  1.4077e+00,\n",
       "                      -1.3015e-01,  9.6358e-01,  2.4014e-01, -1.2701e+00,  1.0681e+00,\n",
       "                      -3.3041e-01,  1.3649e+00,  3.8981e-01,  2.0423e+00,  3.5123e-01,\n",
       "                      -7.8163e-02,  5.9086e-01, -1.1432e-01,  5.0669e-01, -3.0095e-01,\n",
       "                      -7.2901e-01, -2.7575e-01, -1.0081e-01,  2.0931e-01, -4.7829e-01,\n",
       "                       8.0302e-01, -3.4430e-01,  4.1924e-01, -1.0797e+00,  1.8098e+00,\n",
       "                      -3.6343e-01,  1.2234e-01, -9.0182e-01,  6.9700e-01, -1.9075e-01,\n",
       "                       5.2432e-01, -8.4558e-01,  1.2902e-01, -4.9163e-01, -9.6162e-01,\n",
       "                       1.3216e-01, -3.8362e-01,  5.8991e-02,  5.1368e-01,  4.3753e-01,\n",
       "                      -1.3593e+00,  1.9117e-03,  5.6336e-01, -7.4443e-01,  3.5126e-01,\n",
       "                      -2.4323e-01, -8.7782e-01, -1.1254e+00,  1.1652e+00, -3.4902e-01,\n",
       "                      -4.9523e-01,  7.7216e-01, -6.3265e-01,  3.6319e-01,  1.8531e+00,\n",
       "                      -5.0196e-02,  1.4099e+00, -4.2900e-01,  1.3227e+00, -7.3455e-01,\n",
       "                      -4.7603e-02, -2.4670e-01, -5.9397e-01,  7.7773e-01,  6.7361e-01,\n",
       "                      -2.8034e-01, -4.9681e-01, -6.8048e-01,  7.4643e-01,  5.7303e-01,\n",
       "                       1.2503e-01, -7.7150e-01, -9.5572e-01, -2.7975e-01,  1.0987e+00,\n",
       "                      -7.8878e-01,  2.3209e-01,  8.5404e-01, -1.3552e+00, -6.2346e-03,\n",
       "                       2.4299e-01,  5.6798e-01, -3.2182e-01,  6.5779e-01,  6.3932e-01,\n",
       "                      -5.3224e-01, -4.1300e-01,  1.5769e+00,  1.7393e+00, -3.5314e-01,\n",
       "                       6.8507e-01, -3.2446e-01,  1.1954e-01,  5.1300e-01,  1.7024e+00,\n",
       "                      -9.3372e-01,  6.6442e-01, -7.3296e-01, -2.2673e-01, -1.3795e-01,\n",
       "                       1.6366e+00, -6.7658e-02,  1.2855e+00, -3.0289e-01, -3.4353e-01,\n",
       "                       2.9051e-01, -3.9121e-01,  6.7707e-01,  6.0766e-01,  3.6973e-01,\n",
       "                       1.1860e+00, -6.8270e-03,  2.7114e-01,  7.2893e-01, -4.8789e-01,\n",
       "                       5.4525e-01,  3.6537e-01,  1.2762e+00,  1.6638e-01,  1.0142e+00,\n",
       "                       1.2306e+00, -1.4158e-01,  6.7946e-01, -1.2421e+00,  9.7893e-01,\n",
       "                      -4.7790e-01,  1.1081e+00,  1.9921e-01,  1.4469e-01, -1.6417e+00,\n",
       "                      -1.1631e+00, -1.9958e-01, -3.7357e-01,  5.1528e-01, -9.9990e-02,\n",
       "                      -3.1698e-01,  1.7737e-01,  1.8690e+00,  2.1764e+00,  2.8051e-01,\n",
       "                       1.3812e-01, -3.2454e-01, -6.1170e-01,  3.8584e-01,  2.9970e-01,\n",
       "                       3.4919e-01,  1.6907e+00,  9.2833e-01,  1.5700e+00,  9.9259e-01,\n",
       "                       4.9440e-01, -9.1127e-03,  1.1045e+00,  3.9873e-02, -2.7579e-01,\n",
       "                       2.2741e-01, -1.7748e-01,  4.0699e-01, -5.3046e-01,  2.1860e-01,\n",
       "                       1.0890e+00,  1.2784e+00,  1.2657e-02, -3.3592e-01, -3.7402e-01,\n",
       "                       1.9687e-02, -5.3735e-01,  6.8801e-01, -2.2756e-01,  5.9890e-01,\n",
       "                       2.2081e-01,  1.3269e+00, -3.4851e-01,  1.5934e+00, -1.5169e-01,\n",
       "                       6.3468e-01,  9.0842e-01,  2.3509e-02,  4.8159e-01,  1.2611e-01,\n",
       "                       7.8904e-02, -2.9773e-01, -1.0378e+00,  4.0802e-01, -1.0815e+00,\n",
       "                       8.3136e-01,  6.7842e-01,  3.0871e-01,  1.0087e-02, -2.1673e-01,\n",
       "                      -9.6482e-01,  5.1283e-01, -1.8344e+00, -3.0262e-02,  9.3871e-01,\n",
       "                      -2.9544e-01, -7.7994e-01,  5.5080e-01,  9.2058e-01,  7.1099e-02,\n",
       "                       8.4645e-01, -1.1174e-01,  8.7235e-01, -1.6330e+00, -3.0687e-02,\n",
       "                       8.3775e-01,  6.2213e-01, -5.8668e-01,  5.8314e-01, -8.9608e-02,\n",
       "                       1.8449e-02,  1.1665e+00,  1.2014e-01,  1.0053e+00, -9.1364e-02,\n",
       "                       1.0474e+00,  1.0684e+00,  1.1310e+00,  1.2721e-01, -2.9173e-01,\n",
       "                       1.1042e+00,  1.2523e+00,  3.9722e-01,  1.5875e+00,  1.5891e+00,\n",
       "                       2.2935e-01,  1.1146e+00,  1.6909e-01,  8.3438e-02,  2.6632e-01,\n",
       "                       3.0780e+00,  4.3647e-01, -2.6005e-02, -5.9347e-01, -5.0463e-01,\n",
       "                       6.2109e-01, -2.7807e-01, -5.0837e-01,  1.6210e+00,  5.9820e-01,\n",
       "                      -1.7808e+00,  9.2634e-01,  1.3266e+00,  1.6203e-02,  9.8446e-01,\n",
       "                       7.5974e-01,  4.6383e-01,  1.4136e+00,  1.6213e+00,  1.3509e+00,\n",
       "                       6.4792e-02, -1.3988e+00, -3.8376e-01, -5.3019e-01, -2.7185e-02,\n",
       "                      -8.2237e-01, -2.0321e-01,  9.7755e-01,  9.4487e-01,  6.8614e-01,\n",
       "                      -3.1405e-01, -1.1337e+00,  7.5537e-02, -1.8874e-01,  7.3689e-01,\n",
       "                       1.9734e+00,  3.1912e-01,  7.8933e-01,  1.4946e+00, -5.3149e-01,\n",
       "                       1.2585e+00,  7.0854e-01, -4.2833e-01, -4.3341e-01,  1.1723e+00,\n",
       "                      -2.7823e-01, -1.1166e-01,  5.4315e-01,  3.0885e-01,  1.3568e+00,\n",
       "                       4.6879e-01, -6.6982e-01,  6.9454e-01, -1.9562e-01,  1.4492e+00,\n",
       "                      -1.3351e+00,  8.7175e-01, -4.4882e-01, -7.3693e-02,  1.7525e-01,\n",
       "                       2.0844e-01,  2.7239e-01,  1.3629e-02,  2.9597e-02, -1.7872e-01,\n",
       "                       5.5179e-01,  1.9232e-01,  1.3265e-01,  1.5425e+00, -4.4734e-01,\n",
       "                      -9.5643e-01,  1.5068e+00,  4.3773e-01,  7.9358e-01,  1.1931e+00,\n",
       "                      -9.2073e-01,  9.4424e-01, -2.0424e-01,  1.4770e+00,  1.3683e+00,\n",
       "                       8.5371e-01, -1.2835e-01, -8.4512e-01,  1.1859e-01,  7.7656e-01,\n",
       "                      -1.6122e+00,  1.3829e+00, -8.5491e-02, -5.4307e-01,  3.7334e-01,\n",
       "                      -1.4985e+00,  9.3583e-01,  1.0924e+00,  1.4065e-01,  7.9883e-01,\n",
       "                      -8.8362e-01,  1.1597e+00,  7.1365e-01,  8.0629e-01,  7.1567e-01,\n",
       "                      -6.7588e-01, -1.1038e+00,  2.3530e-01,  1.9718e+00,  7.2884e-01,\n",
       "                      -7.2978e-01, -4.9595e-01, -1.0693e+00,  2.4704e-01, -4.9650e-01,\n",
       "                      -8.2231e-01, -7.3261e-01, -1.9882e-01, -3.5862e-01, -1.7786e-01,\n",
       "                       2.6956e-01, -1.0425e+00,  8.2806e-01,  1.8375e+00,  1.9931e-01,\n",
       "                       5.4141e-02,  8.6078e-01, -1.1841e-01,  2.2204e+00,  1.2423e+00,\n",
       "                      -1.1116e+00,  5.9830e-02, -5.6253e-01, -6.2854e-01,  7.5419e-01,\n",
       "                       1.2896e+00, -1.0962e+00, -6.2619e-01,  1.6376e+00,  5.8860e-01,\n",
       "                      -5.2125e-01,  6.0878e-01,  3.6635e-01,  1.0791e-02,  1.2806e-01,\n",
       "                       5.8795e-01, -3.3507e-01,  2.0911e+00,  2.4317e-02,  1.5004e+00,\n",
       "                       1.7138e+00,  7.7417e-01,  4.7521e-01, -2.3995e-01,  6.0111e-01,\n",
       "                      -9.4324e-01, -4.6340e-01,  1.2977e-01,  1.2699e+00,  9.6974e-02,\n",
       "                      -1.3226e+00, -2.9932e-02,  4.6082e-01,  1.8867e-01, -2.2615e-01,\n",
       "                       1.9189e+00,  2.2953e-01, -6.7223e-01,  4.7067e-01,  6.0741e-02,\n",
       "                      -7.9216e-01,  9.2384e-01, -1.5863e+00,  1.0299e-01,  8.0824e-01,\n",
       "                       3.3031e-01,  1.7765e+00,  3.1794e-01,  1.4187e-01, -1.9749e-01,\n",
       "                       1.5199e+00,  6.9549e-01], device='cuda:0')),\n",
       "             ('module.model.22.1.running_var',\n",
       "              tensor([0.6067, 0.7781, 0.4516, 0.5526, 0.4974, 0.3584, 0.6986, 0.4340, 0.5425,\n",
       "                      0.6449, 0.4446, 0.4962, 0.8546, 0.4767, 0.5934, 0.5049, 0.6321, 0.6279,\n",
       "                      0.3637, 0.5567, 0.5667, 0.5804, 0.4227, 0.4395, 0.8270, 0.7905, 0.6302,\n",
       "                      0.5733, 0.5605, 0.5947, 0.4671, 0.5774, 0.4041, 0.5787, 0.5054, 0.4899,\n",
       "                      0.6328, 0.4547, 0.4865, 0.4821, 0.7276, 0.8957, 0.5916, 0.3776, 0.5357,\n",
       "                      0.4747, 0.5410, 0.2856, 0.4612, 0.4006, 0.5984, 0.4757, 0.5399, 0.5731,\n",
       "                      0.6938, 0.4337, 0.4298, 0.5975, 0.5103, 0.7070, 0.4954, 0.3021, 0.8803,\n",
       "                      0.7349, 0.5581, 0.6173, 0.8087, 0.5946, 0.7226, 0.9318, 0.6837, 0.8588,\n",
       "                      0.8185, 0.8630, 0.6994, 0.5858, 0.4924, 0.4840, 0.7609, 0.5941, 0.3660,\n",
       "                      0.9744, 0.5138, 0.5108, 0.8240, 0.5238, 0.7004, 0.6006, 0.9626, 0.6467,\n",
       "                      0.4493, 0.4328, 0.5211, 0.4639, 0.4312, 0.5931, 0.5563, 0.5938, 0.5154,\n",
       "                      0.5692, 0.4618, 0.6539, 0.4423, 0.5634, 0.6284, 0.5719, 0.5538, 0.2615,\n",
       "                      0.6395, 0.7530, 0.8805, 0.6595, 0.6454, 0.6914, 0.4304, 0.5714, 0.7357,\n",
       "                      0.5576, 0.5216, 0.5796, 0.4256, 0.4457, 0.7621, 0.2476, 0.4873, 0.7357,\n",
       "                      0.7861, 0.8493, 0.3809, 0.5978, 0.6558, 0.6532, 0.5322, 0.8273, 0.4075,\n",
       "                      0.3870, 0.4450, 0.5388, 0.2815, 0.5216, 0.4327, 0.6378, 0.6518, 0.6638,\n",
       "                      0.6128, 0.4132, 0.4786, 0.6302, 0.7229, 0.5285, 0.7417, 0.1759, 0.8549,\n",
       "                      0.6020, 0.6019, 0.7857, 0.5726, 0.6225, 0.9526, 0.5514, 0.4867, 0.4035,\n",
       "                      0.8293, 0.5918, 0.5521, 0.8828, 0.3353, 0.5687, 0.5471, 0.4387, 0.6631,\n",
       "                      0.8525, 0.6472, 0.6810, 0.2878, 0.5423, 0.6340, 0.2700, 0.4756, 0.7721,\n",
       "                      0.5431, 0.5431, 0.5006, 0.5171, 0.6260, 0.4431, 0.5379, 0.6868, 0.8062,\n",
       "                      0.5483, 0.7250, 0.4996, 0.4621, 0.5927, 0.6338, 0.5804, 0.5134, 0.5918,\n",
       "                      0.4717, 0.4210, 0.6455, 0.6116, 0.6735, 0.6874, 0.5427, 0.6470, 0.3743,\n",
       "                      0.5766, 0.6458, 0.6506, 0.5660, 0.5349, 0.5666, 0.6375, 0.5231, 0.5559,\n",
       "                      0.6444, 0.5240, 0.5799, 0.5102, 0.4488, 0.7319, 0.8804, 0.6179, 0.4990,\n",
       "                      0.4672, 1.0783, 0.5142, 0.6635, 0.9319, 0.5166, 0.5387, 0.6280, 0.6920,\n",
       "                      0.5569, 0.7793, 0.4613, 0.7926, 0.5343, 0.5918, 0.6023, 0.6129, 1.0839,\n",
       "                      0.3382, 0.3920, 0.6138, 0.6756, 0.5091, 0.7197, 0.8655, 0.5804, 0.3801,\n",
       "                      0.5906, 0.7351, 0.3862, 0.6287, 0.5987, 0.9074, 0.5473, 0.5053, 0.6439,\n",
       "                      0.4785, 0.5855, 0.4567, 0.6960, 0.4356, 0.7310, 0.9067, 0.8691, 0.3616,\n",
       "                      0.5534, 0.5160, 0.4684, 0.5833, 0.5367, 0.6151, 1.0096, 0.8868, 0.5877,\n",
       "                      0.4080, 0.6655, 0.5346, 0.6696, 0.6060, 0.4828, 0.4661, 0.3874, 0.3902,\n",
       "                      0.8436, 0.6713, 0.3093, 0.9984, 0.3845, 0.7871, 0.4421, 0.6223, 0.5512,\n",
       "                      0.9173, 0.5181, 0.5207, 0.5830, 0.6230, 0.7620, 0.5081, 0.4564, 0.5746,\n",
       "                      0.4343, 0.4849, 0.4573, 0.5230, 0.6446, 0.6060, 0.6807, 0.7048, 0.8458,\n",
       "                      0.2614, 0.5878, 0.4274, 0.4858, 0.6004, 0.6285, 0.6819, 0.4980, 0.4210,\n",
       "                      0.4454, 0.5552, 0.5639, 0.5371, 0.5392, 0.8630, 0.5269, 0.5546, 0.7601,\n",
       "                      0.6632, 0.7042, 0.7298, 0.5092, 0.5314, 0.2730, 0.5518, 0.7020, 0.6375,\n",
       "                      0.6519, 0.5000, 0.6327, 0.7665, 0.6962, 0.5527, 0.5535, 0.5364, 0.5017,\n",
       "                      0.4542, 0.6509, 0.8226, 0.5038, 0.5565, 0.5101, 0.3481, 0.6304, 0.4710,\n",
       "                      0.5104, 0.6587, 0.7075, 0.4090, 0.5728, 0.5422, 0.4376, 0.6998, 0.7503,\n",
       "                      0.4854, 0.4562, 0.3820, 0.5662, 0.3598, 0.5796, 0.6019, 0.5454, 0.4201,\n",
       "                      0.7495, 0.4826, 0.2759, 0.5182, 0.5067, 0.7242, 0.4238, 0.7078, 0.5503,\n",
       "                      0.6650, 0.7008, 0.4126, 0.8100, 0.6609, 0.3946, 0.4949, 0.5412, 0.4938,\n",
       "                      0.4876, 0.5035, 0.5269, 0.4560, 1.1750, 0.5666, 0.8117, 0.3920, 0.5523,\n",
       "                      0.7922, 0.5356, 0.5654, 0.5650, 0.4378, 0.6102, 0.6037, 0.2984, 0.6490,\n",
       "                      0.8107, 0.6561, 0.8051, 0.4655, 0.6040, 0.6726, 0.4060, 0.4725, 0.7172,\n",
       "                      0.4857, 0.3716, 0.6094, 0.5998, 0.6325, 0.5411, 0.5770, 0.5967, 0.5185,\n",
       "                      0.6034, 0.4211, 0.5974, 0.4123, 0.3675, 0.6344, 0.7516, 0.7575, 0.4254,\n",
       "                      0.9170, 0.4264, 0.5091, 0.5024, 0.6578, 0.6203, 0.4643, 0.4776, 0.4745,\n",
       "                      0.2681, 0.8748, 0.5129, 0.6324, 0.5113, 1.0261, 0.6317, 0.4849, 0.6847,\n",
       "                      0.6174, 0.6108, 0.6755, 0.5862, 0.6737, 0.9398, 0.6153, 0.8109, 0.9645,\n",
       "                      0.4663, 0.8116, 0.4706, 0.4560, 0.5400, 0.6157, 0.6577, 0.6047, 0.5056,\n",
       "                      0.6053, 0.6245, 0.5141, 0.5504, 0.2808, 0.5166, 0.7185, 0.6896, 0.4756,\n",
       "                      0.6307, 0.6130, 0.7706, 0.5646, 0.4385, 0.5430, 0.5316, 0.5127, 0.5367,\n",
       "                      0.8123, 0.4453, 0.5683, 0.4637, 0.6033, 0.6957, 0.7439, 0.6420, 0.5195,\n",
       "                      0.6373, 0.4976, 0.4491, 0.5639, 0.7223, 0.7608, 1.3622, 0.8617],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.22.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.22.2.clip_val', tensor([5.5313], device='cuda:0')),\n",
       "             ('module.model.23.0.weight',\n",
       "              tensor([[[[-0.3164, -0.3426, -0.2187],\n",
       "                        [-0.3083, -0.3525, -0.2520],\n",
       "                        [-0.2118, -0.2553, -0.1031]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3791,  0.4831,  0.2518],\n",
       "                        [ 0.4740,  0.5716,  0.3193],\n",
       "                        [ 0.2521,  0.3465,  0.1359]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.6062,  0.8240,  0.1763],\n",
       "                        [ 0.0531,  0.1639, -0.1582],\n",
       "                        [-0.8304, -0.8401, -0.6603]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.3339, -0.3696, -0.2350],\n",
       "                        [-0.3857, -0.3888, -0.3225],\n",
       "                        [-0.2833, -0.3575, -0.1724]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4288,  0.5199,  0.2349],\n",
       "                        [ 0.4426,  0.5597,  0.3363],\n",
       "                        [ 0.2807,  0.3283,  0.1923]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4146,  0.5246,  0.2646],\n",
       "                        [ 0.5236,  0.6390,  0.3552],\n",
       "                        [ 0.2774,  0.3911,  0.1564]]]], device='cuda:0')),\n",
       "             ('module.model.23.1.weight',\n",
       "              tensor([0.4223, 0.5577, 0.8615, 0.4467, 0.4759, 0.5213, 0.4878, 0.4902, 0.4290,\n",
       "                      1.2542, 0.6936, 0.5618, 0.6024, 0.4351, 0.4535, 0.6057, 0.4587, 0.3933,\n",
       "                      0.4224, 0.8653, 0.4407, 0.4605, 0.8197, 0.5829, 0.5359, 0.5371, 0.4600,\n",
       "                      0.4234, 0.4561, 0.5484, 0.7286, 0.6992, 0.4865, 0.4718, 0.4437, 0.4181,\n",
       "                      0.7237, 0.4897, 0.4341, 0.4682, 0.7959, 0.4563, 0.4532, 0.4332, 0.4393,\n",
       "                      1.3470, 0.4501, 0.5009, 0.4270, 0.3956, 0.4176, 0.4227, 0.5072, 0.6902,\n",
       "                      0.5574, 0.4682, 0.9127, 0.4498, 1.0891, 0.6396, 0.4386, 0.5632, 0.4767,\n",
       "                      0.9183, 0.9453, 0.5070, 0.4744, 0.7365, 0.5042, 1.2242, 0.6198, 0.5357,\n",
       "                      0.4779, 0.4695, 0.4688, 0.4739, 0.8050, 0.4389, 0.6286, 0.4713, 0.4043,\n",
       "                      0.4753, 0.4862, 0.5679, 1.2009, 0.8933, 0.8795, 0.6153, 0.5931, 0.4806,\n",
       "                      0.7143, 0.4620, 0.4116, 0.4276, 0.6479, 0.8062, 0.6741, 0.4746, 0.4050,\n",
       "                      0.4611, 0.6110, 0.9232, 0.7951, 0.4558, 0.4481, 0.4355, 0.4148, 0.5052,\n",
       "                      0.4657, 0.5083, 0.5720, 0.5123, 0.4536, 0.4493, 0.8165, 0.4585, 0.4562,\n",
       "                      0.4685, 0.4284, 0.4311, 0.4641, 0.4121, 0.5149, 0.5208, 0.4580, 0.4772,\n",
       "                      0.4646, 1.0829, 0.4813, 0.4359, 0.5261, 0.5199, 0.4319, 0.6766, 0.7072,\n",
       "                      0.5054, 0.4456, 0.4271, 0.9023, 0.4388, 0.4603, 0.4670, 0.6125, 0.4716,\n",
       "                      0.4132, 0.5172, 0.4343, 0.4342, 0.4821, 0.5306, 0.5290, 0.6829, 0.6019,\n",
       "                      0.5022, 0.4663, 0.5202, 0.4381, 0.6506, 0.6681, 0.4321, 0.6867, 0.7716,\n",
       "                      0.4561, 0.4409, 0.7003, 0.6325, 0.5279, 0.9688, 0.4321, 0.4257, 0.6348,\n",
       "                      1.0334, 0.6184, 0.9721, 0.5844, 0.4199, 0.4503, 0.4944, 0.4195, 0.9967,\n",
       "                      0.6982, 0.6320, 0.4928, 0.5939, 0.4461, 1.0467, 0.4674, 0.4920, 1.1689,\n",
       "                      0.4267, 0.4031, 0.9381, 0.4801, 0.8781, 0.4773, 0.4621, 0.4699, 0.4601,\n",
       "                      0.4470, 0.4064, 0.4525, 0.5175, 0.4638, 0.4454, 0.4833, 0.9293, 0.5190,\n",
       "                      0.4916, 0.4740, 0.4450, 0.4398, 1.2479, 0.4613, 0.6706, 0.4802, 0.4507,\n",
       "                      0.7811, 0.4472, 0.4294, 0.4533, 0.5987, 0.4536, 0.4791, 0.8887, 0.4576,\n",
       "                      0.3938, 0.5442, 0.4133, 0.5449, 0.9197, 0.6631, 0.4481, 0.5607, 0.4766,\n",
       "                      0.6155, 0.5117, 0.8020, 0.4126, 0.4551, 0.6450, 0.4521, 0.4719, 0.9207,\n",
       "                      0.4461, 0.6115, 0.7538, 0.5196, 0.4952, 0.4349, 0.4633, 0.4885, 0.4988,\n",
       "                      0.4795, 0.4341, 0.3864, 0.4674, 0.9000, 0.4630, 0.4684, 0.4524, 0.4630,\n",
       "                      0.5962, 0.4427, 0.5547, 0.4277, 0.4649, 0.5307, 0.4951, 0.4945, 0.5312,\n",
       "                      1.0722, 0.4527, 0.5510, 0.4336, 0.3982, 0.4775, 0.5160, 0.8164, 0.4570,\n",
       "                      0.5119, 0.5515, 0.5855, 0.4584, 0.4609, 0.8536, 0.8364, 0.5966, 0.5043,\n",
       "                      0.6795, 0.6080, 0.5066, 0.5625, 0.8921, 0.4761, 0.6116, 0.4325, 0.4525,\n",
       "                      0.9380, 0.8937, 0.4134, 0.5349, 0.4658, 0.4797, 0.4160, 0.6847, 0.4978,\n",
       "                      0.4576, 0.4166, 0.4365, 0.4658, 0.4740, 0.7335, 0.4652, 0.5931, 0.4356,\n",
       "                      0.6404, 0.4345, 0.4417, 0.4398, 0.4452, 0.4418, 0.4897, 0.5908, 0.7088,\n",
       "                      0.7325, 0.5011, 0.5804, 1.2193, 0.4435, 1.6343, 0.4593, 0.4323, 0.4728,\n",
       "                      0.5076, 0.4909, 0.6914, 0.4366, 0.5806, 0.9601, 0.4399, 0.8834, 0.4567,\n",
       "                      0.4783, 0.4450, 0.8040, 0.4412, 0.4799, 0.4246, 0.4922, 0.4518, 0.6159,\n",
       "                      0.6947, 0.4618, 0.6126, 0.5440, 0.4326, 0.4746, 1.0901, 0.4973, 0.8169,\n",
       "                      0.4124, 0.4182, 0.4388, 0.7881, 0.4112, 0.8207, 0.4509, 0.4741, 0.5262,\n",
       "                      0.4834, 0.8376, 0.6048, 0.6055, 0.9889, 0.4734, 0.4775, 0.4924, 0.5451,\n",
       "                      0.4622, 0.7002, 1.0156, 0.4407, 0.4197, 0.8047, 0.6605, 0.4609, 0.6451,\n",
       "                      0.5816, 0.4393, 0.5063, 0.4662, 0.5165, 0.4969, 0.5868, 1.0285, 0.6136,\n",
       "                      0.4413, 0.4353, 0.4473, 0.5118, 1.4593, 0.9617, 0.7452, 0.4270, 0.4574,\n",
       "                      0.6591, 0.3971, 0.4584, 0.4036, 0.6725, 0.4367, 0.5162, 1.1483, 0.5593,\n",
       "                      0.5599, 0.7601, 0.4922, 0.7037, 0.4892, 0.4382, 0.4004, 0.4165, 0.4815,\n",
       "                      0.4020, 0.5382, 0.6216, 0.4797, 1.2415, 0.4859, 1.4851, 0.5177, 0.4412,\n",
       "                      0.7745, 0.4948, 0.4844, 0.6378, 0.4264, 0.4499, 0.4968, 0.5224, 0.4817,\n",
       "                      0.7743, 0.4533, 0.4251, 0.4354, 0.4314, 0.4267, 0.9172, 0.4928, 0.5108,\n",
       "                      0.4680, 1.2557, 0.5046, 0.4719, 0.3967, 0.4801, 0.4804, 0.4541, 0.5113,\n",
       "                      0.4632, 0.5457, 0.4443, 0.6312, 0.4696, 0.6101, 0.4702, 0.7988, 0.6852,\n",
       "                      0.4094, 0.4953, 0.4344, 0.9454, 0.4211, 0.5832, 0.6560, 0.4430, 0.4393,\n",
       "                      0.4704, 0.4541, 0.4520, 0.4396, 0.4666, 0.4396, 0.4735, 0.5202, 0.4420,\n",
       "                      1.0293, 1.1492, 0.9271, 0.4798, 0.5733, 0.7084, 1.3800, 0.4484, 0.5201,\n",
       "                      0.4717, 0.6537, 0.9948, 0.4611, 0.4320, 0.4733, 0.4261, 0.4466, 0.4267,\n",
       "                      0.5243, 0.4009, 0.8303, 0.4385, 0.4778, 0.4960, 0.8834, 0.5426],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.23.1.bias',\n",
       "              tensor([ 1.2207,  0.3032,  0.1025,  0.2380,  0.1536,  0.7041,  0.2043,  0.0823,\n",
       "                       0.4838, -0.1974,  0.3449,  0.9749,  0.2445,  0.1587,  2.0134,  0.5636,\n",
       "                       0.1691,  1.6421,  1.3491,  0.0914,  0.5536,  2.0121, -0.1111, -0.0733,\n",
       "                       0.2337,  2.2251,  0.1535,  1.8167,  1.6079,  0.2334,  0.1383, -0.0266,\n",
       "                       0.6744,  2.0614,  0.1762,  1.6268,  0.5368,  0.9918,  0.1668,  0.0217,\n",
       "                       0.2107,  0.1950,  0.1452,  1.0290,  1.3707, -0.3699,  1.7528,  0.9557,\n",
       "                       1.7108,  1.2689,  0.1218,  0.1774,  1.8380,  0.1712,  0.2265,  0.1646,\n",
       "                      -0.1998,  2.3258, -0.0601,  0.3216,  0.1763,  0.1344,  0.2408, -0.3903,\n",
       "                       0.2676,  0.1782,  0.2096, -0.3179,  0.2200,  0.2951,  1.3182,  0.2079,\n",
       "                       0.2154,  0.1842,  0.2119,  0.3058,  0.1851,  0.1501,  0.6142,  0.1983,\n",
       "                       1.0996,  0.4162,  0.5851,  0.6136, -0.0438,  0.2090,  0.2313,  0.0752,\n",
       "                       0.2355,  0.2094,  0.1062,  0.6605,  1.9596,  1.4517,  0.2750,  0.2879,\n",
       "                       0.1774,  0.1991,  0.1458,  0.3607,  0.2626, -0.2745,  0.0279,  1.2439,\n",
       "                       1.3662,  0.1521,  2.0899,  0.8305,  0.1796,  0.2269,  0.2817,  0.2082,\n",
       "                       0.2008,  0.1882,  0.1641,  1.2778,  1.8208,  0.8483,  0.1635,  0.1566,\n",
       "                       0.7249,  0.1360,  1.4079,  0.8413,  0.2055,  0.2405,  2.0204, -0.0197,\n",
       "                       1.0854,  0.2129,  0.1845,  0.2171,  0.1638, -0.0103, -0.0671,  0.7522,\n",
       "                       1.6721,  0.1414, -0.6365,  2.3708,  1.2767,  0.1762,  0.7561,  2.1820,\n",
       "                       1.9419,  1.9322,  1.4990,  0.1834,  2.0782,  0.1544,  0.2341, -0.4072,\n",
       "                       0.2538,  0.2113,  1.8093,  0.0536,  0.1571,  0.2341,  0.1745,  0.1983,\n",
       "                       0.3544, -0.0933,  0.1842,  1.8258,  0.4711,  2.6204,  0.7511, -0.1282,\n",
       "                       1.7888,  1.9690,  0.2774, -0.4458,  0.1586,  0.0114,  0.3725,  1.8287,\n",
       "                       0.1420,  1.2954,  1.9445,  0.1903,  0.2637,  0.7251,  1.4479,  0.1712,\n",
       "                       0.0420, -0.0266,  0.1357,  0.2162,  0.4209,  1.6737,  1.7717, -0.2894,\n",
       "                       0.0961, -0.0055,  0.1652,  0.1209,  0.1334,  0.1907,  1.5943,  0.1499,\n",
       "                       2.2184,  0.2033,  0.2556,  0.1948,  0.1574,  0.5070,  0.5673,  0.1776,\n",
       "                       1.8717,  1.3860,  0.1542,  0.0627,  1.6057,  0.6132,  1.3852,  0.1772,\n",
       "                       0.3462,  0.1315,  1.5222,  0.1840,  0.4483,  1.1363,  0.1875, -0.2088,\n",
       "                       0.1495,  0.1531,  0.2634,  1.8374,  1.6585,  0.0993,  0.4916,  0.1576,\n",
       "                       0.1990,  0.9551,  0.4650,  0.9712, -0.2017,  2.1044,  0.2757,  0.2287,\n",
       "                       2.1664,  1.3273,  0.0624,  0.2264,  0.6515, -0.0448,  1.2713,  0.8867,\n",
       "                       0.1318,  0.1921,  0.2240,  1.6237,  0.2393,  1.8801,  1.2231,  1.0951,\n",
       "                       0.1788,  0.2462,  1.1653,  0.1565,  0.2564,  0.4335,  0.2516,  0.7941,\n",
       "                       1.6318,  0.9776,  0.2219,  0.2806,  0.2791,  0.7255, -0.1436,  0.0643,\n",
       "                       0.7836,  0.1670,  0.1149,  1.9446,  0.2531,  0.6439,  1.1542,  0.9167,\n",
       "                      -0.0537,  1.0226,  0.1903,  0.2212, -0.2114,  0.0107,  0.1474,  0.4338,\n",
       "                       0.7819,  0.1570,  1.1315,  0.2666, -0.2967,  0.2247,  0.9318,  1.9852,\n",
       "                       0.1677,  0.0634, -0.0102,  2.4572,  1.1079,  0.2061,  0.2316,  1.6687,\n",
       "                       0.4231,  0.2074,  0.1531,  2.0093,  0.9970, -0.1182,  0.2836,  0.5089,\n",
       "                       0.1749, -0.0114,  2.0647,  1.1020,  0.2190,  0.1126,  0.2073,  1.8654,\n",
       "                       0.1537,  0.8431,  0.6569, -0.0383,  0.0783,  0.1619,  0.6924, -0.5321,\n",
       "                       0.3246, -0.8399,  1.0754,  0.1730,  0.2309,  0.4363,  2.0148, -0.1795,\n",
       "                       0.1362,  0.2007, -0.0646,  1.2745,  0.0239,  0.1889,  0.2025,  1.8069,\n",
       "                       2.3923,  0.2868,  0.2467,  0.2232,  0.1568,  1.9634,  0.4483,  0.0691,\n",
       "                       0.2034,  0.2907,  1.0426,  0.1538,  1.4245, -0.3008,  0.3811, -0.0134,\n",
       "                       0.3376, -0.0029,  0.1703,  0.0711,  0.1760,  0.5218,  0.7136,  0.2226,\n",
       "                       0.2310,  0.7044, -0.2419,  0.3490,  0.7859, -0.4547,  1.6828,  0.1841,\n",
       "                       0.1940,  0.3998,  0.2088, -0.6722, -0.7301,  1.9401,  2.0468,  0.3881,\n",
       "                       0.3274,  0.1877,  0.2665,  0.2283,  1.7029, -0.1136,  0.2001,  0.2516,\n",
       "                       0.4730,  0.5585,  0.0252,  0.2126,  0.1619,  0.1424,  2.3431,  0.7339,\n",
       "                      -0.1399,  0.1052,  0.5471,  0.1579,  0.5265,  0.2337,  0.1318,  1.2515,\n",
       "                       0.9369,  0.3273,  0.1642,  0.2083, -1.0509,  0.2507,  0.2692,  0.2119,\n",
       "                       0.2559,  0.0623,  1.7438,  0.1706,  1.8016,  0.1457,  0.2053,  1.7159,\n",
       "                       0.7087,  1.8383,  2.0161, -0.2353,  1.8509, -0.2574,  0.2153,  1.4943,\n",
       "                       0.4943,  0.5846,  0.2153,  0.2586,  1.2009,  0.1595,  0.2374,  0.2783,\n",
       "                       0.8392,  0.1364,  0.1728,  1.7983,  0.2771,  0.2003,  2.0933, -0.0225,\n",
       "                       0.9121,  0.7467,  1.0366, -0.2465,  0.2384,  0.2096,  0.1265,  0.2228,\n",
       "                       0.1720,  0.1519,  0.2286,  0.1889,  0.1293,  0.1824,  0.5712,  0.2120,\n",
       "                       0.2888,  1.5485,  0.2314,  2.3325,  0.1464,  0.1855,  0.1574, -0.4454,\n",
       "                       1.9642,  0.1681, -0.2493,  0.2084,  2.0656,  0.2270,  0.2108,  0.1622,\n",
       "                       0.1910,  1.3477,  2.0023,  0.1812,  0.2081,  0.1332,  0.0862,  0.6242,\n",
       "                      -0.0961,  0.1755,  0.2279,  0.1918, -0.5748,  1.1467,  0.1888,  0.2173,\n",
       "                       0.5345,  0.1080,  0.1680,  0.1164,  1.0799,  0.4768,  0.1673,  1.8570,\n",
       "                       0.2366,  1.4208,  0.0430,  1.2750,  1.0073,  1.5766,  0.1848,  0.2463],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.23.1.running_mean',\n",
       "              tensor([-0.4789,  0.4551,  0.1138,  0.3471,  0.2013,  0.5922,  0.2463,  0.4831,\n",
       "                       0.2849, -2.3205, -1.2004, -0.7438,  0.2385,  0.1319, -0.7813,  0.4700,\n",
       "                       0.1937, -0.3664, -1.3964, -2.0744,  0.1971, -0.7369, -2.1928,  0.9802,\n",
       "                       0.3189, -0.9479,  0.2010, -0.6329, -1.3906,  0.2928, -2.2218,  0.6314,\n",
       "                      -2.0660, -0.8860,  0.1768, -1.0133, -0.6814, -1.7303,  0.1794,  0.0718,\n",
       "                       0.1811,  0.3108,  0.1135, -1.9931, -0.5443, -1.2867,  0.4456, -0.4925,\n",
       "                      -0.9815, -1.0451,  0.0828,  0.1739, -0.3326,  0.1734,  0.2848,  0.1707,\n",
       "                      -1.9734, -0.5337, -1.3388,  0.3654,  0.2387,  1.4056,  0.3447,  0.4867,\n",
       "                      -1.2287,  0.2121,  0.2770,  0.1032,  0.2199, -0.8668, -2.0373,  0.2895,\n",
       "                       0.3622,  0.2286,  0.3089,  0.3633,  0.0865,  0.1734, -2.3078,  0.2472,\n",
       "                      -0.8167,  0.2915, -1.9065, -1.3665,  0.3134, -1.2286, -1.4534,  0.5273,\n",
       "                       0.3299,  0.2459, -2.2814, -1.7768, -0.4874, -1.1576, -1.7972, -0.9531,\n",
       "                       0.1787,  0.2546,  0.1193,  0.3232,  0.5199,  0.7225, -2.7001, -1.3195,\n",
       "                      -0.4218,  0.1548, -0.5921, -0.1315,  0.2146,  0.3127,  0.4308,  0.3425,\n",
       "                       0.2047,  0.2453, -1.2377, -0.8984, -0.8140, -0.9696,  0.1740,  0.1795,\n",
       "                      -1.8230,  0.1796, -1.0936, -0.2170,  0.2181,  0.3072, -0.6733,  0.4418,\n",
       "                       0.9477,  0.2583,  0.3662,  0.3066,  0.2778,  0.5282, -0.2813,  0.2696,\n",
       "                      -1.3475,  0.1617,  0.3672, -0.4800, -2.0664,  0.2738, -0.9273, -0.4319,\n",
       "                      -0.6018, -0.8799, -1.1303,  0.1678, -1.0436,  0.2801,  0.2474,  1.0251,\n",
       "                       0.2742,  0.3232, -0.8385,  0.3167,  0.1502,  0.2431,  0.2980,  0.2236,\n",
       "                      -1.1836, -2.3848,  0.2462, -0.8835, -0.6964, -1.5962,  0.3195, -1.9553,\n",
       "                      -0.7997, -0.7561,  0.3728,  0.7183,  0.4381, -1.9366,  1.0200, -0.5570,\n",
       "                       0.1262, -0.1992, -0.9127,  0.2819, -2.5271, -0.6552, -1.0867,  0.1440,\n",
       "                       0.1916, -1.1857,  0.2116,  0.3026, -0.5393, -0.8686, -0.3153, -0.4008,\n",
       "                       0.2564, -2.1616,  0.2128,  0.1527,  0.1011,  0.1760, -1.0675,  0.1273,\n",
       "                      -0.4752,  0.2073,  0.4046,  0.3241,  0.2082, -1.0369, -1.6943,  0.2660,\n",
       "                      -0.6972, -0.9706,  0.1773, -0.8026, -1.5619,  0.2824, -0.6530,  0.2062,\n",
       "                      -1.1158,  0.0984, -0.5049,  0.2149, -1.4183, -0.4574,  0.2064,  0.9933,\n",
       "                       0.1541,  0.1713,  0.3754, -0.9526, -0.8527,  0.6699, -0.9042,  0.1723,\n",
       "                       0.1931, -0.7342, -1.6930, -0.6556,  0.7732, -0.3502,  0.2500,  0.2248,\n",
       "                      -0.6038, -1.6329,  0.7511,  1.2595, -0.8302,  0.4040, -0.6692, -1.2255,\n",
       "                       0.1511,  0.2403,  0.3457, -2.1139,  0.2918, -0.4042,  0.1479, -0.4894,\n",
       "                      -1.1262,  0.2952, -0.8652,  0.1801,  0.4029, -1.5674,  0.3762, -0.7459,\n",
       "                      -0.5065, -1.1659,  0.2886,  0.4386,  0.4637,  0.5054, -1.6556,  0.1229,\n",
       "                       0.0263,  0.2031,  0.0772, -0.6702,  0.3753, -0.8870, -1.1183,  0.3093,\n",
       "                       0.2986, -0.5484,  0.2479,  0.2773, -2.4612, -1.7998,  0.3411, -2.0379,\n",
       "                      -0.3369,  0.1837,  0.1408,  0.3899, -2.4045,  0.3205, -0.5681, -0.6078,\n",
       "                       0.1893, -2.4278, -1.5990, -0.4614, -0.9215,  0.2437,  0.3496, -0.8191,\n",
       "                      -1.1976,  0.2528,  0.2232, -0.7103, -1.3688,  0.2225,  0.4301, -1.0035,\n",
       "                       0.2465,  0.5309, -0.5526, -1.1970,  0.2777,  0.3336,  0.2410, -0.7425,\n",
       "                       0.2022,  0.5330, -1.3016, -2.8133, -2.0707,  0.1720, -1.2505,  0.7583,\n",
       "                       0.2769,  0.7824, -1.4174,  0.1965,  0.3851,  0.3855, -0.8531,  0.4265,\n",
       "                       0.1224,  0.1696, -0.0711, -0.9249,  0.0884,  0.2401,  0.2659, -0.5104,\n",
       "                       0.1355,  0.3041,  0.3707,  0.3760,  0.1026, -0.7273, -1.3497, -2.0787,\n",
       "                       0.2352,  0.3616, -2.2739,  0.1702, -1.3420,  0.6857,  0.3920, -2.0643,\n",
       "                       0.3545,  0.1359,  0.1729, -2.1685,  0.2321, -0.7500, -1.9315,  0.3549,\n",
       "                       0.2884, -2.2446, -2.3195, -1.9595, -0.9067,  2.3590, -1.3529,  0.3008,\n",
       "                       0.1784, -1.9701,  0.2789,  0.7681,  0.7293, -0.7370, -0.4825,  0.4383,\n",
       "                      -1.8690,  0.2526,  0.2193,  0.2671, -0.5408,  0.4414,  0.2462,  0.3281,\n",
       "                      -1.8442, -1.1575, -1.1927,  0.2330,  0.1481,  0.1303, -0.5438, -1.4997,\n",
       "                       0.3903, -1.0806, -0.6439,  0.1346,  0.1559,  0.4271,  0.0838, -1.1335,\n",
       "                      -1.4545, -1.3270,  0.2023,  0.2420, -0.3247,  0.3499,  0.2983,  0.2854,\n",
       "                       0.4241, -0.1556, -0.6160,  0.2587, -0.7026,  0.1657,  0.2786, -0.5969,\n",
       "                      -0.9397, -0.5365, -0.6895, -1.2878, -1.1647, -0.9049,  0.2394, -0.6589,\n",
       "                      -1.0162, -1.4948,  0.3259, -1.3663, -1.0913,  0.1983,  0.2447,  0.3745,\n",
       "                      -1.8817,  0.3409,  0.2481, -0.7590,  0.2757,  0.3920, -0.4578, -1.5548,\n",
       "                      -0.8300, -2.0383,  1.4914,  0.6226,  0.4023,  0.2302,  0.0966,  0.2618,\n",
       "                       0.1790,  0.1919,  0.3331,  0.2720,  0.2673,  0.2604, -0.8697,  0.3197,\n",
       "                       0.3684, -1.1324,  0.2696, -0.4731,  0.1282,  0.3342,  0.1784,  2.6952,\n",
       "                      -0.5063,  0.1517,  0.5337,  0.2827, -0.6254,  0.2588,  0.3003,  0.1467,\n",
       "                       0.2325, -1.5865, -1.0741,  0.2387,  0.2819,  0.1896, -0.9008, -0.4955,\n",
       "                      -2.0695,  0.1957, -0.0360, -1.7212,  0.1315, -1.7241,  0.2326,  0.3111,\n",
       "                      -1.2653, -0.7248,  0.1624,  0.0877, -0.7497,  0.2804,  0.1829, -0.6295,\n",
       "                       0.2698, -0.6746, -1.7731, -1.0543, -0.7850, -0.5799,  0.3975,  0.3655],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.23.1.running_var',\n",
       "              tensor([0.4036, 0.6784, 1.5718, 0.4165, 0.2861, 1.5083, 0.2810, 0.7893, 0.3626,\n",
       "                      0.6019, 1.3191, 0.7104, 0.3990, 0.1238, 0.7222, 1.3110, 0.2347, 0.3793,\n",
       "                      1.2901, 0.6573, 0.2480, 0.6605, 0.2949, 1.5531, 0.5106, 2.5819, 0.3025,\n",
       "                      0.6280, 1.2302, 0.4213, 0.3437, 1.0760, 0.2982, 0.8146, 0.1980, 0.8807,\n",
       "                      1.0573, 0.9759, 0.1933, 0.0704, 0.4351, 0.4422, 0.1204, 0.5906, 0.5800,\n",
       "                      1.4428, 1.1653, 0.5330, 1.0004, 1.0266, 0.0684, 0.1715, 1.5581, 0.3128,\n",
       "                      0.4355, 0.2054, 0.4580, 0.5062, 1.4583, 0.5094, 0.2855, 2.4279, 0.4075,\n",
       "                      0.8804, 2.3655, 0.2409, 0.3506, 0.4318, 0.2341, 1.3850, 6.6272, 0.5314,\n",
       "                      0.5760, 0.3088, 0.4086, 0.4632, 0.1398, 0.2048, 1.8996, 0.2992, 0.7774,\n",
       "                      0.4963, 0.6291, 1.3893, 0.7289, 1.4984, 1.8763, 0.8864, 0.6160, 0.3228,\n",
       "                      0.3248, 0.8296, 0.4456, 1.0243, 1.1026, 1.0333, 0.3997, 0.2844, 0.1057,\n",
       "                      0.3599, 1.3472, 1.2375, 0.4751, 1.0283, 0.3739, 0.1936, 0.6246, 1.1107,\n",
       "                      0.2523, 0.3487, 0.8047, 0.3174, 0.2075, 0.2962, 1.0888, 0.8817, 0.9022,\n",
       "                      0.8915, 0.1691, 0.1935, 0.5285, 0.2580, 1.3308, 0.9472, 0.2214, 0.3416,\n",
       "                      0.5732, 0.9366, 1.8718, 0.2487, 0.5565, 0.4393, 0.4043, 0.9375, 1.7912,\n",
       "                      2.2162, 1.2549, 0.1996, 0.9251, 0.5313, 0.7568, 0.3797, 1.0022, 0.4238,\n",
       "                      0.6001, 2.0932, 1.1924, 0.1480, 1.0408, 0.3394, 0.3021, 1.2791, 0.4594,\n",
       "                      0.5727, 0.7082, 0.3867, 0.1801, 0.4660, 0.5703, 0.2482, 1.6097, 0.2746,\n",
       "                      0.2756, 0.9364, 0.7346, 7.8467, 2.1561, 0.5680, 0.9419, 1.1044, 0.4763,\n",
       "                      1.6177, 0.6453, 0.9836, 2.3213, 0.5359, 0.1712, 0.6260, 0.9146, 0.9562,\n",
       "                      0.4778, 0.7678, 1.0314, 0.2620, 0.1972, 1.5360, 0.2101, 0.4458, 1.1672,\n",
       "                      0.7681, 0.2738, 1.1071, 0.4169, 0.7049, 0.2799, 0.1257, 0.1007, 0.1704,\n",
       "                      0.9742, 0.1117, 0.4759, 0.2623, 0.5216, 0.4928, 0.3195, 2.2618, 0.9113,\n",
       "                      0.3616, 0.6109, 0.9244, 0.2257, 1.2522, 1.1820, 0.7764, 0.7936, 0.2608,\n",
       "                      1.6626, 0.0914, 0.5081, 0.2298, 1.1139, 0.4098, 0.3051, 1.6390, 0.1970,\n",
       "                      0.1945, 0.7939, 0.8669, 0.9598, 1.5093, 1.1346, 0.2023, 0.3409, 0.8389,\n",
       "                      0.8649, 0.9032, 2.2513, 0.2854, 0.2902, 0.3262, 0.5262, 0.8424, 1.7823,\n",
       "                      2.0975, 1.0787, 1.0485, 0.7975, 1.2272, 0.2133, 0.3014, 0.5029, 0.8978,\n",
       "                      0.2994, 0.3754, 1.6094, 0.4527, 1.5292, 0.3780, 0.9048, 0.2353, 0.4927,\n",
       "                      1.0974, 0.3945, 0.9491, 0.4464, 1.4244, 0.3833, 0.7036, 0.6096, 1.6849,\n",
       "                      1.3059, 0.1314, 1.0651, 0.2452, 0.0718, 0.7289, 0.6579, 1.7738, 0.9607,\n",
       "                      0.9173, 0.4316, 0.7426, 0.3086, 0.3087, 0.3219, 0.7434, 1.8267, 0.2774,\n",
       "                      0.3516, 0.3021, 1.4827, 0.6682, 0.3473, 0.4063, 0.7464, 0.6575, 0.2184,\n",
       "                      1.0928, 1.3850, 0.4801, 1.2130, 0.2673, 0.4436, 0.7383, 1.2717, 0.3190,\n",
       "                      0.2808, 0.6350, 1.0446, 0.2233, 0.5401, 1.2094, 0.3547, 0.6918, 0.4564,\n",
       "                      0.9606, 0.2754, 0.4577, 0.2390, 0.6511, 0.2754, 0.9595, 1.4894, 0.4194,\n",
       "                      0.3883, 0.2605, 1.7555, 1.5831, 0.3155, 1.7492, 1.1618, 0.2031, 0.5381,\n",
       "                      0.5191, 0.8657, 0.4684, 0.1186, 0.2507, 0.6725, 0.8508, 0.1283, 0.2791,\n",
       "                      0.3258, 0.4627, 0.5040, 0.3265, 0.4589, 0.4386, 0.0928, 0.8627, 1.4924,\n",
       "                      0.3409, 0.2572, 0.6201, 0.6104, 0.1846, 1.0304, 1.0725, 0.4956, 0.3963,\n",
       "                      0.4632, 0.2077, 0.1938, 0.5692, 0.2309, 0.9505, 0.3543, 0.5434, 0.3310,\n",
       "                      0.3750, 0.2845, 0.3535, 0.9260, 1.6083, 1.1107, 0.5138, 0.1646, 0.5162,\n",
       "                      0.3179, 0.7776, 0.8911, 0.7700, 0.4166, 0.7082, 0.5572, 0.3356, 0.3671,\n",
       "                      0.4137, 0.5369, 0.6582, 0.2940, 0.3864, 0.5372, 1.4146, 1.1489, 0.4287,\n",
       "                      0.1339, 0.1416, 0.4556, 1.3284, 1.7118, 1.3307, 0.9112, 0.1263, 0.2246,\n",
       "                      0.9461, 0.0639, 1.0787, 0.9873, 1.2429, 0.2341, 0.3427, 0.8828, 0.4853,\n",
       "                      0.5106, 0.6002, 0.5600, 1.4334, 0.7188, 0.3198, 0.6346, 0.1870, 0.3749,\n",
       "                      0.5505, 0.9276, 2.5595, 0.6378, 1.2323, 1.3233, 1.3770, 0.2862, 0.7162,\n",
       "                      1.7215, 0.9722, 0.4895, 1.1839, 0.9884, 0.2762, 0.2392, 0.4366, 1.0752,\n",
       "                      0.8056, 0.3422, 0.7823, 0.3331, 0.4575, 0.3547, 1.1256, 0.9419, 0.8031,\n",
       "                      2.2933, 1.2235, 0.6347, 0.2111, 0.0965, 0.2657, 0.2066, 0.2724, 0.5064,\n",
       "                      0.3944, 0.3559, 0.3349, 1.2402, 0.4820, 0.7080, 0.9596, 0.6285, 0.8127,\n",
       "                      0.1098, 0.6301, 0.2017, 3.1681, 0.4446, 0.1655, 0.7857, 0.3491, 0.5888,\n",
       "                      0.2774, 0.3292, 0.1558, 0.2604, 0.8433, 0.9887, 0.3476, 0.5102, 0.2193,\n",
       "                      1.1638, 1.2363, 0.6135, 0.2600, 0.9030, 0.7873, 0.3034, 0.7695, 0.3522,\n",
       "                      0.4051, 1.2869, 0.8504, 0.1941, 0.0955, 0.7040, 0.3410, 0.2297, 0.5590,\n",
       "                      0.3906, 0.6847, 1.0537, 1.1370, 0.8439, 0.5646, 1.0870, 0.5298],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.23.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.23.2.clip_val', tensor([6.5774], device='cuda:0')),\n",
       "             ('module.model.24.0.weight', tensor([[[[-0.0477]],\n",
       "              \n",
       "                       [[ 0.0360]],\n",
       "              \n",
       "                       [[ 0.0537]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1182]],\n",
       "              \n",
       "                       [[ 0.0025]],\n",
       "              \n",
       "                       [[ 0.0652]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0476]],\n",
       "              \n",
       "                       [[-0.0618]],\n",
       "              \n",
       "                       [[-0.0011]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0440]],\n",
       "              \n",
       "                       [[-0.0519]],\n",
       "              \n",
       "                       [[-0.0441]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1028]],\n",
       "              \n",
       "                       [[-0.0534]],\n",
       "              \n",
       "                       [[-0.0052]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0458]],\n",
       "              \n",
       "                       [[-0.0146]],\n",
       "              \n",
       "                       [[ 0.0770]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0187]],\n",
       "              \n",
       "                       [[-0.0016]],\n",
       "              \n",
       "                       [[-0.0269]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0381]],\n",
       "              \n",
       "                       [[ 0.0299]],\n",
       "              \n",
       "                       [[ 0.1198]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0726]],\n",
       "              \n",
       "                       [[ 0.0085]],\n",
       "              \n",
       "                       [[ 0.0375]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0529]],\n",
       "              \n",
       "                       [[-0.0858]],\n",
       "              \n",
       "                       [[-0.0055]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0003]],\n",
       "              \n",
       "                       [[ 0.0372]],\n",
       "              \n",
       "                       [[-0.0165]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0004]],\n",
       "              \n",
       "                       [[ 0.0048]],\n",
       "              \n",
       "                       [[ 0.0035]]]], device='cuda:0')),\n",
       "             ('module.model.24.1.weight',\n",
       "              tensor([0.7085, 0.6520, 0.7465,  ..., 1.0478, 1.0108, 0.7505], device='cuda:0')),\n",
       "             ('module.model.24.1.bias',\n",
       "              tensor([-0.9526, -1.0521, -0.7375,  ..., -0.0784, -0.4146,  0.7943],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.24.1.running_mean',\n",
       "              tensor([-0.5147,  1.3619, -0.7991,  ..., -0.0056, -0.4191, -1.0308],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.24.1.running_var',\n",
       "              tensor([0.3214, 0.3243, 0.3572,  ..., 0.3762, 0.4055, 0.6207], device='cuda:0')),\n",
       "             ('module.model.24.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.24.2.clip_val', tensor([5.2442], device='cuda:0')),\n",
       "             ('module.model.25.0.weight',\n",
       "              tensor([[[[ 0.0870,  0.0472,  0.0886],\n",
       "                        [ 0.0521,  0.0134,  0.0545],\n",
       "                        [ 0.0466,  0.0404,  0.0488]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1401,  0.1036,  0.1287],\n",
       "                        [ 0.1346,  0.0875,  0.1092],\n",
       "                        [ 0.0947,  0.0755,  0.0808]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2986,  0.2293,  0.2465],\n",
       "                        [ 0.3008,  0.1772,  0.2537],\n",
       "                        [ 0.3888,  0.2916,  0.3333]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.3132, -0.2111, -0.2941],\n",
       "                        [-0.2123, -0.0832, -0.1773],\n",
       "                        [-0.2585, -0.1674, -0.2526]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3214, -0.1695, -0.2498],\n",
       "                        [-0.1794, -0.0556, -0.1508],\n",
       "                        [-0.2899, -0.1429, -0.2594]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2433, -0.1980, -0.2041],\n",
       "                        [-0.1874, -0.1390, -0.1659],\n",
       "                        [-0.1805, -0.1757, -0.1605]]]], device='cuda:0')),\n",
       "             ('module.model.25.1.weight',\n",
       "              tensor([0.9145, 0.9226, 0.5019,  ..., 1.4040, 1.0738, 1.6370], device='cuda:0')),\n",
       "             ('module.model.25.1.bias',\n",
       "              tensor([ 0.2031,  0.1392,  0.2213,  ...,  0.1440,  0.6851, -0.6232],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.25.1.running_mean',\n",
       "              tensor([ 0.0125,  0.0039,  0.0976,  ..., -0.4897, -0.2937, -0.9919],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.25.1.running_var',\n",
       "              tensor([1.4087e-03, 3.3561e-04, 3.9042e-02,  ..., 4.6204e-01, 2.2723e-01,\n",
       "                      3.5297e-01], device='cuda:0')),\n",
       "             ('module.model.25.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.25.2.clip_val', tensor([6.5861], device='cuda:0')),\n",
       "             ('module.model.26.0.weight', tensor([[[[ 0.0458]],\n",
       "              \n",
       "                       [[ 0.0384]],\n",
       "              \n",
       "                       [[ 0.0292]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0605]],\n",
       "              \n",
       "                       [[ 0.0584]],\n",
       "              \n",
       "                       [[ 0.0171]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0799]],\n",
       "              \n",
       "                       [[-0.0615]],\n",
       "              \n",
       "                       [[ 0.0183]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0569]],\n",
       "              \n",
       "                       [[ 0.0057]],\n",
       "              \n",
       "                       [[-0.0121]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0574]],\n",
       "              \n",
       "                       [[-0.0726]],\n",
       "              \n",
       "                       [[ 0.0989]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0512]],\n",
       "              \n",
       "                       [[-0.0449]],\n",
       "              \n",
       "                       [[-0.0963]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0325]],\n",
       "              \n",
       "                       [[ 0.0358]],\n",
       "              \n",
       "                       [[-0.0507]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0068]],\n",
       "              \n",
       "                       [[ 0.0016]],\n",
       "              \n",
       "                       [[ 0.0008]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0373]],\n",
       "              \n",
       "                       [[-0.0639]],\n",
       "              \n",
       "                       [[ 0.0922]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0562]],\n",
       "              \n",
       "                       [[-0.0723]],\n",
       "              \n",
       "                       [[ 0.0634]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0751]],\n",
       "              \n",
       "                       [[ 0.0664]],\n",
       "              \n",
       "                       [[ 0.0270]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0049]],\n",
       "              \n",
       "                       [[ 0.0260]],\n",
       "              \n",
       "                       [[ 0.0615]]]], device='cuda:0')),\n",
       "             ('module.model.26.1.weight',\n",
       "              tensor([4.0210, 4.1241, 3.9919,  ..., 3.9459, 4.1153, 4.1261], device='cuda:0')),\n",
       "             ('module.model.26.1.bias',\n",
       "              tensor([-2.6764, -2.7184, -2.9856,  ..., -2.4607, -2.6461, -2.8955],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.26.1.running_mean',\n",
       "              tensor([-0.1373, -1.4943, -1.5319,  ..., -0.8444, -0.6687, -0.5911],\n",
       "                     device='cuda:0')),\n",
       "             ('module.model.26.1.running_var',\n",
       "              tensor([0.6600, 0.7090, 0.6808,  ..., 0.5739, 0.6569, 0.5948], device='cuda:0')),\n",
       "             ('module.model.26.1.num_batches_tracked',\n",
       "              tensor(10010, device='cuda:0')),\n",
       "             ('module.model.26.2.clip_val', tensor([4.8140], device='cuda:0')),\n",
       "             ('module.fc.weight',\n",
       "              tensor([[-0.0387, -0.2215, -0.1238,  ..., -0.1747, -0.2280, -0.0956],\n",
       "                      [ 0.0962, -0.2120, -0.1567,  ..., -0.1136, -0.1915,  0.1217],\n",
       "                      [-0.0768,  0.0567, -0.1333,  ..., -0.0576, -0.0660,  0.0567],\n",
       "                      ...,\n",
       "                      [-0.0529, -0.1566, -0.1451,  ..., -0.1723, -0.0475,  0.0207],\n",
       "                      [-0.1272, -0.0813, -0.0857,  ..., -0.0970, -0.1016, -0.0838],\n",
       "                      [ 0.0210, -0.1514,  0.4402,  ..., -0.1570, -0.1582, -0.0087]],\n",
       "                     device='cuda:0')),\n",
       "             ('module.fc.bias',\n",
       "              tensor([-2.0160e-01,  6.6101e-02, -5.6443e-01, -5.3778e-01,  4.6421e-02,\n",
       "                       1.5119e-02, -3.0287e-01,  2.4275e-02, -2.6844e-01, -2.4689e-01,\n",
       "                      -2.8889e-01, -4.8033e-01, -2.7007e-01, -1.9591e-01, -2.7220e-01,\n",
       "                      -2.2421e-01, -1.5873e-01, -2.0474e-01,  5.4489e-02, -2.4814e-01,\n",
       "                      -1.8105e-01,  2.0187e-01, -9.9182e-02,  5.3974e-02,  6.7687e-02,\n",
       "                      -1.3753e-01,  7.2600e-02, -1.1990e-01,  1.6130e-01,  8.0839e-02,\n",
       "                       3.1104e-02, -4.3382e-02,  1.1245e-01, -3.9317e-01,  1.7040e-01,\n",
       "                      -3.6328e-01,  1.1689e-01, -3.7572e-01,  1.3461e-01, -5.9910e-02,\n",
       "                      -7.2975e-02,  3.3770e-02,  2.5833e-02, -3.5345e-01,  1.1174e-01,\n",
       "                       1.4008e-01,  1.4595e-01, -1.5907e-01, -4.2737e-01,  1.8908e-03,\n",
       "                       2.3063e-01, -4.0970e-01,  1.8101e-01,  3.4035e-01, -6.7041e-02,\n",
       "                      -1.3460e-01, -1.4314e-03, -9.4007e-02,  2.1820e-01,  1.9313e-01,\n",
       "                      -6.5976e-03,  3.3956e-01, -7.1279e-02,  3.2976e-01,  1.9562e-01,\n",
       "                       4.3222e-02, -2.2903e-01, -1.7554e-01,  2.5166e-02,  1.0899e-01,\n",
       "                      -2.7961e-01, -1.7612e-02, -1.9605e-01,  1.1354e-02, -2.0509e-01,\n",
       "                       3.5428e-01,  1.2151e-01,  4.3947e-01,  2.3858e-01,  3.8961e-01,\n",
       "                      -3.9935e-01, -6.5743e-02, -1.7813e-02, -6.8323e-01, -3.7845e-01,\n",
       "                      -3.3601e-02, -3.3029e-01,  1.3192e-01, -6.2812e-02,  2.9423e-01,\n",
       "                      -2.5893e-01, -4.1729e-01, -2.0145e-02, -6.8367e-02, -1.3981e-01,\n",
       "                      -5.1540e-01,  7.3083e-02, -5.5396e-01, -6.2477e-01, -2.5598e-01,\n",
       "                      -3.3210e-01, -4.6598e-01,  3.3485e-02,  5.1055e-01,  2.2443e-01,\n",
       "                      -2.7614e-01,  2.6810e-01, -1.8845e-01, -4.9713e-01, -6.6850e-01,\n",
       "                      -4.3343e-01,  8.8697e-01,  2.1991e-01, -1.4883e-01,  1.0868e-01,\n",
       "                      -6.9995e-01, -3.6374e-01, -3.0517e-01,  2.5654e-02, -2.1052e-01,\n",
       "                      -4.0629e-01, -1.9599e-01, -1.7062e-01, -5.1885e-01, -5.6638e-02,\n",
       "                      -3.4179e-01, -2.6628e-02, -2.9644e-01, -2.7664e-01, -1.9256e-01,\n",
       "                      -4.1190e-01, -3.3236e-01, -2.0970e-01,  7.7812e-02, -5.6732e-01,\n",
       "                      -7.7929e-01, -3.7221e-01, -5.5383e-01, -2.5548e-01, -4.1701e-01,\n",
       "                      -5.3010e-01, -3.4058e-01, -5.4461e-01, -4.2034e-01, -4.3589e-01,\n",
       "                      -1.1921e-01, -7.7941e-02, -5.8222e-01, -2.5481e-01, -2.1863e-01,\n",
       "                       5.9263e-02,  3.2911e-01, -7.3626e-01,  3.1808e-01, -8.5707e-02,\n",
       "                       6.8362e-02,  1.0721e-01, -2.8494e-01, -3.5856e-01, -1.0308e-01,\n",
       "                       1.3466e-02,  2.8866e-01,  5.2866e-01,  8.3760e-02, -8.2402e-02,\n",
       "                      -2.5380e-01, -3.5934e-01, -6.0243e-01,  4.5514e-02, -1.2304e-01,\n",
       "                       1.6520e-01,  5.6148e-01, -3.9146e-02,  5.8156e-02,  4.0037e-01,\n",
       "                      -3.0444e-01, -2.0221e-01, -9.1243e-02,  4.5009e-01,  1.6971e-01,\n",
       "                       1.4588e-01, -1.7671e-01,  1.9554e-01, -2.3411e-01,  1.2952e-01,\n",
       "                       3.9825e-01,  5.8612e-02,  8.4483e-02, -8.2040e-02,  1.2856e-01,\n",
       "                      -1.1009e-01,  3.0620e-01,  3.0392e-01, -2.2920e-01, -8.1988e-02,\n",
       "                       5.2245e-01,  1.7445e-01, -1.5553e-02, -1.1084e-01,  3.4855e-01,\n",
       "                      -3.1274e-01,  1.6772e-01,  1.0404e-01,  4.2121e-01,  3.2942e-01,\n",
       "                       6.4212e-02, -3.3778e-01,  6.0927e-02, -2.5544e-02,  1.7818e-01,\n",
       "                      -1.1908e-01,  1.0066e-01,  6.5090e-02, -2.9358e-01,  1.0807e-01,\n",
       "                      -3.6664e-02,  2.5326e-02,  5.1426e-01,  1.5118e-01,  1.4414e-01,\n",
       "                      -8.0882e-02, -2.3266e-01,  7.9476e-04,  6.4709e-01, -4.9608e-01,\n",
       "                       2.9227e-01, -8.1112e-02,  1.5203e-02, -4.6525e-01,  2.6658e-01,\n",
       "                      -5.7174e-02, -1.8514e-01,  1.6511e-01,  5.1908e-02,  1.1212e-01,\n",
       "                      -1.6671e-01,  1.4691e-01,  3.2419e-01,  1.8632e-01,  4.8099e-01,\n",
       "                      -4.5282e-02, -2.3156e-01,  5.5517e-02,  1.4044e-02, -6.5475e-03,\n",
       "                       2.7734e-01, -3.6195e-01,  1.8417e-01, -1.3250e-01, -2.1525e-01,\n",
       "                       1.0070e-01,  1.8377e-01,  9.0929e-03, -1.2953e-01,  4.0710e-01,\n",
       "                      -1.3917e-01,  4.9087e-02,  2.0358e-01,  3.2081e-02,  1.6624e-02,\n",
       "                       1.7249e-01,  2.0603e-02, -1.9100e-01,  1.4027e-01,  1.3222e-01,\n",
       "                       2.5311e-02,  2.3541e-01, -2.4611e-01, -4.4473e-01, -1.0650e-01,\n",
       "                       2.6464e-01, -2.3641e-01,  9.4346e-02,  5.3030e-02, -3.5186e-01,\n",
       "                      -2.6099e-01, -2.7591e-02, -1.0241e-01, -3.9590e-01,  6.0664e-02,\n",
       "                      -1.9650e-01,  5.1486e-01, -2.1593e-01, -1.6405e-01,  5.0745e-01,\n",
       "                       1.9521e-01,  1.1935e-01, -9.4431e-02, -2.3897e-01, -7.7985e-02,\n",
       "                      -1.8237e-01, -1.6729e-01, -1.8441e-01, -3.5738e-01, -1.6877e-01,\n",
       "                      -9.5594e-02,  5.2214e-02,  1.6155e-01,  1.0383e-01,  2.7108e-01,\n",
       "                      -4.8591e-01, -1.1785e-01, -3.4944e-01, -2.2227e-01, -4.9640e-01,\n",
       "                       3.8506e-01,  2.4899e-01, -2.3030e-01, -2.8470e-01, -3.7624e-01,\n",
       "                      -4.2242e-02, -8.4752e-02,  1.3478e-01,  6.1288e-01,  4.6225e-01,\n",
       "                       1.5464e-01, -7.1567e-02, -2.9592e-01,  1.0432e-01, -2.3141e-01,\n",
       "                      -4.1905e-01, -7.3123e-01, -9.0120e-01, -5.8864e-01, -3.9067e-01,\n",
       "                      -7.0898e-01, -6.8201e-01, -2.4930e-01, -2.3374e-01, -2.4492e-01,\n",
       "                       4.6877e-02, -1.8387e-01,  4.2229e-02,  6.0588e-02, -6.3862e-02,\n",
       "                      -2.7375e-01,  2.6844e-01, -1.6251e-01, -3.9037e-01, -4.2401e-01,\n",
       "                      -1.3939e-01, -4.2065e-02,  1.0177e-02, -1.3312e-01, -9.8690e-02,\n",
       "                      -1.2707e-01,  1.0598e-01, -1.5009e-02,  6.6243e-04, -2.0340e-01,\n",
       "                      -1.4165e-01, -3.8974e-01, -4.8189e-01,  2.2748e-02, -1.4875e-02,\n",
       "                      -2.2550e-01, -1.2476e-02, -2.9782e-01, -1.8016e-02,  3.5627e-01,\n",
       "                      -3.6620e-02,  2.1434e-01,  1.6774e-01,  7.3438e-02,  1.8108e-01,\n",
       "                      -6.1250e-02, -6.3414e-02,  2.8762e-01, -3.0092e-01,  2.5430e-01,\n",
       "                      -1.8423e-01, -2.6566e-01, -1.7057e-01,  1.6907e-01, -1.2216e-01,\n",
       "                      -3.3466e-01, -7.2038e-02, -1.6247e-01,  8.1611e-02, -5.3280e-03,\n",
       "                      -1.8514e-01, -4.8875e-03,  6.7719e-02, -3.7734e-03, -5.2779e-01,\n",
       "                      -2.3402e-01, -2.8275e-01, -2.7477e-01, -2.7884e-01, -3.1991e-01,\n",
       "                      -2.7537e-01,  1.4233e-01, -5.3277e-01, -6.5318e-01, -1.9419e-01,\n",
       "                      -1.6843e-01, -7.3443e-01, -4.0346e-01,  5.1288e-02,  4.5665e-01,\n",
       "                      -2.0087e-01,  2.5102e-01,  1.6545e-01, -2.5433e-01, -6.5929e-01,\n",
       "                       3.8601e-02, -1.6197e-01, -2.0367e-01, -3.6030e-01, -3.0412e-01,\n",
       "                      -8.5811e-02, -1.3927e-01,  2.6726e-01,  2.5064e-01,  3.0826e-01,\n",
       "                      -8.4175e-02,  5.7774e-01, -2.6018e-01,  3.6105e-01,  4.5603e-01,\n",
       "                      -1.2573e-01,  4.2512e-01,  1.1004e-01,  5.8628e-01,  3.9445e-01,\n",
       "                       3.2453e-02, -6.4932e-01, -3.0414e-01,  2.4331e-01,  6.0543e-02,\n",
       "                      -1.8974e-01,  1.0141e-01, -1.3115e-01,  3.2752e-02, -2.3202e-02,\n",
       "                      -4.1056e-02,  2.2112e-01, -1.5587e-01, -7.3254e-02, -1.0548e-01,\n",
       "                       3.6284e-01,  1.1372e-01,  3.8191e-02,  3.8907e-02,  1.0926e-01,\n",
       "                       4.5931e-01,  3.4995e-01,  1.7942e-01,  5.9742e-01,  6.9397e-02,\n",
       "                       1.9022e-01,  6.2061e-04, -1.2920e-01,  1.8492e-01,  9.9804e-02,\n",
       "                       7.4749e-02,  2.9309e-01,  2.2772e-01,  6.2006e-02,  4.4064e-01,\n",
       "                      -2.0767e-02,  3.7126e-01,  1.5982e-02,  5.4993e-02,  5.1312e-01,\n",
       "                      -1.2111e-02, -1.9878e-01, -2.4482e-01,  4.6073e-01, -1.7898e-01,\n",
       "                       2.7718e-01, -1.4798e-01, -8.0608e-02,  6.6569e-02, -2.2112e-01,\n",
       "                       1.0192e-01, -7.5893e-01,  2.0340e-01,  2.3146e-01, -8.8596e-02,\n",
       "                       8.6890e-02,  1.0191e-01, -1.0553e-01, -1.5564e-01, -4.7282e-01,\n",
       "                       7.3518e-02,  1.4407e-02,  3.7086e-01,  3.3777e-02,  1.7048e-01,\n",
       "                      -4.6769e-01,  1.7310e-01,  3.2122e-01, -2.3740e-01,  3.0820e-02,\n",
       "                      -1.5161e-01,  1.8490e-01, -3.3349e-01,  1.2038e-01,  1.1505e-01,\n",
       "                      -1.7869e-01,  1.1924e-01,  1.8970e-02,  5.1347e-02,  3.2820e-01,\n",
       "                      -5.1690e-02,  4.5425e-01, -6.2531e-02, -3.5629e-02, -1.6527e-01,\n",
       "                      -2.9665e-01,  7.4617e-02,  1.2935e-03,  1.3392e-01, -1.3276e-03,\n",
       "                       2.8748e-01, -2.1661e-03, -2.4584e-01, -1.2247e-01,  2.3792e-01,\n",
       "                      -2.4113e-01, -1.3495e-01,  1.9742e-01,  7.9725e-01, -2.5886e-01,\n",
       "                      -1.2213e-01,  2.3616e-01, -6.4100e-02, -3.0784e-01,  4.2613e-01,\n",
       "                       2.7554e-01, -1.5322e-01, -1.3500e-02,  1.9615e-01,  2.0065e-01,\n",
       "                      -1.8639e-01, -1.7066e-01,  9.4917e-02,  1.2032e-01,  5.5245e-01,\n",
       "                      -5.0258e-01, -3.4928e-03,  1.8236e-01,  2.7781e-01, -1.2583e-01,\n",
       "                       1.2911e-01, -1.7643e-01, -7.6532e-01, -3.8405e-01,  2.2444e-01,\n",
       "                      -1.4852e-01,  2.9202e-01, -1.9512e-03, -1.1433e-01, -4.9306e-01,\n",
       "                      -2.1551e-01, -1.2256e-01,  3.1256e-01,  3.1113e-02,  1.5364e-01,\n",
       "                      -1.4967e-01, -5.5426e-02, -1.7409e-01,  5.1331e-01, -4.1021e-01,\n",
       "                      -2.1127e-01, -2.4760e-01,  2.8460e-01,  4.7905e-02, -5.3607e-04,\n",
       "                       5.8895e-02, -3.3596e-03,  1.1941e-01,  2.1242e-02,  1.2347e-01,\n",
       "                       1.8785e-01, -2.2762e-01,  2.5951e-02, -3.2457e-02,  3.3768e-01,\n",
       "                      -2.1470e-01, -2.2520e-01, -1.0555e-01, -5.6319e-03,  4.1936e-01,\n",
       "                       1.1949e-01, -3.5393e-01,  5.2446e-01, -1.2399e-02,  3.0957e-01,\n",
       "                      -1.1492e-01, -7.4066e-02, -2.2839e-01,  2.3696e-01,  2.6515e-02,\n",
       "                      -3.4804e-01,  2.1689e-01,  3.8526e-01, -2.7862e-01,  3.4274e-01,\n",
       "                       2.7346e-01, -6.6890e-02,  1.3602e-01,  1.6907e-01, -1.6816e-03,\n",
       "                       2.2819e-01,  4.5735e-01, -1.8297e-01,  5.7046e-01,  1.9277e-02,\n",
       "                       4.0396e-01, -4.0393e-01, -1.2195e-02, -8.2898e-02,  1.8062e-01,\n",
       "                      -6.1901e-02,  4.5913e-02,  3.0413e-01,  1.6509e-01, -3.0146e-01,\n",
       "                       5.4372e-01,  1.5217e-01,  3.1430e-01,  8.6105e-01,  2.1523e-01,\n",
       "                      -2.1742e-01,  5.0085e-01,  5.0947e-01, -2.4109e-01,  4.1207e-01,\n",
       "                       4.4325e-01, -5.5353e-02,  3.5783e-01,  2.3726e-01,  1.3428e-01,\n",
       "                      -3.0221e-01,  3.3786e-01,  1.8927e-02,  2.8897e-01, -3.1061e-01,\n",
       "                      -5.6349e-01, -7.6496e-02,  1.8589e-02, -6.1672e-02,  5.4355e-01,\n",
       "                      -2.3541e-01,  6.4664e-01, -3.0678e-01,  2.9901e-01,  1.5538e-01,\n",
       "                       3.4464e-01,  2.6756e-02,  1.2115e-01,  2.0223e-01,  1.4452e-01,\n",
       "                       6.3908e-01,  1.9203e-01,  2.0796e-01,  3.2817e-01,  1.7716e-01,\n",
       "                       2.0419e-01, -5.7681e-01,  3.7019e-01, -1.4945e-01,  3.6007e-01,\n",
       "                       3.5029e-01,  3.5060e-02, -8.6034e-03, -3.5652e-01, -3.2117e-01,\n",
       "                       2.0296e-01,  4.2191e-01,  1.8149e-01,  3.4847e-01,  3.5047e-01,\n",
       "                      -2.9174e-01, -4.0094e-01,  6.1480e-01,  4.0262e-01,  8.5509e-02,\n",
       "                       1.1267e-01,  1.8434e-01,  2.4608e-01,  4.0476e-01,  1.4306e-01,\n",
       "                      -3.0807e-01,  2.2785e-01, -3.2978e-01, -4.8456e-01, -5.0162e-01,\n",
       "                      -4.5710e-01, -6.9636e-02,  5.0216e-01, -2.3885e-01, -1.5948e-01,\n",
       "                       4.6588e-01,  2.3666e-01,  3.6003e-03,  6.4005e-02,  4.5415e-03,\n",
       "                       3.3492e-01, -1.3073e-01,  9.4638e-03,  5.7560e-01,  5.6874e-01,\n",
       "                      -3.8838e-01,  1.3619e-01,  3.6209e-02, -4.8622e-02,  2.4130e-02,\n",
       "                      -2.2733e-02,  5.4647e-01,  1.9134e-01, -4.6793e-02,  2.6036e-01,\n",
       "                      -2.4477e-01,  4.8734e-01, -1.5081e-01,  2.2955e-01,  3.0642e-01,\n",
       "                       1.0417e-01, -2.4196e-01,  4.8345e-01, -7.8952e-02, -4.6928e-01,\n",
       "                      -3.1119e-01,  2.5341e-01, -2.5211e-01,  1.4873e-01,  1.7159e-01,\n",
       "                      -1.2193e-01,  3.0505e-01,  5.7282e-02,  1.6564e-01, -2.0085e-01,\n",
       "                      -7.2657e-02,  1.0446e-01,  1.7999e-01,  3.1090e-01,  2.0712e-01,\n",
       "                       6.4119e-02,  9.1429e-02, -1.4251e-01,  4.6816e-01,  1.0496e-01,\n",
       "                       1.0077e-01,  1.5983e-01, -3.0212e-03,  1.7050e-01,  4.6969e-01,\n",
       "                       3.1987e-02,  6.2983e-02,  2.7109e-01,  1.6485e-02, -6.9792e-02,\n",
       "                      -2.5114e-01,  9.4682e-02, -4.6676e-01,  2.1528e-01,  1.3313e-01,\n",
       "                       2.8226e-01,  4.8620e-01, -1.4434e-01,  1.8018e-02,  2.3285e-01,\n",
       "                       1.9702e-01, -3.2947e-01,  2.3582e-01, -1.6145e-01,  3.4794e-01,\n",
       "                      -8.9066e-02, -3.0064e-01,  9.9745e-02,  1.8625e-01,  4.7436e-01,\n",
       "                       1.0205e-02,  2.4865e-01,  7.4109e-01, -3.5969e-01, -2.1295e-01,\n",
       "                      -2.9223e-01, -1.4052e-01,  2.1396e-01,  2.1334e-01,  1.0533e-01,\n",
       "                       3.2809e-01, -5.7428e-02, -2.6171e-01,  1.9401e-01, -1.8813e-01,\n",
       "                      -6.0065e-01,  4.6330e-01,  2.2156e-01, -1.2731e-01,  2.6230e-01,\n",
       "                       4.5571e-02,  1.1212e-01,  1.8420e-01, -2.2140e-01,  3.6106e-01,\n",
       "                      -2.6338e-01, -1.1529e-03, -2.2839e-01, -2.2067e-02,  1.0244e-01,\n",
       "                       1.7249e-01, -5.0345e-02, -1.1670e-01, -1.2413e-01, -7.7367e-02,\n",
       "                      -4.9319e-01, -2.5963e-01,  2.1659e-01,  1.1755e-01, -2.9475e-01,\n",
       "                      -1.5022e-01,  3.2500e-01,  2.3098e-02,  1.3590e-01,  4.3032e-02,\n",
       "                      -4.0036e-01, -8.8089e-02,  1.5675e-01,  4.1952e-01,  3.1175e-01,\n",
       "                       1.6465e-02, -1.9386e-01, -3.3853e-01,  3.1027e-01, -8.3961e-02,\n",
       "                       3.3414e-01,  1.4449e-01, -1.9845e-01, -2.2848e-01,  4.9481e-01,\n",
       "                      -1.6540e-01,  3.5842e-01,  5.4475e-01,  5.0844e-01, -1.8042e-01,\n",
       "                       9.2979e-02,  1.3822e-01,  2.1203e-01,  2.2424e-01,  4.4644e-01,\n",
       "                       6.0022e-01, -1.2771e-01,  1.0767e-01,  1.3886e-01,  4.6586e-02,\n",
       "                       3.7213e-01,  9.4806e-02,  9.6863e-02, -2.6139e-01, -1.6758e-01,\n",
       "                       2.0967e-01, -6.6544e-01, -1.4706e-01,  4.0622e-01,  4.4751e-01,\n",
       "                      -1.8423e-01,  3.1615e-01, -8.9770e-02,  8.4690e-02, -1.4033e-01,\n",
       "                      -4.4318e-02, -8.0652e-02, -2.2347e-02,  4.5510e-01,  3.2995e-02,\n",
       "                       1.4066e-01, -3.8853e-01,  3.6182e-01, -3.3621e-01, -8.6641e-01,\n",
       "                       6.3496e-02,  1.6172e-01,  2.4046e-01, -4.7523e-01,  6.0265e-01,\n",
       "                       6.4627e-01,  9.0512e-02,  2.4544e-01,  5.5394e-02, -4.1355e-03,\n",
       "                       8.4184e-01, -2.5913e-01, -3.3680e-01,  3.7751e-02,  1.4988e-01,\n",
       "                      -1.3570e-01, -2.3220e-01,  2.7252e-01,  4.8707e-01,  5.9880e-02,\n",
       "                       1.3740e-01,  2.6426e-01,  2.0223e-01,  3.2864e-01,  8.5457e-02,\n",
       "                      -1.5194e-01, -1.8826e-01,  2.6279e-01,  4.7193e-01,  4.0822e-01,\n",
       "                       4.7419e-01,  2.5780e-01,  3.4238e-01, -1.5772e-01,  3.1030e-02,\n",
       "                       1.4772e-01,  3.9881e-01, -1.4417e-01,  1.0566e-01, -4.4554e-01,\n",
       "                      -2.1690e-01, -2.1776e-01, -2.7576e-01, -6.9091e-02,  7.9758e-01,\n",
       "                       2.7982e-01, -2.6880e-02,  5.2597e-02,  1.3727e-01, -5.3131e-01,\n",
       "                      -2.1432e-01, -4.6054e-01, -1.3647e-01, -1.6334e-01,  5.2761e-01,\n",
       "                       2.7138e-01, -9.3465e-02, -8.5398e-02, -6.4565e-01,  3.5042e-02,\n",
       "                      -1.2923e-01,  1.1040e-01, -4.2330e-01, -3.4676e-01, -1.6584e-01,\n",
       "                      -1.7492e-01, -2.0798e-01, -1.1206e-02, -1.0051e-01, -2.9191e-01,\n",
       "                      -4.5498e-03, -3.6753e-01, -3.8384e-01,  3.0517e-03, -1.7086e-02,\n",
       "                      -3.4036e-02, -1.5622e-01, -1.1491e-01, -1.3851e-01,  1.1649e-01,\n",
       "                       7.9181e-02, -3.2472e-01, -1.8797e-01,  9.6734e-02, -5.5390e-01,\n",
       "                       8.5909e-02,  2.2853e-01, -2.5400e-01, -3.4625e-01, -3.6944e-01,\n",
       "                       3.5891e-02,  4.5242e-01,  3.4118e-01,  1.2054e-01,  5.4459e-02,\n",
       "                      -9.2624e-02,  3.4427e-01,  8.3813e-02, -4.1893e-01,  1.8975e-01,\n",
       "                       3.1229e-01,  1.9150e-01,  5.0970e-01,  2.2340e-01, -1.9533e-01,\n",
       "                       3.0414e-01,  1.1082e-01,  4.2463e-01, -2.7599e-01, -1.0338e-01,\n",
       "                       1.5143e-01, -4.3614e-01,  1.0787e-01, -1.5819e-01, -5.4056e-02,\n",
       "                      -2.2751e-02, -5.0882e-01, -3.8213e-01, -9.1715e-01, -2.3075e-01,\n",
       "                      -3.1955e-01,  1.0210e-02, -2.7299e-01,  1.1815e-01,  2.4934e-01],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224.0"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data_loader, model,  epoch=0, training=False, quantizer=None):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    max_i = 1\n",
    "    min_i = -1\n",
    "    bit_i = 8\n",
    "    n_steps = (2**bit_i)-1\n",
    "    eps = (max_i-min_i) / n_steps\n",
    "    \n",
    "    for i, (inputs, target) in enumerate(data_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = Variable(inputs.cuda(), volatile=not training)\n",
    "        target_var = Variable(target)\n",
    "        \n",
    "        input_var = input_var.clamp(min_i,max_i).div(eps).round()\n",
    "        \n",
    "        if quantizer is not None:\n",
    "            input_var = input_var.mul(eps)\n",
    "            quantizer.store_and_quantize(training=False )\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        if type(output) is list:\n",
    "            output = output[0]\n",
    "        values, indices = output.max(1)\n",
    "        #print(indices, target)\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if quantizer is not None:\n",
    "            quantizer.restore_real_value()\n",
    "\n",
    "            \n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            print('{phase} - Epoch: [{0}][{1}/{2}]\\t'\n",
    "                         'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                         'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                         'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                         'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                             epoch, i, len(data_loader),\n",
    "                             phase='TRAINING' if training else 'EVALUATING',\n",
    "                             batch_time=batch_time,\n",
    "                             data_time=data_time, top1=top1, top5=top5))\n",
    "\n",
    "    print('Top1: ', top1.avg)\n",
    "    return top1.avg, top5.avg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:22: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][0/1563]\tTime 0.606 (0.606)\tData 0.561 (0.561)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      "EVALUATING - Epoch: [0][100/1563]\tTime 0.038 (0.049)\tData 0.000 (0.013)\tPrec@1 59.375 (69.245)\tPrec@5 87.500 (88.861)\n",
      "EVALUATING - Epoch: [0][200/1563]\tTime 0.035 (0.043)\tData 0.000 (0.008)\tPrec@1 81.250 (70.693)\tPrec@5 96.875 (89.381)\n",
      "EVALUATING - Epoch: [0][300/1563]\tTime 0.035 (0.041)\tData 0.000 (0.006)\tPrec@1 53.125 (70.671)\tPrec@5 96.875 (90.096)\n",
      "EVALUATING - Epoch: [0][400/1563]\tTime 0.036 (0.040)\tData 0.000 (0.004)\tPrec@1 71.875 (70.168)\tPrec@5 90.625 (90.695)\n",
      "EVALUATING - Epoch: [0][500/1563]\tTime 0.035 (0.040)\tData 0.000 (0.005)\tPrec@1 93.750 (70.172)\tPrec@5 100.000 (90.924)\n",
      "EVALUATING - Epoch: [0][600/1563]\tTime 0.035 (0.041)\tData 0.000 (0.005)\tPrec@1 87.500 (70.585)\tPrec@5 96.875 (90.984)\n",
      "EVALUATING - Epoch: [0][700/1563]\tTime 0.035 (0.041)\tData 0.000 (0.005)\tPrec@1 59.375 (69.601)\tPrec@5 87.500 (90.228)\n",
      "EVALUATING - Epoch: [0][800/1563]\tTime 0.035 (0.040)\tData 0.000 (0.005)\tPrec@1 62.500 (67.868)\tPrec@5 84.375 (89.068)\n",
      "EVALUATING - Epoch: [0][900/1563]\tTime 0.035 (0.041)\tData 0.000 (0.005)\tPrec@1 90.625 (67.311)\tPrec@5 100.000 (88.450)\n",
      "EVALUATING - Epoch: [0][1000/1563]\tTime 0.035 (0.041)\tData 0.000 (0.005)\tPrec@1 93.750 (66.434)\tPrec@5 100.000 (87.647)\n",
      "EVALUATING - Epoch: [0][1100/1563]\tTime 0.035 (0.041)\tData 0.000 (0.005)\tPrec@1 71.875 (65.883)\tPrec@5 90.625 (87.168)\n",
      "EVALUATING - Epoch: [0][1200/1563]\tTime 0.037 (0.040)\tData 0.000 (0.005)\tPrec@1 75.000 (65.284)\tPrec@5 81.250 (86.615)\n",
      "EVALUATING - Epoch: [0][1300/1563]\tTime 0.035 (0.040)\tData 0.000 (0.005)\tPrec@1 78.125 (64.950)\tPrec@5 84.375 (86.325)\n",
      "EVALUATING - Epoch: [0][1400/1563]\tTime 0.040 (0.040)\tData 0.000 (0.005)\tPrec@1 62.500 (64.229)\tPrec@5 93.750 (85.905)\n",
      "EVALUATING - Epoch: [0][1500/1563]\tTime 0.035 (0.040)\tData 0.000 (0.004)\tPrec@1 43.750 (64.197)\tPrec@5 78.125 (85.920)\n",
      "Top1:  64.198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64.198, 85.934)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.generate_deployment_model()\n",
    "quantizer.deployment_model.eval()\n",
    "forward(val_loader, quantizer.deployment_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_quant_devel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d_SAME(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
       "    (2): Conv2d_SAME(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "    (3): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
       "    (4): Conv2d_SAME(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (5): ScaledClippedLinearQuantizationChannel(clip_val=3, inplace)\n",
       "    (6): Conv2d_SAME(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (7): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (8): Conv2d_SAME(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (9): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
       "    (10): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "    (11): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
       "    (12): Conv2d_SAME(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (13): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
       "    (14): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "    (15): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (16): Conv2d_SAME(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (17): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (18): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (19): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (20): Conv2d_SAME(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (21): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (22): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "    (23): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (24): Conv2d_SAME(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (25): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (26): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (27): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (28): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (29): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (30): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (31): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (32): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (33): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (34): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (35): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (36): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (37): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (38): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (39): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (40): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (41): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (42): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (43): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (44): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (45): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (46): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "    (47): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (48): Conv2d_SAME(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (49): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (50): Conv2d_SAME(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "    (51): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (52): Conv2d_SAME(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (53): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (54): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    (1): ScaledClippedLinearQuantizationChannel(clip_val=False, inplace)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7733, 0.9876, 0.5983, 0.7105, 0.8488, 0.7820, 0.5255, 0.8953, 0.5265,\n",
       "        0.6629, 0.7659, 0.8644, 0.7197, 0.7027, 0.7039, 0.5955, 0.8282, 0.5596,\n",
       "        0.5022, 0.9477, 0.8367, 0.5117, 0.8185, 0.6013, 0.5532, 0.8462, 0.7449,\n",
       "        0.9446, 0.5171, 0.5420, 0.6003, 0.6302], device='cuda:0')"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_list[0]['quant_act'].M_ZERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) 8 4 8 PerChannelsAsymMinMax ICN\n",
      "2 ) 4 4 8 PerChannelsAsymMinMax ICN\n",
      "3 ) 4 2 8 PerChannelsAsymMinMax ICN\n",
      "4 ) 2 8 8 PerChannelsAsymMinMax ICN\n",
      "5 ) 8 4 8 PerChannelsAsymMinMax ICN\n",
      "6 ) 4 4 8 PerChannelsAsymMinMax ICN\n",
      "7 ) 4 4 8 PerChannelsAsymMinMax ICN\n",
      "8 ) 4 8 8 PerChannelsAsymMinMax ICN\n",
      "9 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "10 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "11 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "12 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "13 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "14 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "15 ) 8 8 4 PerChannelsAsymMinMax ICN\n",
      "16 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "17 ) 8 8 4 PerChannelsAsymMinMax ICN\n",
      "18 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "19 ) 8 8 4 PerChannelsAsymMinMax ICN\n",
      "20 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "21 ) 8 8 4 PerChannelsAsymMinMax ICN\n",
      "22 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "23 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "24 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "25 ) 8 8 4 PerChannelsAsymMinMax ICN\n",
      "26 ) 8 8 8 PerChannelsAsymMinMax ICN\n",
      "27 ) 8 8 2 PerChannelsAsymMinMax ICN\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'act_o_bits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-381-33c9c9b85f42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'act_o_bits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mwt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w_bits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'act_o_bits'"
     ]
    }
   ],
   "source": [
    "inp = 8\n",
    "out = 8\n",
    "wt = 8\n",
    "lay = 1\n",
    "for item in param_list:\n",
    "    out = item['act_o_bits']\n",
    "    wt = item['w_bits']\n",
    "    print(lay, ')', end=' ')\n",
    "    print(inp, out, wt,item['quant_type'], item['fold_type'])\n",
    "    inp = out\n",
    "    lay+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 =  quantizer.deployment_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_quant_devel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d_SAME(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
       "    (2): Conv2d_SAME(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "    (3): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
       "    (4): Conv2d_SAME(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (5): ScaledClippedLinearQuantizationChannel(clip_val=3, inplace)\n",
       "    (6): Conv2d_SAME(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (7): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (8): Conv2d_SAME(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (9): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
       "    (10): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "    (11): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
       "    (12): Conv2d_SAME(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (13): ScaledClippedLinearQuantizationChannel(clip_val=15, inplace)\n",
       "    (14): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "    (15): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (16): Conv2d_SAME(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (17): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (18): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (19): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (20): Conv2d_SAME(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (21): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (22): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "    (23): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (24): Conv2d_SAME(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (25): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (26): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (27): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (28): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (29): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (30): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (31): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (32): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (33): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (34): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (35): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (36): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (37): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (38): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (39): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (40): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (41): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (42): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (43): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (44): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (45): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (46): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "    (47): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (48): Conv2d_SAME(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (49): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (50): Conv2d_SAME(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "    (51): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (52): Conv2d_SAME(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (53): ScaledClippedLinearQuantizationChannel(clip_val=255, inplace)\n",
       "    (54): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    (1): ScaledClippedLinearQuantizationChannel(clip_val=False, inplace)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The entire trained quantized mobilenet\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224.0"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "############### ATTENZIONE VANNO RICALCOLATI I CHECKSUM ###################################\n",
    "################# E RICARICATA L'IMMAGINE SU STM ##########################################\n",
    "###########################################################################################\n",
    "\n",
    "#random image with 3 channel (4th of each pixel default 0)\n",
    "x = torch.Tensor(1,3,224,224)\n",
    "ch_image = x.size(1)\n",
    "dim_im = x.size(2)\n",
    "x.random_(0,255)\n",
    "\n",
    "str_image = '#define INPUT_IMAGE \\\\\\n { '\n",
    "for j in range(dim_im):\n",
    "    for k in range(dim_im):\n",
    "        for p in range(ch_image):\n",
    "            str_image += str(int(x[0][p][j][k])) + ', '\n",
    "        str_image += str(0) + ', '\n",
    "    str_image += '\\\\\\n'\n",
    "str_image = str_image[:-4] + '}'\n",
    "    \n",
    "f = open(\"immagine.h\", \"w\") \n",
    "f.write(str_image) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 --> checksum[0] = 1609436.0\n",
      "layer 2 --> checksum[1] = 1394860.0\n",
      "layer 3 --> checksum[2] = 922244.0\n",
      "layer 4 --> checksum[3] = 15723856.0\n",
      "layer 5 --> checksum[4] = 1779281.0\n",
      "layer 6 --> checksum[5] = 1128059.0\n",
      "layer 7 --> checksum[6] = 990829.0\n",
      "layer 8 --> checksum[7] = 6212367.0\n",
      "layer 9 --> checksum[8] = 12036086.0\n",
      "layer 10 --> checksum[9] = 5739366.0\n",
      "layer 11 --> checksum[10] = 6187564.0\n",
      "layer 12 --> checksum[11] = 2091175.0\n",
      "layer 13 --> checksum[12] = 4809420.0\n",
      "layer 14 --> checksum[13] = 2122381.0\n",
      "layer 15 --> checksum[14] = 2991072.0\n",
      "layer 16 --> checksum[15] = 2222794.0\n",
      "layer 17 --> checksum[16] = 2316926.0\n",
      "layer 18 --> checksum[17] = 2036464.0\n",
      "layer 19 --> checksum[18] = 2237355.0\n",
      "layer 20 --> checksum[19] = 2032239.0\n",
      "layer 21 --> checksum[20] = 1759282.0\n",
      "layer 22 --> checksum[21] = 2194700.0\n",
      "layer 23 --> checksum[22] = 1738510.0\n",
      "layer 24 --> checksum[23] = 518588.0\n",
      "layer 25 --> checksum[24] = 611309.0\n",
      "layer 26 --> checksum[25] = 696271.0\n",
      "layer 27 --> checksum[26] = 130850.0\n"
     ]
    }
   ],
   "source": [
    "#we work with a subset of layers\n",
    "nlayer = 27\n",
    "layer_list = []\n",
    "out_list = []\n",
    "for i in range(nlayer*2):\n",
    "    layer_list.append(m1.model[i])\n",
    "    m2 = nn.Sequential(*layer_list)\n",
    "    #inference of the nlayer network with the random image\n",
    "    if i%2:\n",
    "        y = m2(x.cuda())\n",
    "        out_list.append(y)\n",
    "        print('layer {} --> checksum[{}] = {}' .format(math.ceil(i/2), math.ceil(i/2)-1, out_list[math.ceil(i/2)-1].sum()))\n",
    "#m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) 8 4 8 PerChannelsAsymMinMax ICN Conv2d_SAME(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "2 ) 4 4 8 PerChannelsAsymMinMax ICN Conv2d_SAME(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "3 ) 4 2 8 PerChannelsAsymMinMax ICN Conv2d_SAME(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "4 ) 2 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "5 ) 8 4 8 PerChannelsAsymMinMax ICN Conv2d_SAME(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "6 ) 4 4 8 PerChannelsAsymMinMax ICN Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "7 ) 4 4 8 PerChannelsAsymMinMax ICN Conv2d_SAME(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "8 ) 4 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "9 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "10 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "11 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "12 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "13 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "14 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "15 ) 8 8 4 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "16 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "17 ) 8 8 4 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "18 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "19 ) 8 8 4 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "20 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "21 ) 8 8 4 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "22 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "23 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "24 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "25 ) 8 8 4 PerChannelsAsymMinMax ICN Conv2d_SAME(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "26 ) 8 8 8 PerChannelsAsymMinMax ICN Conv2d_SAME(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "27 ) 8 8 2 PerChannelsAsymMinMax ICN Conv2d_SAME(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "inp = 8\n",
    "out = 8\n",
    "wt = 8\n",
    "i=1\n",
    "for item in param_list[:nlayer]:\n",
    "    out = item['act_o_bits']\n",
    "    wt = item['w_bits']\n",
    "    print(i, ')',inp, out, wt,item['quant_type'], item['fold_type'], item['conv'])\n",
    "    inp = out\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPECTED tensor_out layer1 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0 - 0\n",
      "tensor_out[1] = \t 0 - 0\n",
      "tensor_out[2] = \t 15 - 0\n",
      "tensor_out[3] = \t 15 - 0\n",
      "tensor_out[4] = \t 0 - 0\n",
      "tensor_out[5] = \t 15 - 0\n",
      "tensor_out[6] = \t 5 - 15\n",
      "tensor_out[7] = \t 5 - 0\n",
      "tensor_out[8] = \t 13 - 15\n",
      "tensor_out[9] = \t 1 - 9\n",
      "tensor_out[10] = \t 1 - 0\n",
      "tensor_out[11] = \t 0 - 0\n",
      "tensor_out[12] = \t 11 - 0\n",
      "tensor_out[13] = \t 0 - 0\n",
      "tensor_out[14] = \t 8 - 0\n",
      "tensor_out[15] = \t 15 - 0\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer2 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 1 - 0\n",
      "tensor_out[1] = \t 0 - 1\n",
      "tensor_out[2] = \t 0 - 3\n",
      "tensor_out[3] = \t 15 - 0\n",
      "tensor_out[4] = \t 0 - 0\n",
      "tensor_out[5] = \t 0 - 0\n",
      "tensor_out[6] = \t 13 - 0\n",
      "tensor_out[7] = \t 12 - 0\n",
      "tensor_out[8] = \t 2 - 15\n",
      "tensor_out[9] = \t 0 - 0\n",
      "tensor_out[10] = \t 8 - 0\n",
      "tensor_out[11] = \t 3 - 0\n",
      "tensor_out[12] = \t 0 - 0\n",
      "tensor_out[13] = \t 0 - 13\n",
      "tensor_out[14] = \t 0 - 4\n",
      "tensor_out[15] = \t 10 - 0\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer3 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[1] = \t 3 - 3 - 0 - 1\n",
      "tensor_out[2] = \t 2 - 2 - 1 - 0\n",
      "tensor_out[3] = \t 3 - 3 - 1 - 0\n",
      "tensor_out[4] = \t 0 - 3 - 1 - 0\n",
      "tensor_out[5] = \t 0 - 2 - 2 - 0\n",
      "tensor_out[6] = \t 0 - 0 - 0 - 3\n",
      "tensor_out[7] = \t 0 - 3 - 3 - 0\n",
      "tensor_out[8] = \t 3 - 1 - 3 - 1\n",
      "tensor_out[9] = \t 2 - 0 - 0 - 0\n",
      "tensor_out[10] = \t 0 - 3 - 0 - 0\n",
      "tensor_out[11] = \t 2 - 0 - 0 - 1\n",
      "tensor_out[12] = \t 2 - 3 - 2 - 0\n",
      "tensor_out[13] = \t 3 - 0 - 3 - 0\n",
      "tensor_out[14] = \t 0 - 0 - 0 - 0\n",
      "tensor_out[15] = \t 2 - 0 - 0 - 0\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer4 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 7\n",
      "tensor_out[1] = \t 238\n",
      "tensor_out[2] = \t 224\n",
      "tensor_out[3] = \t 150\n",
      "tensor_out[4] = \t 255\n",
      "tensor_out[5] = \t 17\n",
      "tensor_out[6] = \t 29\n",
      "tensor_out[7] = \t 200\n",
      "tensor_out[8] = \t 0\n",
      "tensor_out[9] = \t 128\n",
      "tensor_out[10] = \t 0\n",
      "tensor_out[11] = \t 0\n",
      "tensor_out[12] = \t 91\n",
      "tensor_out[13] = \t 74\n",
      "tensor_out[14] = \t 96\n",
      "tensor_out[15] = \t 12\n",
      "tensor_out[16] = \t 43\n",
      "tensor_out[17] = \t 43\n",
      "tensor_out[18] = \t 62\n",
      "tensor_out[19] = \t 0\n",
      "tensor_out[20] = \t 0\n",
      "tensor_out[21] = \t 79\n",
      "tensor_out[22] = \t 235\n",
      "tensor_out[23] = \t 0\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 97\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 0\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 133\n",
      "tensor_out[30] = \t 190\n",
      "tensor_out[31] = \t 0\n",
      "tensor_out[32] = \t 255\n",
      "tensor_out[33] = \t 17\n",
      "tensor_out[34] = \t 0\n",
      "tensor_out[35] = \t 62\n",
      "tensor_out[36] = \t 69\n",
      "tensor_out[37] = \t 0\n",
      "tensor_out[38] = \t 133\n",
      "tensor_out[39] = \t 54\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 78\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 0\n",
      "tensor_out[45] = \t 156\n",
      "tensor_out[46] = \t 119\n",
      "tensor_out[47] = \t 0\n",
      "tensor_out[48] = \t 0\n",
      "tensor_out[49] = \t 255\n",
      "tensor_out[50] = \t 157\n",
      "tensor_out[51] = \t 48\n",
      "tensor_out[52] = \t 0\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 98\n",
      "tensor_out[55] = \t 55\n",
      "tensor_out[56] = \t 116\n",
      "tensor_out[57] = \t 220\n",
      "tensor_out[58] = \t 127\n",
      "tensor_out[59] = \t 106\n",
      "tensor_out[60] = \t 0\n",
      "tensor_out[61] = \t 181\n",
      "tensor_out[62] = \t 0\n",
      "tensor_out[63] = \t 116\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer5 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 6 - 14\n",
      "tensor_out[1] = \t 7 - 0\n",
      "tensor_out[2] = \t 0 - 5\n",
      "tensor_out[3] = \t 15 - 7\n",
      "tensor_out[4] = \t 0 - 0\n",
      "tensor_out[5] = \t 6 - 11\n",
      "tensor_out[6] = \t 7 - 15\n",
      "tensor_out[7] = \t 0 - 2\n",
      "tensor_out[8] = \t 11 - 1\n",
      "tensor_out[9] = \t 8 - 9\n",
      "tensor_out[10] = \t 0 - 0\n",
      "tensor_out[11] = \t 1 - 15\n",
      "tensor_out[12] = \t 4 - 10\n",
      "tensor_out[13] = \t 0 - 0\n",
      "tensor_out[14] = \t 0 - 3\n",
      "tensor_out[15] = \t 13 - 0\n",
      "tensor_out[16] = \t 9 - 8\n",
      "tensor_out[17] = \t 0 - 6\n",
      "tensor_out[18] = \t 4 - 6\n",
      "tensor_out[19] = \t 11 - 0\n",
      "tensor_out[20] = \t 0 - 9\n",
      "tensor_out[21] = \t 6 - 0\n",
      "tensor_out[22] = \t 3 - 9\n",
      "tensor_out[23] = \t 15 - 0\n",
      "tensor_out[24] = \t 6 - 13\n",
      "tensor_out[25] = \t 3 - 0\n",
      "tensor_out[26] = \t 0 - 0\n",
      "tensor_out[27] = \t 0 - 10\n",
      "tensor_out[28] = \t 1 - 0\n",
      "tensor_out[29] = \t 1 - 0\n",
      "tensor_out[30] = \t 15 - 0\n",
      "tensor_out[31] = \t 2 - 0\n",
      "tensor_out[32] = \t 4 - 0\n",
      "tensor_out[33] = \t 15 - 0\n",
      "tensor_out[34] = \t 11 - 0\n",
      "tensor_out[35] = \t 0 - 6\n",
      "tensor_out[36] = \t 1 - 3\n",
      "tensor_out[37] = \t 4 - 0\n",
      "tensor_out[38] = \t 9 - 10\n",
      "tensor_out[39] = \t 0 - 14\n",
      "tensor_out[40] = \t 8 - 3\n",
      "tensor_out[41] = \t 1 - 0\n",
      "tensor_out[42] = \t 0 - 8\n",
      "tensor_out[43] = \t 0 - 0\n",
      "tensor_out[44] = \t 5 - 4\n",
      "tensor_out[45] = \t 3 - 0\n",
      "tensor_out[46] = \t 0 - 1\n",
      "tensor_out[47] = \t 6 - 0\n",
      "tensor_out[48] = \t 15 - 5\n",
      "tensor_out[49] = \t 0 - 3\n",
      "tensor_out[50] = \t 1 - 4\n",
      "tensor_out[51] = \t 0 - 2\n",
      "tensor_out[52] = \t 13 - 4\n",
      "tensor_out[53] = \t 0 - 0\n",
      "tensor_out[54] = \t 2 - 0\n",
      "tensor_out[55] = \t 3 - 2\n",
      "tensor_out[56] = \t 3 - 3\n",
      "tensor_out[57] = \t 1 - 7\n",
      "tensor_out[58] = \t 0 - 5\n",
      "tensor_out[59] = \t 2 - 10\n",
      "tensor_out[60] = \t 15 - 0\n",
      "tensor_out[61] = \t 0 - 1\n",
      "tensor_out[62] = \t 15 - 0\n",
      "tensor_out[63] = \t 13 - 7\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer6 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 2 - 15\n",
      "tensor_out[1] = \t 6 - 0\n",
      "tensor_out[2] = \t 0 - 4\n",
      "tensor_out[3] = \t 1 - 0\n",
      "tensor_out[4] = \t 0 - 6\n",
      "tensor_out[5] = \t 1 - 13\n",
      "tensor_out[6] = \t 8 - 15\n",
      "tensor_out[7] = \t 0 - 2\n",
      "tensor_out[8] = \t 0 - 0\n",
      "tensor_out[9] = \t 0 - 0\n",
      "tensor_out[10] = \t 0 - 2\n",
      "tensor_out[11] = \t 0 - 1\n",
      "tensor_out[12] = \t 0 - 1\n",
      "tensor_out[13] = \t 0 - 0\n",
      "tensor_out[14] = \t 0 - 0\n",
      "tensor_out[15] = \t 14 - 5\n",
      "tensor_out[16] = \t 0 - 0\n",
      "tensor_out[17] = \t 3 - 0\n",
      "tensor_out[18] = \t 0 - 7\n",
      "tensor_out[19] = \t 0 - 0\n",
      "tensor_out[20] = \t 0 - 15\n",
      "tensor_out[21] = \t 0 - 1\n",
      "tensor_out[22] = \t 0 - 6\n",
      "tensor_out[23] = \t 15 - 0\n",
      "tensor_out[24] = \t 4 - 0\n",
      "tensor_out[25] = \t 1 - 4\n",
      "tensor_out[26] = \t 3 - 15\n",
      "tensor_out[27] = \t 0 - 11\n",
      "tensor_out[28] = \t 3 - 0\n",
      "tensor_out[29] = \t 0 - 1\n",
      "tensor_out[30] = \t 14 - 0\n",
      "tensor_out[31] = \t 1 - 0\n",
      "tensor_out[32] = \t 5 - 0\n",
      "tensor_out[33] = \t 12 - 0\n",
      "tensor_out[34] = \t 0 - 3\n",
      "tensor_out[35] = \t 4 - 3\n",
      "tensor_out[36] = \t 2 - 3\n",
      "tensor_out[37] = \t 2 - 1\n",
      "tensor_out[38] = \t 0 - 0\n",
      "tensor_out[39] = \t 7 - 13\n",
      "tensor_out[40] = \t 8 - 5\n",
      "tensor_out[41] = \t 2 - 0\n",
      "tensor_out[42] = \t 0 - 4\n",
      "tensor_out[43] = \t 0 - 3\n",
      "tensor_out[44] = \t 0 - 5\n",
      "tensor_out[45] = \t 6 - 0\n",
      "tensor_out[46] = \t 0 - 0\n",
      "tensor_out[47] = \t 1 - 0\n",
      "tensor_out[48] = \t 6 - 12\n",
      "tensor_out[49] = \t 0 - 2\n",
      "tensor_out[50] = \t 0 - 5\n",
      "tensor_out[51] = \t 0 - 7\n",
      "tensor_out[52] = \t 0 - 4\n",
      "tensor_out[53] = \t 0 - 0\n",
      "tensor_out[54] = \t 0 - 3\n",
      "tensor_out[55] = \t 3 - 3\n",
      "tensor_out[56] = \t 1 - 1\n",
      "tensor_out[57] = \t 1 - 11\n",
      "tensor_out[58] = \t 4 - 4\n",
      "tensor_out[59] = \t 5 - 0\n",
      "tensor_out[60] = \t 15 - 2\n",
      "tensor_out[61] = \t 2 - 2\n",
      "tensor_out[62] = \t 7 - 0\n",
      "tensor_out[63] = \t 0 - 15\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer7 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 9 - 9\n",
      "tensor_out[1] = \t 0 - 15\n",
      "tensor_out[2] = \t 2 - 6\n",
      "tensor_out[3] = \t 0 - 4\n",
      "tensor_out[4] = \t 4 - 8\n",
      "tensor_out[5] = \t 1 - 0\n",
      "tensor_out[6] = \t 0 - 2\n",
      "tensor_out[7] = \t 0 - 0\n",
      "tensor_out[8] = \t 12 - 5\n",
      "tensor_out[9] = \t 0 - 0\n",
      "tensor_out[10] = \t 0 - 0\n",
      "tensor_out[11] = \t 0 - 0\n",
      "tensor_out[12] = \t 2 - 0\n",
      "tensor_out[13] = \t 0 - 6\n",
      "tensor_out[14] = \t 0 - 7\n",
      "tensor_out[15] = \t 0 - 10\n",
      "tensor_out[16] = \t 0 - 5\n",
      "tensor_out[17] = \t 2 - 0\n",
      "tensor_out[18] = \t 7 - 1\n",
      "tensor_out[19] = \t 6 - 15\n",
      "tensor_out[20] = \t 3 - 0\n",
      "tensor_out[21] = \t 0 - 6\n",
      "tensor_out[22] = \t 7 - 0\n",
      "tensor_out[23] = \t 15 - 0\n",
      "tensor_out[24] = \t 7 - 0\n",
      "tensor_out[25] = \t 1 - 3\n",
      "tensor_out[26] = \t 0 - 0\n",
      "tensor_out[27] = \t 3 - 0\n",
      "tensor_out[28] = \t 1 - 7\n",
      "tensor_out[29] = \t 0 - 0\n",
      "tensor_out[30] = \t 5 - 0\n",
      "tensor_out[31] = \t 0 - 13\n",
      "tensor_out[32] = \t 0 - 7\n",
      "tensor_out[33] = \t 0 - 0\n",
      "tensor_out[34] = \t 1 - 5\n",
      "tensor_out[35] = \t 0 - 4\n",
      "tensor_out[36] = \t 3 - 0\n",
      "tensor_out[37] = \t 3 - 0\n",
      "tensor_out[38] = \t 6 - 4\n",
      "tensor_out[39] = \t 4 - 6\n",
      "tensor_out[40] = \t 0 - 0\n",
      "tensor_out[41] = \t 2 - 7\n",
      "tensor_out[42] = \t 0 - 5\n",
      "tensor_out[43] = \t 0 - 0\n",
      "tensor_out[44] = \t 0 - 1\n",
      "tensor_out[45] = \t 0 - 0\n",
      "tensor_out[46] = \t 15 - 0\n",
      "tensor_out[47] = \t 0 - 7\n",
      "tensor_out[48] = \t 0 - 11\n",
      "tensor_out[49] = \t 0 - 7\n",
      "tensor_out[50] = \t 11 - 0\n",
      "tensor_out[51] = \t 0 - 0\n",
      "tensor_out[52] = \t 0 - 6\n",
      "tensor_out[53] = \t 3 - 8\n",
      "tensor_out[54] = \t 2 - 6\n",
      "tensor_out[55] = \t 0 - 5\n",
      "tensor_out[56] = \t 6 - 3\n",
      "tensor_out[57] = \t 0 - 0\n",
      "tensor_out[58] = \t 0 - 1\n",
      "tensor_out[59] = \t 3 - 2\n",
      "tensor_out[60] = \t 6 - 0\n",
      "tensor_out[61] = \t 0 - 8\n",
      "tensor_out[62] = \t 5 - 5\n",
      "tensor_out[63] = \t 3 - 0\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer8 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 255\n",
      "tensor_out[1] = \t 158\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 61\n",
      "tensor_out[5] = \t 144\n",
      "tensor_out[6] = \t 104\n",
      "tensor_out[7] = \t 16\n",
      "tensor_out[8] = \t 0\n",
      "tensor_out[9] = \t 102\n",
      "tensor_out[10] = \t 7\n",
      "tensor_out[11] = \t 0\n",
      "tensor_out[12] = \t 57\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 168\n",
      "tensor_out[15] = \t 51\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 35\n",
      "tensor_out[19] = \t 37\n",
      "tensor_out[20] = \t 21\n",
      "tensor_out[21] = \t 68\n",
      "tensor_out[22] = \t 171\n",
      "tensor_out[23] = \t 24\n",
      "tensor_out[24] = \t 205\n",
      "tensor_out[25] = \t 33\n",
      "tensor_out[26] = \t 106\n",
      "tensor_out[27] = \t 113\n",
      "tensor_out[28] = \t 5\n",
      "tensor_out[29] = \t 54\n",
      "tensor_out[30] = \t 0\n",
      "tensor_out[31] = \t 11\n",
      "tensor_out[32] = \t 0\n",
      "tensor_out[33] = \t 101\n",
      "tensor_out[34] = \t 4\n",
      "tensor_out[35] = \t 86\n",
      "tensor_out[36] = \t 62\n",
      "tensor_out[37] = \t 90\n",
      "tensor_out[38] = \t 255\n",
      "tensor_out[39] = \t 255\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 23\n",
      "tensor_out[42] = \t 16\n",
      "tensor_out[43] = \t 11\n",
      "tensor_out[44] = \t 151\n",
      "tensor_out[45] = \t 37\n",
      "tensor_out[46] = \t 93\n",
      "tensor_out[47] = \t 0\n",
      "tensor_out[48] = \t 5\n",
      "tensor_out[49] = \t 235\n",
      "tensor_out[50] = \t 100\n",
      "tensor_out[51] = \t 22\n",
      "tensor_out[52] = \t 107\n",
      "tensor_out[53] = \t 27\n",
      "tensor_out[54] = \t 23\n",
      "tensor_out[55] = \t 0\n",
      "tensor_out[56] = \t 255\n",
      "tensor_out[57] = \t 0\n",
      "tensor_out[58] = \t 55\n",
      "tensor_out[59] = \t 69\n",
      "tensor_out[60] = \t 40\n",
      "tensor_out[61] = \t 26\n",
      "tensor_out[62] = \t 76\n",
      "tensor_out[63] = \t 38\n",
      "tensor_out[64] = \t 0\n",
      "tensor_out[65] = \t 7\n",
      "tensor_out[66] = \t 0\n",
      "tensor_out[67] = \t 23\n",
      "tensor_out[68] = \t 77\n",
      "tensor_out[69] = \t 95\n",
      "tensor_out[70] = \t 48\n",
      "tensor_out[71] = \t 64\n",
      "tensor_out[72] = \t 3\n",
      "tensor_out[73] = \t 153\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 23\n",
      "tensor_out[76] = \t 156\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 111\n",
      "tensor_out[79] = \t 219\n",
      "tensor_out[80] = \t 0\n",
      "tensor_out[81] = \t 74\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 255\n",
      "tensor_out[84] = \t 220\n",
      "tensor_out[85] = \t 0\n",
      "tensor_out[86] = \t 140\n",
      "tensor_out[87] = \t 12\n",
      "tensor_out[88] = \t 96\n",
      "tensor_out[89] = \t 0\n",
      "tensor_out[90] = \t 82\n",
      "tensor_out[91] = \t 89\n",
      "tensor_out[92] = \t 31\n",
      "tensor_out[93] = \t 255\n",
      "tensor_out[94] = \t 0\n",
      "tensor_out[95] = \t 4\n",
      "tensor_out[96] = \t 2\n",
      "tensor_out[97] = \t 1\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 16\n",
      "tensor_out[101] = \t 54\n",
      "tensor_out[102] = \t 35\n",
      "tensor_out[103] = \t 36\n",
      "tensor_out[104] = \t 158\n",
      "tensor_out[105] = \t 31\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 0\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 0\n",
      "tensor_out[110] = \t 89\n",
      "tensor_out[111] = \t 29\n",
      "tensor_out[112] = \t 102\n",
      "tensor_out[113] = \t 119\n",
      "tensor_out[114] = \t 144\n",
      "tensor_out[115] = \t 0\n",
      "tensor_out[116] = \t 0\n",
      "tensor_out[117] = \t 96\n",
      "tensor_out[118] = \t 142\n",
      "tensor_out[119] = \t 55\n",
      "tensor_out[120] = \t 176\n",
      "tensor_out[121] = \t 63\n",
      "tensor_out[122] = \t 124\n",
      "tensor_out[123] = \t 22\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 164\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 142\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer9 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0\n",
      "tensor_out[1] = \t 104\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 168\n",
      "tensor_out[4] = \t 4\n",
      "tensor_out[5] = \t 56\n",
      "tensor_out[6] = \t 71\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 111\n",
      "tensor_out[9] = \t 10\n",
      "tensor_out[10] = \t 41\n",
      "tensor_out[11] = \t 106\n",
      "tensor_out[12] = \t 212\n",
      "tensor_out[13] = \t 90\n",
      "tensor_out[14] = \t 18\n",
      "tensor_out[15] = \t 38\n",
      "tensor_out[16] = \t 12\n",
      "tensor_out[17] = \t 107\n",
      "tensor_out[18] = \t 154\n",
      "tensor_out[19] = \t 255\n",
      "tensor_out[20] = \t 0\n",
      "tensor_out[21] = \t 80\n",
      "tensor_out[22] = \t 73\n",
      "tensor_out[23] = \t 67\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 49\n",
      "tensor_out[26] = \t 102\n",
      "tensor_out[27] = \t 78\n",
      "tensor_out[28] = \t 236\n",
      "tensor_out[29] = \t 63\n",
      "tensor_out[30] = \t 0\n",
      "tensor_out[31] = \t 39\n",
      "tensor_out[32] = \t 110\n",
      "tensor_out[33] = \t 66\n",
      "tensor_out[34] = \t 23\n",
      "tensor_out[35] = \t 95\n",
      "tensor_out[36] = \t 74\n",
      "tensor_out[37] = \t 141\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 178\n",
      "tensor_out[40] = \t 137\n",
      "tensor_out[41] = \t 94\n",
      "tensor_out[42] = \t 61\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 55\n",
      "tensor_out[45] = \t 68\n",
      "tensor_out[46] = \t 0\n",
      "tensor_out[47] = \t 42\n",
      "tensor_out[48] = \t 9\n",
      "tensor_out[49] = \t 128\n",
      "tensor_out[50] = \t 175\n",
      "tensor_out[51] = \t 38\n",
      "tensor_out[52] = \t 150\n",
      "tensor_out[53] = \t 31\n",
      "tensor_out[54] = \t 0\n",
      "tensor_out[55] = \t 92\n",
      "tensor_out[56] = \t 0\n",
      "tensor_out[57] = \t 84\n",
      "tensor_out[58] = \t 197\n",
      "tensor_out[59] = \t 120\n",
      "tensor_out[60] = \t 45\n",
      "tensor_out[61] = \t 123\n",
      "tensor_out[62] = \t 77\n",
      "tensor_out[63] = \t 0\n",
      "tensor_out[64] = \t 55\n",
      "tensor_out[65] = \t 107\n",
      "tensor_out[66] = \t 0\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 10\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 80\n",
      "tensor_out[72] = \t 35\n",
      "tensor_out[73] = \t 219\n",
      "tensor_out[74] = \t 149\n",
      "tensor_out[75] = \t 10\n",
      "tensor_out[76] = \t 0\n",
      "tensor_out[77] = \t 79\n",
      "tensor_out[78] = \t 23\n",
      "tensor_out[79] = \t 255\n",
      "tensor_out[80] = \t 79\n",
      "tensor_out[81] = \t 72\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 89\n",
      "tensor_out[84] = \t 108\n",
      "tensor_out[85] = \t 53\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 0\n",
      "tensor_out[88] = \t 78\n",
      "tensor_out[89] = \t 45\n",
      "tensor_out[90] = \t 18\n",
      "tensor_out[91] = \t 9\n",
      "tensor_out[92] = \t 92\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 213\n",
      "tensor_out[95] = \t 2\n",
      "tensor_out[96] = \t 45\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 75\n",
      "tensor_out[100] = \t 34\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 0\n",
      "tensor_out[103] = \t 125\n",
      "tensor_out[104] = \t 65\n",
      "tensor_out[105] = \t 63\n",
      "tensor_out[106] = \t 255\n",
      "tensor_out[107] = \t 21\n",
      "tensor_out[108] = \t 35\n",
      "tensor_out[109] = \t 139\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 193\n",
      "tensor_out[112] = \t 38\n",
      "tensor_out[113] = \t 89\n",
      "tensor_out[114] = \t 141\n",
      "tensor_out[115] = \t 72\n",
      "tensor_out[116] = \t 172\n",
      "tensor_out[117] = \t 0\n",
      "tensor_out[118] = \t 14\n",
      "tensor_out[119] = \t 137\n",
      "tensor_out[120] = \t 5\n",
      "tensor_out[121] = \t 67\n",
      "tensor_out[122] = \t 167\n",
      "tensor_out[123] = \t 6\n",
      "tensor_out[124] = \t 9\n",
      "tensor_out[125] = \t 62\n",
      "tensor_out[126] = \t 176\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 9\n",
      "tensor_out[129] = \t 200\n",
      "tensor_out[130] = \t 64\n",
      "tensor_out[131] = \t 52\n",
      "tensor_out[132] = \t 186\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 63\n",
      "tensor_out[135] = \t 10\n",
      "tensor_out[136] = \t 35\n",
      "tensor_out[137] = \t 31\n",
      "tensor_out[138] = \t 34\n",
      "tensor_out[139] = \t 79\n",
      "tensor_out[140] = \t 231\n",
      "tensor_out[141] = \t 61\n",
      "tensor_out[142] = \t 0\n",
      "tensor_out[143] = \t 34\n",
      "tensor_out[144] = \t 176\n",
      "tensor_out[145] = \t 67\n",
      "tensor_out[146] = \t 52\n",
      "tensor_out[147] = \t 48\n",
      "tensor_out[148] = \t 0\n",
      "tensor_out[149] = \t 3\n",
      "tensor_out[150] = \t 7\n",
      "tensor_out[151] = \t 19\n",
      "tensor_out[152] = \t 8\n",
      "tensor_out[153] = \t 146\n",
      "tensor_out[154] = \t 133\n",
      "tensor_out[155] = \t 107\n",
      "tensor_out[156] = \t 0\n",
      "tensor_out[157] = \t 61\n",
      "tensor_out[158] = \t 31\n",
      "tensor_out[159] = \t 129\n",
      "tensor_out[160] = \t 146\n",
      "tensor_out[161] = \t 64\n",
      "tensor_out[162] = \t 17\n",
      "tensor_out[163] = \t 0\n",
      "tensor_out[164] = \t 33\n",
      "tensor_out[165] = \t 147\n",
      "tensor_out[166] = \t 118\n",
      "tensor_out[167] = \t 37\n",
      "tensor_out[168] = \t 173\n",
      "tensor_out[169] = \t 67\n",
      "tensor_out[170] = \t 105\n",
      "tensor_out[171] = \t 0\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 0\n",
      "tensor_out[174] = \t 52\n",
      "tensor_out[175] = \t 0\n",
      "tensor_out[176] = \t 101\n",
      "tensor_out[177] = \t 102\n",
      "tensor_out[178] = \t 64\n",
      "tensor_out[179] = \t 224\n",
      "tensor_out[180] = \t 67\n",
      "tensor_out[181] = \t 22\n",
      "tensor_out[182] = \t 115\n",
      "tensor_out[183] = \t 71\n",
      "tensor_out[184] = \t 1\n",
      "tensor_out[185] = \t 0\n",
      "tensor_out[186] = \t 82\n",
      "tensor_out[187] = \t 67\n",
      "tensor_out[188] = \t 0\n",
      "tensor_out[189] = \t 13\n",
      "tensor_out[190] = \t 0\n",
      "tensor_out[191] = \t 100\n",
      "tensor_out[192] = \t 0\n",
      "tensor_out[193] = \t 83\n",
      "tensor_out[194] = \t 116\n",
      "tensor_out[195] = \t 118\n",
      "tensor_out[196] = \t 0\n",
      "tensor_out[197] = \t 110\n",
      "tensor_out[198] = \t 5\n",
      "tensor_out[199] = \t 34\n",
      "tensor_out[200] = \t 0\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 116\n",
      "tensor_out[203] = \t 72\n",
      "tensor_out[204] = \t 1\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 50\n",
      "tensor_out[207] = \t 109\n",
      "tensor_out[208] = \t 88\n",
      "tensor_out[209] = \t 44\n",
      "tensor_out[210] = \t 10\n",
      "tensor_out[211] = \t 16\n",
      "tensor_out[212] = \t 33\n",
      "tensor_out[213] = \t 184\n",
      "tensor_out[214] = \t 24\n",
      "tensor_out[215] = \t 14\n",
      "tensor_out[216] = \t 206\n",
      "tensor_out[217] = \t 49\n",
      "tensor_out[218] = \t 37\n",
      "tensor_out[219] = \t 128\n",
      "tensor_out[220] = \t 63\n",
      "tensor_out[221] = \t 64\n",
      "tensor_out[222] = \t 112\n",
      "tensor_out[223] = \t 0\n",
      "tensor_out[224] = \t 0\n",
      "tensor_out[225] = \t 255\n",
      "tensor_out[226] = \t 0\n",
      "tensor_out[227] = \t 105\n",
      "tensor_out[228] = \t 76\n",
      "tensor_out[229] = \t 50\n",
      "tensor_out[230] = \t 52\n",
      "tensor_out[231] = \t 8\n",
      "tensor_out[232] = \t 0\n",
      "tensor_out[233] = \t 18\n",
      "tensor_out[234] = \t 154\n",
      "tensor_out[235] = \t 73\n",
      "tensor_out[236] = \t 67\n",
      "tensor_out[237] = \t 70\n",
      "tensor_out[238] = \t 115\n",
      "tensor_out[239] = \t 33\n",
      "tensor_out[240] = \t 77\n",
      "tensor_out[241] = \t 49\n",
      "tensor_out[242] = \t 0\n",
      "tensor_out[243] = \t 53\n",
      "tensor_out[244] = \t 99\n",
      "tensor_out[245] = \t 13\n",
      "tensor_out[246] = \t 76\n",
      "tensor_out[247] = \t 43\n",
      "tensor_out[248] = \t 67\n",
      "tensor_out[249] = \t 121\n",
      "tensor_out[250] = \t 26\n",
      "tensor_out[251] = \t 20\n",
      "tensor_out[252] = \t 159\n",
      "tensor_out[253] = \t 82\n",
      "tensor_out[254] = \t 82\n",
      "tensor_out[255] = \t 36\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer10 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0\n",
      "tensor_out[1] = \t 0\n",
      "tensor_out[2] = \t 72\n",
      "tensor_out[3] = \t 205\n",
      "tensor_out[4] = \t 0\n",
      "tensor_out[5] = \t 0\n",
      "tensor_out[6] = \t 37\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 29\n",
      "tensor_out[9] = \t 0\n",
      "tensor_out[10] = \t 0\n",
      "tensor_out[11] = \t 22\n",
      "tensor_out[12] = \t 0\n",
      "tensor_out[13] = \t 8\n",
      "tensor_out[14] = \t 0\n",
      "tensor_out[15] = \t 99\n",
      "tensor_out[16] = \t 56\n",
      "tensor_out[17] = \t 106\n",
      "tensor_out[18] = \t 89\n",
      "tensor_out[19] = \t 244\n",
      "tensor_out[20] = \t 0\n",
      "tensor_out[21] = \t 5\n",
      "tensor_out[22] = \t 6\n",
      "tensor_out[23] = \t 80\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 0\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 35\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 150\n",
      "tensor_out[30] = \t 81\n",
      "tensor_out[31] = \t 96\n",
      "tensor_out[32] = \t 172\n",
      "tensor_out[33] = \t 95\n",
      "tensor_out[34] = \t 122\n",
      "tensor_out[35] = \t 14\n",
      "tensor_out[36] = \t 13\n",
      "tensor_out[37] = \t 0\n",
      "tensor_out[38] = \t 57\n",
      "tensor_out[39] = \t 121\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 43\n",
      "tensor_out[43] = \t 75\n",
      "tensor_out[44] = \t 97\n",
      "tensor_out[45] = \t 0\n",
      "tensor_out[46] = \t 62\n",
      "tensor_out[47] = \t 23\n",
      "tensor_out[48] = \t 130\n",
      "tensor_out[49] = \t 120\n",
      "tensor_out[50] = \t 0\n",
      "tensor_out[51] = \t 0\n",
      "tensor_out[52] = \t 254\n",
      "tensor_out[53] = \t 64\n",
      "tensor_out[54] = \t 36\n",
      "tensor_out[55] = \t 50\n",
      "tensor_out[56] = \t 61\n",
      "tensor_out[57] = \t 39\n",
      "tensor_out[58] = \t 255\n",
      "tensor_out[59] = \t 195\n",
      "tensor_out[60] = \t 0\n",
      "tensor_out[61] = \t 28\n",
      "tensor_out[62] = \t 4\n",
      "tensor_out[63] = \t 12\n",
      "tensor_out[64] = \t 7\n",
      "tensor_out[65] = \t 0\n",
      "tensor_out[66] = \t 0\n",
      "tensor_out[67] = \t 28\n",
      "tensor_out[68] = \t 38\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 8\n",
      "tensor_out[71] = \t 0\n",
      "tensor_out[72] = \t 16\n",
      "tensor_out[73] = \t 71\n",
      "tensor_out[74] = \t 255\n",
      "tensor_out[75] = \t 0\n",
      "tensor_out[76] = \t 62\n",
      "tensor_out[77] = \t 215\n",
      "tensor_out[78] = \t 15\n",
      "tensor_out[79] = \t 255\n",
      "tensor_out[80] = \t 0\n",
      "tensor_out[81] = \t 71\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 0\n",
      "tensor_out[84] = \t 116\n",
      "tensor_out[85] = \t 0\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 50\n",
      "tensor_out[88] = \t 0\n",
      "tensor_out[89] = \t 60\n",
      "tensor_out[90] = \t 88\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 0\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 255\n",
      "tensor_out[95] = \t 0\n",
      "tensor_out[96] = \t 108\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 53\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 27\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 0\n",
      "tensor_out[103] = \t 0\n",
      "tensor_out[104] = \t 0\n",
      "tensor_out[105] = \t 16\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 0\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 255\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 255\n",
      "tensor_out[112] = \t 6\n",
      "tensor_out[113] = \t 16\n",
      "tensor_out[114] = \t 0\n",
      "tensor_out[115] = \t 52\n",
      "tensor_out[116] = \t 179\n",
      "tensor_out[117] = \t 0\n",
      "tensor_out[118] = \t 132\n",
      "tensor_out[119] = \t 0\n",
      "tensor_out[120] = \t 2\n",
      "tensor_out[121] = \t 72\n",
      "tensor_out[122] = \t 96\n",
      "tensor_out[123] = \t 0\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 44\n",
      "tensor_out[126] = \t 89\n",
      "tensor_out[127] = \t 84\n",
      "tensor_out[128] = \t 0\n",
      "tensor_out[129] = \t 0\n",
      "tensor_out[130] = \t 148\n",
      "tensor_out[131] = \t 84\n",
      "tensor_out[132] = \t 241\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 100\n",
      "tensor_out[135] = \t 60\n",
      "tensor_out[136] = \t 9\n",
      "tensor_out[137] = \t 7\n",
      "tensor_out[138] = \t 22\n",
      "tensor_out[139] = \t 0\n",
      "tensor_out[140] = \t 0\n",
      "tensor_out[141] = \t 74\n",
      "tensor_out[142] = \t 0\n",
      "tensor_out[143] = \t 208\n",
      "tensor_out[144] = \t 255\n",
      "tensor_out[145] = \t 59\n",
      "tensor_out[146] = \t 33\n",
      "tensor_out[147] = \t 45\n",
      "tensor_out[148] = \t 10\n",
      "tensor_out[149] = \t 77\n",
      "tensor_out[150] = \t 0\n",
      "tensor_out[151] = \t 7\n",
      "tensor_out[152] = \t 55\n",
      "tensor_out[153] = \t 156\n",
      "tensor_out[154] = \t 115\n",
      "tensor_out[155] = \t 45\n",
      "tensor_out[156] = \t 0\n",
      "tensor_out[157] = \t 75\n",
      "tensor_out[158] = \t 87\n",
      "tensor_out[159] = \t 52\n",
      "tensor_out[160] = \t 1\n",
      "tensor_out[161] = \t 89\n",
      "tensor_out[162] = \t 0\n",
      "tensor_out[163] = \t 0\n",
      "tensor_out[164] = \t 0\n",
      "tensor_out[165] = \t 0\n",
      "tensor_out[166] = \t 0\n",
      "tensor_out[167] = \t 0\n",
      "tensor_out[168] = \t 0\n",
      "tensor_out[169] = \t 113\n",
      "tensor_out[170] = \t 0\n",
      "tensor_out[171] = \t 0\n",
      "tensor_out[172] = \t 60\n",
      "tensor_out[173] = \t 0\n",
      "tensor_out[174] = \t 27\n",
      "tensor_out[175] = \t 53\n",
      "tensor_out[176] = \t 17\n",
      "tensor_out[177] = \t 0\n",
      "tensor_out[178] = \t 0\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 55\n",
      "tensor_out[181] = \t 48\n",
      "tensor_out[182] = \t 149\n",
      "tensor_out[183] = \t 71\n",
      "tensor_out[184] = \t 0\n",
      "tensor_out[185] = \t 0\n",
      "tensor_out[186] = \t 0\n",
      "tensor_out[187] = \t 28\n",
      "tensor_out[188] = \t 0\n",
      "tensor_out[189] = \t 0\n",
      "tensor_out[190] = \t 0\n",
      "tensor_out[191] = \t 0\n",
      "tensor_out[192] = \t 0\n",
      "tensor_out[193] = \t 69\n",
      "tensor_out[194] = \t 150\n",
      "tensor_out[195] = \t 185\n",
      "tensor_out[196] = \t 91\n",
      "tensor_out[197] = \t 231\n",
      "tensor_out[198] = \t 7\n",
      "tensor_out[199] = \t 37\n",
      "tensor_out[200] = \t 41\n",
      "tensor_out[201] = \t 3\n",
      "tensor_out[202] = \t 0\n",
      "tensor_out[203] = \t 18\n",
      "tensor_out[204] = \t 131\n",
      "tensor_out[205] = \t 55\n",
      "tensor_out[206] = \t 0\n",
      "tensor_out[207] = \t 90\n",
      "tensor_out[208] = \t 0\n",
      "tensor_out[209] = \t 148\n",
      "tensor_out[210] = \t 0\n",
      "tensor_out[211] = \t 80\n",
      "tensor_out[212] = \t 5\n",
      "tensor_out[213] = \t 236\n",
      "tensor_out[214] = \t 0\n",
      "tensor_out[215] = \t 19\n",
      "tensor_out[216] = \t 0\n",
      "tensor_out[217] = \t 0\n",
      "tensor_out[218] = \t 0\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 186\n",
      "tensor_out[221] = \t 23\n",
      "tensor_out[222] = \t 167\n",
      "tensor_out[223] = \t 51\n",
      "tensor_out[224] = \t 54\n",
      "tensor_out[225] = \t 255\n",
      "tensor_out[226] = \t 45\n",
      "tensor_out[227] = \t 7\n",
      "tensor_out[228] = \t 38\n",
      "tensor_out[229] = \t 112\n",
      "tensor_out[230] = \t 53\n",
      "tensor_out[231] = \t 0\n",
      "tensor_out[232] = \t 0\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 214\n",
      "tensor_out[235] = \t 63\n",
      "tensor_out[236] = \t 0\n",
      "tensor_out[237] = \t 48\n",
      "tensor_out[238] = \t 174\n",
      "tensor_out[239] = \t 76\n",
      "tensor_out[240] = \t 1\n",
      "tensor_out[241] = \t 0\n",
      "tensor_out[242] = \t 44\n",
      "tensor_out[243] = \t 0\n",
      "tensor_out[244] = \t 156\n",
      "tensor_out[245] = \t 0\n",
      "tensor_out[246] = \t 23\n",
      "tensor_out[247] = \t 45\n",
      "tensor_out[248] = \t 40\n",
      "tensor_out[249] = \t 0\n",
      "tensor_out[250] = \t 0\n",
      "tensor_out[251] = \t 97\n",
      "tensor_out[252] = \t 157\n",
      "tensor_out[253] = \t 79\n",
      "tensor_out[254] = \t 146\n",
      "tensor_out[255] = \t 123\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer11 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 56\n",
      "tensor_out[1] = \t 0\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 8\n",
      "tensor_out[5] = \t 64\n",
      "tensor_out[6] = \t 49\n",
      "tensor_out[7] = \t 58\n",
      "tensor_out[8] = \t 82\n",
      "tensor_out[9] = \t 61\n",
      "tensor_out[10] = \t 0\n",
      "tensor_out[11] = \t 64\n",
      "tensor_out[12] = \t 67\n",
      "tensor_out[13] = \t 90\n",
      "tensor_out[14] = \t 0\n",
      "tensor_out[15] = \t 0\n",
      "tensor_out[16] = \t 165\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 0\n",
      "tensor_out[19] = \t 128\n",
      "tensor_out[20] = \t 64\n",
      "tensor_out[21] = \t 96\n",
      "tensor_out[22] = \t 0\n",
      "tensor_out[23] = \t 0\n",
      "tensor_out[24] = \t 102\n",
      "tensor_out[25] = \t 0\n",
      "tensor_out[26] = \t 47\n",
      "tensor_out[27] = \t 55\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 58\n",
      "tensor_out[30] = \t 0\n",
      "tensor_out[31] = \t 0\n",
      "tensor_out[32] = \t 51\n",
      "tensor_out[33] = \t 191\n",
      "tensor_out[34] = \t 0\n",
      "tensor_out[35] = \t 86\n",
      "tensor_out[36] = \t 0\n",
      "tensor_out[37] = \t 255\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 7\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 71\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 19\n",
      "tensor_out[45] = \t 100\n",
      "tensor_out[46] = \t 0\n",
      "tensor_out[47] = \t 60\n",
      "tensor_out[48] = \t 0\n",
      "tensor_out[49] = \t 47\n",
      "tensor_out[50] = \t 50\n",
      "tensor_out[51] = \t 85\n",
      "tensor_out[52] = \t 189\n",
      "tensor_out[53] = \t 98\n",
      "tensor_out[54] = \t 0\n",
      "tensor_out[55] = \t 0\n",
      "tensor_out[56] = \t 36\n",
      "tensor_out[57] = \t 0\n",
      "tensor_out[58] = \t 0\n",
      "tensor_out[59] = \t 42\n",
      "tensor_out[60] = \t 24\n",
      "tensor_out[61] = \t 14\n",
      "tensor_out[62] = \t 255\n",
      "tensor_out[63] = \t 0\n",
      "tensor_out[64] = \t 50\n",
      "tensor_out[65] = \t 80\n",
      "tensor_out[66] = \t 117\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 119\n",
      "tensor_out[69] = \t 7\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 28\n",
      "tensor_out[72] = \t 40\n",
      "tensor_out[73] = \t 0\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 0\n",
      "tensor_out[76] = \t 0\n",
      "tensor_out[77] = \t 26\n",
      "tensor_out[78] = \t 0\n",
      "tensor_out[79] = \t 0\n",
      "tensor_out[80] = \t 4\n",
      "tensor_out[81] = \t 0\n",
      "tensor_out[82] = \t 109\n",
      "tensor_out[83] = \t 0\n",
      "tensor_out[84] = \t 8\n",
      "tensor_out[85] = \t 0\n",
      "tensor_out[86] = \t 86\n",
      "tensor_out[87] = \t 0\n",
      "tensor_out[88] = \t 83\n",
      "tensor_out[89] = \t 74\n",
      "tensor_out[90] = \t 0\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 71\n",
      "tensor_out[93] = \t 103\n",
      "tensor_out[94] = \t 131\n",
      "tensor_out[95] = \t 0\n",
      "tensor_out[96] = \t 255\n",
      "tensor_out[97] = \t 5\n",
      "tensor_out[98] = \t 42\n",
      "tensor_out[99] = \t 15\n",
      "tensor_out[100] = \t 0\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 78\n",
      "tensor_out[103] = \t 156\n",
      "tensor_out[104] = \t 84\n",
      "tensor_out[105] = \t 255\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 64\n",
      "tensor_out[108] = \t 2\n",
      "tensor_out[109] = \t 109\n",
      "tensor_out[110] = \t 161\n",
      "tensor_out[111] = \t 151\n",
      "tensor_out[112] = \t 24\n",
      "tensor_out[113] = \t 0\n",
      "tensor_out[114] = \t 2\n",
      "tensor_out[115] = \t 37\n",
      "tensor_out[116] = \t 0\n",
      "tensor_out[117] = \t 0\n",
      "tensor_out[118] = \t 126\n",
      "tensor_out[119] = \t 92\n",
      "tensor_out[120] = \t 131\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 0\n",
      "tensor_out[123] = \t 213\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 0\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 15\n",
      "tensor_out[129] = \t 47\n",
      "tensor_out[130] = \t 0\n",
      "tensor_out[131] = \t 6\n",
      "tensor_out[132] = \t 33\n",
      "tensor_out[133] = \t 20\n",
      "tensor_out[134] = \t 0\n",
      "tensor_out[135] = \t 0\n",
      "tensor_out[136] = \t 6\n",
      "tensor_out[137] = \t 255\n",
      "tensor_out[138] = \t 57\n",
      "tensor_out[139] = \t 123\n",
      "tensor_out[140] = \t 97\n",
      "tensor_out[141] = \t 163\n",
      "tensor_out[142] = \t 7\n",
      "tensor_out[143] = \t 69\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 0\n",
      "tensor_out[146] = \t 158\n",
      "tensor_out[147] = \t 0\n",
      "tensor_out[148] = \t 8\n",
      "tensor_out[149] = \t 0\n",
      "tensor_out[150] = \t 0\n",
      "tensor_out[151] = \t 247\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 255\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 37\n",
      "tensor_out[156] = \t 141\n",
      "tensor_out[157] = \t 116\n",
      "tensor_out[158] = \t 15\n",
      "tensor_out[159] = \t 105\n",
      "tensor_out[160] = \t 0\n",
      "tensor_out[161] = \t 0\n",
      "tensor_out[162] = \t 12\n",
      "tensor_out[163] = \t 0\n",
      "tensor_out[164] = \t 154\n",
      "tensor_out[165] = \t 78\n",
      "tensor_out[166] = \t 167\n",
      "tensor_out[167] = \t 0\n",
      "tensor_out[168] = \t 131\n",
      "tensor_out[169] = \t 0\n",
      "tensor_out[170] = \t 48\n",
      "tensor_out[171] = \t 255\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 1\n",
      "tensor_out[174] = \t 0\n",
      "tensor_out[175] = \t 111\n",
      "tensor_out[176] = \t 0\n",
      "tensor_out[177] = \t 92\n",
      "tensor_out[178] = \t 132\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 25\n",
      "tensor_out[181] = \t 111\n",
      "tensor_out[182] = \t 121\n",
      "tensor_out[183] = \t 181\n",
      "tensor_out[184] = \t 0\n",
      "tensor_out[185] = \t 0\n",
      "tensor_out[186] = \t 0\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 42\n",
      "tensor_out[189] = \t 34\n",
      "tensor_out[190] = \t 0\n",
      "tensor_out[191] = \t 61\n",
      "tensor_out[192] = \t 0\n",
      "tensor_out[193] = \t 22\n",
      "tensor_out[194] = \t 87\n",
      "tensor_out[195] = \t 0\n",
      "tensor_out[196] = \t 111\n",
      "tensor_out[197] = \t 0\n",
      "tensor_out[198] = \t 0\n",
      "tensor_out[199] = \t 0\n",
      "tensor_out[200] = \t 90\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 105\n",
      "tensor_out[203] = \t 130\n",
      "tensor_out[204] = \t 51\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 27\n",
      "tensor_out[207] = \t 60\n",
      "tensor_out[208] = \t 0\n",
      "tensor_out[209] = \t 0\n",
      "tensor_out[210] = \t 0\n",
      "tensor_out[211] = \t 203\n",
      "tensor_out[212] = \t 10\n",
      "tensor_out[213] = \t 186\n",
      "tensor_out[214] = \t 0\n",
      "tensor_out[215] = \t 66\n",
      "tensor_out[216] = \t 1\n",
      "tensor_out[217] = \t 186\n",
      "tensor_out[218] = \t 42\n",
      "tensor_out[219] = \t 71\n",
      "tensor_out[220] = \t 102\n",
      "tensor_out[221] = \t 80\n",
      "tensor_out[222] = \t 150\n",
      "tensor_out[223] = \t 72\n",
      "tensor_out[224] = \t 255\n",
      "tensor_out[225] = \t 0\n",
      "tensor_out[226] = \t 0\n",
      "tensor_out[227] = \t 41\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 0\n",
      "tensor_out[231] = \t 157\n",
      "tensor_out[232] = \t 71\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 148\n",
      "tensor_out[235] = \t 0\n",
      "tensor_out[236] = \t 62\n",
      "tensor_out[237] = \t 0\n",
      "tensor_out[238] = \t 83\n",
      "tensor_out[239] = \t 0\n",
      "tensor_out[240] = \t 103\n",
      "tensor_out[241] = \t 0\n",
      "tensor_out[242] = \t 0\n",
      "tensor_out[243] = \t 64\n",
      "tensor_out[244] = \t 0\n",
      "tensor_out[245] = \t 10\n",
      "tensor_out[246] = \t 32\n",
      "tensor_out[247] = \t 0\n",
      "tensor_out[248] = \t 129\n",
      "tensor_out[249] = \t 0\n",
      "tensor_out[250] = \t 0\n",
      "tensor_out[251] = \t 1\n",
      "tensor_out[252] = \t 0\n",
      "tensor_out[253] = \t 0\n",
      "tensor_out[254] = \t 157\n",
      "tensor_out[255] = \t 103\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer12 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 51\n",
      "tensor_out[1] = \t 56\n",
      "tensor_out[2] = \t 22\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 13\n",
      "tensor_out[5] = \t 0\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 66\n",
      "tensor_out[8] = \t 0\n",
      "tensor_out[9] = \t 57\n",
      "tensor_out[10] = \t 179\n",
      "tensor_out[11] = \t 95\n",
      "tensor_out[12] = \t 110\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 117\n",
      "tensor_out[15] = \t 171\n",
      "tensor_out[16] = \t 22\n",
      "tensor_out[17] = \t 177\n",
      "tensor_out[18] = \t 63\n",
      "tensor_out[19] = \t 114\n",
      "tensor_out[20] = \t 0\n",
      "tensor_out[21] = \t 6\n",
      "tensor_out[22] = \t 3\n",
      "tensor_out[23] = \t 8\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 0\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 37\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 85\n",
      "tensor_out[30] = \t 4\n",
      "tensor_out[31] = \t 140\n",
      "tensor_out[32] = \t 85\n",
      "tensor_out[33] = \t 0\n",
      "tensor_out[34] = \t 76\n",
      "tensor_out[35] = \t 30\n",
      "tensor_out[36] = \t 6\n",
      "tensor_out[37] = \t 0\n",
      "tensor_out[38] = \t 7\n",
      "tensor_out[39] = \t 58\n",
      "tensor_out[40] = \t 7\n",
      "tensor_out[41] = \t 121\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 71\n",
      "tensor_out[45] = \t 119\n",
      "tensor_out[46] = \t 102\n",
      "tensor_out[47] = \t 6\n",
      "tensor_out[48] = \t 16\n",
      "tensor_out[49] = \t 0\n",
      "tensor_out[50] = \t 116\n",
      "tensor_out[51] = \t 68\n",
      "tensor_out[52] = \t 0\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 26\n",
      "tensor_out[55] = \t 146\n",
      "tensor_out[56] = \t 24\n",
      "tensor_out[57] = \t 0\n",
      "tensor_out[58] = \t 10\n",
      "tensor_out[59] = \t 8\n",
      "tensor_out[60] = \t 32\n",
      "tensor_out[61] = \t 14\n",
      "tensor_out[62] = \t 0\n",
      "tensor_out[63] = \t 85\n",
      "tensor_out[64] = \t 93\n",
      "tensor_out[65] = \t 0\n",
      "tensor_out[66] = \t 131\n",
      "tensor_out[67] = \t 22\n",
      "tensor_out[68] = \t 89\n",
      "tensor_out[69] = \t 61\n",
      "tensor_out[70] = \t 15\n",
      "tensor_out[71] = \t 21\n",
      "tensor_out[72] = \t 53\n",
      "tensor_out[73] = \t 0\n",
      "tensor_out[74] = \t 14\n",
      "tensor_out[75] = \t 5\n",
      "tensor_out[76] = \t 0\n",
      "tensor_out[77] = \t 37\n",
      "tensor_out[78] = \t 0\n",
      "tensor_out[79] = \t 29\n",
      "tensor_out[80] = \t 115\n",
      "tensor_out[81] = \t 104\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 71\n",
      "tensor_out[84] = \t 3\n",
      "tensor_out[85] = \t 0\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 38\n",
      "tensor_out[88] = \t 0\n",
      "tensor_out[89] = \t 0\n",
      "tensor_out[90] = \t 54\n",
      "tensor_out[91] = \t 8\n",
      "tensor_out[92] = \t 121\n",
      "tensor_out[93] = \t 94\n",
      "tensor_out[94] = \t 0\n",
      "tensor_out[95] = \t 28\n",
      "tensor_out[96] = \t 0\n",
      "tensor_out[97] = \t 27\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 32\n",
      "tensor_out[100] = \t 79\n",
      "tensor_out[101] = \t 19\n",
      "tensor_out[102] = \t 24\n",
      "tensor_out[103] = \t 92\n",
      "tensor_out[104] = \t 20\n",
      "tensor_out[105] = \t 255\n",
      "tensor_out[106] = \t 22\n",
      "tensor_out[107] = \t 75\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 0\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 0\n",
      "tensor_out[112] = \t 155\n",
      "tensor_out[113] = \t 4\n",
      "tensor_out[114] = \t 4\n",
      "tensor_out[115] = \t 60\n",
      "tensor_out[116] = \t 0\n",
      "tensor_out[117] = \t 4\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 5\n",
      "tensor_out[120] = \t 90\n",
      "tensor_out[121] = \t 89\n",
      "tensor_out[122] = \t 90\n",
      "tensor_out[123] = \t 38\n",
      "tensor_out[124] = \t 63\n",
      "tensor_out[125] = \t 0\n",
      "tensor_out[126] = \t 65\n",
      "tensor_out[127] = \t 7\n",
      "tensor_out[128] = \t 21\n",
      "tensor_out[129] = \t 18\n",
      "tensor_out[130] = \t 75\n",
      "tensor_out[131] = \t 173\n",
      "tensor_out[132] = \t 47\n",
      "tensor_out[133] = \t 16\n",
      "tensor_out[134] = \t 103\n",
      "tensor_out[135] = \t 19\n",
      "tensor_out[136] = \t 13\n",
      "tensor_out[137] = \t 240\n",
      "tensor_out[138] = \t 11\n",
      "tensor_out[139] = \t 0\n",
      "tensor_out[140] = \t 0\n",
      "tensor_out[141] = \t 0\n",
      "tensor_out[142] = \t 85\n",
      "tensor_out[143] = \t 43\n",
      "tensor_out[144] = \t 97\n",
      "tensor_out[145] = \t 33\n",
      "tensor_out[146] = \t 70\n",
      "tensor_out[147] = \t 7\n",
      "tensor_out[148] = \t 122\n",
      "tensor_out[149] = \t 0\n",
      "tensor_out[150] = \t 0\n",
      "tensor_out[151] = \t 59\n",
      "tensor_out[152] = \t 109\n",
      "tensor_out[153] = \t 0\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 38\n",
      "tensor_out[156] = \t 0\n",
      "tensor_out[157] = \t 72\n",
      "tensor_out[158] = \t 87\n",
      "tensor_out[159] = \t 14\n",
      "tensor_out[160] = \t 47\n",
      "tensor_out[161] = \t 29\n",
      "tensor_out[162] = \t 25\n",
      "tensor_out[163] = \t 1\n",
      "tensor_out[164] = \t 211\n",
      "tensor_out[165] = \t 29\n",
      "tensor_out[166] = \t 0\n",
      "tensor_out[167] = \t 13\n",
      "tensor_out[168] = \t 0\n",
      "tensor_out[169] = \t 0\n",
      "tensor_out[170] = \t 139\n",
      "tensor_out[171] = \t 255\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 0\n",
      "tensor_out[174] = \t 12\n",
      "tensor_out[175] = \t 53\n",
      "tensor_out[176] = \t 85\n",
      "tensor_out[177] = \t 0\n",
      "tensor_out[178] = \t 143\n",
      "tensor_out[179] = \t 105\n",
      "tensor_out[180] = \t 79\n",
      "tensor_out[181] = \t 0\n",
      "tensor_out[182] = \t 0\n",
      "tensor_out[183] = \t 221\n",
      "tensor_out[184] = \t 40\n",
      "tensor_out[185] = \t 0\n",
      "tensor_out[186] = \t 59\n",
      "tensor_out[187] = \t 5\n",
      "tensor_out[188] = \t 80\n",
      "tensor_out[189] = \t 0\n",
      "tensor_out[190] = \t 32\n",
      "tensor_out[191] = \t 29\n",
      "tensor_out[192] = \t 47\n",
      "tensor_out[193] = \t 106\n",
      "tensor_out[194] = \t 98\n",
      "tensor_out[195] = \t 125\n",
      "tensor_out[196] = \t 56\n",
      "tensor_out[197] = \t 108\n",
      "tensor_out[198] = \t 21\n",
      "tensor_out[199] = \t 81\n",
      "tensor_out[200] = \t 0\n",
      "tensor_out[201] = \t 8\n",
      "tensor_out[202] = \t 76\n",
      "tensor_out[203] = \t 0\n",
      "tensor_out[204] = \t 169\n",
      "tensor_out[205] = \t 96\n",
      "tensor_out[206] = \t 126\n",
      "tensor_out[207] = \t 48\n",
      "tensor_out[208] = \t 0\n",
      "tensor_out[209] = \t 6\n",
      "tensor_out[210] = \t 72\n",
      "tensor_out[211] = \t 153\n",
      "tensor_out[212] = \t 18\n",
      "tensor_out[213] = \t 0\n",
      "tensor_out[214] = \t 46\n",
      "tensor_out[215] = \t 21\n",
      "tensor_out[216] = \t 0\n",
      "tensor_out[217] = \t 252\n",
      "tensor_out[218] = \t 160\n",
      "tensor_out[219] = \t 225\n",
      "tensor_out[220] = \t 0\n",
      "tensor_out[221] = \t 0\n",
      "tensor_out[222] = \t 81\n",
      "tensor_out[223] = \t 202\n",
      "tensor_out[224] = \t 75\n",
      "tensor_out[225] = \t 109\n",
      "tensor_out[226] = \t 0\n",
      "tensor_out[227] = \t 49\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 10\n",
      "tensor_out[230] = \t 15\n",
      "tensor_out[231] = \t 0\n",
      "tensor_out[232] = \t 0\n",
      "tensor_out[233] = \t 1\n",
      "tensor_out[234] = \t 73\n",
      "tensor_out[235] = \t 93\n",
      "tensor_out[236] = \t 1\n",
      "tensor_out[237] = \t 86\n",
      "tensor_out[238] = \t 0\n",
      "tensor_out[239] = \t 9\n",
      "tensor_out[240] = \t 170\n",
      "tensor_out[241] = \t 0\n",
      "tensor_out[242] = \t 5\n",
      "tensor_out[243] = \t 112\n",
      "tensor_out[244] = \t 108\n",
      "tensor_out[245] = \t 39\n",
      "tensor_out[246] = \t 0\n",
      "tensor_out[247] = \t 8\n",
      "tensor_out[248] = \t 143\n",
      "tensor_out[249] = \t 191\n",
      "tensor_out[250] = \t 0\n",
      "tensor_out[251] = \t 35\n",
      "tensor_out[252] = \t 54\n",
      "tensor_out[253] = \t 8\n",
      "tensor_out[254] = \t 0\n",
      "tensor_out[255] = \t 143\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer13 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0\n",
      "tensor_out[1] = \t 159\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 154\n",
      "tensor_out[5] = \t 106\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 114\n",
      "tensor_out[9] = \t 118\n",
      "tensor_out[10] = \t 225\n",
      "tensor_out[11] = \t 213\n",
      "tensor_out[12] = \t 86\n",
      "tensor_out[13] = \t 84\n",
      "tensor_out[14] = \t 59\n",
      "tensor_out[15] = \t 107\n",
      "tensor_out[16] = \t 52\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 8\n",
      "tensor_out[19] = \t 50\n",
      "tensor_out[20] = \t 110\n",
      "tensor_out[21] = \t 255\n",
      "tensor_out[22] = \t 0\n",
      "tensor_out[23] = \t 0\n",
      "tensor_out[24] = \t 126\n",
      "tensor_out[25] = \t 70\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 58\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 0\n",
      "tensor_out[30] = \t 38\n",
      "tensor_out[31] = \t 12\n",
      "tensor_out[32] = \t 23\n",
      "tensor_out[33] = \t 0\n",
      "tensor_out[34] = \t 0\n",
      "tensor_out[35] = \t 87\n",
      "tensor_out[36] = \t 18\n",
      "tensor_out[37] = \t 0\n",
      "tensor_out[38] = \t 122\n",
      "tensor_out[39] = \t 33\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 23\n",
      "tensor_out[42] = \t 34\n",
      "tensor_out[43] = \t 41\n",
      "tensor_out[44] = \t 149\n",
      "tensor_out[45] = \t 0\n",
      "tensor_out[46] = \t 174\n",
      "tensor_out[47] = \t 69\n",
      "tensor_out[48] = \t 41\n",
      "tensor_out[49] = \t 93\n",
      "tensor_out[50] = \t 73\n",
      "tensor_out[51] = \t 0\n",
      "tensor_out[52] = \t 62\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 95\n",
      "tensor_out[55] = \t 0\n",
      "tensor_out[56] = \t 0\n",
      "tensor_out[57] = \t 46\n",
      "tensor_out[58] = \t 123\n",
      "tensor_out[59] = \t 50\n",
      "tensor_out[60] = \t 61\n",
      "tensor_out[61] = \t 65\n",
      "tensor_out[62] = \t 19\n",
      "tensor_out[63] = \t 0\n",
      "tensor_out[64] = \t 76\n",
      "tensor_out[65] = \t 39\n",
      "tensor_out[66] = \t 38\n",
      "tensor_out[67] = \t 61\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 107\n",
      "tensor_out[70] = \t 82\n",
      "tensor_out[71] = \t 0\n",
      "tensor_out[72] = \t 125\n",
      "tensor_out[73] = \t 13\n",
      "tensor_out[74] = \t 129\n",
      "tensor_out[75] = \t 56\n",
      "tensor_out[76] = \t 6\n",
      "tensor_out[77] = \t 124\n",
      "tensor_out[78] = \t 125\n",
      "tensor_out[79] = \t 0\n",
      "tensor_out[80] = \t 2\n",
      "tensor_out[81] = \t 45\n",
      "tensor_out[82] = \t 105\n",
      "tensor_out[83] = \t 255\n",
      "tensor_out[84] = \t 255\n",
      "tensor_out[85] = \t 136\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 35\n",
      "tensor_out[88] = \t 55\n",
      "tensor_out[89] = \t 143\n",
      "tensor_out[90] = \t 5\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 36\n",
      "tensor_out[93] = \t 169\n",
      "tensor_out[94] = \t 161\n",
      "tensor_out[95] = \t 0\n",
      "tensor_out[96] = \t 37\n",
      "tensor_out[97] = \t 167\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 99\n",
      "tensor_out[101] = \t 255\n",
      "tensor_out[102] = \t 194\n",
      "tensor_out[103] = \t 4\n",
      "tensor_out[104] = \t 0\n",
      "tensor_out[105] = \t 146\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 0\n",
      "tensor_out[108] = \t 62\n",
      "tensor_out[109] = \t 22\n",
      "tensor_out[110] = \t 2\n",
      "tensor_out[111] = \t 26\n",
      "tensor_out[112] = \t 0\n",
      "tensor_out[113] = \t 0\n",
      "tensor_out[114] = \t 54\n",
      "tensor_out[115] = \t 158\n",
      "tensor_out[116] = \t 2\n",
      "tensor_out[117] = \t 67\n",
      "tensor_out[118] = \t 104\n",
      "tensor_out[119] = \t 27\n",
      "tensor_out[120] = \t 240\n",
      "tensor_out[121] = \t 78\n",
      "tensor_out[122] = \t 24\n",
      "tensor_out[123] = \t 134\n",
      "tensor_out[124] = \t 87\n",
      "tensor_out[125] = \t 2\n",
      "tensor_out[126] = \t 8\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 0\n",
      "tensor_out[129] = \t 0\n",
      "tensor_out[130] = \t 0\n",
      "tensor_out[131] = \t 0\n",
      "tensor_out[132] = \t 19\n",
      "tensor_out[133] = \t 64\n",
      "tensor_out[134] = \t 0\n",
      "tensor_out[135] = \t 48\n",
      "tensor_out[136] = \t 48\n",
      "tensor_out[137] = \t 72\n",
      "tensor_out[138] = \t 130\n",
      "tensor_out[139] = \t 41\n",
      "tensor_out[140] = \t 73\n",
      "tensor_out[141] = \t 106\n",
      "tensor_out[142] = \t 119\n",
      "tensor_out[143] = \t 0\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 80\n",
      "tensor_out[146] = \t 43\n",
      "tensor_out[147] = \t 41\n",
      "tensor_out[148] = \t 0\n",
      "tensor_out[149] = \t 0\n",
      "tensor_out[150] = \t 14\n",
      "tensor_out[151] = \t 51\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 140\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 0\n",
      "tensor_out[156] = \t 36\n",
      "tensor_out[157] = \t 43\n",
      "tensor_out[158] = \t 76\n",
      "tensor_out[159] = \t 33\n",
      "tensor_out[160] = \t 0\n",
      "tensor_out[161] = \t 133\n",
      "tensor_out[162] = \t 39\n",
      "tensor_out[163] = \t 253\n",
      "tensor_out[164] = \t 103\n",
      "tensor_out[165] = \t 181\n",
      "tensor_out[166] = \t 10\n",
      "tensor_out[167] = \t 112\n",
      "tensor_out[168] = \t 188\n",
      "tensor_out[169] = \t 96\n",
      "tensor_out[170] = \t 38\n",
      "tensor_out[171] = \t 0\n",
      "tensor_out[172] = \t 98\n",
      "tensor_out[173] = \t 136\n",
      "tensor_out[174] = \t 0\n",
      "tensor_out[175] = \t 17\n",
      "tensor_out[176] = \t 110\n",
      "tensor_out[177] = \t 0\n",
      "tensor_out[178] = \t 89\n",
      "tensor_out[179] = \t 164\n",
      "tensor_out[180] = \t 0\n",
      "tensor_out[181] = \t 60\n",
      "tensor_out[182] = \t 0\n",
      "tensor_out[183] = \t 44\n",
      "tensor_out[184] = \t 91\n",
      "tensor_out[185] = \t 0\n",
      "tensor_out[186] = \t 97\n",
      "tensor_out[187] = \t 28\n",
      "tensor_out[188] = \t 79\n",
      "tensor_out[189] = \t 91\n",
      "tensor_out[190] = \t 255\n",
      "tensor_out[191] = \t 0\n",
      "tensor_out[192] = \t 1\n",
      "tensor_out[193] = \t 70\n",
      "tensor_out[194] = \t 0\n",
      "tensor_out[195] = \t 0\n",
      "tensor_out[196] = \t 121\n",
      "tensor_out[197] = \t 98\n",
      "tensor_out[198] = \t 39\n",
      "tensor_out[199] = \t 65\n",
      "tensor_out[200] = \t 0\n",
      "tensor_out[201] = \t 128\n",
      "tensor_out[202] = \t 15\n",
      "tensor_out[203] = \t 62\n",
      "tensor_out[204] = \t 49\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 54\n",
      "tensor_out[207] = \t 0\n",
      "tensor_out[208] = \t 85\n",
      "tensor_out[209] = \t 109\n",
      "tensor_out[210] = \t 199\n",
      "tensor_out[211] = \t 90\n",
      "tensor_out[212] = \t 0\n",
      "tensor_out[213] = \t 0\n",
      "tensor_out[214] = \t 191\n",
      "tensor_out[215] = \t 82\n",
      "tensor_out[216] = \t 27\n",
      "tensor_out[217] = \t 140\n",
      "tensor_out[218] = \t 113\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 105\n",
      "tensor_out[221] = \t 0\n",
      "tensor_out[222] = \t 96\n",
      "tensor_out[223] = \t 0\n",
      "tensor_out[224] = \t 0\n",
      "tensor_out[225] = \t 33\n",
      "tensor_out[226] = \t 93\n",
      "tensor_out[227] = \t 24\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 57\n",
      "tensor_out[231] = \t 60\n",
      "tensor_out[232] = \t 177\n",
      "tensor_out[233] = \t 23\n",
      "tensor_out[234] = \t 0\n",
      "tensor_out[235] = \t 156\n",
      "tensor_out[236] = \t 41\n",
      "tensor_out[237] = \t 6\n",
      "tensor_out[238] = \t 70\n",
      "tensor_out[239] = \t 28\n",
      "tensor_out[240] = \t 85\n",
      "tensor_out[241] = \t 0\n",
      "tensor_out[242] = \t 0\n",
      "tensor_out[243] = \t 79\n",
      "tensor_out[244] = \t 81\n",
      "tensor_out[245] = \t 92\n",
      "tensor_out[246] = \t 135\n",
      "tensor_out[247] = \t 0\n",
      "tensor_out[248] = \t 0\n",
      "tensor_out[249] = \t 0\n",
      "tensor_out[250] = \t 255\n",
      "tensor_out[251] = \t 80\n",
      "tensor_out[252] = \t 0\n",
      "tensor_out[253] = \t 60\n",
      "tensor_out[254] = \t 63\n",
      "tensor_out[255] = \t 40\n",
      "tensor_out[256] = \t 26\n",
      "tensor_out[257] = \t 79\n",
      "tensor_out[258] = \t 1\n",
      "tensor_out[259] = \t 0\n",
      "tensor_out[260] = \t 104\n",
      "tensor_out[261] = \t 54\n",
      "tensor_out[262] = \t 24\n",
      "tensor_out[263] = \t 137\n",
      "tensor_out[264] = \t 128\n",
      "tensor_out[265] = \t 0\n",
      "tensor_out[266] = \t 82\n",
      "tensor_out[267] = \t 0\n",
      "tensor_out[268] = \t 62\n",
      "tensor_out[269] = \t 13\n",
      "tensor_out[270] = \t 24\n",
      "tensor_out[271] = \t 80\n",
      "tensor_out[272] = \t 0\n",
      "tensor_out[273] = \t 0\n",
      "tensor_out[274] = \t 255\n",
      "tensor_out[275] = \t 6\n",
      "tensor_out[276] = \t 231\n",
      "tensor_out[277] = \t 31\n",
      "tensor_out[278] = \t 71\n",
      "tensor_out[279] = \t 0\n",
      "tensor_out[280] = \t 17\n",
      "tensor_out[281] = \t 154\n",
      "tensor_out[282] = \t 0\n",
      "tensor_out[283] = \t 37\n",
      "tensor_out[284] = \t 0\n",
      "tensor_out[285] = \t 0\n",
      "tensor_out[286] = \t 79\n",
      "tensor_out[287] = \t 124\n",
      "tensor_out[288] = \t 64\n",
      "tensor_out[289] = \t 12\n",
      "tensor_out[290] = \t 127\n",
      "tensor_out[291] = \t 62\n",
      "tensor_out[292] = \t 0\n",
      "tensor_out[293] = \t 9\n",
      "tensor_out[294] = \t 15\n",
      "tensor_out[295] = \t 26\n",
      "tensor_out[296] = \t 164\n",
      "tensor_out[297] = \t 16\n",
      "tensor_out[298] = \t 134\n",
      "tensor_out[299] = \t 0\n",
      "tensor_out[300] = \t 41\n",
      "tensor_out[301] = \t 28\n",
      "tensor_out[302] = \t 255\n",
      "tensor_out[303] = \t 0\n",
      "tensor_out[304] = \t 82\n",
      "tensor_out[305] = \t 5\n",
      "tensor_out[306] = \t 0\n",
      "tensor_out[307] = \t 25\n",
      "tensor_out[308] = \t 0\n",
      "tensor_out[309] = \t 0\n",
      "tensor_out[310] = \t 207\n",
      "tensor_out[311] = \t 10\n",
      "tensor_out[312] = \t 35\n",
      "tensor_out[313] = \t 0\n",
      "tensor_out[314] = \t 0\n",
      "tensor_out[315] = \t 39\n",
      "tensor_out[316] = \t 73\n",
      "tensor_out[317] = \t 32\n",
      "tensor_out[318] = \t 0\n",
      "tensor_out[319] = \t 107\n",
      "tensor_out[320] = \t 0\n",
      "tensor_out[321] = \t 2\n",
      "tensor_out[322] = \t 44\n",
      "tensor_out[323] = \t 7\n",
      "tensor_out[324] = \t 104\n",
      "tensor_out[325] = \t 81\n",
      "tensor_out[326] = \t 41\n",
      "tensor_out[327] = \t 0\n",
      "tensor_out[328] = \t 0\n",
      "tensor_out[329] = \t 188\n",
      "tensor_out[330] = \t 83\n",
      "tensor_out[331] = \t 0\n",
      "tensor_out[332] = \t 39\n",
      "tensor_out[333] = \t 9\n",
      "tensor_out[334] = \t 144\n",
      "tensor_out[335] = \t 35\n",
      "tensor_out[336] = \t 40\n",
      "tensor_out[337] = \t 65\n",
      "tensor_out[338] = \t 0\n",
      "tensor_out[339] = \t 0\n",
      "tensor_out[340] = \t 197\n",
      "tensor_out[341] = \t 0\n",
      "tensor_out[342] = \t 6\n",
      "tensor_out[343] = \t 21\n",
      "tensor_out[344] = \t 120\n",
      "tensor_out[345] = \t 0\n",
      "tensor_out[346] = \t 158\n",
      "tensor_out[347] = \t 29\n",
      "tensor_out[348] = \t 77\n",
      "tensor_out[349] = \t 0\n",
      "tensor_out[350] = \t 53\n",
      "tensor_out[351] = \t 0\n",
      "tensor_out[352] = \t 68\n",
      "tensor_out[353] = \t 154\n",
      "tensor_out[354] = \t 31\n",
      "tensor_out[355] = \t 145\n",
      "tensor_out[356] = \t 154\n",
      "tensor_out[357] = \t 95\n",
      "tensor_out[358] = \t 37\n",
      "tensor_out[359] = \t 0\n",
      "tensor_out[360] = \t 51\n",
      "tensor_out[361] = \t 166\n",
      "tensor_out[362] = \t 0\n",
      "tensor_out[363] = \t 86\n",
      "tensor_out[364] = \t 45\n",
      "tensor_out[365] = \t 0\n",
      "tensor_out[366] = \t 58\n",
      "tensor_out[367] = \t 0\n",
      "tensor_out[368] = \t 0\n",
      "tensor_out[369] = \t 16\n",
      "tensor_out[370] = \t 24\n",
      "tensor_out[371] = \t 119\n",
      "tensor_out[372] = \t 0\n",
      "tensor_out[373] = \t 182\n",
      "tensor_out[374] = \t 65\n",
      "tensor_out[375] = \t 63\n",
      "tensor_out[376] = \t 0\n",
      "tensor_out[377] = \t 64\n",
      "tensor_out[378] = \t 57\n",
      "tensor_out[379] = \t 69\n",
      "tensor_out[380] = \t 91\n",
      "tensor_out[381] = \t 35\n",
      "tensor_out[382] = \t 17\n",
      "tensor_out[383] = \t 87\n",
      "tensor_out[384] = \t 125\n",
      "tensor_out[385] = \t 146\n",
      "tensor_out[386] = \t 0\n",
      "tensor_out[387] = \t 0\n",
      "tensor_out[388] = \t 18\n",
      "tensor_out[389] = \t 63\n",
      "tensor_out[390] = \t 0\n",
      "tensor_out[391] = \t 0\n",
      "tensor_out[392] = \t 110\n",
      "tensor_out[393] = \t 0\n",
      "tensor_out[394] = \t 99\n",
      "tensor_out[395] = \t 12\n",
      "tensor_out[396] = \t 177\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 108\n",
      "tensor_out[399] = \t 75\n",
      "tensor_out[400] = \t 0\n",
      "tensor_out[401] = \t 0\n",
      "tensor_out[402] = \t 73\n",
      "tensor_out[403] = \t 71\n",
      "tensor_out[404] = \t 55\n",
      "tensor_out[405] = \t 173\n",
      "tensor_out[406] = \t 102\n",
      "tensor_out[407] = \t 95\n",
      "tensor_out[408] = \t 0\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 44\n",
      "tensor_out[411] = \t 72\n",
      "tensor_out[412] = \t 0\n",
      "tensor_out[413] = \t 0\n",
      "tensor_out[414] = \t 46\n",
      "tensor_out[415] = \t 0\n",
      "tensor_out[416] = \t 52\n",
      "tensor_out[417] = \t 0\n",
      "tensor_out[418] = \t 21\n",
      "tensor_out[419] = \t 42\n",
      "tensor_out[420] = \t 255\n",
      "tensor_out[421] = \t 90\n",
      "tensor_out[422] = \t 45\n",
      "tensor_out[423] = \t 0\n",
      "tensor_out[424] = \t 5\n",
      "tensor_out[425] = \t 226\n",
      "tensor_out[426] = \t 43\n",
      "tensor_out[427] = \t 75\n",
      "tensor_out[428] = \t 119\n",
      "tensor_out[429] = \t 23\n",
      "tensor_out[430] = \t 92\n",
      "tensor_out[431] = \t 0\n",
      "tensor_out[432] = \t 0\n",
      "tensor_out[433] = \t 21\n",
      "tensor_out[434] = \t 0\n",
      "tensor_out[435] = \t 0\n",
      "tensor_out[436] = \t 71\n",
      "tensor_out[437] = \t 62\n",
      "tensor_out[438] = \t 0\n",
      "tensor_out[439] = \t 56\n",
      "tensor_out[440] = \t 156\n",
      "tensor_out[441] = \t 67\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 176\n",
      "tensor_out[444] = \t 140\n",
      "tensor_out[445] = \t 0\n",
      "tensor_out[446] = \t 0\n",
      "tensor_out[447] = \t 123\n",
      "tensor_out[448] = \t 64\n",
      "tensor_out[449] = \t 38\n",
      "tensor_out[450] = \t 222\n",
      "tensor_out[451] = \t 145\n",
      "tensor_out[452] = \t 102\n",
      "tensor_out[453] = \t 155\n",
      "tensor_out[454] = \t 0\n",
      "tensor_out[455] = \t 70\n",
      "tensor_out[456] = \t 88\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 41\n",
      "tensor_out[459] = \t 0\n",
      "tensor_out[460] = \t 90\n",
      "tensor_out[461] = \t 153\n",
      "tensor_out[462] = \t 41\n",
      "tensor_out[463] = \t 85\n",
      "tensor_out[464] = \t 9\n",
      "tensor_out[465] = \t 95\n",
      "tensor_out[466] = \t 153\n",
      "tensor_out[467] = \t 27\n",
      "tensor_out[468] = \t 25\n",
      "tensor_out[469] = \t 0\n",
      "tensor_out[470] = \t 0\n",
      "tensor_out[471] = \t 0\n",
      "tensor_out[472] = \t 27\n",
      "tensor_out[473] = \t 0\n",
      "tensor_out[474] = \t 0\n",
      "tensor_out[475] = \t 57\n",
      "tensor_out[476] = \t 58\n",
      "tensor_out[477] = \t 0\n",
      "tensor_out[478] = \t 65\n",
      "tensor_out[479] = \t 129\n",
      "tensor_out[480] = \t 68\n",
      "tensor_out[481] = \t 0\n",
      "tensor_out[482] = \t 50\n",
      "tensor_out[483] = \t 102\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 102\n",
      "tensor_out[486] = \t 212\n",
      "tensor_out[487] = \t 240\n",
      "tensor_out[488] = \t 53\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 17\n",
      "tensor_out[491] = \t 0\n",
      "tensor_out[492] = \t 0\n",
      "tensor_out[493] = \t 81\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 157\n",
      "tensor_out[496] = \t 21\n",
      "tensor_out[497] = \t 65\n",
      "tensor_out[498] = \t 42\n",
      "tensor_out[499] = \t 40\n",
      "tensor_out[500] = \t 0\n",
      "tensor_out[501] = \t 42\n",
      "tensor_out[502] = \t 104\n",
      "tensor_out[503] = \t 40\n",
      "tensor_out[504] = \t 119\n",
      "tensor_out[505] = \t 49\n",
      "tensor_out[506] = \t 188\n",
      "tensor_out[507] = \t 0\n",
      "tensor_out[508] = \t 0\n",
      "tensor_out[509] = \t 117\n",
      "tensor_out[510] = \t 136\n",
      "tensor_out[511] = \t 122\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer14 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 44\n",
      "tensor_out[1] = \t 0\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 67\n",
      "tensor_out[4] = \t 0\n",
      "tensor_out[5] = \t 101\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 0\n",
      "tensor_out[9] = \t 0\n",
      "tensor_out[10] = \t 0\n",
      "tensor_out[11] = \t 182\n",
      "tensor_out[12] = \t 106\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 7\n",
      "tensor_out[15] = \t 0\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 6\n",
      "tensor_out[18] = \t 0\n",
      "tensor_out[19] = \t 0\n",
      "tensor_out[20] = \t 0\n",
      "tensor_out[21] = \t 196\n",
      "tensor_out[22] = \t 47\n",
      "tensor_out[23] = \t 38\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 0\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 120\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 56\n",
      "tensor_out[30] = \t 51\n",
      "tensor_out[31] = \t 10\n",
      "tensor_out[32] = \t 38\n",
      "tensor_out[33] = \t 0\n",
      "tensor_out[34] = \t 0\n",
      "tensor_out[35] = \t 54\n",
      "tensor_out[36] = \t 112\n",
      "tensor_out[37] = \t 51\n",
      "tensor_out[38] = \t 151\n",
      "tensor_out[39] = \t 0\n",
      "tensor_out[40] = \t 55\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 96\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 0\n",
      "tensor_out[45] = \t 0\n",
      "tensor_out[46] = \t 0\n",
      "tensor_out[47] = \t 0\n",
      "tensor_out[48] = \t 64\n",
      "tensor_out[49] = \t 125\n",
      "tensor_out[50] = \t 35\n",
      "tensor_out[51] = \t 63\n",
      "tensor_out[52] = \t 32\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 12\n",
      "tensor_out[55] = \t 45\n",
      "tensor_out[56] = \t 0\n",
      "tensor_out[57] = \t 25\n",
      "tensor_out[58] = \t 0\n",
      "tensor_out[59] = \t 79\n",
      "tensor_out[60] = \t 34\n",
      "tensor_out[61] = \t 71\n",
      "tensor_out[62] = \t 29\n",
      "tensor_out[63] = \t 12\n",
      "tensor_out[64] = \t 53\n",
      "tensor_out[65] = \t 44\n",
      "tensor_out[66] = \t 44\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 22\n",
      "tensor_out[72] = \t 122\n",
      "tensor_out[73] = \t 0\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 110\n",
      "tensor_out[76] = \t 44\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 0\n",
      "tensor_out[79] = \t 0\n",
      "tensor_out[80] = \t 82\n",
      "tensor_out[81] = \t 27\n",
      "tensor_out[82] = \t 73\n",
      "tensor_out[83] = \t 134\n",
      "tensor_out[84] = \t 145\n",
      "tensor_out[85] = \t 255\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 0\n",
      "tensor_out[88] = \t 0\n",
      "tensor_out[89] = \t 211\n",
      "tensor_out[90] = \t 26\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 0\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 0\n",
      "tensor_out[95] = \t 74\n",
      "tensor_out[96] = \t 0\n",
      "tensor_out[97] = \t 152\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 8\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 190\n",
      "tensor_out[103] = \t 54\n",
      "tensor_out[104] = \t 5\n",
      "tensor_out[105] = \t 26\n",
      "tensor_out[106] = \t 40\n",
      "tensor_out[107] = \t 0\n",
      "tensor_out[108] = \t 47\n",
      "tensor_out[109] = \t 13\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 1\n",
      "tensor_out[112] = \t 37\n",
      "tensor_out[113] = \t 0\n",
      "tensor_out[114] = \t 91\n",
      "tensor_out[115] = \t 0\n",
      "tensor_out[116] = \t 0\n",
      "tensor_out[117] = \t 0\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 28\n",
      "tensor_out[120] = \t 189\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 42\n",
      "tensor_out[123] = \t 0\n",
      "tensor_out[124] = \t 112\n",
      "tensor_out[125] = \t 25\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 0\n",
      "tensor_out[129] = \t 54\n",
      "tensor_out[130] = \t 0\n",
      "tensor_out[131] = \t 0\n",
      "tensor_out[132] = \t 112\n",
      "tensor_out[133] = \t 86\n",
      "tensor_out[134] = \t 0\n",
      "tensor_out[135] = \t 0\n",
      "tensor_out[136] = \t 77\n",
      "tensor_out[137] = \t 106\n",
      "tensor_out[138] = \t 162\n",
      "tensor_out[139] = \t 0\n",
      "tensor_out[140] = \t 0\n",
      "tensor_out[141] = \t 0\n",
      "tensor_out[142] = \t 105\n",
      "tensor_out[143] = \t 1\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 41\n",
      "tensor_out[146] = \t 0\n",
      "tensor_out[147] = \t 0\n",
      "tensor_out[148] = \t 3\n",
      "tensor_out[149] = \t 50\n",
      "tensor_out[150] = \t 41\n",
      "tensor_out[151] = \t 117\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 126\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 0\n",
      "tensor_out[156] = \t 12\n",
      "tensor_out[157] = \t 63\n",
      "tensor_out[158] = \t 8\n",
      "tensor_out[159] = \t 0\n",
      "tensor_out[160] = \t 47\n",
      "tensor_out[161] = \t 154\n",
      "tensor_out[162] = \t 38\n",
      "tensor_out[163] = \t 213\n",
      "tensor_out[164] = \t 0\n",
      "tensor_out[165] = \t 84\n",
      "tensor_out[166] = \t 0\n",
      "tensor_out[167] = \t 0\n",
      "tensor_out[168] = \t 0\n",
      "tensor_out[169] = \t 0\n",
      "tensor_out[170] = \t 23\n",
      "tensor_out[171] = \t 0\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 105\n",
      "tensor_out[174] = \t 0\n",
      "tensor_out[175] = \t 0\n",
      "tensor_out[176] = \t 0\n",
      "tensor_out[177] = \t 0\n",
      "tensor_out[178] = \t 0\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 0\n",
      "tensor_out[181] = \t 0\n",
      "tensor_out[182] = \t 0\n",
      "tensor_out[183] = \t 109\n",
      "tensor_out[184] = \t 0\n",
      "tensor_out[185] = \t 0\n",
      "tensor_out[186] = \t 0\n",
      "tensor_out[187] = \t 70\n",
      "tensor_out[188] = \t 20\n",
      "tensor_out[189] = \t 114\n",
      "tensor_out[190] = \t 180\n",
      "tensor_out[191] = \t 2\n",
      "tensor_out[192] = \t 40\n",
      "tensor_out[193] = \t 0\n",
      "tensor_out[194] = \t 17\n",
      "tensor_out[195] = \t 6\n",
      "tensor_out[196] = \t 159\n",
      "tensor_out[197] = \t 0\n",
      "tensor_out[198] = \t 0\n",
      "tensor_out[199] = \t 0\n",
      "tensor_out[200] = \t 10\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 39\n",
      "tensor_out[203] = \t 0\n",
      "tensor_out[204] = \t 40\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 86\n",
      "tensor_out[207] = \t 8\n",
      "tensor_out[208] = \t 83\n",
      "tensor_out[209] = \t 0\n",
      "tensor_out[210] = \t 149\n",
      "tensor_out[211] = \t 182\n",
      "tensor_out[212] = \t 0\n",
      "tensor_out[213] = \t 0\n",
      "tensor_out[214] = \t 190\n",
      "tensor_out[215] = \t 9\n",
      "tensor_out[216] = \t 11\n",
      "tensor_out[217] = \t 0\n",
      "tensor_out[218] = \t 45\n",
      "tensor_out[219] = \t 1\n",
      "tensor_out[220] = \t 189\n",
      "tensor_out[221] = \t 0\n",
      "tensor_out[222] = \t 0\n",
      "tensor_out[223] = \t 10\n",
      "tensor_out[224] = \t 53\n",
      "tensor_out[225] = \t 24\n",
      "tensor_out[226] = \t 0\n",
      "tensor_out[227] = \t 30\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 55\n",
      "tensor_out[231] = \t 0\n",
      "tensor_out[232] = \t 195\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 0\n",
      "tensor_out[235] = \t 0\n",
      "tensor_out[236] = \t 44\n",
      "tensor_out[237] = \t 0\n",
      "tensor_out[238] = \t 84\n",
      "tensor_out[239] = \t 55\n",
      "tensor_out[240] = \t 137\n",
      "tensor_out[241] = \t 53\n",
      "tensor_out[242] = \t 32\n",
      "tensor_out[243] = \t 23\n",
      "tensor_out[244] = \t 45\n",
      "tensor_out[245] = \t 0\n",
      "tensor_out[246] = \t 225\n",
      "tensor_out[247] = \t 48\n",
      "tensor_out[248] = \t 61\n",
      "tensor_out[249] = \t 0\n",
      "tensor_out[250] = \t 16\n",
      "tensor_out[251] = \t 30\n",
      "tensor_out[252] = \t 0\n",
      "tensor_out[253] = \t 140\n",
      "tensor_out[254] = \t 0\n",
      "tensor_out[255] = \t 52\n",
      "tensor_out[256] = \t 40\n",
      "tensor_out[257] = \t 29\n",
      "tensor_out[258] = \t 5\n",
      "tensor_out[259] = \t 0\n",
      "tensor_out[260] = \t 0\n",
      "tensor_out[261] = \t 0\n",
      "tensor_out[262] = \t 0\n",
      "tensor_out[263] = \t 0\n",
      "tensor_out[264] = \t 216\n",
      "tensor_out[265] = \t 0\n",
      "tensor_out[266] = \t 121\n",
      "tensor_out[267] = \t 10\n",
      "tensor_out[268] = \t 39\n",
      "tensor_out[269] = \t 8\n",
      "tensor_out[270] = \t 31\n",
      "tensor_out[271] = \t 0\n",
      "tensor_out[272] = \t 47\n",
      "tensor_out[273] = \t 0\n",
      "tensor_out[274] = \t 140\n",
      "tensor_out[275] = \t 0\n",
      "tensor_out[276] = \t 0\n",
      "tensor_out[277] = \t 0\n",
      "tensor_out[278] = \t 65\n",
      "tensor_out[279] = \t 46\n",
      "tensor_out[280] = \t 28\n",
      "tensor_out[281] = \t 108\n",
      "tensor_out[282] = \t 17\n",
      "tensor_out[283] = \t 31\n",
      "tensor_out[284] = \t 0\n",
      "tensor_out[285] = \t 94\n",
      "tensor_out[286] = \t 108\n",
      "tensor_out[287] = \t 5\n",
      "tensor_out[288] = \t 0\n",
      "tensor_out[289] = \t 0\n",
      "tensor_out[290] = \t 0\n",
      "tensor_out[291] = \t 110\n",
      "tensor_out[292] = \t 71\n",
      "tensor_out[293] = \t 0\n",
      "tensor_out[294] = \t 99\n",
      "tensor_out[295] = \t 0\n",
      "tensor_out[296] = \t 0\n",
      "tensor_out[297] = \t 22\n",
      "tensor_out[298] = \t 0\n",
      "tensor_out[299] = \t 0\n",
      "tensor_out[300] = \t 0\n",
      "tensor_out[301] = \t 0\n",
      "tensor_out[302] = \t 0\n",
      "tensor_out[303] = \t 0\n",
      "tensor_out[304] = \t 126\n",
      "tensor_out[305] = \t 71\n",
      "tensor_out[306] = \t 64\n",
      "tensor_out[307] = \t 78\n",
      "tensor_out[308] = \t 53\n",
      "tensor_out[309] = \t 42\n",
      "tensor_out[310] = \t 128\n",
      "tensor_out[311] = \t 0\n",
      "tensor_out[312] = \t 24\n",
      "tensor_out[313] = \t 44\n",
      "tensor_out[314] = \t 47\n",
      "tensor_out[315] = \t 0\n",
      "tensor_out[316] = \t 144\n",
      "tensor_out[317] = \t 90\n",
      "tensor_out[318] = \t 0\n",
      "tensor_out[319] = \t 0\n",
      "tensor_out[320] = \t 0\n",
      "tensor_out[321] = \t 94\n",
      "tensor_out[322] = \t 70\n",
      "tensor_out[323] = \t 0\n",
      "tensor_out[324] = \t 151\n",
      "tensor_out[325] = \t 0\n",
      "tensor_out[326] = \t 54\n",
      "tensor_out[327] = \t 0\n",
      "tensor_out[328] = \t 64\n",
      "tensor_out[329] = \t 0\n",
      "tensor_out[330] = \t 132\n",
      "tensor_out[331] = \t 0\n",
      "tensor_out[332] = \t 0\n",
      "tensor_out[333] = \t 0\n",
      "tensor_out[334] = \t 148\n",
      "tensor_out[335] = \t 24\n",
      "tensor_out[336] = \t 110\n",
      "tensor_out[337] = \t 35\n",
      "tensor_out[338] = \t 0\n",
      "tensor_out[339] = \t 0\n",
      "tensor_out[340] = \t 0\n",
      "tensor_out[341] = \t 0\n",
      "tensor_out[342] = \t 8\n",
      "tensor_out[343] = \t 65\n",
      "tensor_out[344] = \t 147\n",
      "tensor_out[345] = \t 3\n",
      "tensor_out[346] = \t 167\n",
      "tensor_out[347] = \t 16\n",
      "tensor_out[348] = \t 0\n",
      "tensor_out[349] = \t 0\n",
      "tensor_out[350] = \t 47\n",
      "tensor_out[351] = \t 0\n",
      "tensor_out[352] = \t 15\n",
      "tensor_out[353] = \t 207\n",
      "tensor_out[354] = \t 0\n",
      "tensor_out[355] = \t 83\n",
      "tensor_out[356] = \t 0\n",
      "tensor_out[357] = \t 0\n",
      "tensor_out[358] = \t 0\n",
      "tensor_out[359] = \t 9\n",
      "tensor_out[360] = \t 0\n",
      "tensor_out[361] = \t 50\n",
      "tensor_out[362] = \t 0\n",
      "tensor_out[363] = \t 0\n",
      "tensor_out[364] = \t 0\n",
      "tensor_out[365] = \t 0\n",
      "tensor_out[366] = \t 0\n",
      "tensor_out[367] = \t 21\n",
      "tensor_out[368] = \t 0\n",
      "tensor_out[369] = \t 0\n",
      "tensor_out[370] = \t 0\n",
      "tensor_out[371] = \t 39\n",
      "tensor_out[372] = \t 0\n",
      "tensor_out[373] = \t 45\n",
      "tensor_out[374] = \t 45\n",
      "tensor_out[375] = \t 0\n",
      "tensor_out[376] = \t 54\n",
      "tensor_out[377] = \t 74\n",
      "tensor_out[378] = \t 0\n",
      "tensor_out[379] = \t 68\n",
      "tensor_out[380] = \t 171\n",
      "tensor_out[381] = \t 15\n",
      "tensor_out[382] = \t 54\n",
      "tensor_out[383] = \t 7\n",
      "tensor_out[384] = \t 41\n",
      "tensor_out[385] = \t 0\n",
      "tensor_out[386] = \t 7\n",
      "tensor_out[387] = \t 0\n",
      "tensor_out[388] = \t 62\n",
      "tensor_out[389] = \t 37\n",
      "tensor_out[390] = \t 47\n",
      "tensor_out[391] = \t 0\n",
      "tensor_out[392] = \t 0\n",
      "tensor_out[393] = \t 48\n",
      "tensor_out[394] = \t 0\n",
      "tensor_out[395] = \t 41\n",
      "tensor_out[396] = \t 0\n",
      "tensor_out[397] = \t 1\n",
      "tensor_out[398] = \t 0\n",
      "tensor_out[399] = \t 25\n",
      "tensor_out[400] = \t 0\n",
      "tensor_out[401] = \t 0\n",
      "tensor_out[402] = \t 61\n",
      "tensor_out[403] = \t 44\n",
      "tensor_out[404] = \t 0\n",
      "tensor_out[405] = \t 0\n",
      "tensor_out[406] = \t 82\n",
      "tensor_out[407] = \t 84\n",
      "tensor_out[408] = \t 47\n",
      "tensor_out[409] = \t 70\n",
      "tensor_out[410] = \t 29\n",
      "tensor_out[411] = \t 66\n",
      "tensor_out[412] = \t 16\n",
      "tensor_out[413] = \t 108\n",
      "tensor_out[414] = \t 0\n",
      "tensor_out[415] = \t 0\n",
      "tensor_out[416] = \t 0\n",
      "tensor_out[417] = \t 0\n",
      "tensor_out[418] = \t 15\n",
      "tensor_out[419] = \t 118\n",
      "tensor_out[420] = \t 72\n",
      "tensor_out[421] = \t 0\n",
      "tensor_out[422] = \t 21\n",
      "tensor_out[423] = \t 18\n",
      "tensor_out[424] = \t 0\n",
      "tensor_out[425] = \t 0\n",
      "tensor_out[426] = \t 0\n",
      "tensor_out[427] = \t 0\n",
      "tensor_out[428] = \t 131\n",
      "tensor_out[429] = \t 95\n",
      "tensor_out[430] = \t 26\n",
      "tensor_out[431] = \t 66\n",
      "tensor_out[432] = \t 0\n",
      "tensor_out[433] = \t 0\n",
      "tensor_out[434] = \t 0\n",
      "tensor_out[435] = \t 0\n",
      "tensor_out[436] = \t 89\n",
      "tensor_out[437] = \t 76\n",
      "tensor_out[438] = \t 10\n",
      "tensor_out[439] = \t 33\n",
      "tensor_out[440] = \t 0\n",
      "tensor_out[441] = \t 0\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 0\n",
      "tensor_out[444] = \t 0\n",
      "tensor_out[445] = \t 8\n",
      "tensor_out[446] = \t 0\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 23\n",
      "tensor_out[449] = \t 0\n",
      "tensor_out[450] = \t 188\n",
      "tensor_out[451] = \t 138\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 71\n",
      "tensor_out[454] = \t 0\n",
      "tensor_out[455] = \t 11\n",
      "tensor_out[456] = \t 0\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 23\n",
      "tensor_out[459] = \t 0\n",
      "tensor_out[460] = \t 64\n",
      "tensor_out[461] = \t 14\n",
      "tensor_out[462] = \t 0\n",
      "tensor_out[463] = \t 53\n",
      "tensor_out[464] = \t 0\n",
      "tensor_out[465] = \t 0\n",
      "tensor_out[466] = \t 0\n",
      "tensor_out[467] = \t 0\n",
      "tensor_out[468] = \t 0\n",
      "tensor_out[469] = \t 0\n",
      "tensor_out[470] = \t 0\n",
      "tensor_out[471] = \t 57\n",
      "tensor_out[472] = \t 12\n",
      "tensor_out[473] = \t 25\n",
      "tensor_out[474] = \t 0\n",
      "tensor_out[475] = \t 54\n",
      "tensor_out[476] = \t 0\n",
      "tensor_out[477] = \t 0\n",
      "tensor_out[478] = \t 27\n",
      "tensor_out[479] = \t 0\n",
      "tensor_out[480] = \t 0\n",
      "tensor_out[481] = \t 2\n",
      "tensor_out[482] = \t 25\n",
      "tensor_out[483] = \t 165\n",
      "tensor_out[484] = \t 85\n",
      "tensor_out[485] = \t 0\n",
      "tensor_out[486] = \t 0\n",
      "tensor_out[487] = \t 177\n",
      "tensor_out[488] = \t 32\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 0\n",
      "tensor_out[491] = \t 0\n",
      "tensor_out[492] = \t 0\n",
      "tensor_out[493] = \t 0\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 205\n",
      "tensor_out[496] = \t 55\n",
      "tensor_out[497] = \t 30\n",
      "tensor_out[498] = \t 104\n",
      "tensor_out[499] = \t 26\n",
      "tensor_out[500] = \t 0\n",
      "tensor_out[501] = \t 9\n",
      "tensor_out[502] = \t 0\n",
      "tensor_out[503] = \t 38\n",
      "tensor_out[504] = \t 0\n",
      "tensor_out[505] = \t 83\n",
      "tensor_out[506] = \t 0\n",
      "tensor_out[507] = \t 110\n",
      "tensor_out[508] = \t 101\n",
      "tensor_out[509] = \t 96\n",
      "tensor_out[510] = \t 0\n",
      "tensor_out[511] = \t 52\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer15 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0\n",
      "tensor_out[1] = \t 0\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 63\n",
      "tensor_out[5] = \t 0\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 105\n",
      "tensor_out[9] = \t 13\n",
      "tensor_out[10] = \t 0\n",
      "tensor_out[11] = \t 0\n",
      "tensor_out[12] = \t 0\n",
      "tensor_out[13] = \t 4\n",
      "tensor_out[14] = \t 0\n",
      "tensor_out[15] = \t 0\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 55\n",
      "tensor_out[19] = \t 0\n",
      "tensor_out[20] = \t 131\n",
      "tensor_out[21] = \t 0\n",
      "tensor_out[22] = \t 0\n",
      "tensor_out[23] = \t 0\n",
      "tensor_out[24] = \t 99\n",
      "tensor_out[25] = \t 18\n",
      "tensor_out[26] = \t 66\n",
      "tensor_out[27] = \t 107\n",
      "tensor_out[28] = \t 179\n",
      "tensor_out[29] = \t 0\n",
      "tensor_out[30] = \t 0\n",
      "tensor_out[31] = \t 0\n",
      "tensor_out[32] = \t 0\n",
      "tensor_out[33] = \t 0\n",
      "tensor_out[34] = \t 30\n",
      "tensor_out[35] = \t 0\n",
      "tensor_out[36] = \t 0\n",
      "tensor_out[37] = \t 113\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 52\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 166\n",
      "tensor_out[44] = \t 0\n",
      "tensor_out[45] = \t 48\n",
      "tensor_out[46] = \t 0\n",
      "tensor_out[47] = \t 0\n",
      "tensor_out[48] = \t 0\n",
      "tensor_out[49] = \t 0\n",
      "tensor_out[50] = \t 0\n",
      "tensor_out[51] = \t 134\n",
      "tensor_out[52] = \t 0\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 135\n",
      "tensor_out[55] = \t 48\n",
      "tensor_out[56] = \t 0\n",
      "tensor_out[57] = \t 0\n",
      "tensor_out[58] = \t 63\n",
      "tensor_out[59] = \t 106\n",
      "tensor_out[60] = \t 0\n",
      "tensor_out[61] = \t 0\n",
      "tensor_out[62] = \t 24\n",
      "tensor_out[63] = \t 156\n",
      "tensor_out[64] = \t 59\n",
      "tensor_out[65] = \t 0\n",
      "tensor_out[66] = \t 0\n",
      "tensor_out[67] = \t 17\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 64\n",
      "tensor_out[71] = \t 0\n",
      "tensor_out[72] = \t 54\n",
      "tensor_out[73] = \t 0\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 16\n",
      "tensor_out[76] = \t 168\n",
      "tensor_out[77] = \t 198\n",
      "tensor_out[78] = \t 58\n",
      "tensor_out[79] = \t 92\n",
      "tensor_out[80] = \t 0\n",
      "tensor_out[81] = \t 0\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 35\n",
      "tensor_out[84] = \t 83\n",
      "tensor_out[85] = \t 21\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 18\n",
      "tensor_out[88] = \t 30\n",
      "tensor_out[89] = \t 0\n",
      "tensor_out[90] = \t 26\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 0\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 0\n",
      "tensor_out[95] = \t 0\n",
      "tensor_out[96] = \t 0\n",
      "tensor_out[97] = \t 13\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 0\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 0\n",
      "tensor_out[103] = \t 24\n",
      "tensor_out[104] = \t 117\n",
      "tensor_out[105] = \t 255\n",
      "tensor_out[106] = \t 57\n",
      "tensor_out[107] = \t 0\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 0\n",
      "tensor_out[110] = \t 81\n",
      "tensor_out[111] = \t 90\n",
      "tensor_out[112] = \t 0\n",
      "tensor_out[113] = \t 52\n",
      "tensor_out[114] = \t 0\n",
      "tensor_out[115] = \t 0\n",
      "tensor_out[116] = \t 0\n",
      "tensor_out[117] = \t 3\n",
      "tensor_out[118] = \t 255\n",
      "tensor_out[119] = \t 0\n",
      "tensor_out[120] = \t 198\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 0\n",
      "tensor_out[123] = \t 0\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 46\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 193\n",
      "tensor_out[129] = \t 87\n",
      "tensor_out[130] = \t 0\n",
      "tensor_out[131] = \t 0\n",
      "tensor_out[132] = \t 0\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 0\n",
      "tensor_out[135] = \t 70\n",
      "tensor_out[136] = \t 34\n",
      "tensor_out[137] = \t 117\n",
      "tensor_out[138] = \t 0\n",
      "tensor_out[139] = \t 42\n",
      "tensor_out[140] = \t 0\n",
      "tensor_out[141] = \t 208\n",
      "tensor_out[142] = \t 80\n",
      "tensor_out[143] = \t 20\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 30\n",
      "tensor_out[146] = \t 0\n",
      "tensor_out[147] = \t 4\n",
      "tensor_out[148] = \t 69\n",
      "tensor_out[149] = \t 23\n",
      "tensor_out[150] = \t 58\n",
      "tensor_out[151] = \t 51\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 27\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 1\n",
      "tensor_out[156] = \t 0\n",
      "tensor_out[157] = \t 101\n",
      "tensor_out[158] = \t 0\n",
      "tensor_out[159] = \t 81\n",
      "tensor_out[160] = \t 73\n",
      "tensor_out[161] = \t 15\n",
      "tensor_out[162] = \t 0\n",
      "tensor_out[163] = \t 0\n",
      "tensor_out[164] = \t 0\n",
      "tensor_out[165] = \t 2\n",
      "tensor_out[166] = \t 0\n",
      "tensor_out[167] = \t 0\n",
      "tensor_out[168] = \t 0\n",
      "tensor_out[169] = \t 31\n",
      "tensor_out[170] = \t 0\n",
      "tensor_out[171] = \t 4\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 35\n",
      "tensor_out[174] = \t 0\n",
      "tensor_out[175] = \t 0\n",
      "tensor_out[176] = \t 0\n",
      "tensor_out[177] = \t 0\n",
      "tensor_out[178] = \t 99\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 0\n",
      "tensor_out[181] = \t 238\n",
      "tensor_out[182] = \t 174\n",
      "tensor_out[183] = \t 0\n",
      "tensor_out[184] = \t 0\n",
      "tensor_out[185] = \t 12\n",
      "tensor_out[186] = \t 16\n",
      "tensor_out[187] = \t 81\n",
      "tensor_out[188] = \t 7\n",
      "tensor_out[189] = \t 126\n",
      "tensor_out[190] = \t 154\n",
      "tensor_out[191] = \t 0\n",
      "tensor_out[192] = \t 0\n",
      "tensor_out[193] = \t 114\n",
      "tensor_out[194] = \t 82\n",
      "tensor_out[195] = \t 5\n",
      "tensor_out[196] = \t 3\n",
      "tensor_out[197] = \t 140\n",
      "tensor_out[198] = \t 67\n",
      "tensor_out[199] = \t 104\n",
      "tensor_out[200] = \t 0\n",
      "tensor_out[201] = \t 21\n",
      "tensor_out[202] = \t 94\n",
      "tensor_out[203] = \t 148\n",
      "tensor_out[204] = \t 0\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 0\n",
      "tensor_out[207] = \t 0\n",
      "tensor_out[208] = \t 0\n",
      "tensor_out[209] = \t 127\n",
      "tensor_out[210] = \t 0\n",
      "tensor_out[211] = \t 69\n",
      "tensor_out[212] = \t 46\n",
      "tensor_out[213] = \t 0\n",
      "tensor_out[214] = \t 132\n",
      "tensor_out[215] = \t 0\n",
      "tensor_out[216] = \t 0\n",
      "tensor_out[217] = \t 31\n",
      "tensor_out[218] = \t 0\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 0\n",
      "tensor_out[221] = \t 17\n",
      "tensor_out[222] = \t 53\n",
      "tensor_out[223] = \t 29\n",
      "tensor_out[224] = \t 76\n",
      "tensor_out[225] = \t 0\n",
      "tensor_out[226] = \t 94\n",
      "tensor_out[227] = \t 0\n",
      "tensor_out[228] = \t 9\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 5\n",
      "tensor_out[231] = \t 0\n",
      "tensor_out[232] = \t 25\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 0\n",
      "tensor_out[235] = \t 0\n",
      "tensor_out[236] = \t 102\n",
      "tensor_out[237] = \t 0\n",
      "tensor_out[238] = \t 0\n",
      "tensor_out[239] = \t 0\n",
      "tensor_out[240] = \t 0\n",
      "tensor_out[241] = \t 0\n",
      "tensor_out[242] = \t 0\n",
      "tensor_out[243] = \t 7\n",
      "tensor_out[244] = \t 41\n",
      "tensor_out[245] = \t 0\n",
      "tensor_out[246] = \t 0\n",
      "tensor_out[247] = \t 43\n",
      "tensor_out[248] = \t 0\n",
      "tensor_out[249] = \t 31\n",
      "tensor_out[250] = \t 13\n",
      "tensor_out[251] = \t 0\n",
      "tensor_out[252] = \t 56\n",
      "tensor_out[253] = \t 0\n",
      "tensor_out[254] = \t 80\n",
      "tensor_out[255] = \t 44\n",
      "tensor_out[256] = \t 93\n",
      "tensor_out[257] = \t 61\n",
      "tensor_out[258] = \t 204\n",
      "tensor_out[259] = \t 43\n",
      "tensor_out[260] = \t 8\n",
      "tensor_out[261] = \t 96\n",
      "tensor_out[262] = \t 0\n",
      "tensor_out[263] = \t 178\n",
      "tensor_out[264] = \t 0\n",
      "tensor_out[265] = \t 0\n",
      "tensor_out[266] = \t 78\n",
      "tensor_out[267] = \t 93\n",
      "tensor_out[268] = \t 0\n",
      "tensor_out[269] = \t 0\n",
      "tensor_out[270] = \t 0\n",
      "tensor_out[271] = \t 92\n",
      "tensor_out[272] = \t 53\n",
      "tensor_out[273] = \t 17\n",
      "tensor_out[274] = \t 0\n",
      "tensor_out[275] = \t 38\n",
      "tensor_out[276] = \t 78\n",
      "tensor_out[277] = \t 112\n",
      "tensor_out[278] = \t 0\n",
      "tensor_out[279] = \t 83\n",
      "tensor_out[280] = \t 149\n",
      "tensor_out[281] = \t 33\n",
      "tensor_out[282] = \t 0\n",
      "tensor_out[283] = \t 28\n",
      "tensor_out[284] = \t 28\n",
      "tensor_out[285] = \t 0\n",
      "tensor_out[286] = \t 77\n",
      "tensor_out[287] = \t 169\n",
      "tensor_out[288] = \t 8\n",
      "tensor_out[289] = \t 0\n",
      "tensor_out[290] = \t 21\n",
      "tensor_out[291] = \t 119\n",
      "tensor_out[292] = \t 120\n",
      "tensor_out[293] = \t 0\n",
      "tensor_out[294] = \t 0\n",
      "tensor_out[295] = \t 0\n",
      "tensor_out[296] = \t 0\n",
      "tensor_out[297] = \t 0\n",
      "tensor_out[298] = \t 39\n",
      "tensor_out[299] = \t 0\n",
      "tensor_out[300] = \t 183\n",
      "tensor_out[301] = \t 74\n",
      "tensor_out[302] = \t 20\n",
      "tensor_out[303] = \t 0\n",
      "tensor_out[304] = \t 142\n",
      "tensor_out[305] = \t 6\n",
      "tensor_out[306] = \t 0\n",
      "tensor_out[307] = \t 128\n",
      "tensor_out[308] = \t 47\n",
      "tensor_out[309] = \t 114\n",
      "tensor_out[310] = \t 0\n",
      "tensor_out[311] = \t 157\n",
      "tensor_out[312] = \t 0\n",
      "tensor_out[313] = \t 0\n",
      "tensor_out[314] = \t 77\n",
      "tensor_out[315] = \t 3\n",
      "tensor_out[316] = \t 70\n",
      "tensor_out[317] = \t 48\n",
      "tensor_out[318] = \t 20\n",
      "tensor_out[319] = \t 104\n",
      "tensor_out[320] = \t 88\n",
      "tensor_out[321] = \t 75\n",
      "tensor_out[322] = \t 0\n",
      "tensor_out[323] = \t 0\n",
      "tensor_out[324] = \t 95\n",
      "tensor_out[325] = \t 0\n",
      "tensor_out[326] = \t 43\n",
      "tensor_out[327] = \t 148\n",
      "tensor_out[328] = \t 0\n",
      "tensor_out[329] = \t 0\n",
      "tensor_out[330] = \t 71\n",
      "tensor_out[331] = \t 71\n",
      "tensor_out[332] = \t 112\n",
      "tensor_out[333] = \t 174\n",
      "tensor_out[334] = \t 0\n",
      "tensor_out[335] = \t 0\n",
      "tensor_out[336] = \t 0\n",
      "tensor_out[337] = \t 0\n",
      "tensor_out[338] = \t 34\n",
      "tensor_out[339] = \t 0\n",
      "tensor_out[340] = \t 80\n",
      "tensor_out[341] = \t 47\n",
      "tensor_out[342] = \t 0\n",
      "tensor_out[343] = \t 0\n",
      "tensor_out[344] = \t 0\n",
      "tensor_out[345] = \t 0\n",
      "tensor_out[346] = \t 63\n",
      "tensor_out[347] = \t 39\n",
      "tensor_out[348] = \t 143\n",
      "tensor_out[349] = \t 34\n",
      "tensor_out[350] = \t 0\n",
      "tensor_out[351] = \t 0\n",
      "tensor_out[352] = \t 0\n",
      "tensor_out[353] = \t 91\n",
      "tensor_out[354] = \t 151\n",
      "tensor_out[355] = \t 91\n",
      "tensor_out[356] = \t 0\n",
      "tensor_out[357] = \t 68\n",
      "tensor_out[358] = \t 0\n",
      "tensor_out[359] = \t 0\n",
      "tensor_out[360] = \t 46\n",
      "tensor_out[361] = \t 105\n",
      "tensor_out[362] = \t 48\n",
      "tensor_out[363] = \t 0\n",
      "tensor_out[364] = \t 0\n",
      "tensor_out[365] = \t 0\n",
      "tensor_out[366] = \t 8\n",
      "tensor_out[367] = \t 108\n",
      "tensor_out[368] = \t 66\n",
      "tensor_out[369] = \t 54\n",
      "tensor_out[370] = \t 0\n",
      "tensor_out[371] = \t 171\n",
      "tensor_out[372] = \t 76\n",
      "tensor_out[373] = \t 0\n",
      "tensor_out[374] = \t 50\n",
      "tensor_out[375] = \t 0\n",
      "tensor_out[376] = \t 5\n",
      "tensor_out[377] = \t 0\n",
      "tensor_out[378] = \t 56\n",
      "tensor_out[379] = \t 0\n",
      "tensor_out[380] = \t 4\n",
      "tensor_out[381] = \t 0\n",
      "tensor_out[382] = \t 51\n",
      "tensor_out[383] = \t 135\n",
      "tensor_out[384] = \t 0\n",
      "tensor_out[385] = \t 34\n",
      "tensor_out[386] = \t 116\n",
      "tensor_out[387] = \t 246\n",
      "tensor_out[388] = \t 0\n",
      "tensor_out[389] = \t 68\n",
      "tensor_out[390] = \t 0\n",
      "tensor_out[391] = \t 0\n",
      "tensor_out[392] = \t 22\n",
      "tensor_out[393] = \t 61\n",
      "tensor_out[394] = \t 161\n",
      "tensor_out[395] = \t 109\n",
      "tensor_out[396] = \t 0\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 99\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 81\n",
      "tensor_out[401] = \t 113\n",
      "tensor_out[402] = \t 14\n",
      "tensor_out[403] = \t 113\n",
      "tensor_out[404] = \t 133\n",
      "tensor_out[405] = \t 0\n",
      "tensor_out[406] = \t 0\n",
      "tensor_out[407] = \t 41\n",
      "tensor_out[408] = \t 49\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 25\n",
      "tensor_out[411] = \t 0\n",
      "tensor_out[412] = \t 69\n",
      "tensor_out[413] = \t 58\n",
      "tensor_out[414] = \t 27\n",
      "tensor_out[415] = \t 15\n",
      "tensor_out[416] = \t 0\n",
      "tensor_out[417] = \t 0\n",
      "tensor_out[418] = \t 99\n",
      "tensor_out[419] = \t 25\n",
      "tensor_out[420] = \t 126\n",
      "tensor_out[421] = \t 0\n",
      "tensor_out[422] = \t 0\n",
      "tensor_out[423] = \t 0\n",
      "tensor_out[424] = \t 56\n",
      "tensor_out[425] = \t 0\n",
      "tensor_out[426] = \t 0\n",
      "tensor_out[427] = \t 156\n",
      "tensor_out[428] = \t 0\n",
      "tensor_out[429] = \t 0\n",
      "tensor_out[430] = \t 0\n",
      "tensor_out[431] = \t 21\n",
      "tensor_out[432] = \t 8\n",
      "tensor_out[433] = \t 0\n",
      "tensor_out[434] = \t 43\n",
      "tensor_out[435] = \t 6\n",
      "tensor_out[436] = \t 61\n",
      "tensor_out[437] = \t 18\n",
      "tensor_out[438] = \t 0\n",
      "tensor_out[439] = \t 29\n",
      "tensor_out[440] = \t 0\n",
      "tensor_out[441] = \t 26\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 106\n",
      "tensor_out[444] = \t 87\n",
      "tensor_out[445] = \t 80\n",
      "tensor_out[446] = \t 28\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 20\n",
      "tensor_out[449] = \t 76\n",
      "tensor_out[450] = \t 57\n",
      "tensor_out[451] = \t 135\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 0\n",
      "tensor_out[454] = \t 0\n",
      "tensor_out[455] = \t 0\n",
      "tensor_out[456] = \t 147\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 144\n",
      "tensor_out[459] = \t 0\n",
      "tensor_out[460] = \t 0\n",
      "tensor_out[461] = \t 0\n",
      "tensor_out[462] = \t 0\n",
      "tensor_out[463] = \t 0\n",
      "tensor_out[464] = \t 92\n",
      "tensor_out[465] = \t 0\n",
      "tensor_out[466] = \t 73\n",
      "tensor_out[467] = \t 173\n",
      "tensor_out[468] = \t 0\n",
      "tensor_out[469] = \t 60\n",
      "tensor_out[470] = \t 71\n",
      "tensor_out[471] = \t 0\n",
      "tensor_out[472] = \t 57\n",
      "tensor_out[473] = \t 134\n",
      "tensor_out[474] = \t 82\n",
      "tensor_out[475] = \t 0\n",
      "tensor_out[476] = \t 0\n",
      "tensor_out[477] = \t 84\n",
      "tensor_out[478] = \t 27\n",
      "tensor_out[479] = \t 0\n",
      "tensor_out[480] = \t 21\n",
      "tensor_out[481] = \t 0\n",
      "tensor_out[482] = \t 0\n",
      "tensor_out[483] = \t 0\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 122\n",
      "tensor_out[486] = \t 54\n",
      "tensor_out[487] = \t 105\n",
      "tensor_out[488] = \t 0\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 0\n",
      "tensor_out[491] = \t 0\n",
      "tensor_out[492] = \t 118\n",
      "tensor_out[493] = \t 165\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 72\n",
      "tensor_out[496] = \t 0\n",
      "tensor_out[497] = \t 128\n",
      "tensor_out[498] = \t 35\n",
      "tensor_out[499] = \t 0\n",
      "tensor_out[500] = \t 0\n",
      "tensor_out[501] = \t 0\n",
      "tensor_out[502] = \t 32\n",
      "tensor_out[503] = \t 0\n",
      "tensor_out[504] = \t 0\n",
      "tensor_out[505] = \t 0\n",
      "tensor_out[506] = \t 58\n",
      "tensor_out[507] = \t 38\n",
      "tensor_out[508] = \t 0\n",
      "tensor_out[509] = \t 0\n",
      "tensor_out[510] = \t 0\n",
      "tensor_out[511] = \t 0\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer16 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 17\n",
      "tensor_out[1] = \t 0\n",
      "tensor_out[2] = \t 14\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 3\n",
      "tensor_out[5] = \t 45\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 77\n",
      "tensor_out[9] = \t 0\n",
      "tensor_out[10] = \t 0\n",
      "tensor_out[11] = \t 72\n",
      "tensor_out[12] = \t 50\n",
      "tensor_out[13] = \t 64\n",
      "tensor_out[14] = \t 4\n",
      "tensor_out[15] = \t 0\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 6\n",
      "tensor_out[18] = \t 0\n",
      "tensor_out[19] = \t 58\n",
      "tensor_out[20] = \t 0\n",
      "tensor_out[21] = \t 0\n",
      "tensor_out[22] = \t 0\n",
      "tensor_out[23] = \t 0\n",
      "tensor_out[24] = \t 87\n",
      "tensor_out[25] = \t 48\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 37\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 0\n",
      "tensor_out[30] = \t 13\n",
      "tensor_out[31] = \t 0\n",
      "tensor_out[32] = \t 0\n",
      "tensor_out[33] = \t 4\n",
      "tensor_out[34] = \t 0\n",
      "tensor_out[35] = \t 0\n",
      "tensor_out[36] = \t 0\n",
      "tensor_out[37] = \t 68\n",
      "tensor_out[38] = \t 10\n",
      "tensor_out[39] = \t 39\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 14\n",
      "tensor_out[43] = \t 34\n",
      "tensor_out[44] = \t 0\n",
      "tensor_out[45] = \t 48\n",
      "tensor_out[46] = \t 0\n",
      "tensor_out[47] = \t 17\n",
      "tensor_out[48] = \t 60\n",
      "tensor_out[49] = \t 0\n",
      "tensor_out[50] = \t 0\n",
      "tensor_out[51] = \t 0\n",
      "tensor_out[52] = \t 0\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 28\n",
      "tensor_out[55] = \t 15\n",
      "tensor_out[56] = \t 3\n",
      "tensor_out[57] = \t 72\n",
      "tensor_out[58] = \t 74\n",
      "tensor_out[59] = \t 0\n",
      "tensor_out[60] = \t 64\n",
      "tensor_out[61] = \t 0\n",
      "tensor_out[62] = \t 104\n",
      "tensor_out[63] = \t 0\n",
      "tensor_out[64] = \t 0\n",
      "tensor_out[65] = \t 63\n",
      "tensor_out[66] = \t 0\n",
      "tensor_out[67] = \t 63\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 0\n",
      "tensor_out[72] = \t 92\n",
      "tensor_out[73] = \t 34\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 48\n",
      "tensor_out[76] = \t 0\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 0\n",
      "tensor_out[79] = \t 40\n",
      "tensor_out[80] = \t 44\n",
      "tensor_out[81] = \t 46\n",
      "tensor_out[82] = \t 32\n",
      "tensor_out[83] = \t 66\n",
      "tensor_out[84] = \t 201\n",
      "tensor_out[85] = \t 0\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 24\n",
      "tensor_out[88] = \t 3\n",
      "tensor_out[89] = \t 0\n",
      "tensor_out[90] = \t 0\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 56\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 0\n",
      "tensor_out[95] = \t 0\n",
      "tensor_out[96] = \t 47\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 52\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 65\n",
      "tensor_out[103] = \t 22\n",
      "tensor_out[104] = \t 0\n",
      "tensor_out[105] = \t 0\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 69\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 0\n",
      "tensor_out[110] = \t 111\n",
      "tensor_out[111] = \t 77\n",
      "tensor_out[112] = \t 7\n",
      "tensor_out[113] = \t 14\n",
      "tensor_out[114] = \t 0\n",
      "tensor_out[115] = \t 3\n",
      "tensor_out[116] = \t 50\n",
      "tensor_out[117] = \t 47\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 59\n",
      "tensor_out[120] = \t 0\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 0\n",
      "tensor_out[123] = \t 37\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 0\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 58\n",
      "tensor_out[128] = \t 0\n",
      "tensor_out[129] = \t 50\n",
      "tensor_out[130] = \t 40\n",
      "tensor_out[131] = \t 19\n",
      "tensor_out[132] = \t 63\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 2\n",
      "tensor_out[135] = \t 0\n",
      "tensor_out[136] = \t 31\n",
      "tensor_out[137] = \t 0\n",
      "tensor_out[138] = \t 0\n",
      "tensor_out[139] = \t 206\n",
      "tensor_out[140] = \t 72\n",
      "tensor_out[141] = \t 15\n",
      "tensor_out[142] = \t 51\n",
      "tensor_out[143] = \t 0\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 10\n",
      "tensor_out[146] = \t 33\n",
      "tensor_out[147] = \t 49\n",
      "tensor_out[148] = \t 50\n",
      "tensor_out[149] = \t 3\n",
      "tensor_out[150] = \t 1\n",
      "tensor_out[151] = \t 0\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 0\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 0\n",
      "tensor_out[156] = \t 0\n",
      "tensor_out[157] = \t 75\n",
      "tensor_out[158] = \t 0\n",
      "tensor_out[159] = \t 225\n",
      "tensor_out[160] = \t 0\n",
      "tensor_out[161] = \t 81\n",
      "tensor_out[162] = \t 8\n",
      "tensor_out[163] = \t 0\n",
      "tensor_out[164] = \t 0\n",
      "tensor_out[165] = \t 49\n",
      "tensor_out[166] = \t 57\n",
      "tensor_out[167] = \t 0\n",
      "tensor_out[168] = \t 0\n",
      "tensor_out[169] = \t 62\n",
      "tensor_out[170] = \t 0\n",
      "tensor_out[171] = \t 16\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 66\n",
      "tensor_out[174] = \t 3\n",
      "tensor_out[175] = \t 32\n",
      "tensor_out[176] = \t 32\n",
      "tensor_out[177] = \t 0\n",
      "tensor_out[178] = \t 0\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 71\n",
      "tensor_out[181] = \t 20\n",
      "tensor_out[182] = \t 0\n",
      "tensor_out[183] = \t 0\n",
      "tensor_out[184] = \t 63\n",
      "tensor_out[185] = \t 17\n",
      "tensor_out[186] = \t 20\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 0\n",
      "tensor_out[189] = \t 0\n",
      "tensor_out[190] = \t 56\n",
      "tensor_out[191] = \t 38\n",
      "tensor_out[192] = \t 0\n",
      "tensor_out[193] = \t 0\n",
      "tensor_out[194] = \t 75\n",
      "tensor_out[195] = \t 26\n",
      "tensor_out[196] = \t 45\n",
      "tensor_out[197] = \t 23\n",
      "tensor_out[198] = \t 19\n",
      "tensor_out[199] = \t 0\n",
      "tensor_out[200] = \t 0\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 110\n",
      "tensor_out[203] = \t 0\n",
      "tensor_out[204] = \t 0\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 0\n",
      "tensor_out[207] = \t 0\n",
      "tensor_out[208] = \t 0\n",
      "tensor_out[209] = \t 77\n",
      "tensor_out[210] = \t 51\n",
      "tensor_out[211] = \t 15\n",
      "tensor_out[212] = \t 12\n",
      "tensor_out[213] = \t 55\n",
      "tensor_out[214] = \t 143\n",
      "tensor_out[215] = \t 0\n",
      "tensor_out[216] = \t 0\n",
      "tensor_out[217] = \t 46\n",
      "tensor_out[218] = \t 0\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 0\n",
      "tensor_out[221] = \t 30\n",
      "tensor_out[222] = \t 0\n",
      "tensor_out[223] = \t 66\n",
      "tensor_out[224] = \t 1\n",
      "tensor_out[225] = \t 61\n",
      "tensor_out[226] = \t 57\n",
      "tensor_out[227] = \t 31\n",
      "tensor_out[228] = \t 51\n",
      "tensor_out[229] = \t 26\n",
      "tensor_out[230] = \t 0\n",
      "tensor_out[231] = \t 10\n",
      "tensor_out[232] = \t 1\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 0\n",
      "tensor_out[235] = \t 0\n",
      "tensor_out[236] = \t 86\n",
      "tensor_out[237] = \t 0\n",
      "tensor_out[238] = \t 0\n",
      "tensor_out[239] = \t 0\n",
      "tensor_out[240] = \t 0\n",
      "tensor_out[241] = \t 12\n",
      "tensor_out[242] = \t 0\n",
      "tensor_out[243] = \t 0\n",
      "tensor_out[244] = \t 0\n",
      "tensor_out[245] = \t 20\n",
      "tensor_out[246] = \t 0\n",
      "tensor_out[247] = \t 59\n",
      "tensor_out[248] = \t 96\n",
      "tensor_out[249] = \t 51\n",
      "tensor_out[250] = \t 3\n",
      "tensor_out[251] = \t 106\n",
      "tensor_out[252] = \t 21\n",
      "tensor_out[253] = \t 0\n",
      "tensor_out[254] = \t 9\n",
      "tensor_out[255] = \t 0\n",
      "tensor_out[256] = \t 171\n",
      "tensor_out[257] = \t 99\n",
      "tensor_out[258] = \t 150\n",
      "tensor_out[259] = \t 39\n",
      "tensor_out[260] = \t 5\n",
      "tensor_out[261] = \t 88\n",
      "tensor_out[262] = \t 0\n",
      "tensor_out[263] = \t 0\n",
      "tensor_out[264] = \t 0\n",
      "tensor_out[265] = \t 0\n",
      "tensor_out[266] = \t 0\n",
      "tensor_out[267] = \t 64\n",
      "tensor_out[268] = \t 0\n",
      "tensor_out[269] = \t 0\n",
      "tensor_out[270] = \t 57\n",
      "tensor_out[271] = \t 0\n",
      "tensor_out[272] = \t 20\n",
      "tensor_out[273] = \t 40\n",
      "tensor_out[274] = \t 0\n",
      "tensor_out[275] = \t 0\n",
      "tensor_out[276] = \t 0\n",
      "tensor_out[277] = \t 72\n",
      "tensor_out[278] = \t 0\n",
      "tensor_out[279] = \t 13\n",
      "tensor_out[280] = \t 64\n",
      "tensor_out[281] = \t 80\n",
      "tensor_out[282] = \t 0\n",
      "tensor_out[283] = \t 41\n",
      "tensor_out[284] = \t 0\n",
      "tensor_out[285] = \t 40\n",
      "tensor_out[286] = \t 45\n",
      "tensor_out[287] = \t 12\n",
      "tensor_out[288] = \t 75\n",
      "tensor_out[289] = \t 85\n",
      "tensor_out[290] = \t 110\n",
      "tensor_out[291] = \t 0\n",
      "tensor_out[292] = \t 5\n",
      "tensor_out[293] = \t 0\n",
      "tensor_out[294] = \t 57\n",
      "tensor_out[295] = \t 3\n",
      "tensor_out[296] = \t 41\n",
      "tensor_out[297] = \t 0\n",
      "tensor_out[298] = \t 0\n",
      "tensor_out[299] = \t 74\n",
      "tensor_out[300] = \t 11\n",
      "tensor_out[301] = \t 28\n",
      "tensor_out[302] = \t 63\n",
      "tensor_out[303] = \t 0\n",
      "tensor_out[304] = \t 0\n",
      "tensor_out[305] = \t 0\n",
      "tensor_out[306] = \t 60\n",
      "tensor_out[307] = \t 202\n",
      "tensor_out[308] = \t 33\n",
      "tensor_out[309] = \t 57\n",
      "tensor_out[310] = \t 57\n",
      "tensor_out[311] = \t 86\n",
      "tensor_out[312] = \t 49\n",
      "tensor_out[313] = \t 0\n",
      "tensor_out[314] = \t 34\n",
      "tensor_out[315] = \t 69\n",
      "tensor_out[316] = \t 27\n",
      "tensor_out[317] = \t 65\n",
      "tensor_out[318] = \t 0\n",
      "tensor_out[319] = \t 40\n",
      "tensor_out[320] = \t 0\n",
      "tensor_out[321] = \t 30\n",
      "tensor_out[322] = \t 0\n",
      "tensor_out[323] = \t 6\n",
      "tensor_out[324] = \t 112\n",
      "tensor_out[325] = \t 0\n",
      "tensor_out[326] = \t 0\n",
      "tensor_out[327] = \t 151\n",
      "tensor_out[328] = \t 1\n",
      "tensor_out[329] = \t 0\n",
      "tensor_out[330] = \t 0\n",
      "tensor_out[331] = \t 3\n",
      "tensor_out[332] = \t 0\n",
      "tensor_out[333] = \t 125\n",
      "tensor_out[334] = \t 0\n",
      "tensor_out[335] = \t 0\n",
      "tensor_out[336] = \t 59\n",
      "tensor_out[337] = \t 0\n",
      "tensor_out[338] = \t 27\n",
      "tensor_out[339] = \t 0\n",
      "tensor_out[340] = \t 8\n",
      "tensor_out[341] = \t 0\n",
      "tensor_out[342] = \t 0\n",
      "tensor_out[343] = \t 0\n",
      "tensor_out[344] = \t 0\n",
      "tensor_out[345] = \t 0\n",
      "tensor_out[346] = \t 46\n",
      "tensor_out[347] = \t 0\n",
      "tensor_out[348] = \t 171\n",
      "tensor_out[349] = \t 0\n",
      "tensor_out[350] = \t 37\n",
      "tensor_out[351] = \t 0\n",
      "tensor_out[352] = \t 0\n",
      "tensor_out[353] = \t 0\n",
      "tensor_out[354] = \t 0\n",
      "tensor_out[355] = \t 8\n",
      "tensor_out[356] = \t 3\n",
      "tensor_out[357] = \t 18\n",
      "tensor_out[358] = \t 0\n",
      "tensor_out[359] = \t 0\n",
      "tensor_out[360] = \t 5\n",
      "tensor_out[361] = \t 0\n",
      "tensor_out[362] = \t 80\n",
      "tensor_out[363] = \t 22\n",
      "tensor_out[364] = \t 51\n",
      "tensor_out[365] = \t 58\n",
      "tensor_out[366] = \t 31\n",
      "tensor_out[367] = \t 0\n",
      "tensor_out[368] = \t 47\n",
      "tensor_out[369] = \t 50\n",
      "tensor_out[370] = \t 0\n",
      "tensor_out[371] = \t 0\n",
      "tensor_out[372] = \t 46\n",
      "tensor_out[373] = \t 0\n",
      "tensor_out[374] = \t 8\n",
      "tensor_out[375] = \t 0\n",
      "tensor_out[376] = \t 0\n",
      "tensor_out[377] = \t 0\n",
      "tensor_out[378] = \t 32\n",
      "tensor_out[379] = \t 79\n",
      "tensor_out[380] = \t 0\n",
      "tensor_out[381] = \t 43\n",
      "tensor_out[382] = \t 18\n",
      "tensor_out[383] = \t 8\n",
      "tensor_out[384] = \t 0\n",
      "tensor_out[385] = \t 11\n",
      "tensor_out[386] = \t 0\n",
      "tensor_out[387] = \t 49\n",
      "tensor_out[388] = \t 27\n",
      "tensor_out[389] = \t 73\n",
      "tensor_out[390] = \t 0\n",
      "tensor_out[391] = \t 0\n",
      "tensor_out[392] = \t 0\n",
      "tensor_out[393] = \t 40\n",
      "tensor_out[394] = \t 0\n",
      "tensor_out[395] = \t 54\n",
      "tensor_out[396] = \t 0\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 0\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 84\n",
      "tensor_out[401] = \t 29\n",
      "tensor_out[402] = \t 27\n",
      "tensor_out[403] = \t 0\n",
      "tensor_out[404] = \t 15\n",
      "tensor_out[405] = \t 0\n",
      "tensor_out[406] = \t 6\n",
      "tensor_out[407] = \t 29\n",
      "tensor_out[408] = \t 16\n",
      "tensor_out[409] = \t 70\n",
      "tensor_out[410] = \t 5\n",
      "tensor_out[411] = \t 55\n",
      "tensor_out[412] = \t 121\n",
      "tensor_out[413] = \t 92\n",
      "tensor_out[414] = \t 0\n",
      "tensor_out[415] = \t 37\n",
      "tensor_out[416] = \t 28\n",
      "tensor_out[417] = \t 42\n",
      "tensor_out[418] = \t 23\n",
      "tensor_out[419] = \t 0\n",
      "tensor_out[420] = \t 0\n",
      "tensor_out[421] = \t 16\n",
      "tensor_out[422] = \t 0\n",
      "tensor_out[423] = \t 38\n",
      "tensor_out[424] = \t 0\n",
      "tensor_out[425] = \t 63\n",
      "tensor_out[426] = \t 46\n",
      "tensor_out[427] = \t 124\n",
      "tensor_out[428] = \t 0\n",
      "tensor_out[429] = \t 0\n",
      "tensor_out[430] = \t 0\n",
      "tensor_out[431] = \t 0\n",
      "tensor_out[432] = \t 0\n",
      "tensor_out[433] = \t 0\n",
      "tensor_out[434] = \t 0\n",
      "tensor_out[435] = \t 0\n",
      "tensor_out[436] = \t 19\n",
      "tensor_out[437] = \t 95\n",
      "tensor_out[438] = \t 94\n",
      "tensor_out[439] = \t 52\n",
      "tensor_out[440] = \t 0\n",
      "tensor_out[441] = \t 69\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 63\n",
      "tensor_out[444] = \t 0\n",
      "tensor_out[445] = \t 60\n",
      "tensor_out[446] = \t 0\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 0\n",
      "tensor_out[449] = \t 10\n",
      "tensor_out[450] = \t 89\n",
      "tensor_out[451] = \t 0\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 76\n",
      "tensor_out[454] = \t 0\n",
      "tensor_out[455] = \t 0\n",
      "tensor_out[456] = \t 150\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 0\n",
      "tensor_out[459] = \t 0\n",
      "tensor_out[460] = \t 26\n",
      "tensor_out[461] = \t 0\n",
      "tensor_out[462] = \t 0\n",
      "tensor_out[463] = \t 0\n",
      "tensor_out[464] = \t 10\n",
      "tensor_out[465] = \t 0\n",
      "tensor_out[466] = \t 0\n",
      "tensor_out[467] = \t 0\n",
      "tensor_out[468] = \t 0\n",
      "tensor_out[469] = \t 0\n",
      "tensor_out[470] = \t 0\n",
      "tensor_out[471] = \t 0\n",
      "tensor_out[472] = \t 35\n",
      "tensor_out[473] = \t 133\n",
      "tensor_out[474] = \t 0\n",
      "tensor_out[475] = \t 0\n",
      "tensor_out[476] = \t 0\n",
      "tensor_out[477] = \t 0\n",
      "tensor_out[478] = \t 0\n",
      "tensor_out[479] = \t 0\n",
      "tensor_out[480] = \t 33\n",
      "tensor_out[481] = \t 57\n",
      "tensor_out[482] = \t 15\n",
      "tensor_out[483] = \t 27\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 11\n",
      "tensor_out[486] = \t 36\n",
      "tensor_out[487] = \t 29\n",
      "tensor_out[488] = \t 40\n",
      "tensor_out[489] = \t 61\n",
      "tensor_out[490] = \t 0\n",
      "tensor_out[491] = \t 55\n",
      "tensor_out[492] = \t 202\n",
      "tensor_out[493] = \t 73\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 0\n",
      "tensor_out[496] = \t 0\n",
      "tensor_out[497] = \t 0\n",
      "tensor_out[498] = \t 0\n",
      "tensor_out[499] = \t 0\n",
      "tensor_out[500] = \t 57\n",
      "tensor_out[501] = \t 40\n",
      "tensor_out[502] = \t 0\n",
      "tensor_out[503] = \t 59\n",
      "tensor_out[504] = \t 44\n",
      "tensor_out[505] = \t 68\n",
      "tensor_out[506] = \t 115\n",
      "tensor_out[507] = \t 71\n",
      "tensor_out[508] = \t 28\n",
      "tensor_out[509] = \t 0\n",
      "tensor_out[510] = \t 75\n",
      "tensor_out[511] = \t 0\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer17 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0\n",
      "tensor_out[1] = \t 0\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 7\n",
      "tensor_out[4] = \t 0\n",
      "tensor_out[5] = \t 0\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 150\n",
      "tensor_out[9] = \t 0\n",
      "tensor_out[10] = \t 89\n",
      "tensor_out[11] = \t 0\n",
      "tensor_out[12] = \t 0\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 16\n",
      "tensor_out[15] = \t 17\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 84\n",
      "tensor_out[18] = \t 7\n",
      "tensor_out[19] = \t 102\n",
      "tensor_out[20] = \t 0\n",
      "tensor_out[21] = \t 101\n",
      "tensor_out[22] = \t 0\n",
      "tensor_out[23] = \t 31\n",
      "tensor_out[24] = \t 94\n",
      "tensor_out[25] = \t 31\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 78\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 0\n",
      "tensor_out[30] = \t 0\n",
      "tensor_out[31] = \t 0\n",
      "tensor_out[32] = \t 0\n",
      "tensor_out[33] = \t 129\n",
      "tensor_out[34] = \t 0\n",
      "tensor_out[35] = \t 0\n",
      "tensor_out[36] = \t 0\n",
      "tensor_out[37] = \t 24\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 18\n",
      "tensor_out[40] = \t 131\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 45\n",
      "tensor_out[44] = \t 35\n",
      "tensor_out[45] = \t 0\n",
      "tensor_out[46] = \t 0\n",
      "tensor_out[47] = \t 72\n",
      "tensor_out[48] = \t 0\n",
      "tensor_out[49] = \t 0\n",
      "tensor_out[50] = \t 0\n",
      "tensor_out[51] = \t 0\n",
      "tensor_out[52] = \t 0\n",
      "tensor_out[53] = \t 94\n",
      "tensor_out[54] = \t 81\n",
      "tensor_out[55] = \t 81\n",
      "tensor_out[56] = \t 162\n",
      "tensor_out[57] = \t 0\n",
      "tensor_out[58] = \t 110\n",
      "tensor_out[59] = \t 0\n",
      "tensor_out[60] = \t 119\n",
      "tensor_out[61] = \t 0\n",
      "tensor_out[62] = \t 4\n",
      "tensor_out[63] = \t 16\n",
      "tensor_out[64] = \t 0\n",
      "tensor_out[65] = \t 89\n",
      "tensor_out[66] = \t 64\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 6\n",
      "tensor_out[70] = \t 210\n",
      "tensor_out[71] = \t 99\n",
      "tensor_out[72] = \t 19\n",
      "tensor_out[73] = \t 23\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 0\n",
      "tensor_out[76] = \t 14\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 0\n",
      "tensor_out[79] = \t 36\n",
      "tensor_out[80] = \t 16\n",
      "tensor_out[81] = \t 0\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 104\n",
      "tensor_out[84] = \t 0\n",
      "tensor_out[85] = \t 38\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 80\n",
      "tensor_out[88] = \t 15\n",
      "tensor_out[89] = \t 12\n",
      "tensor_out[90] = \t 0\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 75\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 0\n",
      "tensor_out[95] = \t 91\n",
      "tensor_out[96] = \t 1\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 7\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 31\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 15\n",
      "tensor_out[103] = \t 4\n",
      "tensor_out[104] = \t 0\n",
      "tensor_out[105] = \t 24\n",
      "tensor_out[106] = \t 39\n",
      "tensor_out[107] = \t 15\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 39\n",
      "tensor_out[110] = \t 255\n",
      "tensor_out[111] = \t 115\n",
      "tensor_out[112] = \t 37\n",
      "tensor_out[113] = \t 118\n",
      "tensor_out[114] = \t 35\n",
      "tensor_out[115] = \t 135\n",
      "tensor_out[116] = \t 46\n",
      "tensor_out[117] = \t 0\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 53\n",
      "tensor_out[120] = \t 0\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 0\n",
      "tensor_out[123] = \t 22\n",
      "tensor_out[124] = \t 36\n",
      "tensor_out[125] = \t 98\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 106\n",
      "tensor_out[129] = \t 37\n",
      "tensor_out[130] = \t 0\n",
      "tensor_out[131] = \t 84\n",
      "tensor_out[132] = \t 27\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 0\n",
      "tensor_out[135] = \t 0\n",
      "tensor_out[136] = \t 35\n",
      "tensor_out[137] = \t 0\n",
      "tensor_out[138] = \t 0\n",
      "tensor_out[139] = \t 0\n",
      "tensor_out[140] = \t 0\n",
      "tensor_out[141] = \t 0\n",
      "tensor_out[142] = \t 30\n",
      "tensor_out[143] = \t 0\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 0\n",
      "tensor_out[146] = \t 0\n",
      "tensor_out[147] = \t 36\n",
      "tensor_out[148] = \t 48\n",
      "tensor_out[149] = \t 29\n",
      "tensor_out[150] = \t 42\n",
      "tensor_out[151] = \t 0\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 13\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 0\n",
      "tensor_out[156] = \t 0\n",
      "tensor_out[157] = \t 106\n",
      "tensor_out[158] = \t 0\n",
      "tensor_out[159] = \t 94\n",
      "tensor_out[160] = \t 0\n",
      "tensor_out[161] = \t 102\n",
      "tensor_out[162] = \t 0\n",
      "tensor_out[163] = \t 0\n",
      "tensor_out[164] = \t 0\n",
      "tensor_out[165] = \t 8\n",
      "tensor_out[166] = \t 39\n",
      "tensor_out[167] = \t 0\n",
      "tensor_out[168] = \t 0\n",
      "tensor_out[169] = \t 20\n",
      "tensor_out[170] = \t 84\n",
      "tensor_out[171] = \t 0\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 0\n",
      "tensor_out[174] = \t 0\n",
      "tensor_out[175] = \t 67\n",
      "tensor_out[176] = \t 63\n",
      "tensor_out[177] = \t 20\n",
      "tensor_out[178] = \t 0\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 55\n",
      "tensor_out[181] = \t 0\n",
      "tensor_out[182] = \t 109\n",
      "tensor_out[183] = \t 0\n",
      "tensor_out[184] = \t 1\n",
      "tensor_out[185] = \t 0\n",
      "tensor_out[186] = \t 0\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 61\n",
      "tensor_out[189] = \t 43\n",
      "tensor_out[190] = \t 41\n",
      "tensor_out[191] = \t 88\n",
      "tensor_out[192] = \t 67\n",
      "tensor_out[193] = \t 10\n",
      "tensor_out[194] = \t 51\n",
      "tensor_out[195] = \t 117\n",
      "tensor_out[196] = \t 0\n",
      "tensor_out[197] = \t 106\n",
      "tensor_out[198] = \t 0\n",
      "tensor_out[199] = \t 1\n",
      "tensor_out[200] = \t 136\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 0\n",
      "tensor_out[203] = \t 0\n",
      "tensor_out[204] = \t 68\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 0\n",
      "tensor_out[207] = \t 0\n",
      "tensor_out[208] = \t 38\n",
      "tensor_out[209] = \t 54\n",
      "tensor_out[210] = \t 52\n",
      "tensor_out[211] = \t 69\n",
      "tensor_out[212] = \t 0\n",
      "tensor_out[213] = \t 0\n",
      "tensor_out[214] = \t 0\n",
      "tensor_out[215] = \t 0\n",
      "tensor_out[216] = \t 0\n",
      "tensor_out[217] = \t 4\n",
      "tensor_out[218] = \t 57\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 0\n",
      "tensor_out[221] = \t 43\n",
      "tensor_out[222] = \t 136\n",
      "tensor_out[223] = \t 0\n",
      "tensor_out[224] = \t 0\n",
      "tensor_out[225] = \t 60\n",
      "tensor_out[226] = \t 38\n",
      "tensor_out[227] = \t 132\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 1\n",
      "tensor_out[231] = \t 0\n",
      "tensor_out[232] = \t 20\n",
      "tensor_out[233] = \t 100\n",
      "tensor_out[234] = \t 78\n",
      "tensor_out[235] = \t 69\n",
      "tensor_out[236] = \t 9\n",
      "tensor_out[237] = \t 42\n",
      "tensor_out[238] = \t 30\n",
      "tensor_out[239] = \t 0\n",
      "tensor_out[240] = \t 196\n",
      "tensor_out[241] = \t 65\n",
      "tensor_out[242] = \t 101\n",
      "tensor_out[243] = \t 0\n",
      "tensor_out[244] = \t 0\n",
      "tensor_out[245] = \t 104\n",
      "tensor_out[246] = \t 120\n",
      "tensor_out[247] = \t 0\n",
      "tensor_out[248] = \t 119\n",
      "tensor_out[249] = \t 0\n",
      "tensor_out[250] = \t 0\n",
      "tensor_out[251] = \t 0\n",
      "tensor_out[252] = \t 64\n",
      "tensor_out[253] = \t 0\n",
      "tensor_out[254] = \t 42\n",
      "tensor_out[255] = \t 70\n",
      "tensor_out[256] = \t 0\n",
      "tensor_out[257] = \t 0\n",
      "tensor_out[258] = \t 20\n",
      "tensor_out[259] = \t 79\n",
      "tensor_out[260] = \t 116\n",
      "tensor_out[261] = \t 0\n",
      "tensor_out[262] = \t 0\n",
      "tensor_out[263] = \t 0\n",
      "tensor_out[264] = \t 42\n",
      "tensor_out[265] = \t 107\n",
      "tensor_out[266] = \t 143\n",
      "tensor_out[267] = \t 142\n",
      "tensor_out[268] = \t 0\n",
      "tensor_out[269] = \t 0\n",
      "tensor_out[270] = \t 0\n",
      "tensor_out[271] = \t 29\n",
      "tensor_out[272] = \t 0\n",
      "tensor_out[273] = \t 74\n",
      "tensor_out[274] = \t 115\n",
      "tensor_out[275] = \t 0\n",
      "tensor_out[276] = \t 40\n",
      "tensor_out[277] = \t 121\n",
      "tensor_out[278] = \t 0\n",
      "tensor_out[279] = \t 0\n",
      "tensor_out[280] = \t 0\n",
      "tensor_out[281] = \t 45\n",
      "tensor_out[282] = \t 0\n",
      "tensor_out[283] = \t 63\n",
      "tensor_out[284] = \t 0\n",
      "tensor_out[285] = \t 0\n",
      "tensor_out[286] = \t 0\n",
      "tensor_out[287] = \t 0\n",
      "tensor_out[288] = \t 0\n",
      "tensor_out[289] = \t 0\n",
      "tensor_out[290] = \t 46\n",
      "tensor_out[291] = \t 0\n",
      "tensor_out[292] = \t 0\n",
      "tensor_out[293] = \t 28\n",
      "tensor_out[294] = \t 0\n",
      "tensor_out[295] = \t 45\n",
      "tensor_out[296] = \t 11\n",
      "tensor_out[297] = \t 26\n",
      "tensor_out[298] = \t 0\n",
      "tensor_out[299] = \t 99\n",
      "tensor_out[300] = \t 21\n",
      "tensor_out[301] = \t 35\n",
      "tensor_out[302] = \t 0\n",
      "tensor_out[303] = \t 13\n",
      "tensor_out[304] = \t 0\n",
      "tensor_out[305] = \t 5\n",
      "tensor_out[306] = \t 43\n",
      "tensor_out[307] = \t 0\n",
      "tensor_out[308] = \t 105\n",
      "tensor_out[309] = \t 6\n",
      "tensor_out[310] = \t 0\n",
      "tensor_out[311] = \t 0\n",
      "tensor_out[312] = \t 0\n",
      "tensor_out[313] = \t 137\n",
      "tensor_out[314] = \t 0\n",
      "tensor_out[315] = \t 0\n",
      "tensor_out[316] = \t 0\n",
      "tensor_out[317] = \t 0\n",
      "tensor_out[318] = \t 15\n",
      "tensor_out[319] = \t 0\n",
      "tensor_out[320] = \t 123\n",
      "tensor_out[321] = \t 0\n",
      "tensor_out[322] = \t 0\n",
      "tensor_out[323] = \t 14\n",
      "tensor_out[324] = \t 24\n",
      "tensor_out[325] = \t 0\n",
      "tensor_out[326] = \t 10\n",
      "tensor_out[327] = \t 60\n",
      "tensor_out[328] = \t 0\n",
      "tensor_out[329] = \t 0\n",
      "tensor_out[330] = \t 7\n",
      "tensor_out[331] = \t 0\n",
      "tensor_out[332] = \t 106\n",
      "tensor_out[333] = \t 31\n",
      "tensor_out[334] = \t 0\n",
      "tensor_out[335] = \t 41\n",
      "tensor_out[336] = \t 0\n",
      "tensor_out[337] = \t 0\n",
      "tensor_out[338] = \t 0\n",
      "tensor_out[339] = \t 0\n",
      "tensor_out[340] = \t 66\n",
      "tensor_out[341] = \t 6\n",
      "tensor_out[342] = \t 13\n",
      "tensor_out[343] = \t 0\n",
      "tensor_out[344] = \t 21\n",
      "tensor_out[345] = \t 3\n",
      "tensor_out[346] = \t 66\n",
      "tensor_out[347] = \t 41\n",
      "tensor_out[348] = \t 24\n",
      "tensor_out[349] = \t 78\n",
      "tensor_out[350] = \t 77\n",
      "tensor_out[351] = \t 76\n",
      "tensor_out[352] = \t 0\n",
      "tensor_out[353] = \t 48\n",
      "tensor_out[354] = \t 0\n",
      "tensor_out[355] = \t 0\n",
      "tensor_out[356] = \t 0\n",
      "tensor_out[357] = \t 0\n",
      "tensor_out[358] = \t 0\n",
      "tensor_out[359] = \t 0\n",
      "tensor_out[360] = \t 0\n",
      "tensor_out[361] = \t 0\n",
      "tensor_out[362] = \t 203\n",
      "tensor_out[363] = \t 17\n",
      "tensor_out[364] = \t 0\n",
      "tensor_out[365] = \t 0\n",
      "tensor_out[366] = \t 51\n",
      "tensor_out[367] = \t 0\n",
      "tensor_out[368] = \t 29\n",
      "tensor_out[369] = \t 0\n",
      "tensor_out[370] = \t 72\n",
      "tensor_out[371] = \t 0\n",
      "tensor_out[372] = \t 11\n",
      "tensor_out[373] = \t 27\n",
      "tensor_out[374] = \t 0\n",
      "tensor_out[375] = \t 0\n",
      "tensor_out[376] = \t 0\n",
      "tensor_out[377] = \t 72\n",
      "tensor_out[378] = \t 0\n",
      "tensor_out[379] = \t 0\n",
      "tensor_out[380] = \t 0\n",
      "tensor_out[381] = \t 69\n",
      "tensor_out[382] = \t 0\n",
      "tensor_out[383] = \t 24\n",
      "tensor_out[384] = \t 0\n",
      "tensor_out[385] = \t 0\n",
      "tensor_out[386] = \t 35\n",
      "tensor_out[387] = \t 72\n",
      "tensor_out[388] = \t 0\n",
      "tensor_out[389] = \t 0\n",
      "tensor_out[390] = \t 12\n",
      "tensor_out[391] = \t 69\n",
      "tensor_out[392] = \t 83\n",
      "tensor_out[393] = \t 0\n",
      "tensor_out[394] = \t 0\n",
      "tensor_out[395] = \t 0\n",
      "tensor_out[396] = \t 0\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 0\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 43\n",
      "tensor_out[401] = \t 22\n",
      "tensor_out[402] = \t 0\n",
      "tensor_out[403] = \t 91\n",
      "tensor_out[404] = \t 41\n",
      "tensor_out[405] = \t 37\n",
      "tensor_out[406] = \t 0\n",
      "tensor_out[407] = \t 36\n",
      "tensor_out[408] = \t 53\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 49\n",
      "tensor_out[411] = \t 64\n",
      "tensor_out[412] = \t 2\n",
      "tensor_out[413] = \t 0\n",
      "tensor_out[414] = \t 75\n",
      "tensor_out[415] = \t 0\n",
      "tensor_out[416] = \t 0\n",
      "tensor_out[417] = \t 0\n",
      "tensor_out[418] = \t 152\n",
      "tensor_out[419] = \t 34\n",
      "tensor_out[420] = \t 73\n",
      "tensor_out[421] = \t 0\n",
      "tensor_out[422] = \t 0\n",
      "tensor_out[423] = \t 27\n",
      "tensor_out[424] = \t 0\n",
      "tensor_out[425] = \t 104\n",
      "tensor_out[426] = \t 0\n",
      "tensor_out[427] = \t 7\n",
      "tensor_out[428] = \t 33\n",
      "tensor_out[429] = \t 23\n",
      "tensor_out[430] = \t 75\n",
      "tensor_out[431] = \t 0\n",
      "tensor_out[432] = \t 0\n",
      "tensor_out[433] = \t 64\n",
      "tensor_out[434] = \t 0\n",
      "tensor_out[435] = \t 29\n",
      "tensor_out[436] = \t 25\n",
      "tensor_out[437] = \t 0\n",
      "tensor_out[438] = \t 48\n",
      "tensor_out[439] = \t 0\n",
      "tensor_out[440] = \t 107\n",
      "tensor_out[441] = \t 4\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 0\n",
      "tensor_out[444] = \t 0\n",
      "tensor_out[445] = \t 0\n",
      "tensor_out[446] = \t 0\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 0\n",
      "tensor_out[449] = \t 41\n",
      "tensor_out[450] = \t 90\n",
      "tensor_out[451] = \t 0\n",
      "tensor_out[452] = \t 100\n",
      "tensor_out[453] = \t 0\n",
      "tensor_out[454] = \t 0\n",
      "tensor_out[455] = \t 7\n",
      "tensor_out[456] = \t 54\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 0\n",
      "tensor_out[459] = \t 0\n",
      "tensor_out[460] = \t 0\n",
      "tensor_out[461] = \t 0\n",
      "tensor_out[462] = \t 8\n",
      "tensor_out[463] = \t 6\n",
      "tensor_out[464] = \t 49\n",
      "tensor_out[465] = \t 0\n",
      "tensor_out[466] = \t 52\n",
      "tensor_out[467] = \t 0\n",
      "tensor_out[468] = \t 84\n",
      "tensor_out[469] = \t 96\n",
      "tensor_out[470] = \t 63\n",
      "tensor_out[471] = \t 40\n",
      "tensor_out[472] = \t 88\n",
      "tensor_out[473] = \t 0\n",
      "tensor_out[474] = \t 0\n",
      "tensor_out[475] = \t 0\n",
      "tensor_out[476] = \t 0\n",
      "tensor_out[477] = \t 0\n",
      "tensor_out[478] = \t 26\n",
      "tensor_out[479] = \t 0\n",
      "tensor_out[480] = \t 0\n",
      "tensor_out[481] = \t 0\n",
      "tensor_out[482] = \t 72\n",
      "tensor_out[483] = \t 0\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 0\n",
      "tensor_out[486] = \t 29\n",
      "tensor_out[487] = \t 0\n",
      "tensor_out[488] = \t 0\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 0\n",
      "tensor_out[491] = \t 0\n",
      "tensor_out[492] = \t 0\n",
      "tensor_out[493] = \t 60\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 0\n",
      "tensor_out[496] = \t 102\n",
      "tensor_out[497] = \t 78\n",
      "tensor_out[498] = \t 0\n",
      "tensor_out[499] = \t 0\n",
      "tensor_out[500] = \t 116\n",
      "tensor_out[501] = \t 0\n",
      "tensor_out[502] = \t 0\n",
      "tensor_out[503] = \t 0\n",
      "tensor_out[504] = \t 0\n",
      "tensor_out[505] = \t 20\n",
      "tensor_out[506] = \t 34\n",
      "tensor_out[507] = \t 0\n",
      "tensor_out[508] = \t 0\n",
      "tensor_out[509] = \t 102\n",
      "tensor_out[510] = \t 0\n",
      "tensor_out[511] = \t 29\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer18 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 39\n",
      "tensor_out[1] = \t 0\n",
      "tensor_out[2] = \t 43\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 40\n",
      "tensor_out[5] = \t 50\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 51\n",
      "tensor_out[8] = \t 0\n",
      "tensor_out[9] = \t 0\n",
      "tensor_out[10] = \t 0\n",
      "tensor_out[11] = \t 0\n",
      "tensor_out[12] = \t 0\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 0\n",
      "tensor_out[15] = \t 0\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 0\n",
      "tensor_out[19] = \t 21\n",
      "tensor_out[20] = \t 39\n",
      "tensor_out[21] = \t 123\n",
      "tensor_out[22] = \t 0\n",
      "tensor_out[23] = \t 0\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 14\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 2\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 0\n",
      "tensor_out[30] = \t 0\n",
      "tensor_out[31] = \t 36\n",
      "tensor_out[32] = \t 14\n",
      "tensor_out[33] = \t 70\n",
      "tensor_out[34] = \t 51\n",
      "tensor_out[35] = \t 0\n",
      "tensor_out[36] = \t 46\n",
      "tensor_out[37] = \t 62\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 45\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 101\n",
      "tensor_out[45] = \t 20\n",
      "tensor_out[46] = \t 0\n",
      "tensor_out[47] = \t 39\n",
      "tensor_out[48] = \t 0\n",
      "tensor_out[49] = \t 9\n",
      "tensor_out[50] = \t 53\n",
      "tensor_out[51] = \t 74\n",
      "tensor_out[52] = \t 0\n",
      "tensor_out[53] = \t 46\n",
      "tensor_out[54] = \t 0\n",
      "tensor_out[55] = \t 0\n",
      "tensor_out[56] = \t 204\n",
      "tensor_out[57] = \t 0\n",
      "tensor_out[58] = \t 51\n",
      "tensor_out[59] = \t 0\n",
      "tensor_out[60] = \t 0\n",
      "tensor_out[61] = \t 62\n",
      "tensor_out[62] = \t 0\n",
      "tensor_out[63] = \t 63\n",
      "tensor_out[64] = \t 46\n",
      "tensor_out[65] = \t 131\n",
      "tensor_out[66] = \t 11\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 62\n",
      "tensor_out[69] = \t 39\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 0\n",
      "tensor_out[72] = \t 0\n",
      "tensor_out[73] = \t 0\n",
      "tensor_out[74] = \t 46\n",
      "tensor_out[75] = \t 0\n",
      "tensor_out[76] = \t 40\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 0\n",
      "tensor_out[79] = \t 39\n",
      "tensor_out[80] = \t 13\n",
      "tensor_out[81] = \t 2\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 14\n",
      "tensor_out[84] = \t 0\n",
      "tensor_out[85] = \t 0\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 0\n",
      "tensor_out[88] = \t 47\n",
      "tensor_out[89] = \t 57\n",
      "tensor_out[90] = \t 0\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 38\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 43\n",
      "tensor_out[95] = \t 0\n",
      "tensor_out[96] = \t 0\n",
      "tensor_out[97] = \t 21\n",
      "tensor_out[98] = \t 51\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 0\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 0\n",
      "tensor_out[103] = \t 0\n",
      "tensor_out[104] = \t 0\n",
      "tensor_out[105] = \t 0\n",
      "tensor_out[106] = \t 21\n",
      "tensor_out[107] = \t 7\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 5\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 0\n",
      "tensor_out[112] = \t 18\n",
      "tensor_out[113] = \t 110\n",
      "tensor_out[114] = \t 6\n",
      "tensor_out[115] = \t 0\n",
      "tensor_out[116] = \t 91\n",
      "tensor_out[117] = \t 0\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 31\n",
      "tensor_out[120] = \t 0\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 0\n",
      "tensor_out[123] = \t 28\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 41\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 0\n",
      "tensor_out[129] = \t 75\n",
      "tensor_out[130] = \t 50\n",
      "tensor_out[131] = \t 136\n",
      "tensor_out[132] = \t 6\n",
      "tensor_out[133] = \t 54\n",
      "tensor_out[134] = \t 0\n",
      "tensor_out[135] = \t 0\n",
      "tensor_out[136] = \t 0\n",
      "tensor_out[137] = \t 0\n",
      "tensor_out[138] = \t 0\n",
      "tensor_out[139] = \t 47\n",
      "tensor_out[140] = \t 0\n",
      "tensor_out[141] = \t 45\n",
      "tensor_out[142] = \t 0\n",
      "tensor_out[143] = \t 49\n",
      "tensor_out[144] = \t 46\n",
      "tensor_out[145] = \t 52\n",
      "tensor_out[146] = \t 0\n",
      "tensor_out[147] = \t 24\n",
      "tensor_out[148] = \t 5\n",
      "tensor_out[149] = \t 0\n",
      "tensor_out[150] = \t 0\n",
      "tensor_out[151] = \t 0\n",
      "tensor_out[152] = \t 10\n",
      "tensor_out[153] = \t 25\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 0\n",
      "tensor_out[156] = \t 0\n",
      "tensor_out[157] = \t 107\n",
      "tensor_out[158] = \t 0\n",
      "tensor_out[159] = \t 58\n",
      "tensor_out[160] = \t 0\n",
      "tensor_out[161] = \t 0\n",
      "tensor_out[162] = \t 0\n",
      "tensor_out[163] = \t 75\n",
      "tensor_out[164] = \t 0\n",
      "tensor_out[165] = \t 31\n",
      "tensor_out[166] = \t 47\n",
      "tensor_out[167] = \t 0\n",
      "tensor_out[168] = \t 60\n",
      "tensor_out[169] = \t 50\n",
      "tensor_out[170] = \t 16\n",
      "tensor_out[171] = \t 0\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 12\n",
      "tensor_out[174] = \t 3\n",
      "tensor_out[175] = \t 5\n",
      "tensor_out[176] = \t 80\n",
      "tensor_out[177] = \t 13\n",
      "tensor_out[178] = \t 9\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 13\n",
      "tensor_out[181] = \t 0\n",
      "tensor_out[182] = \t 83\n",
      "tensor_out[183] = \t 0\n",
      "tensor_out[184] = \t 69\n",
      "tensor_out[185] = \t 61\n",
      "tensor_out[186] = \t 0\n",
      "tensor_out[187] = \t 2\n",
      "tensor_out[188] = \t 216\n",
      "tensor_out[189] = \t 37\n",
      "tensor_out[190] = \t 0\n",
      "tensor_out[191] = \t 47\n",
      "tensor_out[192] = \t 46\n",
      "tensor_out[193] = \t 36\n",
      "tensor_out[194] = \t 4\n",
      "tensor_out[195] = \t 90\n",
      "tensor_out[196] = \t 0\n",
      "tensor_out[197] = \t 0\n",
      "tensor_out[198] = \t 63\n",
      "tensor_out[199] = \t 0\n",
      "tensor_out[200] = \t 30\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 0\n",
      "tensor_out[203] = \t 0\n",
      "tensor_out[204] = \t 151\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 45\n",
      "tensor_out[207] = \t 0\n",
      "tensor_out[208] = \t 68\n",
      "tensor_out[209] = \t 17\n",
      "tensor_out[210] = \t 0\n",
      "tensor_out[211] = \t 0\n",
      "tensor_out[212] = \t 0\n",
      "tensor_out[213] = \t 0\n",
      "tensor_out[214] = \t 14\n",
      "tensor_out[215] = \t 0\n",
      "tensor_out[216] = \t 0\n",
      "tensor_out[217] = \t 9\n",
      "tensor_out[218] = \t 54\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 0\n",
      "tensor_out[221] = \t 51\n",
      "tensor_out[222] = \t 40\n",
      "tensor_out[223] = \t 0\n",
      "tensor_out[224] = \t 114\n",
      "tensor_out[225] = \t 0\n",
      "tensor_out[226] = \t 34\n",
      "tensor_out[227] = \t 0\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 0\n",
      "tensor_out[231] = \t 68\n",
      "tensor_out[232] = \t 0\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 60\n",
      "tensor_out[235] = \t 38\n",
      "tensor_out[236] = \t 53\n",
      "tensor_out[237] = \t 45\n",
      "tensor_out[238] = \t 80\n",
      "tensor_out[239] = \t 0\n",
      "tensor_out[240] = \t 0\n",
      "tensor_out[241] = \t 66\n",
      "tensor_out[242] = \t 0\n",
      "tensor_out[243] = \t 0\n",
      "tensor_out[244] = \t 0\n",
      "tensor_out[245] = \t 143\n",
      "tensor_out[246] = \t 92\n",
      "tensor_out[247] = \t 50\n",
      "tensor_out[248] = \t 46\n",
      "tensor_out[249] = \t 39\n",
      "tensor_out[250] = \t 0\n",
      "tensor_out[251] = \t 49\n",
      "tensor_out[252] = \t 80\n",
      "tensor_out[253] = \t 0\n",
      "tensor_out[254] = \t 16\n",
      "tensor_out[255] = \t 0\n",
      "tensor_out[256] = \t 65\n",
      "tensor_out[257] = \t 60\n",
      "tensor_out[258] = \t 0\n",
      "tensor_out[259] = \t 68\n",
      "tensor_out[260] = \t 107\n",
      "tensor_out[261] = \t 0\n",
      "tensor_out[262] = \t 0\n",
      "tensor_out[263] = \t 38\n",
      "tensor_out[264] = \t 0\n",
      "tensor_out[265] = \t 116\n",
      "tensor_out[266] = \t 181\n",
      "tensor_out[267] = \t 70\n",
      "tensor_out[268] = \t 0\n",
      "tensor_out[269] = \t 3\n",
      "tensor_out[270] = \t 0\n",
      "tensor_out[271] = \t 43\n",
      "tensor_out[272] = \t 0\n",
      "tensor_out[273] = \t 24\n",
      "tensor_out[274] = \t 70\n",
      "tensor_out[275] = \t 0\n",
      "tensor_out[276] = \t 0\n",
      "tensor_out[277] = \t 0\n",
      "tensor_out[278] = \t 0\n",
      "tensor_out[279] = \t 22\n",
      "tensor_out[280] = \t 0\n",
      "tensor_out[281] = \t 1\n",
      "tensor_out[282] = \t 0\n",
      "tensor_out[283] = \t 0\n",
      "tensor_out[284] = \t 57\n",
      "tensor_out[285] = \t 0\n",
      "tensor_out[286] = \t 0\n",
      "tensor_out[287] = \t 0\n",
      "tensor_out[288] = \t 0\n",
      "tensor_out[289] = \t 16\n",
      "tensor_out[290] = \t 91\n",
      "tensor_out[291] = \t 0\n",
      "tensor_out[292] = \t 0\n",
      "tensor_out[293] = \t 0\n",
      "tensor_out[294] = \t 13\n",
      "tensor_out[295] = \t 14\n",
      "tensor_out[296] = \t 0\n",
      "tensor_out[297] = \t 0\n",
      "tensor_out[298] = \t 58\n",
      "tensor_out[299] = \t 100\n",
      "tensor_out[300] = \t 0\n",
      "tensor_out[301] = \t 11\n",
      "tensor_out[302] = \t 0\n",
      "tensor_out[303] = \t 0\n",
      "tensor_out[304] = \t 0\n",
      "tensor_out[305] = \t 0\n",
      "tensor_out[306] = \t 0\n",
      "tensor_out[307] = \t 0\n",
      "tensor_out[308] = \t 18\n",
      "tensor_out[309] = \t 75\n",
      "tensor_out[310] = \t 57\n",
      "tensor_out[311] = \t 2\n",
      "tensor_out[312] = \t 68\n",
      "tensor_out[313] = \t 62\n",
      "tensor_out[314] = \t 60\n",
      "tensor_out[315] = \t 45\n",
      "tensor_out[316] = \t 45\n",
      "tensor_out[317] = \t 49\n",
      "tensor_out[318] = \t 85\n",
      "tensor_out[319] = \t 47\n",
      "tensor_out[320] = \t 0\n",
      "tensor_out[321] = \t 0\n",
      "tensor_out[322] = \t 0\n",
      "tensor_out[323] = \t 49\n",
      "tensor_out[324] = \t 29\n",
      "tensor_out[325] = \t 53\n",
      "tensor_out[326] = \t 25\n",
      "tensor_out[327] = \t 11\n",
      "tensor_out[328] = \t 0\n",
      "tensor_out[329] = \t 60\n",
      "tensor_out[330] = \t 0\n",
      "tensor_out[331] = \t 44\n",
      "tensor_out[332] = \t 30\n",
      "tensor_out[333] = \t 0\n",
      "tensor_out[334] = \t 17\n",
      "tensor_out[335] = \t 43\n",
      "tensor_out[336] = \t 5\n",
      "tensor_out[337] = \t 0\n",
      "tensor_out[338] = \t 0\n",
      "tensor_out[339] = \t 32\n",
      "tensor_out[340] = \t 0\n",
      "tensor_out[341] = \t 14\n",
      "tensor_out[342] = \t 23\n",
      "tensor_out[343] = \t 0\n",
      "tensor_out[344] = \t 33\n",
      "tensor_out[345] = \t 40\n",
      "tensor_out[346] = \t 23\n",
      "tensor_out[347] = \t 0\n",
      "tensor_out[348] = \t 0\n",
      "tensor_out[349] = \t 112\n",
      "tensor_out[350] = \t 47\n",
      "tensor_out[351] = \t 0\n",
      "tensor_out[352] = \t 32\n",
      "tensor_out[353] = \t 0\n",
      "tensor_out[354] = \t 0\n",
      "tensor_out[355] = \t 0\n",
      "tensor_out[356] = \t 0\n",
      "tensor_out[357] = \t 0\n",
      "tensor_out[358] = \t 37\n",
      "tensor_out[359] = \t 0\n",
      "tensor_out[360] = \t 0\n",
      "tensor_out[361] = \t 0\n",
      "tensor_out[362] = \t 79\n",
      "tensor_out[363] = \t 0\n",
      "tensor_out[364] = \t 0\n",
      "tensor_out[365] = \t 0\n",
      "tensor_out[366] = \t 68\n",
      "tensor_out[367] = \t 120\n",
      "tensor_out[368] = \t 53\n",
      "tensor_out[369] = \t 0\n",
      "tensor_out[370] = \t 42\n",
      "tensor_out[371] = \t 0\n",
      "tensor_out[372] = \t 45\n",
      "tensor_out[373] = \t 52\n",
      "tensor_out[374] = \t 0\n",
      "tensor_out[375] = \t 0\n",
      "tensor_out[376] = \t 19\n",
      "tensor_out[377] = \t 43\n",
      "tensor_out[378] = \t 61\n",
      "tensor_out[379] = \t 40\n",
      "tensor_out[380] = \t 0\n",
      "tensor_out[381] = \t 6\n",
      "tensor_out[382] = \t 47\n",
      "tensor_out[383] = \t 29\n",
      "tensor_out[384] = \t 50\n",
      "tensor_out[385] = \t 0\n",
      "tensor_out[386] = \t 0\n",
      "tensor_out[387] = \t 0\n",
      "tensor_out[388] = \t 55\n",
      "tensor_out[389] = \t 0\n",
      "tensor_out[390] = \t 0\n",
      "tensor_out[391] = \t 57\n",
      "tensor_out[392] = \t 64\n",
      "tensor_out[393] = \t 0\n",
      "tensor_out[394] = \t 48\n",
      "tensor_out[395] = \t 0\n",
      "tensor_out[396] = \t 45\n",
      "tensor_out[397] = \t 19\n",
      "tensor_out[398] = \t 0\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 117\n",
      "tensor_out[401] = \t 57\n",
      "tensor_out[402] = \t 73\n",
      "tensor_out[403] = \t 22\n",
      "tensor_out[404] = \t 56\n",
      "tensor_out[405] = \t 15\n",
      "tensor_out[406] = \t 0\n",
      "tensor_out[407] = \t 41\n",
      "tensor_out[408] = \t 0\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 0\n",
      "tensor_out[411] = \t 34\n",
      "tensor_out[412] = \t 1\n",
      "tensor_out[413] = \t 0\n",
      "tensor_out[414] = \t 8\n",
      "tensor_out[415] = \t 0\n",
      "tensor_out[416] = \t 0\n",
      "tensor_out[417] = \t 0\n",
      "tensor_out[418] = \t 36\n",
      "tensor_out[419] = \t 30\n",
      "tensor_out[420] = \t 17\n",
      "tensor_out[421] = \t 0\n",
      "tensor_out[422] = \t 0\n",
      "tensor_out[423] = \t 13\n",
      "tensor_out[424] = \t 0\n",
      "tensor_out[425] = \t 70\n",
      "tensor_out[426] = \t 66\n",
      "tensor_out[427] = \t 8\n",
      "tensor_out[428] = \t 0\n",
      "tensor_out[429] = \t 13\n",
      "tensor_out[430] = \t 0\n",
      "tensor_out[431] = \t 54\n",
      "tensor_out[432] = \t 54\n",
      "tensor_out[433] = \t 27\n",
      "tensor_out[434] = \t 41\n",
      "tensor_out[435] = \t 16\n",
      "tensor_out[436] = \t 28\n",
      "tensor_out[437] = \t 0\n",
      "tensor_out[438] = \t 66\n",
      "tensor_out[439] = \t 34\n",
      "tensor_out[440] = \t 0\n",
      "tensor_out[441] = \t 14\n",
      "tensor_out[442] = \t 4\n",
      "tensor_out[443] = \t 0\n",
      "tensor_out[444] = \t 1\n",
      "tensor_out[445] = \t 59\n",
      "tensor_out[446] = \t 62\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 0\n",
      "tensor_out[449] = \t 19\n",
      "tensor_out[450] = \t 44\n",
      "tensor_out[451] = \t 0\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 0\n",
      "tensor_out[454] = \t 58\n",
      "tensor_out[455] = \t 80\n",
      "tensor_out[456] = \t 21\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 0\n",
      "tensor_out[459] = \t 0\n",
      "tensor_out[460] = \t 33\n",
      "tensor_out[461] = \t 2\n",
      "tensor_out[462] = \t 0\n",
      "tensor_out[463] = \t 64\n",
      "tensor_out[464] = \t 17\n",
      "tensor_out[465] = \t 0\n",
      "tensor_out[466] = \t 70\n",
      "tensor_out[467] = \t 0\n",
      "tensor_out[468] = \t 18\n",
      "tensor_out[469] = \t 93\n",
      "tensor_out[470] = \t 76\n",
      "tensor_out[471] = \t 42\n",
      "tensor_out[472] = \t 11\n",
      "tensor_out[473] = \t 12\n",
      "tensor_out[474] = \t 0\n",
      "tensor_out[475] = \t 0\n",
      "tensor_out[476] = \t 0\n",
      "tensor_out[477] = \t 4\n",
      "tensor_out[478] = \t 57\n",
      "tensor_out[479] = \t 0\n",
      "tensor_out[480] = \t 5\n",
      "tensor_out[481] = \t 0\n",
      "tensor_out[482] = \t 56\n",
      "tensor_out[483] = \t 0\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 58\n",
      "tensor_out[486] = \t 26\n",
      "tensor_out[487] = \t 0\n",
      "tensor_out[488] = \t 60\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 0\n",
      "tensor_out[491] = \t 0\n",
      "tensor_out[492] = \t 0\n",
      "tensor_out[493] = \t 4\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 0\n",
      "tensor_out[496] = \t 0\n",
      "tensor_out[497] = \t 0\n",
      "tensor_out[498] = \t 0\n",
      "tensor_out[499] = \t 46\n",
      "tensor_out[500] = \t 0\n",
      "tensor_out[501] = \t 0\n",
      "tensor_out[502] = \t 0\n",
      "tensor_out[503] = \t 59\n",
      "tensor_out[504] = \t 0\n",
      "tensor_out[505] = \t 10\n",
      "tensor_out[506] = \t 42\n",
      "tensor_out[507] = \t 44\n",
      "tensor_out[508] = \t 0\n",
      "tensor_out[509] = \t 139\n",
      "tensor_out[510] = \t 0\n",
      "tensor_out[511] = \t 0\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer19 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 52\n",
      "tensor_out[1] = \t 166\n",
      "tensor_out[2] = \t 13\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 0\n",
      "tensor_out[5] = \t 10\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 15\n",
      "tensor_out[9] = \t 68\n",
      "tensor_out[10] = \t 25\n",
      "tensor_out[11] = \t 22\n",
      "tensor_out[12] = \t 30\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 12\n",
      "tensor_out[15] = \t 0\n",
      "tensor_out[16] = \t 2\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 0\n",
      "tensor_out[19] = \t 0\n",
      "tensor_out[20] = \t 16\n",
      "tensor_out[21] = \t 0\n",
      "tensor_out[22] = \t 39\n",
      "tensor_out[23] = \t 137\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 85\n",
      "tensor_out[26] = \t 61\n",
      "tensor_out[27] = \t 0\n",
      "tensor_out[28] = \t 51\n",
      "tensor_out[29] = \t 0\n",
      "tensor_out[30] = \t 73\n",
      "tensor_out[31] = \t 75\n",
      "tensor_out[32] = \t 36\n",
      "tensor_out[33] = \t 11\n",
      "tensor_out[34] = \t 0\n",
      "tensor_out[35] = \t 0\n",
      "tensor_out[36] = \t 0\n",
      "tensor_out[37] = \t 0\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 0\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 0\n",
      "tensor_out[45] = \t 0\n",
      "tensor_out[46] = \t 39\n",
      "tensor_out[47] = \t 210\n",
      "tensor_out[48] = \t 40\n",
      "tensor_out[49] = \t 0\n",
      "tensor_out[50] = \t 0\n",
      "tensor_out[51] = \t 0\n",
      "tensor_out[52] = \t 20\n",
      "tensor_out[53] = \t 51\n",
      "tensor_out[54] = \t 0\n",
      "tensor_out[55] = \t 40\n",
      "tensor_out[56] = \t 0\n",
      "tensor_out[57] = \t 0\n",
      "tensor_out[58] = \t 85\n",
      "tensor_out[59] = \t 0\n",
      "tensor_out[60] = \t 54\n",
      "tensor_out[61] = \t 0\n",
      "tensor_out[62] = \t 76\n",
      "tensor_out[63] = \t 128\n",
      "tensor_out[64] = \t 0\n",
      "tensor_out[65] = \t 69\n",
      "tensor_out[66] = \t 0\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 39\n",
      "tensor_out[72] = \t 0\n",
      "tensor_out[73] = \t 0\n",
      "tensor_out[74] = \t 253\n",
      "tensor_out[75] = \t 0\n",
      "tensor_out[76] = \t 7\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 0\n",
      "tensor_out[79] = \t 0\n",
      "tensor_out[80] = \t 0\n",
      "tensor_out[81] = \t 0\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 0\n",
      "tensor_out[84] = \t 0\n",
      "tensor_out[85] = \t 0\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 43\n",
      "tensor_out[88] = \t 0\n",
      "tensor_out[89] = \t 45\n",
      "tensor_out[90] = \t 0\n",
      "tensor_out[91] = \t 45\n",
      "tensor_out[92] = \t 19\n",
      "tensor_out[93] = \t 89\n",
      "tensor_out[94] = \t 70\n",
      "tensor_out[95] = \t 0\n",
      "tensor_out[96] = \t 0\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 0\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 0\n",
      "tensor_out[103] = \t 46\n",
      "tensor_out[104] = \t 0\n",
      "tensor_out[105] = \t 83\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 58\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 83\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 15\n",
      "tensor_out[112] = \t 0\n",
      "tensor_out[113] = \t 0\n",
      "tensor_out[114] = \t 20\n",
      "tensor_out[115] = \t 111\n",
      "tensor_out[116] = \t 0\n",
      "tensor_out[117] = \t 80\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 0\n",
      "tensor_out[120] = \t 0\n",
      "tensor_out[121] = \t 66\n",
      "tensor_out[122] = \t 26\n",
      "tensor_out[123] = \t 146\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 0\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 0\n",
      "tensor_out[129] = \t 0\n",
      "tensor_out[130] = \t 0\n",
      "tensor_out[131] = \t 8\n",
      "tensor_out[132] = \t 15\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 32\n",
      "tensor_out[135] = \t 59\n",
      "tensor_out[136] = \t 104\n",
      "tensor_out[137] = \t 63\n",
      "tensor_out[138] = \t 0\n",
      "tensor_out[139] = \t 44\n",
      "tensor_out[140] = \t 39\n",
      "tensor_out[141] = \t 68\n",
      "tensor_out[142] = \t 123\n",
      "tensor_out[143] = \t 108\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 40\n",
      "tensor_out[146] = \t 0\n",
      "tensor_out[147] = \t 22\n",
      "tensor_out[148] = \t 0\n",
      "tensor_out[149] = \t 0\n",
      "tensor_out[150] = \t 0\n",
      "tensor_out[151] = \t 41\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 0\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 0\n",
      "tensor_out[156] = \t 31\n",
      "tensor_out[157] = \t 12\n",
      "tensor_out[158] = \t 0\n",
      "tensor_out[159] = \t 70\n",
      "tensor_out[160] = \t 3\n",
      "tensor_out[161] = \t 68\n",
      "tensor_out[162] = \t 36\n",
      "tensor_out[163] = \t 0\n",
      "tensor_out[164] = \t 124\n",
      "tensor_out[165] = \t 0\n",
      "tensor_out[166] = \t 58\n",
      "tensor_out[167] = \t 83\n",
      "tensor_out[168] = \t 48\n",
      "tensor_out[169] = \t 19\n",
      "tensor_out[170] = \t 0\n",
      "tensor_out[171] = \t 0\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 0\n",
      "tensor_out[174] = \t 39\n",
      "tensor_out[175] = \t 0\n",
      "tensor_out[176] = \t 0\n",
      "tensor_out[177] = \t 33\n",
      "tensor_out[178] = \t 16\n",
      "tensor_out[179] = \t 7\n",
      "tensor_out[180] = \t 0\n",
      "tensor_out[181] = \t 0\n",
      "tensor_out[182] = \t 37\n",
      "tensor_out[183] = \t 0\n",
      "tensor_out[184] = \t 97\n",
      "tensor_out[185] = \t 53\n",
      "tensor_out[186] = \t 0\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 22\n",
      "tensor_out[189] = \t 0\n",
      "tensor_out[190] = \t 12\n",
      "tensor_out[191] = \t 0\n",
      "tensor_out[192] = \t 0\n",
      "tensor_out[193] = \t 64\n",
      "tensor_out[194] = \t 0\n",
      "tensor_out[195] = \t 71\n",
      "tensor_out[196] = \t 33\n",
      "tensor_out[197] = \t 134\n",
      "tensor_out[198] = \t 0\n",
      "tensor_out[199] = \t 21\n",
      "tensor_out[200] = \t 119\n",
      "tensor_out[201] = \t 139\n",
      "tensor_out[202] = \t 59\n",
      "tensor_out[203] = \t 65\n",
      "tensor_out[204] = \t 0\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 26\n",
      "tensor_out[207] = \t 45\n",
      "tensor_out[208] = \t 60\n",
      "tensor_out[209] = \t 0\n",
      "tensor_out[210] = \t 76\n",
      "tensor_out[211] = \t 8\n",
      "tensor_out[212] = \t 145\n",
      "tensor_out[213] = \t 18\n",
      "tensor_out[214] = \t 0\n",
      "tensor_out[215] = \t 0\n",
      "tensor_out[216] = \t 50\n",
      "tensor_out[217] = \t 0\n",
      "tensor_out[218] = \t 65\n",
      "tensor_out[219] = \t 104\n",
      "tensor_out[220] = \t 7\n",
      "tensor_out[221] = \t 0\n",
      "tensor_out[222] = \t 0\n",
      "tensor_out[223] = \t 0\n",
      "tensor_out[224] = \t 0\n",
      "tensor_out[225] = \t 0\n",
      "tensor_out[226] = \t 26\n",
      "tensor_out[227] = \t 0\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 0\n",
      "tensor_out[231] = \t 0\n",
      "tensor_out[232] = \t 0\n",
      "tensor_out[233] = \t 46\n",
      "tensor_out[234] = \t 11\n",
      "tensor_out[235] = \t 2\n",
      "tensor_out[236] = \t 1\n",
      "tensor_out[237] = \t 86\n",
      "tensor_out[238] = \t 16\n",
      "tensor_out[239] = \t 0\n",
      "tensor_out[240] = \t 33\n",
      "tensor_out[241] = \t 15\n",
      "tensor_out[242] = \t 47\n",
      "tensor_out[243] = \t 58\n",
      "tensor_out[244] = \t 69\n",
      "tensor_out[245] = \t 94\n",
      "tensor_out[246] = \t 0\n",
      "tensor_out[247] = \t 74\n",
      "tensor_out[248] = \t 0\n",
      "tensor_out[249] = \t 52\n",
      "tensor_out[250] = \t 38\n",
      "tensor_out[251] = \t 0\n",
      "tensor_out[252] = \t 0\n",
      "tensor_out[253] = \t 0\n",
      "tensor_out[254] = \t 45\n",
      "tensor_out[255] = \t 0\n",
      "tensor_out[256] = \t 74\n",
      "tensor_out[257] = \t 0\n",
      "tensor_out[258] = \t 0\n",
      "tensor_out[259] = \t 0\n",
      "tensor_out[260] = \t 101\n",
      "tensor_out[261] = \t 0\n",
      "tensor_out[262] = \t 105\n",
      "tensor_out[263] = \t 0\n",
      "tensor_out[264] = \t 0\n",
      "tensor_out[265] = \t 0\n",
      "tensor_out[266] = \t 0\n",
      "tensor_out[267] = \t 0\n",
      "tensor_out[268] = \t 0\n",
      "tensor_out[269] = \t 154\n",
      "tensor_out[270] = \t 97\n",
      "tensor_out[271] = \t 34\n",
      "tensor_out[272] = \t 10\n",
      "tensor_out[273] = \t 29\n",
      "tensor_out[274] = \t 183\n",
      "tensor_out[275] = \t 60\n",
      "tensor_out[276] = \t 5\n",
      "tensor_out[277] = \t 86\n",
      "tensor_out[278] = \t 0\n",
      "tensor_out[279] = \t 88\n",
      "tensor_out[280] = \t 9\n",
      "tensor_out[281] = \t 0\n",
      "tensor_out[282] = \t 97\n",
      "tensor_out[283] = \t 0\n",
      "tensor_out[284] = \t 78\n",
      "tensor_out[285] = \t 77\n",
      "tensor_out[286] = \t 0\n",
      "tensor_out[287] = \t 0\n",
      "tensor_out[288] = \t 0\n",
      "tensor_out[289] = \t 46\n",
      "tensor_out[290] = \t 0\n",
      "tensor_out[291] = \t 75\n",
      "tensor_out[292] = \t 4\n",
      "tensor_out[293] = \t 222\n",
      "tensor_out[294] = \t 0\n",
      "tensor_out[295] = \t 20\n",
      "tensor_out[296] = \t 7\n",
      "tensor_out[297] = \t 0\n",
      "tensor_out[298] = \t 0\n",
      "tensor_out[299] = \t 0\n",
      "tensor_out[300] = \t 0\n",
      "tensor_out[301] = \t 22\n",
      "tensor_out[302] = \t 0\n",
      "tensor_out[303] = \t 56\n",
      "tensor_out[304] = \t 45\n",
      "tensor_out[305] = \t 0\n",
      "tensor_out[306] = \t 101\n",
      "tensor_out[307] = \t 44\n",
      "tensor_out[308] = \t 61\n",
      "tensor_out[309] = \t 0\n",
      "tensor_out[310] = \t 0\n",
      "tensor_out[311] = \t 0\n",
      "tensor_out[312] = \t 0\n",
      "tensor_out[313] = \t 0\n",
      "tensor_out[314] = \t 0\n",
      "tensor_out[315] = \t 0\n",
      "tensor_out[316] = \t 0\n",
      "tensor_out[317] = \t 0\n",
      "tensor_out[318] = \t 21\n",
      "tensor_out[319] = \t 0\n",
      "tensor_out[320] = \t 0\n",
      "tensor_out[321] = \t 38\n",
      "tensor_out[322] = \t 15\n",
      "tensor_out[323] = \t 113\n",
      "tensor_out[324] = \t 0\n",
      "tensor_out[325] = \t 97\n",
      "tensor_out[326] = \t 14\n",
      "tensor_out[327] = \t 75\n",
      "tensor_out[328] = \t 0\n",
      "tensor_out[329] = \t 0\n",
      "tensor_out[330] = \t 0\n",
      "tensor_out[331] = \t 51\n",
      "tensor_out[332] = \t 0\n",
      "tensor_out[333] = \t 15\n",
      "tensor_out[334] = \t 15\n",
      "tensor_out[335] = \t 6\n",
      "tensor_out[336] = \t 0\n",
      "tensor_out[337] = \t 60\n",
      "tensor_out[338] = \t 0\n",
      "tensor_out[339] = \t 38\n",
      "tensor_out[340] = \t 45\n",
      "tensor_out[341] = \t 0\n",
      "tensor_out[342] = \t 0\n",
      "tensor_out[343] = \t 0\n",
      "tensor_out[344] = \t 0\n",
      "tensor_out[345] = \t 107\n",
      "tensor_out[346] = \t 48\n",
      "tensor_out[347] = \t 69\n",
      "tensor_out[348] = \t 0\n",
      "tensor_out[349] = \t 0\n",
      "tensor_out[350] = \t 0\n",
      "tensor_out[351] = \t 2\n",
      "tensor_out[352] = \t 20\n",
      "tensor_out[353] = \t 37\n",
      "tensor_out[354] = \t 78\n",
      "tensor_out[355] = \t 0\n",
      "tensor_out[356] = \t 0\n",
      "tensor_out[357] = \t 19\n",
      "tensor_out[358] = \t 27\n",
      "tensor_out[359] = \t 0\n",
      "tensor_out[360] = \t 0\n",
      "tensor_out[361] = \t 0\n",
      "tensor_out[362] = \t 164\n",
      "tensor_out[363] = \t 13\n",
      "tensor_out[364] = \t 0\n",
      "tensor_out[365] = \t 51\n",
      "tensor_out[366] = \t 35\n",
      "tensor_out[367] = \t 0\n",
      "tensor_out[368] = \t 0\n",
      "tensor_out[369] = \t 44\n",
      "tensor_out[370] = \t 0\n",
      "tensor_out[371] = \t 0\n",
      "tensor_out[372] = \t 84\n",
      "tensor_out[373] = \t 148\n",
      "tensor_out[374] = \t 38\n",
      "tensor_out[375] = \t 0\n",
      "tensor_out[376] = \t 62\n",
      "tensor_out[377] = \t 182\n",
      "tensor_out[378] = \t 119\n",
      "tensor_out[379] = \t 51\n",
      "tensor_out[380] = \t 42\n",
      "tensor_out[381] = \t 0\n",
      "tensor_out[382] = \t 62\n",
      "tensor_out[383] = \t 0\n",
      "tensor_out[384] = \t 29\n",
      "tensor_out[385] = \t 0\n",
      "tensor_out[386] = \t 13\n",
      "tensor_out[387] = \t 0\n",
      "tensor_out[388] = \t 65\n",
      "tensor_out[389] = \t 30\n",
      "tensor_out[390] = \t 0\n",
      "tensor_out[391] = \t 75\n",
      "tensor_out[392] = \t 0\n",
      "tensor_out[393] = \t 0\n",
      "tensor_out[394] = \t 0\n",
      "tensor_out[395] = \t 95\n",
      "tensor_out[396] = \t 67\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 29\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 0\n",
      "tensor_out[401] = \t 0\n",
      "tensor_out[402] = \t 0\n",
      "tensor_out[403] = \t 0\n",
      "tensor_out[404] = \t 0\n",
      "tensor_out[405] = \t 0\n",
      "tensor_out[406] = \t 0\n",
      "tensor_out[407] = \t 0\n",
      "tensor_out[408] = \t 0\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 66\n",
      "tensor_out[411] = \t 0\n",
      "tensor_out[412] = \t 0\n",
      "tensor_out[413] = \t 60\n",
      "tensor_out[414] = \t 64\n",
      "tensor_out[415] = \t 0\n",
      "tensor_out[416] = \t 0\n",
      "tensor_out[417] = \t 0\n",
      "tensor_out[418] = \t 0\n",
      "tensor_out[419] = \t 62\n",
      "tensor_out[420] = \t 18\n",
      "tensor_out[421] = \t 0\n",
      "tensor_out[422] = \t 15\n",
      "tensor_out[423] = \t 13\n",
      "tensor_out[424] = \t 54\n",
      "tensor_out[425] = \t 0\n",
      "tensor_out[426] = \t 97\n",
      "tensor_out[427] = \t 21\n",
      "tensor_out[428] = \t 0\n",
      "tensor_out[429] = \t 34\n",
      "tensor_out[430] = \t 5\n",
      "tensor_out[431] = \t 95\n",
      "tensor_out[432] = \t 0\n",
      "tensor_out[433] = \t 0\n",
      "tensor_out[434] = \t 90\n",
      "tensor_out[435] = \t 0\n",
      "tensor_out[436] = \t 0\n",
      "tensor_out[437] = \t 0\n",
      "tensor_out[438] = \t 1\n",
      "tensor_out[439] = \t 32\n",
      "tensor_out[440] = \t 20\n",
      "tensor_out[441] = \t 0\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 0\n",
      "tensor_out[444] = \t 0\n",
      "tensor_out[445] = \t 65\n",
      "tensor_out[446] = \t 0\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 0\n",
      "tensor_out[449] = \t 128\n",
      "tensor_out[450] = \t 0\n",
      "tensor_out[451] = \t 53\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 0\n",
      "tensor_out[454] = \t 11\n",
      "tensor_out[455] = \t 0\n",
      "tensor_out[456] = \t 0\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 107\n",
      "tensor_out[459] = \t 0\n",
      "tensor_out[460] = \t 65\n",
      "tensor_out[461] = \t 0\n",
      "tensor_out[462] = \t 98\n",
      "tensor_out[463] = \t 63\n",
      "tensor_out[464] = \t 47\n",
      "tensor_out[465] = \t 0\n",
      "tensor_out[466] = \t 78\n",
      "tensor_out[467] = \t 74\n",
      "tensor_out[468] = \t 0\n",
      "tensor_out[469] = \t 0\n",
      "tensor_out[470] = \t 0\n",
      "tensor_out[471] = \t 101\n",
      "tensor_out[472] = \t 0\n",
      "tensor_out[473] = \t 0\n",
      "tensor_out[474] = \t 0\n",
      "tensor_out[475] = \t 0\n",
      "tensor_out[476] = \t 104\n",
      "tensor_out[477] = \t 0\n",
      "tensor_out[478] = \t 0\n",
      "tensor_out[479] = \t 33\n",
      "tensor_out[480] = \t 14\n",
      "tensor_out[481] = \t 0\n",
      "tensor_out[482] = \t 44\n",
      "tensor_out[483] = \t 82\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 17\n",
      "tensor_out[486] = \t 0\n",
      "tensor_out[487] = \t 0\n",
      "tensor_out[488] = \t 0\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 0\n",
      "tensor_out[491] = \t 0\n",
      "tensor_out[492] = \t 56\n",
      "tensor_out[493] = \t 3\n",
      "tensor_out[494] = \t 19\n",
      "tensor_out[495] = \t 0\n",
      "tensor_out[496] = \t 0\n",
      "tensor_out[497] = \t 0\n",
      "tensor_out[498] = \t 0\n",
      "tensor_out[499] = \t 46\n",
      "tensor_out[500] = \t 0\n",
      "tensor_out[501] = \t 0\n",
      "tensor_out[502] = \t 0\n",
      "tensor_out[503] = \t 96\n",
      "tensor_out[504] = \t 26\n",
      "tensor_out[505] = \t 67\n",
      "tensor_out[506] = \t 0\n",
      "tensor_out[507] = \t 91\n",
      "tensor_out[508] = \t 0\n",
      "tensor_out[509] = \t 0\n",
      "tensor_out[510] = \t 80\n",
      "tensor_out[511] = \t 73\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer20 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 26\n",
      "tensor_out[1] = \t 47\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 56\n",
      "tensor_out[4] = \t 42\n",
      "tensor_out[5] = \t 83\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 0\n",
      "tensor_out[9] = \t 52\n",
      "tensor_out[10] = \t 36\n",
      "tensor_out[11] = \t 50\n",
      "tensor_out[12] = \t 4\n",
      "tensor_out[13] = \t 41\n",
      "tensor_out[14] = \t 0\n",
      "tensor_out[15] = \t 42\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 0\n",
      "tensor_out[19] = \t 0\n",
      "tensor_out[20] = \t 0\n",
      "tensor_out[21] = \t 0\n",
      "tensor_out[22] = \t 39\n",
      "tensor_out[23] = \t 0\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 0\n",
      "tensor_out[26] = \t 40\n",
      "tensor_out[27] = \t 44\n",
      "tensor_out[28] = \t 45\n",
      "tensor_out[29] = \t 0\n",
      "tensor_out[30] = \t 62\n",
      "tensor_out[31] = \t 70\n",
      "tensor_out[32] = \t 50\n",
      "tensor_out[33] = \t 31\n",
      "tensor_out[34] = \t 0\n",
      "tensor_out[35] = \t 0\n",
      "tensor_out[36] = \t 0\n",
      "tensor_out[37] = \t 47\n",
      "tensor_out[38] = \t 35\n",
      "tensor_out[39] = \t 53\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 56\n",
      "tensor_out[45] = \t 0\n",
      "tensor_out[46] = \t 46\n",
      "tensor_out[47] = \t 86\n",
      "tensor_out[48] = \t 21\n",
      "tensor_out[49] = \t 0\n",
      "tensor_out[50] = \t 77\n",
      "tensor_out[51] = \t 0\n",
      "tensor_out[52] = \t 2\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 30\n",
      "tensor_out[55] = \t 40\n",
      "tensor_out[56] = \t 59\n",
      "tensor_out[57] = \t 57\n",
      "tensor_out[58] = \t 119\n",
      "tensor_out[59] = \t 42\n",
      "tensor_out[60] = \t 22\n",
      "tensor_out[61] = \t 0\n",
      "tensor_out[62] = \t 35\n",
      "tensor_out[63] = \t 2\n",
      "tensor_out[64] = \t 0\n",
      "tensor_out[65] = \t 2\n",
      "tensor_out[66] = \t 0\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 51\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 47\n",
      "tensor_out[72] = \t 40\n",
      "tensor_out[73] = \t 4\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 0\n",
      "tensor_out[76] = \t 85\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 0\n",
      "tensor_out[79] = \t 36\n",
      "tensor_out[80] = \t 0\n",
      "tensor_out[81] = \t 0\n",
      "tensor_out[82] = \t 58\n",
      "tensor_out[83] = \t 11\n",
      "tensor_out[84] = \t 7\n",
      "tensor_out[85] = \t 44\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 14\n",
      "tensor_out[88] = \t 0\n",
      "tensor_out[89] = \t 30\n",
      "tensor_out[90] = \t 28\n",
      "tensor_out[91] = \t 6\n",
      "tensor_out[92] = \t 0\n",
      "tensor_out[93] = \t 77\n",
      "tensor_out[94] = \t 0\n",
      "tensor_out[95] = \t 7\n",
      "tensor_out[96] = \t 47\n",
      "tensor_out[97] = \t 66\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 92\n",
      "tensor_out[100] = \t 0\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 23\n",
      "tensor_out[103] = \t 3\n",
      "tensor_out[104] = \t 0\n",
      "tensor_out[105] = \t 61\n",
      "tensor_out[106] = \t 42\n",
      "tensor_out[107] = \t 75\n",
      "tensor_out[108] = \t 54\n",
      "tensor_out[109] = \t 0\n",
      "tensor_out[110] = \t 44\n",
      "tensor_out[111] = \t 0\n",
      "tensor_out[112] = \t 0\n",
      "tensor_out[113] = \t 0\n",
      "tensor_out[114] = \t 13\n",
      "tensor_out[115] = \t 0\n",
      "tensor_out[116] = \t 16\n",
      "tensor_out[117] = \t 118\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 65\n",
      "tensor_out[120] = \t 0\n",
      "tensor_out[121] = \t 45\n",
      "tensor_out[122] = \t 0\n",
      "tensor_out[123] = \t 0\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 0\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 0\n",
      "tensor_out[129] = \t 0\n",
      "tensor_out[130] = \t 79\n",
      "tensor_out[131] = \t 0\n",
      "tensor_out[132] = \t 4\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 66\n",
      "tensor_out[135] = \t 0\n",
      "tensor_out[136] = \t 0\n",
      "tensor_out[137] = \t 43\n",
      "tensor_out[138] = \t 0\n",
      "tensor_out[139] = \t 38\n",
      "tensor_out[140] = \t 0\n",
      "tensor_out[141] = \t 60\n",
      "tensor_out[142] = \t 117\n",
      "tensor_out[143] = \t 0\n",
      "tensor_out[144] = \t 48\n",
      "tensor_out[145] = \t 71\n",
      "tensor_out[146] = \t 91\n",
      "tensor_out[147] = \t 20\n",
      "tensor_out[148] = \t 0\n",
      "tensor_out[149] = \t 0\n",
      "tensor_out[150] = \t 0\n",
      "tensor_out[151] = \t 56\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 0\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 0\n",
      "tensor_out[156] = \t 46\n",
      "tensor_out[157] = \t 0\n",
      "tensor_out[158] = \t 0\n",
      "tensor_out[159] = \t 26\n",
      "tensor_out[160] = \t 77\n",
      "tensor_out[161] = \t 0\n",
      "tensor_out[162] = \t 22\n",
      "tensor_out[163] = \t 0\n",
      "tensor_out[164] = \t 0\n",
      "tensor_out[165] = \t 0\n",
      "tensor_out[166] = \t 0\n",
      "tensor_out[167] = \t 36\n",
      "tensor_out[168] = \t 14\n",
      "tensor_out[169] = \t 60\n",
      "tensor_out[170] = \t 0\n",
      "tensor_out[171] = \t 22\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 0\n",
      "tensor_out[174] = \t 38\n",
      "tensor_out[175] = \t 53\n",
      "tensor_out[176] = \t 42\n",
      "tensor_out[177] = \t 7\n",
      "tensor_out[178] = \t 21\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 60\n",
      "tensor_out[181] = \t 0\n",
      "tensor_out[182] = \t 70\n",
      "tensor_out[183] = \t 0\n",
      "tensor_out[184] = \t 82\n",
      "tensor_out[185] = \t 42\n",
      "tensor_out[186] = \t 48\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 0\n",
      "tensor_out[189] = \t 0\n",
      "tensor_out[190] = \t 0\n",
      "tensor_out[191] = \t 0\n",
      "tensor_out[192] = \t 42\n",
      "tensor_out[193] = \t 0\n",
      "tensor_out[194] = \t 0\n",
      "tensor_out[195] = \t 33\n",
      "tensor_out[196] = \t 46\n",
      "tensor_out[197] = \t 65\n",
      "tensor_out[198] = \t 45\n",
      "tensor_out[199] = \t 43\n",
      "tensor_out[200] = \t 31\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 43\n",
      "tensor_out[203] = \t 0\n",
      "tensor_out[204] = \t 0\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 0\n",
      "tensor_out[207] = \t 94\n",
      "tensor_out[208] = \t 53\n",
      "tensor_out[209] = \t 69\n",
      "tensor_out[210] = \t 100\n",
      "tensor_out[211] = \t 37\n",
      "tensor_out[212] = \t 102\n",
      "tensor_out[213] = \t 0\n",
      "tensor_out[214] = \t 0\n",
      "tensor_out[215] = \t 0\n",
      "tensor_out[216] = \t 50\n",
      "tensor_out[217] = \t 44\n",
      "tensor_out[218] = \t 67\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 0\n",
      "tensor_out[221] = \t 0\n",
      "tensor_out[222] = \t 0\n",
      "tensor_out[223] = \t 0\n",
      "tensor_out[224] = \t 0\n",
      "tensor_out[225] = \t 45\n",
      "tensor_out[226] = \t 7\n",
      "tensor_out[227] = \t 0\n",
      "tensor_out[228] = \t 55\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 51\n",
      "tensor_out[231] = \t 0\n",
      "tensor_out[232] = \t 41\n",
      "tensor_out[233] = \t 57\n",
      "tensor_out[234] = \t 16\n",
      "tensor_out[235] = \t 0\n",
      "tensor_out[236] = \t 0\n",
      "tensor_out[237] = \t 39\n",
      "tensor_out[238] = \t 0\n",
      "tensor_out[239] = \t 23\n",
      "tensor_out[240] = \t 0\n",
      "tensor_out[241] = \t 0\n",
      "tensor_out[242] = \t 25\n",
      "tensor_out[243] = \t 74\n",
      "tensor_out[244] = \t 40\n",
      "tensor_out[245] = \t 133\n",
      "tensor_out[246] = \t 0\n",
      "tensor_out[247] = \t 0\n",
      "tensor_out[248] = \t 70\n",
      "tensor_out[249] = \t 18\n",
      "tensor_out[250] = \t 50\n",
      "tensor_out[251] = \t 6\n",
      "tensor_out[252] = \t 0\n",
      "tensor_out[253] = \t 0\n",
      "tensor_out[254] = \t 26\n",
      "tensor_out[255] = \t 0\n",
      "tensor_out[256] = \t 35\n",
      "tensor_out[257] = \t 0\n",
      "tensor_out[258] = \t 81\n",
      "tensor_out[259] = \t 0\n",
      "tensor_out[260] = \t 137\n",
      "tensor_out[261] = \t 0\n",
      "tensor_out[262] = \t 141\n",
      "tensor_out[263] = \t 0\n",
      "tensor_out[264] = \t 56\n",
      "tensor_out[265] = \t 37\n",
      "tensor_out[266] = \t 0\n",
      "tensor_out[267] = \t 49\n",
      "tensor_out[268] = \t 0\n",
      "tensor_out[269] = \t 91\n",
      "tensor_out[270] = \t 84\n",
      "tensor_out[271] = \t 73\n",
      "tensor_out[272] = \t 59\n",
      "tensor_out[273] = \t 40\n",
      "tensor_out[274] = \t 26\n",
      "tensor_out[275] = \t 23\n",
      "tensor_out[276] = \t 8\n",
      "tensor_out[277] = \t 28\n",
      "tensor_out[278] = \t 0\n",
      "tensor_out[279] = \t 0\n",
      "tensor_out[280] = \t 15\n",
      "tensor_out[281] = \t 74\n",
      "tensor_out[282] = \t 160\n",
      "tensor_out[283] = \t 118\n",
      "tensor_out[284] = \t 98\n",
      "tensor_out[285] = \t 58\n",
      "tensor_out[286] = \t 0\n",
      "tensor_out[287] = \t 55\n",
      "tensor_out[288] = \t 0\n",
      "tensor_out[289] = \t 41\n",
      "tensor_out[290] = \t 73\n",
      "tensor_out[291] = \t 27\n",
      "tensor_out[292] = \t 28\n",
      "tensor_out[293] = \t 0\n",
      "tensor_out[294] = \t 0\n",
      "tensor_out[295] = \t 0\n",
      "tensor_out[296] = \t 0\n",
      "tensor_out[297] = \t 45\n",
      "tensor_out[298] = \t 0\n",
      "tensor_out[299] = \t 4\n",
      "tensor_out[300] = \t 51\n",
      "tensor_out[301] = \t 0\n",
      "tensor_out[302] = \t 16\n",
      "tensor_out[303] = \t 48\n",
      "tensor_out[304] = \t 9\n",
      "tensor_out[305] = \t 0\n",
      "tensor_out[306] = \t 64\n",
      "tensor_out[307] = \t 53\n",
      "tensor_out[308] = \t 6\n",
      "tensor_out[309] = \t 45\n",
      "tensor_out[310] = \t 46\n",
      "tensor_out[311] = \t 0\n",
      "tensor_out[312] = \t 0\n",
      "tensor_out[313] = \t 41\n",
      "tensor_out[314] = \t 53\n",
      "tensor_out[315] = \t 0\n",
      "tensor_out[316] = \t 74\n",
      "tensor_out[317] = \t 0\n",
      "tensor_out[318] = \t 47\n",
      "tensor_out[319] = \t 61\n",
      "tensor_out[320] = \t 0\n",
      "tensor_out[321] = \t 39\n",
      "tensor_out[322] = \t 0\n",
      "tensor_out[323] = \t 72\n",
      "tensor_out[324] = \t 64\n",
      "tensor_out[325] = \t 95\n",
      "tensor_out[326] = \t 35\n",
      "tensor_out[327] = \t 106\n",
      "tensor_out[328] = \t 0\n",
      "tensor_out[329] = \t 0\n",
      "tensor_out[330] = \t 39\n",
      "tensor_out[331] = \t 33\n",
      "tensor_out[332] = \t 71\n",
      "tensor_out[333] = \t 0\n",
      "tensor_out[334] = \t 0\n",
      "tensor_out[335] = \t 51\n",
      "tensor_out[336] = \t 0\n",
      "tensor_out[337] = \t 0\n",
      "tensor_out[338] = \t 0\n",
      "tensor_out[339] = \t 35\n",
      "tensor_out[340] = \t 0\n",
      "tensor_out[341] = \t 52\n",
      "tensor_out[342] = \t 55\n",
      "tensor_out[343] = \t 0\n",
      "tensor_out[344] = \t 0\n",
      "tensor_out[345] = \t 127\n",
      "tensor_out[346] = \t 25\n",
      "tensor_out[347] = \t 0\n",
      "tensor_out[348] = \t 0\n",
      "tensor_out[349] = \t 77\n",
      "tensor_out[350] = \t 53\n",
      "tensor_out[351] = \t 0\n",
      "tensor_out[352] = \t 0\n",
      "tensor_out[353] = \t 40\n",
      "tensor_out[354] = \t 114\n",
      "tensor_out[355] = \t 0\n",
      "tensor_out[356] = \t 0\n",
      "tensor_out[357] = \t 5\n",
      "tensor_out[358] = \t 33\n",
      "tensor_out[359] = \t 46\n",
      "tensor_out[360] = \t 23\n",
      "tensor_out[361] = \t 0\n",
      "tensor_out[362] = \t 0\n",
      "tensor_out[363] = \t 20\n",
      "tensor_out[364] = \t 0\n",
      "tensor_out[365] = \t 55\n",
      "tensor_out[366] = \t 1\n",
      "tensor_out[367] = \t 0\n",
      "tensor_out[368] = \t 0\n",
      "tensor_out[369] = \t 28\n",
      "tensor_out[370] = \t 0\n",
      "tensor_out[371] = \t 0\n",
      "tensor_out[372] = \t 44\n",
      "tensor_out[373] = \t 0\n",
      "tensor_out[374] = \t 5\n",
      "tensor_out[375] = \t 0\n",
      "tensor_out[376] = \t 4\n",
      "tensor_out[377] = \t 129\n",
      "tensor_out[378] = \t 0\n",
      "tensor_out[379] = \t 32\n",
      "tensor_out[380] = \t 24\n",
      "tensor_out[381] = \t 41\n",
      "tensor_out[382] = \t 36\n",
      "tensor_out[383] = \t 0\n",
      "tensor_out[384] = \t 0\n",
      "tensor_out[385] = \t 0\n",
      "tensor_out[386] = \t 0\n",
      "tensor_out[387] = \t 7\n",
      "tensor_out[388] = \t 67\n",
      "tensor_out[389] = \t 16\n",
      "tensor_out[390] = \t 48\n",
      "tensor_out[391] = \t 43\n",
      "tensor_out[392] = \t 0\n",
      "tensor_out[393] = \t 0\n",
      "tensor_out[394] = \t 0\n",
      "tensor_out[395] = \t 74\n",
      "tensor_out[396] = \t 0\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 76\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 58\n",
      "tensor_out[401] = \t 0\n",
      "tensor_out[402] = \t 0\n",
      "tensor_out[403] = \t 0\n",
      "tensor_out[404] = \t 91\n",
      "tensor_out[405] = \t 51\n",
      "tensor_out[406] = \t 0\n",
      "tensor_out[407] = \t 0\n",
      "tensor_out[408] = \t 0\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 49\n",
      "tensor_out[411] = \t 0\n",
      "tensor_out[412] = \t 0\n",
      "tensor_out[413] = \t 87\n",
      "tensor_out[414] = \t 88\n",
      "tensor_out[415] = \t 25\n",
      "tensor_out[416] = \t 0\n",
      "tensor_out[417] = \t 38\n",
      "tensor_out[418] = \t 44\n",
      "tensor_out[419] = \t 0\n",
      "tensor_out[420] = \t 64\n",
      "tensor_out[421] = \t 35\n",
      "tensor_out[422] = \t 49\n",
      "tensor_out[423] = \t 63\n",
      "tensor_out[424] = \t 0\n",
      "tensor_out[425] = \t 53\n",
      "tensor_out[426] = \t 3\n",
      "tensor_out[427] = \t 61\n",
      "tensor_out[428] = \t 0\n",
      "tensor_out[429] = \t 34\n",
      "tensor_out[430] = \t 38\n",
      "tensor_out[431] = \t 0\n",
      "tensor_out[432] = \t 57\n",
      "tensor_out[433] = \t 65\n",
      "tensor_out[434] = \t 16\n",
      "tensor_out[435] = \t 0\n",
      "tensor_out[436] = \t 0\n",
      "tensor_out[437] = \t 43\n",
      "tensor_out[438] = \t 54\n",
      "tensor_out[439] = \t 28\n",
      "tensor_out[440] = \t 59\n",
      "tensor_out[441] = \t 33\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 0\n",
      "tensor_out[444] = \t 54\n",
      "tensor_out[445] = \t 45\n",
      "tensor_out[446] = \t 0\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 0\n",
      "tensor_out[449] = \t 0\n",
      "tensor_out[450] = \t 0\n",
      "tensor_out[451] = \t 40\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 0\n",
      "tensor_out[454] = \t 0\n",
      "tensor_out[455] = \t 0\n",
      "tensor_out[456] = \t 0\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 0\n",
      "tensor_out[459] = \t 46\n",
      "tensor_out[460] = \t 53\n",
      "tensor_out[461] = \t 0\n",
      "tensor_out[462] = \t 129\n",
      "tensor_out[463] = \t 16\n",
      "tensor_out[464] = \t 45\n",
      "tensor_out[465] = \t 0\n",
      "tensor_out[466] = \t 65\n",
      "tensor_out[467] = \t 14\n",
      "tensor_out[468] = \t 39\n",
      "tensor_out[469] = \t 0\n",
      "tensor_out[470] = \t 0\n",
      "tensor_out[471] = \t 0\n",
      "tensor_out[472] = \t 0\n",
      "tensor_out[473] = \t 44\n",
      "tensor_out[474] = \t 0\n",
      "tensor_out[475] = \t 0\n",
      "tensor_out[476] = \t 41\n",
      "tensor_out[477] = \t 66\n",
      "tensor_out[478] = \t 0\n",
      "tensor_out[479] = \t 81\n",
      "tensor_out[480] = \t 0\n",
      "tensor_out[481] = \t 0\n",
      "tensor_out[482] = \t 28\n",
      "tensor_out[483] = \t 68\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 95\n",
      "tensor_out[486] = \t 42\n",
      "tensor_out[487] = \t 0\n",
      "tensor_out[488] = \t 0\n",
      "tensor_out[489] = \t 50\n",
      "tensor_out[490] = \t 0\n",
      "tensor_out[491] = \t 47\n",
      "tensor_out[492] = \t 24\n",
      "tensor_out[493] = \t 53\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 0\n",
      "tensor_out[496] = \t 52\n",
      "tensor_out[497] = \t 35\n",
      "tensor_out[498] = \t 46\n",
      "tensor_out[499] = \t 33\n",
      "tensor_out[500] = \t 0\n",
      "tensor_out[501] = \t 11\n",
      "tensor_out[502] = \t 44\n",
      "tensor_out[503] = \t 0\n",
      "tensor_out[504] = \t 0\n",
      "tensor_out[505] = \t 0\n",
      "tensor_out[506] = \t 0\n",
      "tensor_out[507] = \t 0\n",
      "tensor_out[508] = \t 41\n",
      "tensor_out[509] = \t 48\n",
      "tensor_out[510] = \t 0\n",
      "tensor_out[511] = \t 38\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer21 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0\n",
      "tensor_out[1] = \t 105\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 0\n",
      "tensor_out[5] = \t 0\n",
      "tensor_out[6] = \t 58\n",
      "tensor_out[7] = \t 53\n",
      "tensor_out[8] = \t 0\n",
      "tensor_out[9] = \t 80\n",
      "tensor_out[10] = \t 18\n",
      "tensor_out[11] = \t 67\n",
      "tensor_out[12] = \t 92\n",
      "tensor_out[13] = \t 8\n",
      "tensor_out[14] = \t 0\n",
      "tensor_out[15] = \t 17\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 0\n",
      "tensor_out[19] = \t 8\n",
      "tensor_out[20] = \t 35\n",
      "tensor_out[21] = \t 0\n",
      "tensor_out[22] = \t 0\n",
      "tensor_out[23] = \t 24\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 97\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 0\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 0\n",
      "tensor_out[30] = \t 85\n",
      "tensor_out[31] = \t 0\n",
      "tensor_out[32] = \t 0\n",
      "tensor_out[33] = \t 0\n",
      "tensor_out[34] = \t 122\n",
      "tensor_out[35] = \t 0\n",
      "tensor_out[36] = \t 0\n",
      "tensor_out[37] = \t 11\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 0\n",
      "tensor_out[40] = \t 18\n",
      "tensor_out[41] = \t 161\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 96\n",
      "tensor_out[44] = \t 0\n",
      "tensor_out[45] = \t 139\n",
      "tensor_out[46] = \t 6\n",
      "tensor_out[47] = \t 16\n",
      "tensor_out[48] = \t 19\n",
      "tensor_out[49] = \t 0\n",
      "tensor_out[50] = \t 0\n",
      "tensor_out[51] = \t 56\n",
      "tensor_out[52] = \t 15\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 26\n",
      "tensor_out[55] = \t 0\n",
      "tensor_out[56] = \t 24\n",
      "tensor_out[57] = \t 5\n",
      "tensor_out[58] = \t 151\n",
      "tensor_out[59] = \t 62\n",
      "tensor_out[60] = \t 174\n",
      "tensor_out[61] = \t 0\n",
      "tensor_out[62] = \t 0\n",
      "tensor_out[63] = \t 0\n",
      "tensor_out[64] = \t 0\n",
      "tensor_out[65] = \t 0\n",
      "tensor_out[66] = \t 34\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 0\n",
      "tensor_out[72] = \t 0\n",
      "tensor_out[73] = \t 21\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 0\n",
      "tensor_out[76] = \t 0\n",
      "tensor_out[77] = \t 36\n",
      "tensor_out[78] = \t 0\n",
      "tensor_out[79] = \t 0\n",
      "tensor_out[80] = \t 46\n",
      "tensor_out[81] = \t 0\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 5\n",
      "tensor_out[84] = \t 37\n",
      "tensor_out[85] = \t 40\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 0\n",
      "tensor_out[88] = \t 4\n",
      "tensor_out[89] = \t 0\n",
      "tensor_out[90] = \t 53\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 1\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 69\n",
      "tensor_out[95] = \t 31\n",
      "tensor_out[96] = \t 31\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 50\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 0\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 126\n",
      "tensor_out[103] = \t 0\n",
      "tensor_out[104] = \t 40\n",
      "tensor_out[105] = \t 82\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 7\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 0\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 0\n",
      "tensor_out[112] = \t 0\n",
      "tensor_out[113] = \t 44\n",
      "tensor_out[114] = \t 126\n",
      "tensor_out[115] = \t 0\n",
      "tensor_out[116] = \t 109\n",
      "tensor_out[117] = \t 71\n",
      "tensor_out[118] = \t 31\n",
      "tensor_out[119] = \t 0\n",
      "tensor_out[120] = \t 0\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 13\n",
      "tensor_out[123] = \t 0\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 0\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 12\n",
      "tensor_out[129] = \t 34\n",
      "tensor_out[130] = \t 7\n",
      "tensor_out[131] = \t 0\n",
      "tensor_out[132] = \t 0\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 0\n",
      "tensor_out[135] = \t 27\n",
      "tensor_out[136] = \t 56\n",
      "tensor_out[137] = \t 0\n",
      "tensor_out[138] = \t 0\n",
      "tensor_out[139] = \t 0\n",
      "tensor_out[140] = \t 0\n",
      "tensor_out[141] = \t 0\n",
      "tensor_out[142] = \t 50\n",
      "tensor_out[143] = \t 0\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 0\n",
      "tensor_out[146] = \t 0\n",
      "tensor_out[147] = \t 0\n",
      "tensor_out[148] = \t 70\n",
      "tensor_out[149] = \t 0\n",
      "tensor_out[150] = \t 0\n",
      "tensor_out[151] = \t 27\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 0\n",
      "tensor_out[154] = \t 51\n",
      "tensor_out[155] = \t 0\n",
      "tensor_out[156] = \t 65\n",
      "tensor_out[157] = \t 82\n",
      "tensor_out[158] = \t 190\n",
      "tensor_out[159] = \t 38\n",
      "tensor_out[160] = \t 0\n",
      "tensor_out[161] = \t 37\n",
      "tensor_out[162] = \t 111\n",
      "tensor_out[163] = \t 9\n",
      "tensor_out[164] = \t 1\n",
      "tensor_out[165] = \t 0\n",
      "tensor_out[166] = \t 0\n",
      "tensor_out[167] = \t 0\n",
      "tensor_out[168] = \t 78\n",
      "tensor_out[169] = \t 91\n",
      "tensor_out[170] = \t 0\n",
      "tensor_out[171] = \t 88\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 27\n",
      "tensor_out[174] = \t 25\n",
      "tensor_out[175] = \t 0\n",
      "tensor_out[176] = \t 38\n",
      "tensor_out[177] = \t 31\n",
      "tensor_out[178] = \t 0\n",
      "tensor_out[179] = \t 62\n",
      "tensor_out[180] = \t 21\n",
      "tensor_out[181] = \t 0\n",
      "tensor_out[182] = \t 9\n",
      "tensor_out[183] = \t 0\n",
      "tensor_out[184] = \t 67\n",
      "tensor_out[185] = \t 138\n",
      "tensor_out[186] = \t 86\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 55\n",
      "tensor_out[189] = \t 42\n",
      "tensor_out[190] = \t 0\n",
      "tensor_out[191] = \t 37\n",
      "tensor_out[192] = \t 0\n",
      "tensor_out[193] = \t 0\n",
      "tensor_out[194] = \t 0\n",
      "tensor_out[195] = \t 126\n",
      "tensor_out[196] = \t 0\n",
      "tensor_out[197] = \t 0\n",
      "tensor_out[198] = \t 0\n",
      "tensor_out[199] = \t 46\n",
      "tensor_out[200] = \t 0\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 0\n",
      "tensor_out[203] = \t 35\n",
      "tensor_out[204] = \t 35\n",
      "tensor_out[205] = \t 8\n",
      "tensor_out[206] = \t 0\n",
      "tensor_out[207] = \t 56\n",
      "tensor_out[208] = \t 0\n",
      "tensor_out[209] = \t 0\n",
      "tensor_out[210] = \t 109\n",
      "tensor_out[211] = \t 0\n",
      "tensor_out[212] = \t 0\n",
      "tensor_out[213] = \t 0\n",
      "tensor_out[214] = \t 44\n",
      "tensor_out[215] = \t 45\n",
      "tensor_out[216] = \t 52\n",
      "tensor_out[217] = \t 0\n",
      "tensor_out[218] = \t 12\n",
      "tensor_out[219] = \t 77\n",
      "tensor_out[220] = \t 0\n",
      "tensor_out[221] = \t 0\n",
      "tensor_out[222] = \t 30\n",
      "tensor_out[223] = \t 0\n",
      "tensor_out[224] = \t 79\n",
      "tensor_out[225] = \t 0\n",
      "tensor_out[226] = \t 33\n",
      "tensor_out[227] = \t 68\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 23\n",
      "tensor_out[230] = \t 43\n",
      "tensor_out[231] = \t 100\n",
      "tensor_out[232] = \t 0\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 0\n",
      "tensor_out[235] = \t 52\n",
      "tensor_out[236] = \t 114\n",
      "tensor_out[237] = \t 0\n",
      "tensor_out[238] = \t 84\n",
      "tensor_out[239] = \t 53\n",
      "tensor_out[240] = \t 0\n",
      "tensor_out[241] = \t 0\n",
      "tensor_out[242] = \t 34\n",
      "tensor_out[243] = \t 129\n",
      "tensor_out[244] = \t 0\n",
      "tensor_out[245] = \t 0\n",
      "tensor_out[246] = \t 0\n",
      "tensor_out[247] = \t 0\n",
      "tensor_out[248] = \t 4\n",
      "tensor_out[249] = \t 0\n",
      "tensor_out[250] = \t 21\n",
      "tensor_out[251] = \t 0\n",
      "tensor_out[252] = \t 0\n",
      "tensor_out[253] = \t 0\n",
      "tensor_out[254] = \t 26\n",
      "tensor_out[255] = \t 0\n",
      "tensor_out[256] = \t 0\n",
      "tensor_out[257] = \t 0\n",
      "tensor_out[258] = \t 59\n",
      "tensor_out[259] = \t 0\n",
      "tensor_out[260] = \t 0\n",
      "tensor_out[261] = \t 71\n",
      "tensor_out[262] = \t 17\n",
      "tensor_out[263] = \t 26\n",
      "tensor_out[264] = \t 37\n",
      "tensor_out[265] = \t 28\n",
      "tensor_out[266] = \t 0\n",
      "tensor_out[267] = \t 0\n",
      "tensor_out[268] = \t 0\n",
      "tensor_out[269] = \t 65\n",
      "tensor_out[270] = \t 0\n",
      "tensor_out[271] = \t 0\n",
      "tensor_out[272] = \t 24\n",
      "tensor_out[273] = \t 0\n",
      "tensor_out[274] = \t 0\n",
      "tensor_out[275] = \t 0\n",
      "tensor_out[276] = \t 0\n",
      "tensor_out[277] = \t 0\n",
      "tensor_out[278] = \t 36\n",
      "tensor_out[279] = \t 61\n",
      "tensor_out[280] = \t 0\n",
      "tensor_out[281] = \t 23\n",
      "tensor_out[282] = \t 0\n",
      "tensor_out[283] = \t 39\n",
      "tensor_out[284] = \t 101\n",
      "tensor_out[285] = \t 0\n",
      "tensor_out[286] = \t 35\n",
      "tensor_out[287] = \t 99\n",
      "tensor_out[288] = \t 0\n",
      "tensor_out[289] = \t 0\n",
      "tensor_out[290] = \t 0\n",
      "tensor_out[291] = \t 18\n",
      "tensor_out[292] = \t 0\n",
      "tensor_out[293] = \t 0\n",
      "tensor_out[294] = \t 18\n",
      "tensor_out[295] = \t 0\n",
      "tensor_out[296] = \t 0\n",
      "tensor_out[297] = \t 150\n",
      "tensor_out[298] = \t 95\n",
      "tensor_out[299] = \t 0\n",
      "tensor_out[300] = \t 75\n",
      "tensor_out[301] = \t 0\n",
      "tensor_out[302] = \t 50\n",
      "tensor_out[303] = \t 0\n",
      "tensor_out[304] = \t 118\n",
      "tensor_out[305] = \t 0\n",
      "tensor_out[306] = \t 8\n",
      "tensor_out[307] = \t 57\n",
      "tensor_out[308] = \t 0\n",
      "tensor_out[309] = \t 0\n",
      "tensor_out[310] = \t 0\n",
      "tensor_out[311] = \t 0\n",
      "tensor_out[312] = \t 0\n",
      "tensor_out[313] = \t 0\n",
      "tensor_out[314] = \t 0\n",
      "tensor_out[315] = \t 0\n",
      "tensor_out[316] = \t 15\n",
      "tensor_out[317] = \t 0\n",
      "tensor_out[318] = \t 21\n",
      "tensor_out[319] = \t 39\n",
      "tensor_out[320] = \t 60\n",
      "tensor_out[321] = \t 5\n",
      "tensor_out[322] = \t 0\n",
      "tensor_out[323] = \t 77\n",
      "tensor_out[324] = \t 0\n",
      "tensor_out[325] = \t 0\n",
      "tensor_out[326] = \t 14\n",
      "tensor_out[327] = \t 0\n",
      "tensor_out[328] = \t 69\n",
      "tensor_out[329] = \t 0\n",
      "tensor_out[330] = \t 15\n",
      "tensor_out[331] = \t 30\n",
      "tensor_out[332] = \t 174\n",
      "tensor_out[333] = \t 136\n",
      "tensor_out[334] = \t 0\n",
      "tensor_out[335] = \t 35\n",
      "tensor_out[336] = \t 0\n",
      "tensor_out[337] = \t 27\n",
      "tensor_out[338] = \t 0\n",
      "tensor_out[339] = \t 240\n",
      "tensor_out[340] = \t 103\n",
      "tensor_out[341] = \t 0\n",
      "tensor_out[342] = \t 0\n",
      "tensor_out[343] = \t 0\n",
      "tensor_out[344] = \t 0\n",
      "tensor_out[345] = \t 0\n",
      "tensor_out[346] = \t 0\n",
      "tensor_out[347] = \t 18\n",
      "tensor_out[348] = \t 97\n",
      "tensor_out[349] = \t 0\n",
      "tensor_out[350] = \t 129\n",
      "tensor_out[351] = \t 0\n",
      "tensor_out[352] = \t 0\n",
      "tensor_out[353] = \t 0\n",
      "tensor_out[354] = \t 98\n",
      "tensor_out[355] = \t 165\n",
      "tensor_out[356] = \t 77\n",
      "tensor_out[357] = \t 33\n",
      "tensor_out[358] = \t 80\n",
      "tensor_out[359] = \t 0\n",
      "tensor_out[360] = \t 0\n",
      "tensor_out[361] = \t 6\n",
      "tensor_out[362] = \t 32\n",
      "tensor_out[363] = \t 0\n",
      "tensor_out[364] = \t 0\n",
      "tensor_out[365] = \t 0\n",
      "tensor_out[366] = \t 0\n",
      "tensor_out[367] = \t 47\n",
      "tensor_out[368] = \t 0\n",
      "tensor_out[369] = \t 0\n",
      "tensor_out[370] = \t 6\n",
      "tensor_out[371] = \t 0\n",
      "tensor_out[372] = \t 9\n",
      "tensor_out[373] = \t 78\n",
      "tensor_out[374] = \t 0\n",
      "tensor_out[375] = \t 0\n",
      "tensor_out[376] = \t 1\n",
      "tensor_out[377] = \t 0\n",
      "tensor_out[378] = \t 0\n",
      "tensor_out[379] = \t 83\n",
      "tensor_out[380] = \t 16\n",
      "tensor_out[381] = \t 55\n",
      "tensor_out[382] = \t 0\n",
      "tensor_out[383] = \t 0\n",
      "tensor_out[384] = \t 0\n",
      "tensor_out[385] = \t 0\n",
      "tensor_out[386] = \t 0\n",
      "tensor_out[387] = \t 27\n",
      "tensor_out[388] = \t 47\n",
      "tensor_out[389] = \t 103\n",
      "tensor_out[390] = \t 0\n",
      "tensor_out[391] = \t 0\n",
      "tensor_out[392] = \t 0\n",
      "tensor_out[393] = \t 23\n",
      "tensor_out[394] = \t 0\n",
      "tensor_out[395] = \t 63\n",
      "tensor_out[396] = \t 0\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 0\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 0\n",
      "tensor_out[401] = \t 48\n",
      "tensor_out[402] = \t 116\n",
      "tensor_out[403] = \t 0\n",
      "tensor_out[404] = \t 34\n",
      "tensor_out[405] = \t 0\n",
      "tensor_out[406] = \t 0\n",
      "tensor_out[407] = \t 0\n",
      "tensor_out[408] = \t 0\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 0\n",
      "tensor_out[411] = \t 26\n",
      "tensor_out[412] = \t 0\n",
      "tensor_out[413] = \t 24\n",
      "tensor_out[414] = \t 0\n",
      "tensor_out[415] = \t 77\n",
      "tensor_out[416] = \t 0\n",
      "tensor_out[417] = \t 58\n",
      "tensor_out[418] = \t 0\n",
      "tensor_out[419] = \t 0\n",
      "tensor_out[420] = \t 53\n",
      "tensor_out[421] = \t 65\n",
      "tensor_out[422] = \t 0\n",
      "tensor_out[423] = \t 18\n",
      "tensor_out[424] = \t 2\n",
      "tensor_out[425] = \t 0\n",
      "tensor_out[426] = \t 8\n",
      "tensor_out[427] = \t 0\n",
      "tensor_out[428] = \t 119\n",
      "tensor_out[429] = \t 100\n",
      "tensor_out[430] = \t 0\n",
      "tensor_out[431] = \t 95\n",
      "tensor_out[432] = \t 110\n",
      "tensor_out[433] = \t 70\n",
      "tensor_out[434] = \t 0\n",
      "tensor_out[435] = \t 9\n",
      "tensor_out[436] = \t 0\n",
      "tensor_out[437] = \t 45\n",
      "tensor_out[438] = \t 0\n",
      "tensor_out[439] = \t 40\n",
      "tensor_out[440] = \t 0\n",
      "tensor_out[441] = \t 34\n",
      "tensor_out[442] = \t 77\n",
      "tensor_out[443] = \t 0\n",
      "tensor_out[444] = \t 0\n",
      "tensor_out[445] = \t 0\n",
      "tensor_out[446] = \t 36\n",
      "tensor_out[447] = \t 8\n",
      "tensor_out[448] = \t 0\n",
      "tensor_out[449] = \t 0\n",
      "tensor_out[450] = \t 63\n",
      "tensor_out[451] = \t 0\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 63\n",
      "tensor_out[454] = \t 0\n",
      "tensor_out[455] = \t 88\n",
      "tensor_out[456] = \t 0\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 0\n",
      "tensor_out[459] = \t 0\n",
      "tensor_out[460] = \t 61\n",
      "tensor_out[461] = \t 0\n",
      "tensor_out[462] = \t 0\n",
      "tensor_out[463] = \t 20\n",
      "tensor_out[464] = \t 23\n",
      "tensor_out[465] = \t 0\n",
      "tensor_out[466] = \t 0\n",
      "tensor_out[467] = \t 11\n",
      "tensor_out[468] = \t 0\n",
      "tensor_out[469] = \t 9\n",
      "tensor_out[470] = \t 85\n",
      "tensor_out[471] = \t 103\n",
      "tensor_out[472] = \t 0\n",
      "tensor_out[473] = \t 0\n",
      "tensor_out[474] = \t 85\n",
      "tensor_out[475] = \t 0\n",
      "tensor_out[476] = \t 0\n",
      "tensor_out[477] = \t 51\n",
      "tensor_out[478] = \t 0\n",
      "tensor_out[479] = \t 14\n",
      "tensor_out[480] = \t 0\n",
      "tensor_out[481] = \t 0\n",
      "tensor_out[482] = \t 21\n",
      "tensor_out[483] = \t 0\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 98\n",
      "tensor_out[486] = \t 18\n",
      "tensor_out[487] = \t 0\n",
      "tensor_out[488] = \t 0\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 42\n",
      "tensor_out[491] = \t 0\n",
      "tensor_out[492] = \t 29\n",
      "tensor_out[493] = \t 0\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 0\n",
      "tensor_out[496] = \t 0\n",
      "tensor_out[497] = \t 67\n",
      "tensor_out[498] = \t 0\n",
      "tensor_out[499] = \t 0\n",
      "tensor_out[500] = \t 87\n",
      "tensor_out[501] = \t 0\n",
      "tensor_out[502] = \t 0\n",
      "tensor_out[503] = \t 33\n",
      "tensor_out[504] = \t 5\n",
      "tensor_out[505] = \t 29\n",
      "tensor_out[506] = \t 0\n",
      "tensor_out[507] = \t 0\n",
      "tensor_out[508] = \t 0\n",
      "tensor_out[509] = \t 14\n",
      "tensor_out[510] = \t 0\n",
      "tensor_out[511] = \t 2\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer22 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0\n",
      "tensor_out[1] = \t 61\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 0\n",
      "tensor_out[5] = \t 0\n",
      "tensor_out[6] = \t 17\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 58\n",
      "tensor_out[9] = \t 6\n",
      "tensor_out[10] = \t 40\n",
      "tensor_out[11] = \t 18\n",
      "tensor_out[12] = \t 18\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 24\n",
      "tensor_out[15] = \t 39\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 49\n",
      "tensor_out[18] = \t 0\n",
      "tensor_out[19] = \t 47\n",
      "tensor_out[20] = \t 65\n",
      "tensor_out[21] = \t 0\n",
      "tensor_out[22] = \t 109\n",
      "tensor_out[23] = \t 115\n",
      "tensor_out[24] = \t 61\n",
      "tensor_out[25] = \t 0\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 0\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 0\n",
      "tensor_out[30] = \t 19\n",
      "tensor_out[31] = \t 0\n",
      "tensor_out[32] = \t 0\n",
      "tensor_out[33] = \t 0\n",
      "tensor_out[34] = \t 48\n",
      "tensor_out[35] = \t 0\n",
      "tensor_out[36] = \t 52\n",
      "tensor_out[37] = \t 4\n",
      "tensor_out[38] = \t 40\n",
      "tensor_out[39] = \t 0\n",
      "tensor_out[40] = \t 34\n",
      "tensor_out[41] = \t 106\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 151\n",
      "tensor_out[44] = \t 0\n",
      "tensor_out[45] = \t 0\n",
      "tensor_out[46] = \t 45\n",
      "tensor_out[47] = \t 13\n",
      "tensor_out[48] = \t 24\n",
      "tensor_out[49] = \t 28\n",
      "tensor_out[50] = \t 33\n",
      "tensor_out[51] = \t 29\n",
      "tensor_out[52] = \t 33\n",
      "tensor_out[53] = \t 44\n",
      "tensor_out[54] = \t 0\n",
      "tensor_out[55] = \t 0\n",
      "tensor_out[56] = \t 0\n",
      "tensor_out[57] = \t 0\n",
      "tensor_out[58] = \t 0\n",
      "tensor_out[59] = \t 0\n",
      "tensor_out[60] = \t 7\n",
      "tensor_out[61] = \t 80\n",
      "tensor_out[62] = \t 0\n",
      "tensor_out[63] = \t 0\n",
      "tensor_out[64] = \t 48\n",
      "tensor_out[65] = \t 0\n",
      "tensor_out[66] = \t 2\n",
      "tensor_out[67] = \t 49\n",
      "tensor_out[68] = \t 58\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 0\n",
      "tensor_out[72] = \t 45\n",
      "tensor_out[73] = \t 43\n",
      "tensor_out[74] = \t 47\n",
      "tensor_out[75] = \t 50\n",
      "tensor_out[76] = \t 52\n",
      "tensor_out[77] = \t 29\n",
      "tensor_out[78] = \t 47\n",
      "tensor_out[79] = \t 0\n",
      "tensor_out[80] = \t 0\n",
      "tensor_out[81] = \t 52\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 0\n",
      "tensor_out[84] = \t 63\n",
      "tensor_out[85] = \t 36\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 0\n",
      "tensor_out[88] = \t 82\n",
      "tensor_out[89] = \t 0\n",
      "tensor_out[90] = \t 36\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 0\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 151\n",
      "tensor_out[95] = \t 26\n",
      "tensor_out[96] = \t 22\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 44\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 87\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 24\n",
      "tensor_out[103] = \t 56\n",
      "tensor_out[104] = \t 20\n",
      "tensor_out[105] = \t 0\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 0\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 16\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 0\n",
      "tensor_out[112] = \t 46\n",
      "tensor_out[113] = \t 8\n",
      "tensor_out[114] = \t 37\n",
      "tensor_out[115] = \t 0\n",
      "tensor_out[116] = \t 85\n",
      "tensor_out[117] = \t 20\n",
      "tensor_out[118] = \t 15\n",
      "tensor_out[119] = \t 70\n",
      "tensor_out[120] = \t 0\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 52\n",
      "tensor_out[123] = \t 0\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 57\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 54\n",
      "tensor_out[129] = \t 44\n",
      "tensor_out[130] = \t 0\n",
      "tensor_out[131] = \t 0\n",
      "tensor_out[132] = \t 0\n",
      "tensor_out[133] = \t 69\n",
      "tensor_out[134] = \t 67\n",
      "tensor_out[135] = \t 93\n",
      "tensor_out[136] = \t 85\n",
      "tensor_out[137] = \t 96\n",
      "tensor_out[138] = \t 0\n",
      "tensor_out[139] = \t 0\n",
      "tensor_out[140] = \t 0\n",
      "tensor_out[141] = \t 80\n",
      "tensor_out[142] = \t 40\n",
      "tensor_out[143] = \t 0\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 0\n",
      "tensor_out[146] = \t 0\n",
      "tensor_out[147] = \t 0\n",
      "tensor_out[148] = \t 19\n",
      "tensor_out[149] = \t 0\n",
      "tensor_out[150] = \t 0\n",
      "tensor_out[151] = \t 53\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 60\n",
      "tensor_out[154] = \t 12\n",
      "tensor_out[155] = \t 60\n",
      "tensor_out[156] = \t 28\n",
      "tensor_out[157] = \t 42\n",
      "tensor_out[158] = \t 236\n",
      "tensor_out[159] = \t 0\n",
      "tensor_out[160] = \t 0\n",
      "tensor_out[161] = \t 0\n",
      "tensor_out[162] = \t 78\n",
      "tensor_out[163] = \t 65\n",
      "tensor_out[164] = \t 0\n",
      "tensor_out[165] = \t 0\n",
      "tensor_out[166] = \t 53\n",
      "tensor_out[167] = \t 75\n",
      "tensor_out[168] = \t 61\n",
      "tensor_out[169] = \t 28\n",
      "tensor_out[170] = \t 42\n",
      "tensor_out[171] = \t 146\n",
      "tensor_out[172] = \t 16\n",
      "tensor_out[173] = \t 36\n",
      "tensor_out[174] = \t 1\n",
      "tensor_out[175] = \t 0\n",
      "tensor_out[176] = \t 115\n",
      "tensor_out[177] = \t 38\n",
      "tensor_out[178] = \t 0\n",
      "tensor_out[179] = \t 19\n",
      "tensor_out[180] = \t 18\n",
      "tensor_out[181] = \t 48\n",
      "tensor_out[182] = \t 66\n",
      "tensor_out[183] = \t 2\n",
      "tensor_out[184] = \t 0\n",
      "tensor_out[185] = \t 86\n",
      "tensor_out[186] = \t 9\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 59\n",
      "tensor_out[189] = \t 37\n",
      "tensor_out[190] = \t 53\n",
      "tensor_out[191] = \t 26\n",
      "tensor_out[192] = \t 0\n",
      "tensor_out[193] = \t 46\n",
      "tensor_out[194] = \t 74\n",
      "tensor_out[195] = \t 167\n",
      "tensor_out[196] = \t 50\n",
      "tensor_out[197] = \t 0\n",
      "tensor_out[198] = \t 0\n",
      "tensor_out[199] = \t 43\n",
      "tensor_out[200] = \t 0\n",
      "tensor_out[201] = \t 59\n",
      "tensor_out[202] = \t 46\n",
      "tensor_out[203] = \t 37\n",
      "tensor_out[204] = \t 0\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 0\n",
      "tensor_out[207] = \t 56\n",
      "tensor_out[208] = \t 0\n",
      "tensor_out[209] = \t 58\n",
      "tensor_out[210] = \t 0\n",
      "tensor_out[211] = \t 0\n",
      "tensor_out[212] = \t 0\n",
      "tensor_out[213] = \t 0\n",
      "tensor_out[214] = \t 21\n",
      "tensor_out[215] = \t 17\n",
      "tensor_out[216] = \t 93\n",
      "tensor_out[217] = \t 0\n",
      "tensor_out[218] = \t 64\n",
      "tensor_out[219] = \t 63\n",
      "tensor_out[220] = \t 0\n",
      "tensor_out[221] = \t 0\n",
      "tensor_out[222] = \t 45\n",
      "tensor_out[223] = \t 0\n",
      "tensor_out[224] = \t 68\n",
      "tensor_out[225] = \t 85\n",
      "tensor_out[226] = \t 8\n",
      "tensor_out[227] = \t 25\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 15\n",
      "tensor_out[230] = \t 52\n",
      "tensor_out[231] = \t 23\n",
      "tensor_out[232] = \t 45\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 0\n",
      "tensor_out[235] = \t 0\n",
      "tensor_out[236] = \t 5\n",
      "tensor_out[237] = \t 46\n",
      "tensor_out[238] = \t 22\n",
      "tensor_out[239] = \t 58\n",
      "tensor_out[240] = \t 95\n",
      "tensor_out[241] = \t 75\n",
      "tensor_out[242] = \t 0\n",
      "tensor_out[243] = \t 149\n",
      "tensor_out[244] = \t 17\n",
      "tensor_out[245] = \t 52\n",
      "tensor_out[246] = \t 50\n",
      "tensor_out[247] = \t 3\n",
      "tensor_out[248] = \t 94\n",
      "tensor_out[249] = \t 56\n",
      "tensor_out[250] = \t 27\n",
      "tensor_out[251] = \t 54\n",
      "tensor_out[252] = \t 0\n",
      "tensor_out[253] = \t 58\n",
      "tensor_out[254] = \t 50\n",
      "tensor_out[255] = \t 0\n",
      "tensor_out[256] = \t 2\n",
      "tensor_out[257] = \t 0\n",
      "tensor_out[258] = \t 41\n",
      "tensor_out[259] = \t 0\n",
      "tensor_out[260] = \t 48\n",
      "tensor_out[261] = \t 77\n",
      "tensor_out[262] = \t 9\n",
      "tensor_out[263] = \t 17\n",
      "tensor_out[264] = \t 18\n",
      "tensor_out[265] = \t 14\n",
      "tensor_out[266] = \t 62\n",
      "tensor_out[267] = \t 0\n",
      "tensor_out[268] = \t 32\n",
      "tensor_out[269] = \t 23\n",
      "tensor_out[270] = \t 50\n",
      "tensor_out[271] = \t 50\n",
      "tensor_out[272] = \t 68\n",
      "tensor_out[273] = \t 49\n",
      "tensor_out[274] = \t 4\n",
      "tensor_out[275] = \t 72\n",
      "tensor_out[276] = \t 65\n",
      "tensor_out[277] = \t 52\n",
      "tensor_out[278] = \t 79\n",
      "tensor_out[279] = \t 49\n",
      "tensor_out[280] = \t 47\n",
      "tensor_out[281] = \t 0\n",
      "tensor_out[282] = \t 0\n",
      "tensor_out[283] = \t 26\n",
      "tensor_out[284] = \t 0\n",
      "tensor_out[285] = \t 0\n",
      "tensor_out[286] = \t 35\n",
      "tensor_out[287] = \t 0\n",
      "tensor_out[288] = \t 50\n",
      "tensor_out[289] = \t 0\n",
      "tensor_out[290] = \t 0\n",
      "tensor_out[291] = \t 66\n",
      "tensor_out[292] = \t 0\n",
      "tensor_out[293] = \t 44\n",
      "tensor_out[294] = \t 0\n",
      "tensor_out[295] = \t 24\n",
      "tensor_out[296] = \t 0\n",
      "tensor_out[297] = \t 60\n",
      "tensor_out[298] = \t 17\n",
      "tensor_out[299] = \t 0\n",
      "tensor_out[300] = \t 15\n",
      "tensor_out[301] = \t 98\n",
      "tensor_out[302] = \t 33\n",
      "tensor_out[303] = \t 79\n",
      "tensor_out[304] = \t 141\n",
      "tensor_out[305] = \t 0\n",
      "tensor_out[306] = \t 26\n",
      "tensor_out[307] = \t 0\n",
      "tensor_out[308] = \t 0\n",
      "tensor_out[309] = \t 0\n",
      "tensor_out[310] = \t 0\n",
      "tensor_out[311] = \t 43\n",
      "tensor_out[312] = \t 43\n",
      "tensor_out[313] = \t 32\n",
      "tensor_out[314] = \t 47\n",
      "tensor_out[315] = \t 31\n",
      "tensor_out[316] = \t 0\n",
      "tensor_out[317] = \t 0\n",
      "tensor_out[318] = \t 18\n",
      "tensor_out[319] = \t 0\n",
      "tensor_out[320] = \t 104\n",
      "tensor_out[321] = \t 43\n",
      "tensor_out[322] = \t 0\n",
      "tensor_out[323] = \t 42\n",
      "tensor_out[324] = \t 0\n",
      "tensor_out[325] = \t 70\n",
      "tensor_out[326] = \t 0\n",
      "tensor_out[327] = \t 50\n",
      "tensor_out[328] = \t 40\n",
      "tensor_out[329] = \t 0\n",
      "tensor_out[330] = \t 0\n",
      "tensor_out[331] = \t 0\n",
      "tensor_out[332] = \t 119\n",
      "tensor_out[333] = \t 20\n",
      "tensor_out[334] = \t 0\n",
      "tensor_out[335] = \t 57\n",
      "tensor_out[336] = \t 52\n",
      "tensor_out[337] = \t 55\n",
      "tensor_out[338] = \t 0\n",
      "tensor_out[339] = \t 0\n",
      "tensor_out[340] = \t 77\n",
      "tensor_out[341] = \t 0\n",
      "tensor_out[342] = \t 50\n",
      "tensor_out[343] = \t 80\n",
      "tensor_out[344] = \t 0\n",
      "tensor_out[345] = \t 54\n",
      "tensor_out[346] = \t 53\n",
      "tensor_out[347] = \t 22\n",
      "tensor_out[348] = \t 0\n",
      "tensor_out[349] = \t 0\n",
      "tensor_out[350] = \t 67\n",
      "tensor_out[351] = \t 0\n",
      "tensor_out[352] = \t 10\n",
      "tensor_out[353] = \t 0\n",
      "tensor_out[354] = \t 0\n",
      "tensor_out[355] = \t 54\n",
      "tensor_out[356] = \t 0\n",
      "tensor_out[357] = \t 0\n",
      "tensor_out[358] = \t 117\n",
      "tensor_out[359] = \t 0\n",
      "tensor_out[360] = \t 4\n",
      "tensor_out[361] = \t 25\n",
      "tensor_out[362] = \t 13\n",
      "tensor_out[363] = \t 47\n",
      "tensor_out[364] = \t 46\n",
      "tensor_out[365] = \t 68\n",
      "tensor_out[366] = \t 6\n",
      "tensor_out[367] = \t 25\n",
      "tensor_out[368] = \t 0\n",
      "tensor_out[369] = \t 0\n",
      "tensor_out[370] = \t 0\n",
      "tensor_out[371] = \t 62\n",
      "tensor_out[372] = \t 0\n",
      "tensor_out[373] = \t 75\n",
      "tensor_out[374] = \t 48\n",
      "tensor_out[375] = \t 5\n",
      "tensor_out[376] = \t 0\n",
      "tensor_out[377] = \t 0\n",
      "tensor_out[378] = \t 0\n",
      "tensor_out[379] = \t 49\n",
      "tensor_out[380] = \t 32\n",
      "tensor_out[381] = \t 0\n",
      "tensor_out[382] = \t 50\n",
      "tensor_out[383] = \t 0\n",
      "tensor_out[384] = \t 0\n",
      "tensor_out[385] = \t 55\n",
      "tensor_out[386] = \t 0\n",
      "tensor_out[387] = \t 38\n",
      "tensor_out[388] = \t 26\n",
      "tensor_out[389] = \t 0\n",
      "tensor_out[390] = \t 54\n",
      "tensor_out[391] = \t 0\n",
      "tensor_out[392] = \t 0\n",
      "tensor_out[393] = \t 6\n",
      "tensor_out[394] = \t 0\n",
      "tensor_out[395] = \t 28\n",
      "tensor_out[396] = \t 8\n",
      "tensor_out[397] = \t 47\n",
      "tensor_out[398] = \t 48\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 7\n",
      "tensor_out[401] = \t 0\n",
      "tensor_out[402] = \t 0\n",
      "tensor_out[403] = \t 0\n",
      "tensor_out[404] = \t 0\n",
      "tensor_out[405] = \t 62\n",
      "tensor_out[406] = \t 0\n",
      "tensor_out[407] = \t 0\n",
      "tensor_out[408] = \t 45\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 52\n",
      "tensor_out[411] = \t 0\n",
      "tensor_out[412] = \t 0\n",
      "tensor_out[413] = \t 44\n",
      "tensor_out[414] = \t 43\n",
      "tensor_out[415] = \t 0\n",
      "tensor_out[416] = \t 0\n",
      "tensor_out[417] = \t 87\n",
      "tensor_out[418] = \t 0\n",
      "tensor_out[419] = \t 22\n",
      "tensor_out[420] = \t 26\n",
      "tensor_out[421] = \t 5\n",
      "tensor_out[422] = \t 0\n",
      "tensor_out[423] = \t 17\n",
      "tensor_out[424] = \t 52\n",
      "tensor_out[425] = \t 89\n",
      "tensor_out[426] = \t 24\n",
      "tensor_out[427] = \t 0\n",
      "tensor_out[428] = \t 53\n",
      "tensor_out[429] = \t 0\n",
      "tensor_out[430] = \t 77\n",
      "tensor_out[431] = \t 0\n",
      "tensor_out[432] = \t 138\n",
      "tensor_out[433] = \t 84\n",
      "tensor_out[434] = \t 0\n",
      "tensor_out[435] = \t 50\n",
      "tensor_out[436] = \t 38\n",
      "tensor_out[437] = \t 0\n",
      "tensor_out[438] = \t 0\n",
      "tensor_out[439] = \t 0\n",
      "tensor_out[440] = \t 0\n",
      "tensor_out[441] = \t 56\n",
      "tensor_out[442] = \t 33\n",
      "tensor_out[443] = \t 0\n",
      "tensor_out[444] = \t 59\n",
      "tensor_out[445] = \t 0\n",
      "tensor_out[446] = \t 41\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 36\n",
      "tensor_out[449] = \t 0\n",
      "tensor_out[450] = \t 0\n",
      "tensor_out[451] = \t 0\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 32\n",
      "tensor_out[454] = \t 51\n",
      "tensor_out[455] = \t 0\n",
      "tensor_out[456] = \t 62\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 42\n",
      "tensor_out[459] = \t 52\n",
      "tensor_out[460] = \t 128\n",
      "tensor_out[461] = \t 0\n",
      "tensor_out[462] = \t 0\n",
      "tensor_out[463] = \t 12\n",
      "tensor_out[464] = \t 67\n",
      "tensor_out[465] = \t 50\n",
      "tensor_out[466] = \t 110\n",
      "tensor_out[467] = \t 0\n",
      "tensor_out[468] = \t 0\n",
      "tensor_out[469] = \t 0\n",
      "tensor_out[470] = \t 0\n",
      "tensor_out[471] = \t 16\n",
      "tensor_out[472] = \t 0\n",
      "tensor_out[473] = \t 0\n",
      "tensor_out[474] = \t 73\n",
      "tensor_out[475] = \t 0\n",
      "tensor_out[476] = \t 0\n",
      "tensor_out[477] = \t 18\n",
      "tensor_out[478] = \t 50\n",
      "tensor_out[479] = \t 0\n",
      "tensor_out[480] = \t 0\n",
      "tensor_out[481] = \t 0\n",
      "tensor_out[482] = \t 55\n",
      "tensor_out[483] = \t 0\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 64\n",
      "tensor_out[486] = \t 68\n",
      "tensor_out[487] = \t 0\n",
      "tensor_out[488] = \t 0\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 23\n",
      "tensor_out[491] = \t 53\n",
      "tensor_out[492] = \t 82\n",
      "tensor_out[493] = \t 0\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 0\n",
      "tensor_out[496] = \t 0\n",
      "tensor_out[497] = \t 0\n",
      "tensor_out[498] = \t 0\n",
      "tensor_out[499] = \t 0\n",
      "tensor_out[500] = \t 10\n",
      "tensor_out[501] = \t 54\n",
      "tensor_out[502] = \t 0\n",
      "tensor_out[503] = \t 45\n",
      "tensor_out[504] = \t 0\n",
      "tensor_out[505] = \t 10\n",
      "tensor_out[506] = \t 31\n",
      "tensor_out[507] = \t 0\n",
      "tensor_out[508] = \t 53\n",
      "tensor_out[509] = \t 53\n",
      "tensor_out[510] = \t 0\n",
      "tensor_out[511] = \t 0\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer23 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0\n",
      "tensor_out[1] = \t 41\n",
      "tensor_out[2] = \t 58\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 0\n",
      "tensor_out[5] = \t 61\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 12\n",
      "tensor_out[9] = \t 71\n",
      "tensor_out[10] = \t 0\n",
      "tensor_out[11] = \t 32\n",
      "tensor_out[12] = \t 0\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 78\n",
      "tensor_out[15] = \t 38\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 11\n",
      "tensor_out[19] = \t 27\n",
      "tensor_out[20] = \t 7\n",
      "tensor_out[21] = \t 68\n",
      "tensor_out[22] = \t 64\n",
      "tensor_out[23] = \t 35\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 132\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 55\n",
      "tensor_out[28] = \t 117\n",
      "tensor_out[29] = \t 0\n",
      "tensor_out[30] = \t 44\n",
      "tensor_out[31] = \t 0\n",
      "tensor_out[32] = \t 41\n",
      "tensor_out[33] = \t 60\n",
      "tensor_out[34] = \t 0\n",
      "tensor_out[35] = \t 38\n",
      "tensor_out[36] = \t 3\n",
      "tensor_out[37] = \t 46\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 0\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 4\n",
      "tensor_out[43] = \t 94\n",
      "tensor_out[44] = \t 10\n",
      "tensor_out[45] = \t 34\n",
      "tensor_out[46] = \t 64\n",
      "tensor_out[47] = \t 102\n",
      "tensor_out[48] = \t 51\n",
      "tensor_out[49] = \t 50\n",
      "tensor_out[50] = \t 0\n",
      "tensor_out[51] = \t 0\n",
      "tensor_out[52] = \t 38\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 0\n",
      "tensor_out[55] = \t 0\n",
      "tensor_out[56] = \t 64\n",
      "tensor_out[57] = \t 31\n",
      "tensor_out[58] = \t 0\n",
      "tensor_out[59] = \t 24\n",
      "tensor_out[60] = \t 0\n",
      "tensor_out[61] = \t 0\n",
      "tensor_out[62] = \t 0\n",
      "tensor_out[63] = \t 0\n",
      "tensor_out[64] = \t 7\n",
      "tensor_out[65] = \t 0\n",
      "tensor_out[66] = \t 0\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 0\n",
      "tensor_out[72] = \t 0\n",
      "tensor_out[73] = \t 0\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 0\n",
      "tensor_out[76] = \t 0\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 143\n",
      "tensor_out[79] = \t 0\n",
      "tensor_out[80] = \t 0\n",
      "tensor_out[81] = \t 0\n",
      "tensor_out[82] = \t 94\n",
      "tensor_out[83] = \t 8\n",
      "tensor_out[84] = \t 9\n",
      "tensor_out[85] = \t 10\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 0\n",
      "tensor_out[88] = \t 0\n",
      "tensor_out[89] = \t 0\n",
      "tensor_out[90] = \t 39\n",
      "tensor_out[91] = \t 73\n",
      "tensor_out[92] = \t 41\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 33\n",
      "tensor_out[95] = \t 80\n",
      "tensor_out[96] = \t 0\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 0\n",
      "tensor_out[101] = \t 67\n",
      "tensor_out[102] = \t 27\n",
      "tensor_out[103] = \t 38\n",
      "tensor_out[104] = \t 0\n",
      "tensor_out[105] = \t 0\n",
      "tensor_out[106] = \t 94\n",
      "tensor_out[107] = \t 114\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 0\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 0\n",
      "tensor_out[112] = \t 0\n",
      "tensor_out[113] = \t 0\n",
      "tensor_out[114] = \t 6\n",
      "tensor_out[115] = \t 0\n",
      "tensor_out[116] = \t 121\n",
      "tensor_out[117] = \t 41\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 12\n",
      "tensor_out[120] = \t 58\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 0\n",
      "tensor_out[123] = \t 78\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 39\n",
      "tensor_out[126] = \t 12\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 16\n",
      "tensor_out[129] = \t 0\n",
      "tensor_out[130] = \t 0\n",
      "tensor_out[131] = \t 0\n",
      "tensor_out[132] = \t 0\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 139\n",
      "tensor_out[135] = \t 45\n",
      "tensor_out[136] = \t 67\n",
      "tensor_out[137] = \t 0\n",
      "tensor_out[138] = \t 11\n",
      "tensor_out[139] = \t 25\n",
      "tensor_out[140] = \t 26\n",
      "tensor_out[141] = \t 0\n",
      "tensor_out[142] = \t 0\n",
      "tensor_out[143] = \t 63\n",
      "tensor_out[144] = \t 33\n",
      "tensor_out[145] = \t 166\n",
      "tensor_out[146] = \t 61\n",
      "tensor_out[147] = \t 0\n",
      "tensor_out[148] = \t 58\n",
      "tensor_out[149] = \t 12\n",
      "tensor_out[150] = \t 0\n",
      "tensor_out[151] = \t 0\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 0\n",
      "tensor_out[154] = \t 78\n",
      "tensor_out[155] = \t 47\n",
      "tensor_out[156] = \t 0\n",
      "tensor_out[157] = \t 0\n",
      "tensor_out[158] = \t 0\n",
      "tensor_out[159] = \t 0\n",
      "tensor_out[160] = \t 45\n",
      "tensor_out[161] = \t 29\n",
      "tensor_out[162] = \t 0\n",
      "tensor_out[163] = \t 36\n",
      "tensor_out[164] = \t 0\n",
      "tensor_out[165] = \t 105\n",
      "tensor_out[166] = \t 77\n",
      "tensor_out[167] = \t 60\n",
      "tensor_out[168] = \t 0\n",
      "tensor_out[169] = \t 0\n",
      "tensor_out[170] = \t 11\n",
      "tensor_out[171] = \t 0\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 0\n",
      "tensor_out[174] = \t 54\n",
      "tensor_out[175] = \t 0\n",
      "tensor_out[176] = \t 0\n",
      "tensor_out[177] = \t 72\n",
      "tensor_out[178] = \t 117\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 52\n",
      "tensor_out[181] = \t 0\n",
      "tensor_out[182] = \t 107\n",
      "tensor_out[183] = \t 0\n",
      "tensor_out[184] = \t 0\n",
      "tensor_out[185] = \t 34\n",
      "tensor_out[186] = \t 0\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 0\n",
      "tensor_out[189] = \t 101\n",
      "tensor_out[190] = \t 0\n",
      "tensor_out[191] = \t 63\n",
      "tensor_out[192] = \t 22\n",
      "tensor_out[193] = \t 19\n",
      "tensor_out[194] = \t 0\n",
      "tensor_out[195] = \t 0\n",
      "tensor_out[196] = \t 0\n",
      "tensor_out[197] = \t 0\n",
      "tensor_out[198] = \t 0\n",
      "tensor_out[199] = \t 0\n",
      "tensor_out[200] = \t 0\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 0\n",
      "tensor_out[203] = \t 0\n",
      "tensor_out[204] = \t 0\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 0\n",
      "tensor_out[207] = \t 0\n",
      "tensor_out[208] = \t 0\n",
      "tensor_out[209] = \t 0\n",
      "tensor_out[210] = \t 1\n",
      "tensor_out[211] = \t 0\n",
      "tensor_out[212] = \t 128\n",
      "tensor_out[213] = \t 20\n",
      "tensor_out[214] = \t 9\n",
      "tensor_out[215] = \t 0\n",
      "tensor_out[216] = \t 78\n",
      "tensor_out[217] = \t 0\n",
      "tensor_out[218] = \t 0\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 85\n",
      "tensor_out[221] = \t 0\n",
      "tensor_out[222] = \t 0\n",
      "tensor_out[223] = \t 84\n",
      "tensor_out[224] = \t 0\n",
      "tensor_out[225] = \t 0\n",
      "tensor_out[226] = \t 0\n",
      "tensor_out[227] = \t 111\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 35\n",
      "tensor_out[230] = \t 0\n",
      "tensor_out[231] = \t 0\n",
      "tensor_out[232] = \t 0\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 0\n",
      "tensor_out[235] = \t 0\n",
      "tensor_out[236] = \t 0\n",
      "tensor_out[237] = \t 0\n",
      "tensor_out[238] = \t 0\n",
      "tensor_out[239] = \t 0\n",
      "tensor_out[240] = \t 126\n",
      "tensor_out[241] = \t 8\n",
      "tensor_out[242] = \t 1\n",
      "tensor_out[243] = \t 0\n",
      "tensor_out[244] = \t 0\n",
      "tensor_out[245] = \t 0\n",
      "tensor_out[246] = \t 0\n",
      "tensor_out[247] = \t 40\n",
      "tensor_out[248] = \t 0\n",
      "tensor_out[249] = \t 0\n",
      "tensor_out[250] = \t 10\n",
      "tensor_out[251] = \t 122\n",
      "tensor_out[252] = \t 0\n",
      "tensor_out[253] = \t 0\n",
      "tensor_out[254] = \t 0\n",
      "tensor_out[255] = \t 0\n",
      "tensor_out[256] = \t 0\n",
      "tensor_out[257] = \t 0\n",
      "tensor_out[258] = \t 0\n",
      "tensor_out[259] = \t 0\n",
      "tensor_out[260] = \t 0\n",
      "tensor_out[261] = \t 45\n",
      "tensor_out[262] = \t 0\n",
      "tensor_out[263] = \t 16\n",
      "tensor_out[264] = \t 129\n",
      "tensor_out[265] = \t 0\n",
      "tensor_out[266] = \t 32\n",
      "tensor_out[267] = \t 0\n",
      "tensor_out[268] = \t 19\n",
      "tensor_out[269] = \t 0\n",
      "tensor_out[270] = \t 85\n",
      "tensor_out[271] = \t 0\n",
      "tensor_out[272] = \t 0\n",
      "tensor_out[273] = \t 0\n",
      "tensor_out[274] = \t 0\n",
      "tensor_out[275] = \t 48\n",
      "tensor_out[276] = \t 0\n",
      "tensor_out[277] = \t 0\n",
      "tensor_out[278] = \t 116\n",
      "tensor_out[279] = \t 90\n",
      "tensor_out[280] = \t 0\n",
      "tensor_out[281] = \t 0\n",
      "tensor_out[282] = \t 0\n",
      "tensor_out[283] = \t 0\n",
      "tensor_out[284] = \t 32\n",
      "tensor_out[285] = \t 9\n",
      "tensor_out[286] = \t 141\n",
      "tensor_out[287] = \t 19\n",
      "tensor_out[288] = \t 0\n",
      "tensor_out[289] = \t 0\n",
      "tensor_out[290] = \t 116\n",
      "tensor_out[291] = \t 0\n",
      "tensor_out[292] = \t 41\n",
      "tensor_out[293] = \t 0\n",
      "tensor_out[294] = \t 2\n",
      "tensor_out[295] = \t 0\n",
      "tensor_out[296] = \t 0\n",
      "tensor_out[297] = \t 0\n",
      "tensor_out[298] = \t 0\n",
      "tensor_out[299] = \t 34\n",
      "tensor_out[300] = \t 24\n",
      "tensor_out[301] = \t 0\n",
      "tensor_out[302] = \t 0\n",
      "tensor_out[303] = \t 16\n",
      "tensor_out[304] = \t 0\n",
      "tensor_out[305] = \t 0\n",
      "tensor_out[306] = \t 0\n",
      "tensor_out[307] = \t 95\n",
      "tensor_out[308] = \t 65\n",
      "tensor_out[309] = \t 0\n",
      "tensor_out[310] = \t 24\n",
      "tensor_out[311] = \t 0\n",
      "tensor_out[312] = \t 0\n",
      "tensor_out[313] = \t 0\n",
      "tensor_out[314] = \t 5\n",
      "tensor_out[315] = \t 108\n",
      "tensor_out[316] = \t 23\n",
      "tensor_out[317] = \t 0\n",
      "tensor_out[318] = \t 0\n",
      "tensor_out[319] = \t 42\n",
      "tensor_out[320] = \t 0\n",
      "tensor_out[321] = \t 141\n",
      "tensor_out[322] = \t 0\n",
      "tensor_out[323] = \t 27\n",
      "tensor_out[324] = \t 45\n",
      "tensor_out[325] = \t 0\n",
      "tensor_out[326] = \t 54\n",
      "tensor_out[327] = \t 0\n",
      "tensor_out[328] = \t 0\n",
      "tensor_out[329] = \t 99\n",
      "tensor_out[330] = \t 103\n",
      "tensor_out[331] = \t 0\n",
      "tensor_out[332] = \t 0\n",
      "tensor_out[333] = \t 0\n",
      "tensor_out[334] = \t 102\n",
      "tensor_out[335] = \t 111\n",
      "tensor_out[336] = \t 0\n",
      "tensor_out[337] = \t 0\n",
      "tensor_out[338] = \t 104\n",
      "tensor_out[339] = \t 0\n",
      "tensor_out[340] = \t 0\n",
      "tensor_out[341] = \t 0\n",
      "tensor_out[342] = \t 0\n",
      "tensor_out[343] = \t 28\n",
      "tensor_out[344] = \t 0\n",
      "tensor_out[345] = \t 0\n",
      "tensor_out[346] = \t 0\n",
      "tensor_out[347] = \t 26\n",
      "tensor_out[348] = \t 0\n",
      "tensor_out[349] = \t 103\n",
      "tensor_out[350] = \t 102\n",
      "tensor_out[351] = \t 29\n",
      "tensor_out[352] = \t 0\n",
      "tensor_out[353] = \t 43\n",
      "tensor_out[354] = \t 102\n",
      "tensor_out[355] = \t 35\n",
      "tensor_out[356] = \t 83\n",
      "tensor_out[357] = \t 0\n",
      "tensor_out[358] = \t 3\n",
      "tensor_out[359] = \t 0\n",
      "tensor_out[360] = \t 0\n",
      "tensor_out[361] = \t 0\n",
      "tensor_out[362] = \t 0\n",
      "tensor_out[363] = \t 67\n",
      "tensor_out[364] = \t 0\n",
      "tensor_out[365] = \t 0\n",
      "tensor_out[366] = \t 60\n",
      "tensor_out[367] = \t 0\n",
      "tensor_out[368] = \t 29\n",
      "tensor_out[369] = \t 42\n",
      "tensor_out[370] = \t 31\n",
      "tensor_out[371] = \t 12\n",
      "tensor_out[372] = \t 109\n",
      "tensor_out[373] = \t 108\n",
      "tensor_out[374] = \t 0\n",
      "tensor_out[375] = \t 0\n",
      "tensor_out[376] = \t 0\n",
      "tensor_out[377] = \t 37\n",
      "tensor_out[378] = \t 0\n",
      "tensor_out[379] = \t 70\n",
      "tensor_out[380] = \t 79\n",
      "tensor_out[381] = \t 106\n",
      "tensor_out[382] = \t 22\n",
      "tensor_out[383] = \t 0\n",
      "tensor_out[384] = \t 35\n",
      "tensor_out[385] = \t 0\n",
      "tensor_out[386] = \t 0\n",
      "tensor_out[387] = \t 11\n",
      "tensor_out[388] = \t 0\n",
      "tensor_out[389] = \t 0\n",
      "tensor_out[390] = \t 0\n",
      "tensor_out[391] = \t 69\n",
      "tensor_out[392] = \t 35\n",
      "tensor_out[393] = \t 0\n",
      "tensor_out[394] = \t 29\n",
      "tensor_out[395] = \t 15\n",
      "tensor_out[396] = \t 0\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 35\n",
      "tensor_out[399] = \t 35\n",
      "tensor_out[400] = \t 0\n",
      "tensor_out[401] = \t 0\n",
      "tensor_out[402] = \t 0\n",
      "tensor_out[403] = \t 0\n",
      "tensor_out[404] = \t 0\n",
      "tensor_out[405] = \t 0\n",
      "tensor_out[406] = \t 0\n",
      "tensor_out[407] = \t 56\n",
      "tensor_out[408] = \t 154\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 0\n",
      "tensor_out[411] = \t 0\n",
      "tensor_out[412] = \t 115\n",
      "tensor_out[413] = \t 0\n",
      "tensor_out[414] = \t 17\n",
      "tensor_out[415] = \t 51\n",
      "tensor_out[416] = \t 0\n",
      "tensor_out[417] = \t 0\n",
      "tensor_out[418] = \t 0\n",
      "tensor_out[419] = \t 0\n",
      "tensor_out[420] = \t 56\n",
      "tensor_out[421] = \t 0\n",
      "tensor_out[422] = \t 0\n",
      "tensor_out[423] = \t 48\n",
      "tensor_out[424] = \t 5\n",
      "tensor_out[425] = \t 0\n",
      "tensor_out[426] = \t 51\n",
      "tensor_out[427] = \t 0\n",
      "tensor_out[428] = \t 86\n",
      "tensor_out[429] = \t 0\n",
      "tensor_out[430] = \t 0\n",
      "tensor_out[431] = \t 52\n",
      "tensor_out[432] = \t 32\n",
      "tensor_out[433] = \t 66\n",
      "tensor_out[434] = \t 34\n",
      "tensor_out[435] = \t 54\n",
      "tensor_out[436] = \t 55\n",
      "tensor_out[437] = \t 0\n",
      "tensor_out[438] = \t 1\n",
      "tensor_out[439] = \t 2\n",
      "tensor_out[440] = \t 45\n",
      "tensor_out[441] = \t 18\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 47\n",
      "tensor_out[444] = \t 0\n",
      "tensor_out[445] = \t 0\n",
      "tensor_out[446] = \t 0\n",
      "tensor_out[447] = \t 73\n",
      "tensor_out[448] = \t 0\n",
      "tensor_out[449] = \t 37\n",
      "tensor_out[450] = \t 82\n",
      "tensor_out[451] = \t 0\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 0\n",
      "tensor_out[454] = \t 0\n",
      "tensor_out[455] = \t 0\n",
      "tensor_out[456] = \t 0\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 0\n",
      "tensor_out[459] = \t 9\n",
      "tensor_out[460] = \t 0\n",
      "tensor_out[461] = \t 0\n",
      "tensor_out[462] = \t 0\n",
      "tensor_out[463] = \t 0\n",
      "tensor_out[464] = \t 0\n",
      "tensor_out[465] = \t 47\n",
      "tensor_out[466] = \t 0\n",
      "tensor_out[467] = \t 0\n",
      "tensor_out[468] = \t 42\n",
      "tensor_out[469] = \t 0\n",
      "tensor_out[470] = \t 0\n",
      "tensor_out[471] = \t 98\n",
      "tensor_out[472] = \t 13\n",
      "tensor_out[473] = \t 0\n",
      "tensor_out[474] = \t 0\n",
      "tensor_out[475] = \t 0\n",
      "tensor_out[476] = \t 0\n",
      "tensor_out[477] = \t 0\n",
      "tensor_out[478] = \t 0\n",
      "tensor_out[479] = \t 0\n",
      "tensor_out[480] = \t 0\n",
      "tensor_out[481] = \t 165\n",
      "tensor_out[482] = \t 112\n",
      "tensor_out[483] = \t 0\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 4\n",
      "tensor_out[486] = \t 0\n",
      "tensor_out[487] = \t 3\n",
      "tensor_out[488] = \t 47\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 14\n",
      "tensor_out[491] = \t 42\n",
      "tensor_out[492] = \t 0\n",
      "tensor_out[493] = \t 117\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 0\n",
      "tensor_out[496] = \t 20\n",
      "tensor_out[497] = \t 7\n",
      "tensor_out[498] = \t 0\n",
      "tensor_out[499] = \t 0\n",
      "tensor_out[500] = \t 0\n",
      "tensor_out[501] = \t 0\n",
      "tensor_out[502] = \t 0\n",
      "tensor_out[503] = \t 83\n",
      "tensor_out[504] = \t 0\n",
      "tensor_out[505] = \t 42\n",
      "tensor_out[506] = \t 60\n",
      "tensor_out[507] = \t 24\n",
      "tensor_out[508] = \t 0\n",
      "tensor_out[509] = \t 0\n",
      "tensor_out[510] = \t 22\n",
      "tensor_out[511] = \t 0\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer24 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 59\n",
      "tensor_out[1] = \t 100\n",
      "tensor_out[2] = \t 33\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 0\n",
      "tensor_out[5] = \t 44\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 41\n",
      "tensor_out[9] = \t 0\n",
      "tensor_out[10] = \t 40\n",
      "tensor_out[11] = \t 48\n",
      "tensor_out[12] = \t 2\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 10\n",
      "tensor_out[15] = \t 28\n",
      "tensor_out[16] = \t 1\n",
      "tensor_out[17] = \t 69\n",
      "tensor_out[18] = \t 67\n",
      "tensor_out[19] = \t 0\n",
      "tensor_out[20] = \t 18\n",
      "tensor_out[21] = \t 85\n",
      "tensor_out[22] = \t 0\n",
      "tensor_out[23] = \t 0\n",
      "tensor_out[24] = \t 11\n",
      "tensor_out[25] = \t 52\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 36\n",
      "tensor_out[28] = \t 10\n",
      "tensor_out[29] = \t 2\n",
      "tensor_out[30] = \t 0\n",
      "tensor_out[31] = \t 0\n",
      "tensor_out[32] = \t 9\n",
      "tensor_out[33] = \t 31\n",
      "tensor_out[34] = \t 0\n",
      "tensor_out[35] = \t 55\n",
      "tensor_out[36] = \t 0\n",
      "tensor_out[37] = \t 18\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 0\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 30\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 28\n",
      "tensor_out[45] = \t 30\n",
      "tensor_out[46] = \t 92\n",
      "tensor_out[47] = \t 22\n",
      "tensor_out[48] = \t 70\n",
      "tensor_out[49] = \t 34\n",
      "tensor_out[50] = \t 0\n",
      "tensor_out[51] = \t 0\n",
      "tensor_out[52] = \t 65\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 5\n",
      "tensor_out[55] = \t 0\n",
      "tensor_out[56] = \t 0\n",
      "tensor_out[57] = \t 71\n",
      "tensor_out[58] = \t 38\n",
      "tensor_out[59] = \t 11\n",
      "tensor_out[60] = \t 0\n",
      "tensor_out[61] = \t 0\n",
      "tensor_out[62] = \t 0\n",
      "tensor_out[63] = \t 0\n",
      "tensor_out[64] = \t 0\n",
      "tensor_out[65] = \t 0\n",
      "tensor_out[66] = \t 0\n",
      "tensor_out[67] = \t 144\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 3\n",
      "tensor_out[70] = \t 69\n",
      "tensor_out[71] = \t 4\n",
      "tensor_out[72] = \t 0\n",
      "tensor_out[73] = \t 0\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 2\n",
      "tensor_out[76] = \t 0\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 0\n",
      "tensor_out[79] = \t 0\n",
      "tensor_out[80] = \t 57\n",
      "tensor_out[81] = \t 8\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 24\n",
      "tensor_out[84] = \t 0\n",
      "tensor_out[85] = \t 36\n",
      "tensor_out[86] = \t 23\n",
      "tensor_out[87] = \t 1\n",
      "tensor_out[88] = \t 28\n",
      "tensor_out[89] = \t 0\n",
      "tensor_out[90] = \t 0\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 28\n",
      "tensor_out[93] = \t 65\n",
      "tensor_out[94] = \t 32\n",
      "tensor_out[95] = \t 13\n",
      "tensor_out[96] = \t 0\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 12\n",
      "tensor_out[99] = \t 4\n",
      "tensor_out[100] = \t 0\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 34\n",
      "tensor_out[103] = \t 2\n",
      "tensor_out[104] = \t 64\n",
      "tensor_out[105] = \t 0\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 0\n",
      "tensor_out[108] = \t 5\n",
      "tensor_out[109] = \t 0\n",
      "tensor_out[110] = \t 1\n",
      "tensor_out[111] = \t 0\n",
      "tensor_out[112] = \t 0\n",
      "tensor_out[113] = \t 0\n",
      "tensor_out[114] = \t 26\n",
      "tensor_out[115] = \t 53\n",
      "tensor_out[116] = \t 0\n",
      "tensor_out[117] = \t 40\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 25\n",
      "tensor_out[120] = \t 11\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 59\n",
      "tensor_out[123] = \t 36\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 32\n",
      "tensor_out[126] = \t 92\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 37\n",
      "tensor_out[129] = \t 0\n",
      "tensor_out[130] = \t 11\n",
      "tensor_out[131] = \t 0\n",
      "tensor_out[132] = \t 0\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 0\n",
      "tensor_out[135] = \t 29\n",
      "tensor_out[136] = \t 52\n",
      "tensor_out[137] = \t 0\n",
      "tensor_out[138] = \t 0\n",
      "tensor_out[139] = \t 78\n",
      "tensor_out[140] = \t 36\n",
      "tensor_out[141] = \t 16\n",
      "tensor_out[142] = \t 27\n",
      "tensor_out[143] = \t 56\n",
      "tensor_out[144] = \t 83\n",
      "tensor_out[145] = \t 0\n",
      "tensor_out[146] = \t 40\n",
      "tensor_out[147] = \t 0\n",
      "tensor_out[148] = \t 24\n",
      "tensor_out[149] = \t 14\n",
      "tensor_out[150] = \t 36\n",
      "tensor_out[151] = \t 0\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 6\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 160\n",
      "tensor_out[156] = \t 0\n",
      "tensor_out[157] = \t 0\n",
      "tensor_out[158] = \t 0\n",
      "tensor_out[159] = \t 0\n",
      "tensor_out[160] = \t 0\n",
      "tensor_out[161] = \t 45\n",
      "tensor_out[162] = \t 71\n",
      "tensor_out[163] = \t 65\n",
      "tensor_out[164] = \t 40\n",
      "tensor_out[165] = \t 88\n",
      "tensor_out[166] = \t 0\n",
      "tensor_out[167] = \t 0\n",
      "tensor_out[168] = \t 48\n",
      "tensor_out[169] = \t 84\n",
      "tensor_out[170] = \t 3\n",
      "tensor_out[171] = \t 0\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 45\n",
      "tensor_out[174] = \t 19\n",
      "tensor_out[175] = \t 83\n",
      "tensor_out[176] = \t 10\n",
      "tensor_out[177] = \t 4\n",
      "tensor_out[178] = \t 40\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 0\n",
      "tensor_out[181] = \t 46\n",
      "tensor_out[182] = \t 10\n",
      "tensor_out[183] = \t 0\n",
      "tensor_out[184] = \t 0\n",
      "tensor_out[185] = \t 0\n",
      "tensor_out[186] = \t 0\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 39\n",
      "tensor_out[189] = \t 0\n",
      "tensor_out[190] = \t 78\n",
      "tensor_out[191] = \t 0\n",
      "tensor_out[192] = \t 13\n",
      "tensor_out[193] = \t 37\n",
      "tensor_out[194] = \t 0\n",
      "tensor_out[195] = \t 0\n",
      "tensor_out[196] = \t 0\n",
      "tensor_out[197] = \t 0\n",
      "tensor_out[198] = \t 63\n",
      "tensor_out[199] = \t 0\n",
      "tensor_out[200] = \t 94\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 0\n",
      "tensor_out[203] = \t 0\n",
      "tensor_out[204] = \t 0\n",
      "tensor_out[205] = \t 36\n",
      "tensor_out[206] = \t 35\n",
      "tensor_out[207] = \t 0\n",
      "tensor_out[208] = \t 82\n",
      "tensor_out[209] = \t 63\n",
      "tensor_out[210] = \t 1\n",
      "tensor_out[211] = \t 37\n",
      "tensor_out[212] = \t 0\n",
      "tensor_out[213] = \t 88\n",
      "tensor_out[214] = \t 56\n",
      "tensor_out[215] = \t 0\n",
      "tensor_out[216] = \t 0\n",
      "tensor_out[217] = \t 0\n",
      "tensor_out[218] = \t 64\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 0\n",
      "tensor_out[221] = \t 52\n",
      "tensor_out[222] = \t 0\n",
      "tensor_out[223] = \t 100\n",
      "tensor_out[224] = \t 0\n",
      "tensor_out[225] = \t 0\n",
      "tensor_out[226] = \t 1\n",
      "tensor_out[227] = \t 23\n",
      "tensor_out[228] = \t 24\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 0\n",
      "tensor_out[231] = \t 0\n",
      "tensor_out[232] = \t 0\n",
      "tensor_out[233] = \t 49\n",
      "tensor_out[234] = \t 61\n",
      "tensor_out[235] = \t 33\n",
      "tensor_out[236] = \t 51\n",
      "tensor_out[237] = \t 92\n",
      "tensor_out[238] = \t 2\n",
      "tensor_out[239] = \t 0\n",
      "tensor_out[240] = \t 0\n",
      "tensor_out[241] = \t 38\n",
      "tensor_out[242] = \t 85\n",
      "tensor_out[243] = \t 0\n",
      "tensor_out[244] = \t 43\n",
      "tensor_out[245] = \t 0\n",
      "tensor_out[246] = \t 58\n",
      "tensor_out[247] = \t 27\n",
      "tensor_out[248] = \t 0\n",
      "tensor_out[249] = \t 0\n",
      "tensor_out[250] = \t 45\n",
      "tensor_out[251] = \t 13\n",
      "tensor_out[252] = \t 0\n",
      "tensor_out[253] = \t 76\n",
      "tensor_out[254] = \t 59\n",
      "tensor_out[255] = \t 55\n",
      "tensor_out[256] = \t 38\n",
      "tensor_out[257] = \t 0\n",
      "tensor_out[258] = \t 60\n",
      "tensor_out[259] = \t 0\n",
      "tensor_out[260] = \t 0\n",
      "tensor_out[261] = \t 12\n",
      "tensor_out[262] = \t 1\n",
      "tensor_out[263] = \t 10\n",
      "tensor_out[264] = \t 0\n",
      "tensor_out[265] = \t 55\n",
      "tensor_out[266] = \t 14\n",
      "tensor_out[267] = \t 0\n",
      "tensor_out[268] = \t 18\n",
      "tensor_out[269] = \t 15\n",
      "tensor_out[270] = \t 0\n",
      "tensor_out[271] = \t 0\n",
      "tensor_out[272] = \t 29\n",
      "tensor_out[273] = \t 0\n",
      "tensor_out[274] = \t 0\n",
      "tensor_out[275] = \t 34\n",
      "tensor_out[276] = \t 0\n",
      "tensor_out[277] = \t 24\n",
      "tensor_out[278] = \t 15\n",
      "tensor_out[279] = \t 49\n",
      "tensor_out[280] = \t 0\n",
      "tensor_out[281] = \t 50\n",
      "tensor_out[282] = \t 0\n",
      "tensor_out[283] = \t 0\n",
      "tensor_out[284] = \t 36\n",
      "tensor_out[285] = \t 31\n",
      "tensor_out[286] = \t 54\n",
      "tensor_out[287] = \t 18\n",
      "tensor_out[288] = \t 35\n",
      "tensor_out[289] = \t 0\n",
      "tensor_out[290] = \t 29\n",
      "tensor_out[291] = \t 6\n",
      "tensor_out[292] = \t 0\n",
      "tensor_out[293] = \t 0\n",
      "tensor_out[294] = \t 30\n",
      "tensor_out[295] = \t 71\n",
      "tensor_out[296] = \t 27\n",
      "tensor_out[297] = \t 71\n",
      "tensor_out[298] = \t 46\n",
      "tensor_out[299] = \t 40\n",
      "tensor_out[300] = \t 10\n",
      "tensor_out[301] = \t 0\n",
      "tensor_out[302] = \t 23\n",
      "tensor_out[303] = \t 70\n",
      "tensor_out[304] = \t 0\n",
      "tensor_out[305] = \t 23\n",
      "tensor_out[306] = \t 0\n",
      "tensor_out[307] = \t 55\n",
      "tensor_out[308] = \t 40\n",
      "tensor_out[309] = \t 0\n",
      "tensor_out[310] = \t 7\n",
      "tensor_out[311] = \t 23\n",
      "tensor_out[312] = \t 0\n",
      "tensor_out[313] = \t 0\n",
      "tensor_out[314] = \t 93\n",
      "tensor_out[315] = \t 0\n",
      "tensor_out[316] = \t 5\n",
      "tensor_out[317] = \t 3\n",
      "tensor_out[318] = \t 0\n",
      "tensor_out[319] = \t 71\n",
      "tensor_out[320] = \t 0\n",
      "tensor_out[321] = \t 37\n",
      "tensor_out[322] = \t 42\n",
      "tensor_out[323] = \t 56\n",
      "tensor_out[324] = \t 0\n",
      "tensor_out[325] = \t 9\n",
      "tensor_out[326] = \t 0\n",
      "tensor_out[327] = \t 0\n",
      "tensor_out[328] = \t 4\n",
      "tensor_out[329] = \t 72\n",
      "tensor_out[330] = \t 2\n",
      "tensor_out[331] = \t 1\n",
      "tensor_out[332] = \t 0\n",
      "tensor_out[333] = \t 6\n",
      "tensor_out[334] = \t 0\n",
      "tensor_out[335] = \t 70\n",
      "tensor_out[336] = \t 0\n",
      "tensor_out[337] = \t 0\n",
      "tensor_out[338] = \t 0\n",
      "tensor_out[339] = \t 66\n",
      "tensor_out[340] = \t 0\n",
      "tensor_out[341] = \t 9\n",
      "tensor_out[342] = \t 0\n",
      "tensor_out[343] = \t 78\n",
      "tensor_out[344] = \t 86\n",
      "tensor_out[345] = \t 2\n",
      "tensor_out[346] = \t 0\n",
      "tensor_out[347] = \t 74\n",
      "tensor_out[348] = \t 0\n",
      "tensor_out[349] = \t 51\n",
      "tensor_out[350] = \t 0\n",
      "tensor_out[351] = \t 1\n",
      "tensor_out[352] = \t 0\n",
      "tensor_out[353] = \t 81\n",
      "tensor_out[354] = \t 0\n",
      "tensor_out[355] = \t 91\n",
      "tensor_out[356] = \t 13\n",
      "tensor_out[357] = \t 125\n",
      "tensor_out[358] = \t 10\n",
      "tensor_out[359] = \t 95\n",
      "tensor_out[360] = \t 4\n",
      "tensor_out[361] = \t 0\n",
      "tensor_out[362] = \t 0\n",
      "tensor_out[363] = \t 0\n",
      "tensor_out[364] = \t 32\n",
      "tensor_out[365] = \t 44\n",
      "tensor_out[366] = \t 0\n",
      "tensor_out[367] = \t 0\n",
      "tensor_out[368] = \t 169\n",
      "tensor_out[369] = \t 18\n",
      "tensor_out[370] = \t 58\n",
      "tensor_out[371] = \t 83\n",
      "tensor_out[372] = \t 6\n",
      "tensor_out[373] = \t 88\n",
      "tensor_out[374] = \t 65\n",
      "tensor_out[375] = \t 0\n",
      "tensor_out[376] = \t 0\n",
      "tensor_out[377] = \t 19\n",
      "tensor_out[378] = \t 17\n",
      "tensor_out[379] = \t 0\n",
      "tensor_out[380] = \t 173\n",
      "tensor_out[381] = \t 0\n",
      "tensor_out[382] = \t 82\n",
      "tensor_out[383] = \t 0\n",
      "tensor_out[384] = \t 13\n",
      "tensor_out[385] = \t 0\n",
      "tensor_out[386] = \t 1\n",
      "tensor_out[387] = \t 3\n",
      "tensor_out[388] = \t 72\n",
      "tensor_out[389] = \t 21\n",
      "tensor_out[390] = \t 0\n",
      "tensor_out[391] = \t 160\n",
      "tensor_out[392] = \t 43\n",
      "tensor_out[393] = \t 43\n",
      "tensor_out[394] = \t 0\n",
      "tensor_out[395] = \t 187\n",
      "tensor_out[396] = \t 0\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 0\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 0\n",
      "tensor_out[401] = \t 36\n",
      "tensor_out[402] = \t 37\n",
      "tensor_out[403] = \t 0\n",
      "tensor_out[404] = \t 14\n",
      "tensor_out[405] = \t 0\n",
      "tensor_out[406] = \t 0\n",
      "tensor_out[407] = \t 53\n",
      "tensor_out[408] = \t 0\n",
      "tensor_out[409] = \t 41\n",
      "tensor_out[410] = \t 0\n",
      "tensor_out[411] = \t 0\n",
      "tensor_out[412] = \t 0\n",
      "tensor_out[413] = \t 21\n",
      "tensor_out[414] = \t 6\n",
      "tensor_out[415] = \t 33\n",
      "tensor_out[416] = \t 0\n",
      "tensor_out[417] = \t 10\n",
      "tensor_out[418] = \t 72\n",
      "tensor_out[419] = \t 0\n",
      "tensor_out[420] = \t 60\n",
      "tensor_out[421] = \t 0\n",
      "tensor_out[422] = \t 0\n",
      "tensor_out[423] = \t 44\n",
      "tensor_out[424] = \t 34\n",
      "tensor_out[425] = \t 79\n",
      "tensor_out[426] = \t 0\n",
      "tensor_out[427] = \t 46\n",
      "tensor_out[428] = \t 32\n",
      "tensor_out[429] = \t 31\n",
      "tensor_out[430] = \t 0\n",
      "tensor_out[431] = \t 60\n",
      "tensor_out[432] = \t 8\n",
      "tensor_out[433] = \t 0\n",
      "tensor_out[434] = \t 105\n",
      "tensor_out[435] = \t 16\n",
      "tensor_out[436] = \t 44\n",
      "tensor_out[437] = \t 0\n",
      "tensor_out[438] = \t 0\n",
      "tensor_out[439] = \t 0\n",
      "tensor_out[440] = \t 13\n",
      "tensor_out[441] = \t 86\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 28\n",
      "tensor_out[444] = \t 2\n",
      "tensor_out[445] = \t 0\n",
      "tensor_out[446] = \t 93\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 47\n",
      "tensor_out[449] = \t 20\n",
      "tensor_out[450] = \t 39\n",
      "tensor_out[451] = \t 0\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 14\n",
      "tensor_out[454] = \t 0\n",
      "tensor_out[455] = \t 0\n",
      "tensor_out[456] = \t 0\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 0\n",
      "tensor_out[459] = \t 28\n",
      "tensor_out[460] = \t 24\n",
      "tensor_out[461] = \t 0\n",
      "tensor_out[462] = \t 41\n",
      "tensor_out[463] = \t 0\n",
      "tensor_out[464] = \t 0\n",
      "tensor_out[465] = \t 57\n",
      "tensor_out[466] = \t 34\n",
      "tensor_out[467] = \t 104\n",
      "tensor_out[468] = \t 39\n",
      "tensor_out[469] = \t 0\n",
      "tensor_out[470] = \t 0\n",
      "tensor_out[471] = \t 54\n",
      "tensor_out[472] = \t 78\n",
      "tensor_out[473] = \t 0\n",
      "tensor_out[474] = \t 81\n",
      "tensor_out[475] = \t 0\n",
      "tensor_out[476] = \t 93\n",
      "tensor_out[477] = \t 0\n",
      "tensor_out[478] = \t 21\n",
      "tensor_out[479] = \t 0\n",
      "tensor_out[480] = \t 0\n",
      "tensor_out[481] = \t 0\n",
      "tensor_out[482] = \t 34\n",
      "tensor_out[483] = \t 0\n",
      "tensor_out[484] = \t 11\n",
      "tensor_out[485] = \t 95\n",
      "tensor_out[486] = \t 36\n",
      "tensor_out[487] = \t 0\n",
      "tensor_out[488] = \t 12\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 0\n",
      "tensor_out[491] = \t 21\n",
      "tensor_out[492] = \t 0\n",
      "tensor_out[493] = \t 0\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 3\n",
      "tensor_out[496] = \t 25\n",
      "tensor_out[497] = \t 17\n",
      "tensor_out[498] = \t 0\n",
      "tensor_out[499] = \t 0\n",
      "tensor_out[500] = \t 25\n",
      "tensor_out[501] = \t 17\n",
      "tensor_out[502] = \t 0\n",
      "tensor_out[503] = \t 0\n",
      "tensor_out[504] = \t 0\n",
      "tensor_out[505] = \t 62\n",
      "tensor_out[506] = \t 0\n",
      "tensor_out[507] = \t 18\n",
      "tensor_out[508] = \t 13\n",
      "tensor_out[509] = \t 10\n",
      "tensor_out[510] = \t 6\n",
      "tensor_out[511] = \t 32\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer25 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0\n",
      "tensor_out[1] = \t 0\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 0\n",
      "tensor_out[5] = \t 0\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 29\n",
      "tensor_out[9] = \t 0\n",
      "tensor_out[10] = \t 85\n",
      "tensor_out[11] = \t 0\n",
      "tensor_out[12] = \t 0\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 49\n",
      "tensor_out[15] = \t 140\n",
      "tensor_out[16] = \t 67\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 255\n",
      "tensor_out[19] = \t 0\n",
      "tensor_out[20] = \t 0\n",
      "tensor_out[21] = \t 0\n",
      "tensor_out[22] = \t 0\n",
      "tensor_out[23] = \t 104\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 83\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 0\n",
      "tensor_out[28] = \t 13\n",
      "tensor_out[29] = \t 64\n",
      "tensor_out[30] = \t 0\n",
      "tensor_out[31] = \t 176\n",
      "tensor_out[32] = \t 80\n",
      "tensor_out[33] = \t 0\n",
      "tensor_out[34] = \t 47\n",
      "tensor_out[35] = \t 0\n",
      "tensor_out[36] = \t 0\n",
      "tensor_out[37] = \t 0\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 0\n",
      "tensor_out[40] = \t 98\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 0\n",
      "tensor_out[45] = \t 0\n",
      "tensor_out[46] = \t 0\n",
      "tensor_out[47] = \t 0\n",
      "tensor_out[48] = \t 35\n",
      "tensor_out[49] = \t 79\n",
      "tensor_out[50] = \t 0\n",
      "tensor_out[51] = \t 0\n",
      "tensor_out[52] = \t 0\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 0\n",
      "tensor_out[55] = \t 1\n",
      "tensor_out[56] = \t 0\n",
      "tensor_out[57] = \t 0\n",
      "tensor_out[58] = \t 118\n",
      "tensor_out[59] = \t 105\n",
      "tensor_out[60] = \t 0\n",
      "tensor_out[61] = \t 0\n",
      "tensor_out[62] = \t 0\n",
      "tensor_out[63] = \t 0\n",
      "tensor_out[64] = \t 134\n",
      "tensor_out[65] = \t 33\n",
      "tensor_out[66] = \t 7\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 79\n",
      "tensor_out[72] = \t 0\n",
      "tensor_out[73] = \t 0\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 49\n",
      "tensor_out[76] = \t 0\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 18\n",
      "tensor_out[79] = \t 116\n",
      "tensor_out[80] = \t 143\n",
      "tensor_out[81] = \t 0\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 75\n",
      "tensor_out[84] = \t 5\n",
      "tensor_out[85] = \t 0\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 159\n",
      "tensor_out[88] = \t 57\n",
      "tensor_out[89] = \t 0\n",
      "tensor_out[90] = \t 0\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 64\n",
      "tensor_out[93] = \t 97\n",
      "tensor_out[94] = \t 0\n",
      "tensor_out[95] = \t 127\n",
      "tensor_out[96] = \t 0\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 0\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 0\n",
      "tensor_out[103] = \t 30\n",
      "tensor_out[104] = \t 0\n",
      "tensor_out[105] = \t 0\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 0\n",
      "tensor_out[108] = \t 92\n",
      "tensor_out[109] = \t 43\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 0\n",
      "tensor_out[112] = \t 0\n",
      "tensor_out[113] = \t 0\n",
      "tensor_out[114] = \t 0\n",
      "tensor_out[115] = \t 0\n",
      "tensor_out[116] = \t 32\n",
      "tensor_out[117] = \t 0\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 0\n",
      "tensor_out[120] = \t 0\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 124\n",
      "tensor_out[123] = \t 81\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 168\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 108\n",
      "tensor_out[128] = \t 65\n",
      "tensor_out[129] = \t 0\n",
      "tensor_out[130] = \t 255\n",
      "tensor_out[131] = \t 61\n",
      "tensor_out[132] = \t 174\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 0\n",
      "tensor_out[135] = \t 53\n",
      "tensor_out[136] = \t 0\n",
      "tensor_out[137] = \t 3\n",
      "tensor_out[138] = \t 146\n",
      "tensor_out[139] = \t 0\n",
      "tensor_out[140] = \t 13\n",
      "tensor_out[141] = \t 114\n",
      "tensor_out[142] = \t 62\n",
      "tensor_out[143] = \t 0\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 82\n",
      "tensor_out[146] = \t 0\n",
      "tensor_out[147] = \t 0\n",
      "tensor_out[148] = \t 0\n",
      "tensor_out[149] = \t 0\n",
      "tensor_out[150] = \t 35\n",
      "tensor_out[151] = \t 0\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 147\n",
      "tensor_out[154] = \t 5\n",
      "tensor_out[155] = \t 6\n",
      "tensor_out[156] = \t 129\n",
      "tensor_out[157] = \t 0\n",
      "tensor_out[158] = \t 86\n",
      "tensor_out[159] = \t 0\n",
      "tensor_out[160] = \t 123\n",
      "tensor_out[161] = \t 0\n",
      "tensor_out[162] = \t 0\n",
      "tensor_out[163] = \t 157\n",
      "tensor_out[164] = \t 43\n",
      "tensor_out[165] = \t 0\n",
      "tensor_out[166] = \t 53\n",
      "tensor_out[167] = \t 16\n",
      "tensor_out[168] = \t 45\n",
      "tensor_out[169] = \t 48\n",
      "tensor_out[170] = \t 0\n",
      "tensor_out[171] = \t 0\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 0\n",
      "tensor_out[174] = \t 0\n",
      "tensor_out[175] = \t 0\n",
      "tensor_out[176] = \t 0\n",
      "tensor_out[177] = \t 0\n",
      "tensor_out[178] = \t 0\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 199\n",
      "tensor_out[181] = \t 64\n",
      "tensor_out[182] = \t 0\n",
      "tensor_out[183] = \t 4\n",
      "tensor_out[184] = \t 0\n",
      "tensor_out[185] = \t 18\n",
      "tensor_out[186] = \t 10\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 0\n",
      "tensor_out[189] = \t 0\n",
      "tensor_out[190] = \t 0\n",
      "tensor_out[191] = \t 47\n",
      "tensor_out[192] = \t 82\n",
      "tensor_out[193] = \t 0\n",
      "tensor_out[194] = \t 0\n",
      "tensor_out[195] = \t 86\n",
      "tensor_out[196] = \t 0\n",
      "tensor_out[197] = \t 0\n",
      "tensor_out[198] = \t 0\n",
      "tensor_out[199] = \t 0\n",
      "tensor_out[200] = \t 23\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 10\n",
      "tensor_out[203] = \t 114\n",
      "tensor_out[204] = \t 0\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 26\n",
      "tensor_out[207] = \t 0\n",
      "tensor_out[208] = \t 0\n",
      "tensor_out[209] = \t 15\n",
      "tensor_out[210] = \t 0\n",
      "tensor_out[211] = \t 0\n",
      "tensor_out[212] = \t 0\n",
      "tensor_out[213] = \t 41\n",
      "tensor_out[214] = \t 8\n",
      "tensor_out[215] = \t 0\n",
      "tensor_out[216] = \t 101\n",
      "tensor_out[217] = \t 0\n",
      "tensor_out[218] = \t 48\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 55\n",
      "tensor_out[221] = \t 0\n",
      "tensor_out[222] = \t 0\n",
      "tensor_out[223] = \t 0\n",
      "tensor_out[224] = \t 180\n",
      "tensor_out[225] = \t 128\n",
      "tensor_out[226] = \t 0\n",
      "tensor_out[227] = \t 0\n",
      "tensor_out[228] = \t 93\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 0\n",
      "tensor_out[231] = \t 31\n",
      "tensor_out[232] = \t 0\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 49\n",
      "tensor_out[235] = \t 0\n",
      "tensor_out[236] = \t 0\n",
      "tensor_out[237] = \t 0\n",
      "tensor_out[238] = \t 28\n",
      "tensor_out[239] = \t 130\n",
      "tensor_out[240] = \t 46\n",
      "tensor_out[241] = \t 0\n",
      "tensor_out[242] = \t 0\n",
      "tensor_out[243] = \t 0\n",
      "tensor_out[244] = \t 0\n",
      "tensor_out[245] = \t 84\n",
      "tensor_out[246] = \t 176\n",
      "tensor_out[247] = \t 0\n",
      "tensor_out[248] = \t 0\n",
      "tensor_out[249] = \t 0\n",
      "tensor_out[250] = \t 0\n",
      "tensor_out[251] = \t 144\n",
      "tensor_out[252] = \t 122\n",
      "tensor_out[253] = \t 148\n",
      "tensor_out[254] = \t 82\n",
      "tensor_out[255] = \t 0\n",
      "tensor_out[256] = \t 148\n",
      "tensor_out[257] = \t 65\n",
      "tensor_out[258] = \t 55\n",
      "tensor_out[259] = \t 0\n",
      "tensor_out[260] = \t 0\n",
      "tensor_out[261] = \t 0\n",
      "tensor_out[262] = \t 0\n",
      "tensor_out[263] = \t 0\n",
      "tensor_out[264] = \t 0\n",
      "tensor_out[265] = \t 0\n",
      "tensor_out[266] = \t 115\n",
      "tensor_out[267] = \t 66\n",
      "tensor_out[268] = \t 0\n",
      "tensor_out[269] = \t 130\n",
      "tensor_out[270] = \t 0\n",
      "tensor_out[271] = \t 0\n",
      "tensor_out[272] = \t 0\n",
      "tensor_out[273] = \t 45\n",
      "tensor_out[274] = \t 0\n",
      "tensor_out[275] = \t 0\n",
      "tensor_out[276] = \t 60\n",
      "tensor_out[277] = \t 0\n",
      "tensor_out[278] = \t 0\n",
      "tensor_out[279] = \t 0\n",
      "tensor_out[280] = \t 0\n",
      "tensor_out[281] = \t 0\n",
      "tensor_out[282] = \t 0\n",
      "tensor_out[283] = \t 86\n",
      "tensor_out[284] = \t 108\n",
      "tensor_out[285] = \t 109\n",
      "tensor_out[286] = \t 0\n",
      "tensor_out[287] = \t 0\n",
      "tensor_out[288] = \t 0\n",
      "tensor_out[289] = \t 7\n",
      "tensor_out[290] = \t 0\n",
      "tensor_out[291] = \t 0\n",
      "tensor_out[292] = \t 0\n",
      "tensor_out[293] = \t 0\n",
      "tensor_out[294] = \t 0\n",
      "tensor_out[295] = \t 61\n",
      "tensor_out[296] = \t 0\n",
      "tensor_out[297] = \t 45\n",
      "tensor_out[298] = \t 85\n",
      "tensor_out[299] = \t 1\n",
      "tensor_out[300] = \t 11\n",
      "tensor_out[301] = \t 39\n",
      "tensor_out[302] = \t 0\n",
      "tensor_out[303] = \t 34\n",
      "tensor_out[304] = \t 48\n",
      "tensor_out[305] = \t 0\n",
      "tensor_out[306] = \t 0\n",
      "tensor_out[307] = \t 236\n",
      "tensor_out[308] = \t 76\n",
      "tensor_out[309] = \t 0\n",
      "tensor_out[310] = \t 90\n",
      "tensor_out[311] = \t 0\n",
      "tensor_out[312] = \t 0\n",
      "tensor_out[313] = \t 39\n",
      "tensor_out[314] = \t 0\n",
      "tensor_out[315] = \t 59\n",
      "tensor_out[316] = \t 83\n",
      "tensor_out[317] = \t 0\n",
      "tensor_out[318] = \t 0\n",
      "tensor_out[319] = \t 0\n",
      "tensor_out[320] = \t 0\n",
      "tensor_out[321] = \t 198\n",
      "tensor_out[322] = \t 0\n",
      "tensor_out[323] = \t 20\n",
      "tensor_out[324] = \t 0\n",
      "tensor_out[325] = \t 0\n",
      "tensor_out[326] = \t 0\n",
      "tensor_out[327] = \t 0\n",
      "tensor_out[328] = \t 78\n",
      "tensor_out[329] = \t 0\n",
      "tensor_out[330] = \t 14\n",
      "tensor_out[331] = \t 152\n",
      "tensor_out[332] = \t 0\n",
      "tensor_out[333] = \t 0\n",
      "tensor_out[334] = \t 0\n",
      "tensor_out[335] = \t 0\n",
      "tensor_out[336] = \t 85\n",
      "tensor_out[337] = \t 94\n",
      "tensor_out[338] = \t 0\n",
      "tensor_out[339] = \t 0\n",
      "tensor_out[340] = \t 0\n",
      "tensor_out[341] = \t 35\n",
      "tensor_out[342] = \t 112\n",
      "tensor_out[343] = \t 0\n",
      "tensor_out[344] = \t 0\n",
      "tensor_out[345] = \t 0\n",
      "tensor_out[346] = \t 0\n",
      "tensor_out[347] = \t 5\n",
      "tensor_out[348] = \t 0\n",
      "tensor_out[349] = \t 43\n",
      "tensor_out[350] = \t 0\n",
      "tensor_out[351] = \t 0\n",
      "tensor_out[352] = \t 0\n",
      "tensor_out[353] = \t 0\n",
      "tensor_out[354] = \t 0\n",
      "tensor_out[355] = \t 0\n",
      "tensor_out[356] = \t 0\n",
      "tensor_out[357] = \t 0\n",
      "tensor_out[358] = \t 0\n",
      "tensor_out[359] = \t 141\n",
      "tensor_out[360] = \t 0\n",
      "tensor_out[361] = \t 0\n",
      "tensor_out[362] = \t 0\n",
      "tensor_out[363] = \t 0\n",
      "tensor_out[364] = \t 0\n",
      "tensor_out[365] = \t 0\n",
      "tensor_out[366] = \t 0\n",
      "tensor_out[367] = \t 0\n",
      "tensor_out[368] = \t 16\n",
      "tensor_out[369] = \t 0\n",
      "tensor_out[370] = \t 0\n",
      "tensor_out[371] = \t 0\n",
      "tensor_out[372] = \t 84\n",
      "tensor_out[373] = \t 0\n",
      "tensor_out[374] = \t 0\n",
      "tensor_out[375] = \t 52\n",
      "tensor_out[376] = \t 49\n",
      "tensor_out[377] = \t 0\n",
      "tensor_out[378] = \t 84\n",
      "tensor_out[379] = \t 120\n",
      "tensor_out[380] = \t 0\n",
      "tensor_out[381] = \t 0\n",
      "tensor_out[382] = \t 0\n",
      "tensor_out[383] = \t 0\n",
      "tensor_out[384] = \t 0\n",
      "tensor_out[385] = \t 154\n",
      "tensor_out[386] = \t 0\n",
      "tensor_out[387] = \t 76\n",
      "tensor_out[388] = \t 0\n",
      "tensor_out[389] = \t 0\n",
      "tensor_out[390] = \t 0\n",
      "tensor_out[391] = \t 159\n",
      "tensor_out[392] = \t 0\n",
      "tensor_out[393] = \t 0\n",
      "tensor_out[394] = \t 37\n",
      "tensor_out[395] = \t 152\n",
      "tensor_out[396] = \t 0\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 0\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 0\n",
      "tensor_out[401] = \t 0\n",
      "tensor_out[402] = \t 0\n",
      "tensor_out[403] = \t 0\n",
      "tensor_out[404] = \t 102\n",
      "tensor_out[405] = \t 0\n",
      "tensor_out[406] = \t 135\n",
      "tensor_out[407] = \t 11\n",
      "tensor_out[408] = \t 0\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 135\n",
      "tensor_out[411] = \t 0\n",
      "tensor_out[412] = \t 0\n",
      "tensor_out[413] = \t 11\n",
      "tensor_out[414] = \t 18\n",
      "tensor_out[415] = \t 0\n",
      "tensor_out[416] = \t 94\n",
      "tensor_out[417] = \t 35\n",
      "tensor_out[418] = \t 23\n",
      "tensor_out[419] = \t 58\n",
      "tensor_out[420] = \t 0\n",
      "tensor_out[421] = \t 70\n",
      "tensor_out[422] = \t 144\n",
      "tensor_out[423] = \t 0\n",
      "tensor_out[424] = \t 0\n",
      "tensor_out[425] = \t 0\n",
      "tensor_out[426] = \t 121\n",
      "tensor_out[427] = \t 112\n",
      "tensor_out[428] = \t 0\n",
      "tensor_out[429] = \t 0\n",
      "tensor_out[430] = \t 28\n",
      "tensor_out[431] = \t 36\n",
      "tensor_out[432] = \t 0\n",
      "tensor_out[433] = \t 59\n",
      "tensor_out[434] = \t 0\n",
      "tensor_out[435] = \t 0\n",
      "tensor_out[436] = \t 6\n",
      "tensor_out[437] = \t 45\n",
      "tensor_out[438] = \t 0\n",
      "tensor_out[439] = \t 20\n",
      "tensor_out[440] = \t 0\n",
      "tensor_out[441] = \t 0\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 0\n",
      "tensor_out[444] = \t 0\n",
      "tensor_out[445] = \t 0\n",
      "tensor_out[446] = \t 27\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 0\n",
      "tensor_out[449] = \t 0\n",
      "tensor_out[450] = \t 0\n",
      "tensor_out[451] = \t 0\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 0\n",
      "tensor_out[454] = \t 0\n",
      "tensor_out[455] = \t 0\n",
      "tensor_out[456] = \t 0\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 0\n",
      "tensor_out[459] = \t 0\n",
      "tensor_out[460] = \t 0\n",
      "tensor_out[461] = \t 14\n",
      "tensor_out[462] = \t 18\n",
      "tensor_out[463] = \t 0\n",
      "tensor_out[464] = \t 8\n",
      "tensor_out[465] = \t 0\n",
      "tensor_out[466] = \t 24\n",
      "tensor_out[467] = \t 0\n",
      "tensor_out[468] = \t 124\n",
      "tensor_out[469] = \t 0\n",
      "tensor_out[470] = \t 0\n",
      "tensor_out[471] = \t 0\n",
      "tensor_out[472] = \t 0\n",
      "tensor_out[473] = \t 25\n",
      "tensor_out[474] = \t 0\n",
      "tensor_out[475] = \t 126\n",
      "tensor_out[476] = \t 38\n",
      "tensor_out[477] = \t 0\n",
      "tensor_out[478] = \t 9\n",
      "tensor_out[479] = \t 9\n",
      "tensor_out[480] = \t 0\n",
      "tensor_out[481] = \t 92\n",
      "tensor_out[482] = \t 176\n",
      "tensor_out[483] = \t 0\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 8\n",
      "tensor_out[486] = \t 155\n",
      "tensor_out[487] = \t 0\n",
      "tensor_out[488] = \t 79\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 0\n",
      "tensor_out[491] = \t 211\n",
      "tensor_out[492] = \t 0\n",
      "tensor_out[493] = \t 147\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 37\n",
      "tensor_out[496] = \t 0\n",
      "tensor_out[497] = \t 136\n",
      "tensor_out[498] = \t 208\n",
      "tensor_out[499] = \t 0\n",
      "tensor_out[500] = \t 0\n",
      "tensor_out[501] = \t 0\n",
      "tensor_out[502] = \t 0\n",
      "tensor_out[503] = \t 78\n",
      "tensor_out[504] = \t 142\n",
      "tensor_out[505] = \t 69\n",
      "tensor_out[506] = \t 0\n",
      "tensor_out[507] = \t 0\n",
      "tensor_out[508] = \t 0\n",
      "tensor_out[509] = \t 0\n",
      "tensor_out[510] = \t 53\n",
      "tensor_out[511] = \t 0\n",
      "tensor_out[512] = \t 0\n",
      "tensor_out[513] = \t 19\n",
      "tensor_out[514] = \t 0\n",
      "tensor_out[515] = \t 74\n",
      "tensor_out[516] = \t 0\n",
      "tensor_out[517] = \t 95\n",
      "tensor_out[518] = \t 0\n",
      "tensor_out[519] = \t 68\n",
      "tensor_out[520] = \t 72\n",
      "tensor_out[521] = \t 19\n",
      "tensor_out[522] = \t 0\n",
      "tensor_out[523] = \t 0\n",
      "tensor_out[524] = \t 221\n",
      "tensor_out[525] = \t 181\n",
      "tensor_out[526] = \t 0\n",
      "tensor_out[527] = \t 0\n",
      "tensor_out[528] = \t 92\n",
      "tensor_out[529] = \t 114\n",
      "tensor_out[530] = \t 0\n",
      "tensor_out[531] = \t 0\n",
      "tensor_out[532] = \t 125\n",
      "tensor_out[533] = \t 0\n",
      "tensor_out[534] = \t 17\n",
      "tensor_out[535] = \t 0\n",
      "tensor_out[536] = \t 0\n",
      "tensor_out[537] = \t 0\n",
      "tensor_out[538] = \t 0\n",
      "tensor_out[539] = \t 87\n",
      "tensor_out[540] = \t 61\n",
      "tensor_out[541] = \t 0\n",
      "tensor_out[542] = \t 0\n",
      "tensor_out[543] = \t 0\n",
      "tensor_out[544] = \t 0\n",
      "tensor_out[545] = \t 0\n",
      "tensor_out[546] = \t 0\n",
      "tensor_out[547] = \t 168\n",
      "tensor_out[548] = \t 193\n",
      "tensor_out[549] = \t 0\n",
      "tensor_out[550] = \t 0\n",
      "tensor_out[551] = \t 110\n",
      "tensor_out[552] = \t 0\n",
      "tensor_out[553] = \t 70\n",
      "tensor_out[554] = \t 0\n",
      "tensor_out[555] = \t 0\n",
      "tensor_out[556] = \t 0\n",
      "tensor_out[557] = \t 0\n",
      "tensor_out[558] = \t 0\n",
      "tensor_out[559] = \t 0\n",
      "tensor_out[560] = \t 11\n",
      "tensor_out[561] = \t 0\n",
      "tensor_out[562] = \t 0\n",
      "tensor_out[563] = \t 0\n",
      "tensor_out[564] = \t 127\n",
      "tensor_out[565] = \t 0\n",
      "tensor_out[566] = \t 77\n",
      "tensor_out[567] = \t 0\n",
      "tensor_out[568] = \t 0\n",
      "tensor_out[569] = \t 0\n",
      "tensor_out[570] = \t 0\n",
      "tensor_out[571] = \t 0\n",
      "tensor_out[572] = \t 0\n",
      "tensor_out[573] = \t 0\n",
      "tensor_out[574] = \t 0\n",
      "tensor_out[575] = \t 9\n",
      "tensor_out[576] = \t 0\n",
      "tensor_out[577] = \t 11\n",
      "tensor_out[578] = \t 73\n",
      "tensor_out[579] = \t 0\n",
      "tensor_out[580] = \t 0\n",
      "tensor_out[581] = \t 66\n",
      "tensor_out[582] = \t 0\n",
      "tensor_out[583] = \t 55\n",
      "tensor_out[584] = \t 0\n",
      "tensor_out[585] = \t 0\n",
      "tensor_out[586] = \t 0\n",
      "tensor_out[587] = \t 0\n",
      "tensor_out[588] = \t 0\n",
      "tensor_out[589] = \t 0\n",
      "tensor_out[590] = \t 0\n",
      "tensor_out[591] = \t 255\n",
      "tensor_out[592] = \t 0\n",
      "tensor_out[593] = \t 176\n",
      "tensor_out[594] = \t 42\n",
      "tensor_out[595] = \t 0\n",
      "tensor_out[596] = \t 0\n",
      "tensor_out[597] = \t 0\n",
      "tensor_out[598] = \t 161\n",
      "tensor_out[599] = \t 0\n",
      "tensor_out[600] = \t 79\n",
      "tensor_out[601] = \t 76\n",
      "tensor_out[602] = \t 0\n",
      "tensor_out[603] = \t 0\n",
      "tensor_out[604] = \t 0\n",
      "tensor_out[605] = \t 0\n",
      "tensor_out[606] = \t 56\n",
      "tensor_out[607] = \t 0\n",
      "tensor_out[608] = \t 48\n",
      "tensor_out[609] = \t 7\n",
      "tensor_out[610] = \t 71\n",
      "tensor_out[611] = \t 0\n",
      "tensor_out[612] = \t 45\n",
      "tensor_out[613] = \t 159\n",
      "tensor_out[614] = \t 0\n",
      "tensor_out[615] = \t 0\n",
      "tensor_out[616] = \t 0\n",
      "tensor_out[617] = \t 40\n",
      "tensor_out[618] = \t 47\n",
      "tensor_out[619] = \t 9\n",
      "tensor_out[620] = \t 0\n",
      "tensor_out[621] = \t 0\n",
      "tensor_out[622] = \t 0\n",
      "tensor_out[623] = \t 15\n",
      "tensor_out[624] = \t 0\n",
      "tensor_out[625] = \t 0\n",
      "tensor_out[626] = \t 0\n",
      "tensor_out[627] = \t 0\n",
      "tensor_out[628] = \t 0\n",
      "tensor_out[629] = \t 47\n",
      "tensor_out[630] = \t 45\n",
      "tensor_out[631] = \t 84\n",
      "tensor_out[632] = \t 80\n",
      "tensor_out[633] = \t 0\n",
      "tensor_out[634] = \t 0\n",
      "tensor_out[635] = \t 0\n",
      "tensor_out[636] = \t 203\n",
      "tensor_out[637] = \t 0\n",
      "tensor_out[638] = \t 41\n",
      "tensor_out[639] = \t 0\n",
      "tensor_out[640] = \t 0\n",
      "tensor_out[641] = \t 0\n",
      "tensor_out[642] = \t 0\n",
      "tensor_out[643] = \t 0\n",
      "tensor_out[644] = \t 0\n",
      "tensor_out[645] = \t 0\n",
      "tensor_out[646] = \t 1\n",
      "tensor_out[647] = \t 0\n",
      "tensor_out[648] = \t 0\n",
      "tensor_out[649] = \t 0\n",
      "tensor_out[650] = \t 0\n",
      "tensor_out[651] = \t 0\n",
      "tensor_out[652] = \t 52\n",
      "tensor_out[653] = \t 0\n",
      "tensor_out[654] = \t 0\n",
      "tensor_out[655] = \t 0\n",
      "tensor_out[656] = \t 0\n",
      "tensor_out[657] = \t 116\n",
      "tensor_out[658] = \t 4\n",
      "tensor_out[659] = \t 103\n",
      "tensor_out[660] = \t 0\n",
      "tensor_out[661] = \t 0\n",
      "tensor_out[662] = \t 0\n",
      "tensor_out[663] = \t 0\n",
      "tensor_out[664] = \t 0\n",
      "tensor_out[665] = \t 72\n",
      "tensor_out[666] = \t 0\n",
      "tensor_out[667] = \t 0\n",
      "tensor_out[668] = \t 0\n",
      "tensor_out[669] = \t 0\n",
      "tensor_out[670] = \t 0\n",
      "tensor_out[671] = \t 27\n",
      "tensor_out[672] = \t 0\n",
      "tensor_out[673] = \t 0\n",
      "tensor_out[674] = \t 88\n",
      "tensor_out[675] = \t 51\n",
      "tensor_out[676] = \t 0\n",
      "tensor_out[677] = \t 174\n",
      "tensor_out[678] = \t 145\n",
      "tensor_out[679] = \t 0\n",
      "tensor_out[680] = \t 0\n",
      "tensor_out[681] = \t 0\n",
      "tensor_out[682] = \t 53\n",
      "tensor_out[683] = \t 81\n",
      "tensor_out[684] = \t 0\n",
      "tensor_out[685] = \t 0\n",
      "tensor_out[686] = \t 74\n",
      "tensor_out[687] = \t 0\n",
      "tensor_out[688] = \t 0\n",
      "tensor_out[689] = \t 0\n",
      "tensor_out[690] = \t 0\n",
      "tensor_out[691] = \t 41\n",
      "tensor_out[692] = \t 0\n",
      "tensor_out[693] = \t 0\n",
      "tensor_out[694] = \t 80\n",
      "tensor_out[695] = \t 0\n",
      "tensor_out[696] = \t 22\n",
      "tensor_out[697] = \t 0\n",
      "tensor_out[698] = \t 0\n",
      "tensor_out[699] = \t 0\n",
      "tensor_out[700] = \t 255\n",
      "tensor_out[701] = \t 0\n",
      "tensor_out[702] = \t 0\n",
      "tensor_out[703] = \t 0\n",
      "tensor_out[704] = \t 0\n",
      "tensor_out[705] = \t 138\n",
      "tensor_out[706] = \t 0\n",
      "tensor_out[707] = \t 0\n",
      "tensor_out[708] = \t 0\n",
      "tensor_out[709] = \t 0\n",
      "tensor_out[710] = \t 0\n",
      "tensor_out[711] = \t 73\n",
      "tensor_out[712] = \t 169\n",
      "tensor_out[713] = \t 0\n",
      "tensor_out[714] = \t 60\n",
      "tensor_out[715] = \t 116\n",
      "tensor_out[716] = \t 87\n",
      "tensor_out[717] = \t 0\n",
      "tensor_out[718] = \t 14\n",
      "tensor_out[719] = \t 0\n",
      "tensor_out[720] = \t 0\n",
      "tensor_out[721] = \t 0\n",
      "tensor_out[722] = \t 85\n",
      "tensor_out[723] = \t 0\n",
      "tensor_out[724] = \t 0\n",
      "tensor_out[725] = \t 0\n",
      "tensor_out[726] = \t 80\n",
      "tensor_out[727] = \t 152\n",
      "tensor_out[728] = \t 20\n",
      "tensor_out[729] = \t 0\n",
      "tensor_out[730] = \t 0\n",
      "tensor_out[731] = \t 0\n",
      "tensor_out[732] = \t 0\n",
      "tensor_out[733] = \t 0\n",
      "tensor_out[734] = \t 18\n",
      "tensor_out[735] = \t 78\n",
      "tensor_out[736] = \t 0\n",
      "tensor_out[737] = \t 183\n",
      "tensor_out[738] = \t 0\n",
      "tensor_out[739] = \t 110\n",
      "tensor_out[740] = \t 30\n",
      "tensor_out[741] = \t 0\n",
      "tensor_out[742] = \t 0\n",
      "tensor_out[743] = \t 0\n",
      "tensor_out[744] = \t 0\n",
      "tensor_out[745] = \t 0\n",
      "tensor_out[746] = \t 0\n",
      "tensor_out[747] = \t 5\n",
      "tensor_out[748] = \t 0\n",
      "tensor_out[749] = \t 143\n",
      "tensor_out[750] = \t 0\n",
      "tensor_out[751] = \t 77\n",
      "tensor_out[752] = \t 0\n",
      "tensor_out[753] = \t 111\n",
      "tensor_out[754] = \t 0\n",
      "tensor_out[755] = \t 22\n",
      "tensor_out[756] = \t 0\n",
      "tensor_out[757] = \t 0\n",
      "tensor_out[758] = \t 111\n",
      "tensor_out[759] = \t 0\n",
      "tensor_out[760] = \t 155\n",
      "tensor_out[761] = \t 57\n",
      "tensor_out[762] = \t 0\n",
      "tensor_out[763] = \t 0\n",
      "tensor_out[764] = \t 0\n",
      "tensor_out[765] = \t 0\n",
      "tensor_out[766] = \t 58\n",
      "tensor_out[767] = \t 20\n",
      "tensor_out[768] = \t 21\n",
      "tensor_out[769] = \t 0\n",
      "tensor_out[770] = \t 0\n",
      "tensor_out[771] = \t 0\n",
      "tensor_out[772] = \t 6\n",
      "tensor_out[773] = \t 44\n",
      "tensor_out[774] = \t 0\n",
      "tensor_out[775] = \t 0\n",
      "tensor_out[776] = \t 0\n",
      "tensor_out[777] = \t 0\n",
      "tensor_out[778] = \t 0\n",
      "tensor_out[779] = \t 27\n",
      "tensor_out[780] = \t 0\n",
      "tensor_out[781] = \t 203\n",
      "tensor_out[782] = \t 46\n",
      "tensor_out[783] = \t 0\n",
      "tensor_out[784] = \t 5\n",
      "tensor_out[785] = \t 73\n",
      "tensor_out[786] = \t 0\n",
      "tensor_out[787] = \t 0\n",
      "tensor_out[788] = \t 115\n",
      "tensor_out[789] = \t 10\n",
      "tensor_out[790] = \t 0\n",
      "tensor_out[791] = \t 47\n",
      "tensor_out[792] = \t 254\n",
      "tensor_out[793] = \t 0\n",
      "tensor_out[794] = \t 168\n",
      "tensor_out[795] = \t 0\n",
      "tensor_out[796] = \t 0\n",
      "tensor_out[797] = \t 72\n",
      "tensor_out[798] = \t 0\n",
      "tensor_out[799] = \t 0\n",
      "tensor_out[800] = \t 0\n",
      "tensor_out[801] = \t 0\n",
      "tensor_out[802] = \t 4\n",
      "tensor_out[803] = \t 255\n",
      "tensor_out[804] = \t 0\n",
      "tensor_out[805] = \t 0\n",
      "tensor_out[806] = \t 0\n",
      "tensor_out[807] = \t 0\n",
      "tensor_out[808] = \t 104\n",
      "tensor_out[809] = \t 0\n",
      "tensor_out[810] = \t 110\n",
      "tensor_out[811] = \t 0\n",
      "tensor_out[812] = \t 123\n",
      "tensor_out[813] = \t 0\n",
      "tensor_out[814] = \t 5\n",
      "tensor_out[815] = \t 0\n",
      "tensor_out[816] = \t 0\n",
      "tensor_out[817] = \t 0\n",
      "tensor_out[818] = \t 0\n",
      "tensor_out[819] = \t 77\n",
      "tensor_out[820] = \t 32\n",
      "tensor_out[821] = \t 0\n",
      "tensor_out[822] = \t 0\n",
      "tensor_out[823] = \t 0\n",
      "tensor_out[824] = \t 31\n",
      "tensor_out[825] = \t 0\n",
      "tensor_out[826] = \t 29\n",
      "tensor_out[827] = \t 0\n",
      "tensor_out[828] = \t 0\n",
      "tensor_out[829] = \t 0\n",
      "tensor_out[830] = \t 0\n",
      "tensor_out[831] = \t 17\n",
      "tensor_out[832] = \t 0\n",
      "tensor_out[833] = \t 0\n",
      "tensor_out[834] = \t 152\n",
      "tensor_out[835] = \t 0\n",
      "tensor_out[836] = \t 0\n",
      "tensor_out[837] = \t 0\n",
      "tensor_out[838] = \t 0\n",
      "tensor_out[839] = \t 0\n",
      "tensor_out[840] = \t 0\n",
      "tensor_out[841] = \t 121\n",
      "tensor_out[842] = \t 0\n",
      "tensor_out[843] = \t 62\n",
      "tensor_out[844] = \t 0\n",
      "tensor_out[845] = \t 0\n",
      "tensor_out[846] = \t 255\n",
      "tensor_out[847] = \t 0\n",
      "tensor_out[848] = \t 225\n",
      "tensor_out[849] = \t 91\n",
      "tensor_out[850] = \t 111\n",
      "tensor_out[851] = \t 43\n",
      "tensor_out[852] = \t 0\n",
      "tensor_out[853] = \t 10\n",
      "tensor_out[854] = \t 0\n",
      "tensor_out[855] = \t 0\n",
      "tensor_out[856] = \t 48\n",
      "tensor_out[857] = \t 0\n",
      "tensor_out[858] = \t 51\n",
      "tensor_out[859] = \t 44\n",
      "tensor_out[860] = \t 48\n",
      "tensor_out[861] = \t 44\n",
      "tensor_out[862] = \t 0\n",
      "tensor_out[863] = \t 0\n",
      "tensor_out[864] = \t 39\n",
      "tensor_out[865] = \t 0\n",
      "tensor_out[866] = \t 0\n",
      "tensor_out[867] = \t 0\n",
      "tensor_out[868] = \t 0\n",
      "tensor_out[869] = \t 0\n",
      "tensor_out[870] = \t 0\n",
      "tensor_out[871] = \t 133\n",
      "tensor_out[872] = \t 0\n",
      "tensor_out[873] = \t 0\n",
      "tensor_out[874] = \t 96\n",
      "tensor_out[875] = \t 0\n",
      "tensor_out[876] = \t 0\n",
      "tensor_out[877] = \t 11\n",
      "tensor_out[878] = \t 0\n",
      "tensor_out[879] = \t 0\n",
      "tensor_out[880] = \t 0\n",
      "tensor_out[881] = \t 0\n",
      "tensor_out[882] = \t 0\n",
      "tensor_out[883] = \t 0\n",
      "tensor_out[884] = \t 0\n",
      "tensor_out[885] = \t 0\n",
      "tensor_out[886] = \t 0\n",
      "tensor_out[887] = \t 0\n",
      "tensor_out[888] = \t 55\n",
      "tensor_out[889] = \t 17\n",
      "tensor_out[890] = \t 0\n",
      "tensor_out[891] = \t 0\n",
      "tensor_out[892] = \t 7\n",
      "tensor_out[893] = \t 0\n",
      "tensor_out[894] = \t 0\n",
      "tensor_out[895] = \t 0\n",
      "tensor_out[896] = \t 0\n",
      "tensor_out[897] = \t 0\n",
      "tensor_out[898] = \t 0\n",
      "tensor_out[899] = \t 0\n",
      "tensor_out[900] = \t 0\n",
      "tensor_out[901] = \t 10\n",
      "tensor_out[902] = \t 0\n",
      "tensor_out[903] = \t 44\n",
      "tensor_out[904] = \t 0\n",
      "tensor_out[905] = \t 0\n",
      "tensor_out[906] = \t 2\n",
      "tensor_out[907] = \t 154\n",
      "tensor_out[908] = \t 17\n",
      "tensor_out[909] = \t 41\n",
      "tensor_out[910] = \t 109\n",
      "tensor_out[911] = \t 0\n",
      "tensor_out[912] = \t 5\n",
      "tensor_out[913] = \t 0\n",
      "tensor_out[914] = \t 0\n",
      "tensor_out[915] = \t 0\n",
      "tensor_out[916] = \t 0\n",
      "tensor_out[917] = \t 0\n",
      "tensor_out[918] = \t 178\n",
      "tensor_out[919] = \t 87\n",
      "tensor_out[920] = \t 0\n",
      "tensor_out[921] = \t 45\n",
      "tensor_out[922] = \t 0\n",
      "tensor_out[923] = \t 0\n",
      "tensor_out[924] = \t 0\n",
      "tensor_out[925] = \t 0\n",
      "tensor_out[926] = \t 0\n",
      "tensor_out[927] = \t 93\n",
      "tensor_out[928] = \t 102\n",
      "tensor_out[929] = \t 0\n",
      "tensor_out[930] = \t 78\n",
      "tensor_out[931] = \t 0\n",
      "tensor_out[932] = \t 0\n",
      "tensor_out[933] = \t 0\n",
      "tensor_out[934] = \t 0\n",
      "tensor_out[935] = \t 0\n",
      "tensor_out[936] = \t 0\n",
      "tensor_out[937] = \t 40\n",
      "tensor_out[938] = \t 0\n",
      "tensor_out[939] = \t 0\n",
      "tensor_out[940] = \t 127\n",
      "tensor_out[941] = \t 0\n",
      "tensor_out[942] = \t 215\n",
      "tensor_out[943] = \t 0\n",
      "tensor_out[944] = \t 255\n",
      "tensor_out[945] = \t 83\n",
      "tensor_out[946] = \t 0\n",
      "tensor_out[947] = \t 0\n",
      "tensor_out[948] = \t 0\n",
      "tensor_out[949] = \t 101\n",
      "tensor_out[950] = \t 0\n",
      "tensor_out[951] = \t 146\n",
      "tensor_out[952] = \t 19\n",
      "tensor_out[953] = \t 25\n",
      "tensor_out[954] = \t 44\n",
      "tensor_out[955] = \t 0\n",
      "tensor_out[956] = \t 0\n",
      "tensor_out[957] = \t 0\n",
      "tensor_out[958] = \t 167\n",
      "tensor_out[959] = \t 0\n",
      "tensor_out[960] = \t 0\n",
      "tensor_out[961] = \t 0\n",
      "tensor_out[962] = \t 0\n",
      "tensor_out[963] = \t 0\n",
      "tensor_out[964] = \t 0\n",
      "tensor_out[965] = \t 128\n",
      "tensor_out[966] = \t 0\n",
      "tensor_out[967] = \t 0\n",
      "tensor_out[968] = \t 0\n",
      "tensor_out[969] = \t 48\n",
      "tensor_out[970] = \t 0\n",
      "tensor_out[971] = \t 0\n",
      "tensor_out[972] = \t 0\n",
      "tensor_out[973] = \t 0\n",
      "tensor_out[974] = \t 178\n",
      "tensor_out[975] = \t 0\n",
      "tensor_out[976] = \t 2\n",
      "tensor_out[977] = \t 0\n",
      "tensor_out[978] = \t 21\n",
      "tensor_out[979] = \t 58\n",
      "tensor_out[980] = \t 48\n",
      "tensor_out[981] = \t 0\n",
      "tensor_out[982] = \t 0\n",
      "tensor_out[983] = \t 0\n",
      "tensor_out[984] = \t 151\n",
      "tensor_out[985] = \t 0\n",
      "tensor_out[986] = \t 30\n",
      "tensor_out[987] = \t 0\n",
      "tensor_out[988] = \t 0\n",
      "tensor_out[989] = \t 94\n",
      "tensor_out[990] = \t 50\n",
      "tensor_out[991] = \t 21\n",
      "tensor_out[992] = \t 0\n",
      "tensor_out[993] = \t 0\n",
      "tensor_out[994] = \t 151\n",
      "tensor_out[995] = \t 0\n",
      "tensor_out[996] = \t 57\n",
      "tensor_out[997] = \t 0\n",
      "tensor_out[998] = \t 41\n",
      "tensor_out[999] = \t 0\n",
      "tensor_out[1000] = \t 0\n",
      "tensor_out[1001] = \t 154\n",
      "tensor_out[1002] = \t 0\n",
      "tensor_out[1003] = \t 0\n",
      "tensor_out[1004] = \t 14\n",
      "tensor_out[1005] = \t 0\n",
      "tensor_out[1006] = \t 0\n",
      "tensor_out[1007] = \t 90\n",
      "tensor_out[1008] = \t 102\n",
      "tensor_out[1009] = \t 0\n",
      "tensor_out[1010] = \t 0\n",
      "tensor_out[1011] = \t 0\n",
      "tensor_out[1012] = \t 0\n",
      "tensor_out[1013] = \t 59\n",
      "tensor_out[1014] = \t 0\n",
      "tensor_out[1015] = \t 0\n",
      "tensor_out[1016] = \t 0\n",
      "tensor_out[1017] = \t 0\n",
      "tensor_out[1018] = \t 0\n",
      "tensor_out[1019] = \t 0\n",
      "tensor_out[1020] = \t 0\n",
      "tensor_out[1021] = \t 0\n",
      "tensor_out[1022] = \t 0\n",
      "tensor_out[1023] = \t 144\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer26 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0\n",
      "tensor_out[1] = \t 1\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 0\n",
      "tensor_out[5] = \t 0\n",
      "tensor_out[6] = \t 1\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 41\n",
      "tensor_out[9] = \t 48\n",
      "tensor_out[10] = \t 33\n",
      "tensor_out[11] = \t 0\n",
      "tensor_out[12] = \t 2\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 0\n",
      "tensor_out[15] = \t 9\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 0\n",
      "tensor_out[19] = \t 2\n",
      "tensor_out[20] = \t 2\n",
      "tensor_out[21] = \t 0\n",
      "tensor_out[22] = \t 0\n",
      "tensor_out[23] = \t 6\n",
      "tensor_out[24] = \t 1\n",
      "tensor_out[25] = \t 32\n",
      "tensor_out[26] = \t 1\n",
      "tensor_out[27] = \t 0\n",
      "tensor_out[28] = \t 6\n",
      "tensor_out[29] = \t 37\n",
      "tensor_out[30] = \t 0\n",
      "tensor_out[31] = \t 6\n",
      "tensor_out[32] = \t 29\n",
      "tensor_out[33] = \t 0\n",
      "tensor_out[34] = \t 53\n",
      "tensor_out[35] = \t 0\n",
      "tensor_out[36] = \t 4\n",
      "tensor_out[37] = \t 0\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 1\n",
      "tensor_out[40] = \t 1\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 0\n",
      "tensor_out[45] = \t 45\n",
      "tensor_out[46] = \t 0\n",
      "tensor_out[47] = \t 46\n",
      "tensor_out[48] = \t 0\n",
      "tensor_out[49] = \t 0\n",
      "tensor_out[50] = \t 0\n",
      "tensor_out[51] = \t 52\n",
      "tensor_out[52] = \t 0\n",
      "tensor_out[53] = \t 45\n",
      "tensor_out[54] = \t 0\n",
      "tensor_out[55] = \t 45\n",
      "tensor_out[56] = \t 0\n",
      "tensor_out[57] = \t 0\n",
      "tensor_out[58] = \t 0\n",
      "tensor_out[59] = \t 32\n",
      "tensor_out[60] = \t 0\n",
      "tensor_out[61] = \t 0\n",
      "tensor_out[62] = \t 0\n",
      "tensor_out[63] = \t 1\n",
      "tensor_out[64] = \t 0\n",
      "tensor_out[65] = \t 0\n",
      "tensor_out[66] = \t 42\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 1\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 19\n",
      "tensor_out[72] = \t 0\n",
      "tensor_out[73] = \t 0\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 53\n",
      "tensor_out[76] = \t 0\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 62\n",
      "tensor_out[79] = \t 17\n",
      "tensor_out[80] = \t 0\n",
      "tensor_out[81] = \t 45\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 9\n",
      "tensor_out[84] = \t 0\n",
      "tensor_out[85] = \t 0\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 0\n",
      "tensor_out[88] = \t 32\n",
      "tensor_out[89] = \t 1\n",
      "tensor_out[90] = \t 0\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 7\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 1\n",
      "tensor_out[95] = \t 21\n",
      "tensor_out[96] = \t 1\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 4\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 1\n",
      "tensor_out[103] = \t 0\n",
      "tensor_out[104] = \t 46\n",
      "tensor_out[105] = \t 0\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 0\n",
      "tensor_out[108] = \t 23\n",
      "tensor_out[109] = \t 23\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 48\n",
      "tensor_out[112] = \t 0\n",
      "tensor_out[113] = \t 0\n",
      "tensor_out[114] = \t 0\n",
      "tensor_out[115] = \t 0\n",
      "tensor_out[116] = \t 10\n",
      "tensor_out[117] = \t 0\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 60\n",
      "tensor_out[120] = \t 44\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 0\n",
      "tensor_out[123] = \t 42\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 0\n",
      "tensor_out[126] = \t 43\n",
      "tensor_out[127] = \t 42\n",
      "tensor_out[128] = \t 0\n",
      "tensor_out[129] = \t 0\n",
      "tensor_out[130] = \t 0\n",
      "tensor_out[131] = \t 34\n",
      "tensor_out[132] = \t 0\n",
      "tensor_out[133] = \t 54\n",
      "tensor_out[134] = \t 0\n",
      "tensor_out[135] = \t 19\n",
      "tensor_out[136] = \t 53\n",
      "tensor_out[137] = \t 0\n",
      "tensor_out[138] = \t 0\n",
      "tensor_out[139] = \t 0\n",
      "tensor_out[140] = \t 45\n",
      "tensor_out[141] = \t 0\n",
      "tensor_out[142] = \t 34\n",
      "tensor_out[143] = \t 34\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 18\n",
      "tensor_out[146] = \t 0\n",
      "tensor_out[147] = \t 0\n",
      "tensor_out[148] = \t 0\n",
      "tensor_out[149] = \t 0\n",
      "tensor_out[150] = \t 45\n",
      "tensor_out[151] = \t 0\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 0\n",
      "tensor_out[154] = \t 35\n",
      "tensor_out[155] = \t 5\n",
      "tensor_out[156] = \t 5\n",
      "tensor_out[157] = \t 0\n",
      "tensor_out[158] = \t 28\n",
      "tensor_out[159] = \t 1\n",
      "tensor_out[160] = \t 10\n",
      "tensor_out[161] = \t 0\n",
      "tensor_out[162] = \t 0\n",
      "tensor_out[163] = \t 14\n",
      "tensor_out[164] = \t 54\n",
      "tensor_out[165] = \t 0\n",
      "tensor_out[166] = \t 0\n",
      "tensor_out[167] = \t 66\n",
      "tensor_out[168] = \t 27\n",
      "tensor_out[169] = \t 5\n",
      "tensor_out[170] = \t 0\n",
      "tensor_out[171] = \t 51\n",
      "tensor_out[172] = \t 84\n",
      "tensor_out[173] = \t 31\n",
      "tensor_out[174] = \t 0\n",
      "tensor_out[175] = \t 0\n",
      "tensor_out[176] = \t 0\n",
      "tensor_out[177] = \t 64\n",
      "tensor_out[178] = \t 68\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 0\n",
      "tensor_out[181] = \t 51\n",
      "tensor_out[182] = \t 0\n",
      "tensor_out[183] = \t 43\n",
      "tensor_out[184] = \t 53\n",
      "tensor_out[185] = \t 33\n",
      "tensor_out[186] = \t 48\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 43\n",
      "tensor_out[189] = \t 0\n",
      "tensor_out[190] = \t 0\n",
      "tensor_out[191] = \t 47\n",
      "tensor_out[192] = \t 44\n",
      "tensor_out[193] = \t 0\n",
      "tensor_out[194] = \t 1\n",
      "tensor_out[195] = \t 0\n",
      "tensor_out[196] = \t 0\n",
      "tensor_out[197] = \t 0\n",
      "tensor_out[198] = \t 5\n",
      "tensor_out[199] = \t 1\n",
      "tensor_out[200] = \t 41\n",
      "tensor_out[201] = \t 48\n",
      "tensor_out[202] = \t 54\n",
      "tensor_out[203] = \t 0\n",
      "tensor_out[204] = \t 0\n",
      "tensor_out[205] = \t 51\n",
      "tensor_out[206] = \t 37\n",
      "tensor_out[207] = \t 1\n",
      "tensor_out[208] = \t 0\n",
      "tensor_out[209] = \t 0\n",
      "tensor_out[210] = \t 0\n",
      "tensor_out[211] = \t 1\n",
      "tensor_out[212] = \t 0\n",
      "tensor_out[213] = \t 0\n",
      "tensor_out[214] = \t 0\n",
      "tensor_out[215] = \t 0\n",
      "tensor_out[216] = \t 0\n",
      "tensor_out[217] = \t 1\n",
      "tensor_out[218] = \t 34\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 16\n",
      "tensor_out[221] = \t 0\n",
      "tensor_out[222] = \t 0\n",
      "tensor_out[223] = \t 0\n",
      "tensor_out[224] = \t 7\n",
      "tensor_out[225] = \t 0\n",
      "tensor_out[226] = \t 0\n",
      "tensor_out[227] = \t 0\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 0\n",
      "tensor_out[231] = \t 35\n",
      "tensor_out[232] = \t 0\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 52\n",
      "tensor_out[235] = \t 0\n",
      "tensor_out[236] = \t 0\n",
      "tensor_out[237] = \t 0\n",
      "tensor_out[238] = \t 31\n",
      "tensor_out[239] = \t 39\n",
      "tensor_out[240] = \t 9\n",
      "tensor_out[241] = \t 0\n",
      "tensor_out[242] = \t 1\n",
      "tensor_out[243] = \t 55\n",
      "tensor_out[244] = \t 0\n",
      "tensor_out[245] = \t 105\n",
      "tensor_out[246] = \t 14\n",
      "tensor_out[247] = \t 0\n",
      "tensor_out[248] = \t 0\n",
      "tensor_out[249] = \t 0\n",
      "tensor_out[250] = \t 0\n",
      "tensor_out[251] = \t 0\n",
      "tensor_out[252] = \t 7\n",
      "tensor_out[253] = \t 0\n",
      "tensor_out[254] = \t 64\n",
      "tensor_out[255] = \t 52\n",
      "tensor_out[256] = \t 13\n",
      "tensor_out[257] = \t 32\n",
      "tensor_out[258] = \t 20\n",
      "tensor_out[259] = \t 0\n",
      "tensor_out[260] = \t 0\n",
      "tensor_out[261] = \t 0\n",
      "tensor_out[262] = \t 0\n",
      "tensor_out[263] = \t 0\n",
      "tensor_out[264] = \t 0\n",
      "tensor_out[265] = \t 50\n",
      "tensor_out[266] = \t 19\n",
      "tensor_out[267] = \t 38\n",
      "tensor_out[268] = \t 0\n",
      "tensor_out[269] = \t 1\n",
      "tensor_out[270] = \t 1\n",
      "tensor_out[271] = \t 1\n",
      "tensor_out[272] = \t 0\n",
      "tensor_out[273] = \t 30\n",
      "tensor_out[274] = \t 0\n",
      "tensor_out[275] = \t 0\n",
      "tensor_out[276] = \t 37\n",
      "tensor_out[277] = \t 8\n",
      "tensor_out[278] = \t 2\n",
      "tensor_out[279] = \t 47\n",
      "tensor_out[280] = \t 0\n",
      "tensor_out[281] = \t 57\n",
      "tensor_out[282] = \t 51\n",
      "tensor_out[283] = \t 0\n",
      "tensor_out[284] = \t 0\n",
      "tensor_out[285] = \t 42\n",
      "tensor_out[286] = \t 1\n",
      "tensor_out[287] = \t 0\n",
      "tensor_out[288] = \t 0\n",
      "tensor_out[289] = \t 7\n",
      "tensor_out[290] = \t 45\n",
      "tensor_out[291] = \t 1\n",
      "tensor_out[292] = \t 48\n",
      "tensor_out[293] = \t 3\n",
      "tensor_out[294] = \t 0\n",
      "tensor_out[295] = \t 30\n",
      "tensor_out[296] = \t 51\n",
      "tensor_out[297] = \t 0\n",
      "tensor_out[298] = \t 5\n",
      "tensor_out[299] = \t 64\n",
      "tensor_out[300] = \t 0\n",
      "tensor_out[301] = \t 48\n",
      "tensor_out[302] = \t 0\n",
      "tensor_out[303] = \t 28\n",
      "tensor_out[304] = \t 19\n",
      "tensor_out[305] = \t 0\n",
      "tensor_out[306] = \t 1\n",
      "tensor_out[307] = \t 0\n",
      "tensor_out[308] = \t 53\n",
      "tensor_out[309] = \t 44\n",
      "tensor_out[310] = \t 42\n",
      "tensor_out[311] = \t 0\n",
      "tensor_out[312] = \t 1\n",
      "tensor_out[313] = \t 48\n",
      "tensor_out[314] = \t 0\n",
      "tensor_out[315] = \t 40\n",
      "tensor_out[316] = \t 36\n",
      "tensor_out[317] = \t 0\n",
      "tensor_out[318] = \t 0\n",
      "tensor_out[319] = \t 0\n",
      "tensor_out[320] = \t 0\n",
      "tensor_out[321] = \t 14\n",
      "tensor_out[322] = \t 0\n",
      "tensor_out[323] = \t 104\n",
      "tensor_out[324] = \t 0\n",
      "tensor_out[325] = \t 0\n",
      "tensor_out[326] = \t 0\n",
      "tensor_out[327] = \t 0\n",
      "tensor_out[328] = \t 19\n",
      "tensor_out[329] = \t 43\n",
      "tensor_out[330] = \t 67\n",
      "tensor_out[331] = \t 0\n",
      "tensor_out[332] = \t 0\n",
      "tensor_out[333] = \t 56\n",
      "tensor_out[334] = \t 1\n",
      "tensor_out[335] = \t 0\n",
      "tensor_out[336] = \t 4\n",
      "tensor_out[337] = \t 0\n",
      "tensor_out[338] = \t 46\n",
      "tensor_out[339] = \t 0\n",
      "tensor_out[340] = \t 0\n",
      "tensor_out[341] = \t 77\n",
      "tensor_out[342] = \t 18\n",
      "tensor_out[343] = \t 0\n",
      "tensor_out[344] = \t 0\n",
      "tensor_out[345] = \t 0\n",
      "tensor_out[346] = \t 44\n",
      "tensor_out[347] = \t 1\n",
      "tensor_out[348] = \t 0\n",
      "tensor_out[349] = \t 65\n",
      "tensor_out[350] = \t 0\n",
      "tensor_out[351] = \t 0\n",
      "tensor_out[352] = \t 0\n",
      "tensor_out[353] = \t 52\n",
      "tensor_out[354] = \t 68\n",
      "tensor_out[355] = \t 0\n",
      "tensor_out[356] = \t 0\n",
      "tensor_out[357] = \t 0\n",
      "tensor_out[358] = \t 0\n",
      "tensor_out[359] = \t 0\n",
      "tensor_out[360] = \t 1\n",
      "tensor_out[361] = \t 53\n",
      "tensor_out[362] = \t 0\n",
      "tensor_out[363] = \t 0\n",
      "tensor_out[364] = \t 2\n",
      "tensor_out[365] = \t 0\n",
      "tensor_out[366] = \t 56\n",
      "tensor_out[367] = \t 1\n",
      "tensor_out[368] = \t 43\n",
      "tensor_out[369] = \t 63\n",
      "tensor_out[370] = \t 0\n",
      "tensor_out[371] = \t 0\n",
      "tensor_out[372] = \t 39\n",
      "tensor_out[373] = \t 0\n",
      "tensor_out[374] = \t 0\n",
      "tensor_out[375] = \t 27\n",
      "tensor_out[376] = \t 21\n",
      "tensor_out[377] = \t 2\n",
      "tensor_out[378] = \t 0\n",
      "tensor_out[379] = \t 18\n",
      "tensor_out[380] = \t 3\n",
      "tensor_out[381] = \t 2\n",
      "tensor_out[382] = \t 0\n",
      "tensor_out[383] = \t 0\n",
      "tensor_out[384] = \t 0\n",
      "tensor_out[385] = \t 2\n",
      "tensor_out[386] = \t 25\n",
      "tensor_out[387] = \t 44\n",
      "tensor_out[388] = \t 0\n",
      "tensor_out[389] = \t 0\n",
      "tensor_out[390] = \t 43\n",
      "tensor_out[391] = \t 0\n",
      "tensor_out[392] = \t 0\n",
      "tensor_out[393] = \t 2\n",
      "tensor_out[394] = \t 45\n",
      "tensor_out[395] = \t 2\n",
      "tensor_out[396] = \t 1\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 0\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 45\n",
      "tensor_out[401] = \t 64\n",
      "tensor_out[402] = \t 0\n",
      "tensor_out[403] = \t 50\n",
      "tensor_out[404] = \t 0\n",
      "tensor_out[405] = \t 0\n",
      "tensor_out[406] = \t 3\n",
      "tensor_out[407] = \t 0\n",
      "tensor_out[408] = \t 47\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 0\n",
      "tensor_out[411] = \t 54\n",
      "tensor_out[412] = \t 4\n",
      "tensor_out[413] = \t 41\n",
      "tensor_out[414] = \t 41\n",
      "tensor_out[415] = \t 0\n",
      "tensor_out[416] = \t 29\n",
      "tensor_out[417] = \t 0\n",
      "tensor_out[418] = \t 64\n",
      "tensor_out[419] = \t 26\n",
      "tensor_out[420] = \t 0\n",
      "tensor_out[421] = \t 0\n",
      "tensor_out[422] = \t 12\n",
      "tensor_out[423] = \t 7\n",
      "tensor_out[424] = \t 50\n",
      "tensor_out[425] = \t 0\n",
      "tensor_out[426] = \t 0\n",
      "tensor_out[427] = \t 14\n",
      "tensor_out[428] = \t 0\n",
      "tensor_out[429] = \t 0\n",
      "tensor_out[430] = \t 48\n",
      "tensor_out[431] = \t 34\n",
      "tensor_out[432] = \t 47\n",
      "tensor_out[433] = \t 42\n",
      "tensor_out[434] = \t 0\n",
      "tensor_out[435] = \t 58\n",
      "tensor_out[436] = \t 9\n",
      "tensor_out[437] = \t 38\n",
      "tensor_out[438] = \t 0\n",
      "tensor_out[439] = \t 75\n",
      "tensor_out[440] = \t 1\n",
      "tensor_out[441] = \t 0\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 47\n",
      "tensor_out[444] = \t 0\n",
      "tensor_out[445] = \t 0\n",
      "tensor_out[446] = \t 0\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 49\n",
      "tensor_out[449] = \t 0\n",
      "tensor_out[450] = \t 0\n",
      "tensor_out[451] = \t 0\n",
      "tensor_out[452] = \t 44\n",
      "tensor_out[453] = \t 50\n",
      "tensor_out[454] = \t 3\n",
      "tensor_out[455] = \t 36\n",
      "tensor_out[456] = \t 0\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 0\n",
      "tensor_out[459] = \t 0\n",
      "tensor_out[460] = \t 0\n",
      "tensor_out[461] = \t 8\n",
      "tensor_out[462] = \t 0\n",
      "tensor_out[463] = \t 1\n",
      "tensor_out[464] = \t 43\n",
      "tensor_out[465] = \t 1\n",
      "tensor_out[466] = \t 0\n",
      "tensor_out[467] = \t 50\n",
      "tensor_out[468] = \t 0\n",
      "tensor_out[469] = \t 0\n",
      "tensor_out[470] = \t 1\n",
      "tensor_out[471] = \t 0\n",
      "tensor_out[472] = \t 0\n",
      "tensor_out[473] = \t 0\n",
      "tensor_out[474] = \t 0\n",
      "tensor_out[475] = \t 5\n",
      "tensor_out[476] = \t 7\n",
      "tensor_out[477] = \t 0\n",
      "tensor_out[478] = \t 89\n",
      "tensor_out[479] = \t 3\n",
      "tensor_out[480] = \t 44\n",
      "tensor_out[481] = \t 0\n",
      "tensor_out[482] = \t 0\n",
      "tensor_out[483] = \t 0\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 25\n",
      "tensor_out[486] = \t 12\n",
      "tensor_out[487] = \t 47\n",
      "tensor_out[488] = \t 44\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 51\n",
      "tensor_out[491] = \t 11\n",
      "tensor_out[492] = \t 0\n",
      "tensor_out[493] = \t 0\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 74\n",
      "tensor_out[496] = \t 0\n",
      "tensor_out[497] = \t 41\n",
      "tensor_out[498] = \t 0\n",
      "tensor_out[499] = \t 0\n",
      "tensor_out[500] = \t 0\n",
      "tensor_out[501] = \t 0\n",
      "tensor_out[502] = \t 1\n",
      "tensor_out[503] = \t 27\n",
      "tensor_out[504] = \t 27\n",
      "tensor_out[505] = \t 58\n",
      "tensor_out[506] = \t 0\n",
      "tensor_out[507] = \t 0\n",
      "tensor_out[508] = \t 0\n",
      "tensor_out[509] = \t 0\n",
      "tensor_out[510] = \t 7\n",
      "tensor_out[511] = \t 0\n",
      "tensor_out[512] = \t 0\n",
      "tensor_out[513] = \t 4\n",
      "tensor_out[514] = \t 45\n",
      "tensor_out[515] = \t 1\n",
      "tensor_out[516] = \t 1\n",
      "tensor_out[517] = \t 33\n",
      "tensor_out[518] = \t 1\n",
      "tensor_out[519] = \t 0\n",
      "tensor_out[520] = \t 255\n",
      "tensor_out[521] = \t 54\n",
      "tensor_out[522] = \t 0\n",
      "tensor_out[523] = \t 0\n",
      "tensor_out[524] = \t 0\n",
      "tensor_out[525] = \t 0\n",
      "tensor_out[526] = \t 0\n",
      "tensor_out[527] = \t 0\n",
      "tensor_out[528] = \t 22\n",
      "tensor_out[529] = \t 10\n",
      "tensor_out[530] = \t 67\n",
      "tensor_out[531] = \t 0\n",
      "tensor_out[532] = \t 0\n",
      "tensor_out[533] = \t 0\n",
      "tensor_out[534] = \t 51\n",
      "tensor_out[535] = \t 0\n",
      "tensor_out[536] = \t 0\n",
      "tensor_out[537] = \t 0\n",
      "tensor_out[538] = \t 0\n",
      "tensor_out[539] = \t 0\n",
      "tensor_out[540] = \t 64\n",
      "tensor_out[541] = \t 0\n",
      "tensor_out[542] = \t 0\n",
      "tensor_out[543] = \t 0\n",
      "tensor_out[544] = \t 2\n",
      "tensor_out[545] = \t 0\n",
      "tensor_out[546] = \t 0\n",
      "tensor_out[547] = \t 3\n",
      "tensor_out[548] = \t 18\n",
      "tensor_out[549] = \t 0\n",
      "tensor_out[550] = \t 0\n",
      "tensor_out[551] = \t 23\n",
      "tensor_out[552] = \t 0\n",
      "tensor_out[553] = \t 39\n",
      "tensor_out[554] = \t 0\n",
      "tensor_out[555] = \t 56\n",
      "tensor_out[556] = \t 43\n",
      "tensor_out[557] = \t 0\n",
      "tensor_out[558] = \t 0\n",
      "tensor_out[559] = \t 45\n",
      "tensor_out[560] = \t 39\n",
      "tensor_out[561] = \t 0\n",
      "tensor_out[562] = \t 1\n",
      "tensor_out[563] = \t 0\n",
      "tensor_out[564] = \t 50\n",
      "tensor_out[565] = \t 0\n",
      "tensor_out[566] = \t 6\n",
      "tensor_out[567] = \t 0\n",
      "tensor_out[568] = \t 0\n",
      "tensor_out[569] = \t 1\n",
      "tensor_out[570] = \t 0\n",
      "tensor_out[571] = \t 0\n",
      "tensor_out[572] = \t 0\n",
      "tensor_out[573] = \t 0\n",
      "tensor_out[574] = \t 0\n",
      "tensor_out[575] = \t 34\n",
      "tensor_out[576] = \t 0\n",
      "tensor_out[577] = \t 25\n",
      "tensor_out[578] = \t 37\n",
      "tensor_out[579] = \t 0\n",
      "tensor_out[580] = \t 2\n",
      "tensor_out[581] = \t 23\n",
      "tensor_out[582] = \t 42\n",
      "tensor_out[583] = \t 41\n",
      "tensor_out[584] = \t 0\n",
      "tensor_out[585] = \t 0\n",
      "tensor_out[586] = \t 0\n",
      "tensor_out[587] = \t 0\n",
      "tensor_out[588] = \t 0\n",
      "tensor_out[589] = \t 0\n",
      "tensor_out[590] = \t 45\n",
      "tensor_out[591] = \t 0\n",
      "tensor_out[592] = \t 2\n",
      "tensor_out[593] = \t 0\n",
      "tensor_out[594] = \t 38\n",
      "tensor_out[595] = \t 0\n",
      "tensor_out[596] = \t 48\n",
      "tensor_out[597] = \t 0\n",
      "tensor_out[598] = \t 0\n",
      "tensor_out[599] = \t 0\n",
      "tensor_out[600] = \t 53\n",
      "tensor_out[601] = \t 0\n",
      "tensor_out[602] = \t 0\n",
      "tensor_out[603] = \t 46\n",
      "tensor_out[604] = \t 0\n",
      "tensor_out[605] = \t 1\n",
      "tensor_out[606] = \t 11\n",
      "tensor_out[607] = \t 1\n",
      "tensor_out[608] = \t 0\n",
      "tensor_out[609] = \t 57\n",
      "tensor_out[610] = \t 2\n",
      "tensor_out[611] = \t 4\n",
      "tensor_out[612] = \t 0\n",
      "tensor_out[613] = \t 0\n",
      "tensor_out[614] = \t 49\n",
      "tensor_out[615] = \t 50\n",
      "tensor_out[616] = \t 0\n",
      "tensor_out[617] = \t 47\n",
      "tensor_out[618] = \t 55\n",
      "tensor_out[619] = \t 4\n",
      "tensor_out[620] = \t 0\n",
      "tensor_out[621] = \t 0\n",
      "tensor_out[622] = \t 0\n",
      "tensor_out[623] = \t 5\n",
      "tensor_out[624] = \t 55\n",
      "tensor_out[625] = \t 1\n",
      "tensor_out[626] = \t 1\n",
      "tensor_out[627] = \t 0\n",
      "tensor_out[628] = \t 1\n",
      "tensor_out[629] = \t 41\n",
      "tensor_out[630] = \t 30\n",
      "tensor_out[631] = \t 0\n",
      "tensor_out[632] = \t 0\n",
      "tensor_out[633] = \t 0\n",
      "tensor_out[634] = \t 1\n",
      "tensor_out[635] = \t 0\n",
      "tensor_out[636] = \t 0\n",
      "tensor_out[637] = \t 0\n",
      "tensor_out[638] = \t 52\n",
      "tensor_out[639] = \t 0\n",
      "tensor_out[640] = \t 50\n",
      "tensor_out[641] = \t 0\n",
      "tensor_out[642] = \t 44\n",
      "tensor_out[643] = \t 0\n",
      "tensor_out[644] = \t 0\n",
      "tensor_out[645] = \t 0\n",
      "tensor_out[646] = \t 0\n",
      "tensor_out[647] = \t 42\n",
      "tensor_out[648] = \t 0\n",
      "tensor_out[649] = \t 0\n",
      "tensor_out[650] = \t 0\n",
      "tensor_out[651] = \t 0\n",
      "tensor_out[652] = \t 0\n",
      "tensor_out[653] = \t 0\n",
      "tensor_out[654] = \t 0\n",
      "tensor_out[655] = \t 7\n",
      "tensor_out[656] = \t 5\n",
      "tensor_out[657] = \t 16\n",
      "tensor_out[658] = \t 49\n",
      "tensor_out[659] = \t 0\n",
      "tensor_out[660] = \t 55\n",
      "tensor_out[661] = \t 0\n",
      "tensor_out[662] = \t 51\n",
      "tensor_out[663] = \t 0\n",
      "tensor_out[664] = \t 0\n",
      "tensor_out[665] = \t 0\n",
      "tensor_out[666] = \t 1\n",
      "tensor_out[667] = \t 0\n",
      "tensor_out[668] = \t 51\n",
      "tensor_out[669] = \t 0\n",
      "tensor_out[670] = \t 0\n",
      "tensor_out[671] = \t 9\n",
      "tensor_out[672] = \t 0\n",
      "tensor_out[673] = \t 1\n",
      "tensor_out[674] = \t 0\n",
      "tensor_out[675] = \t 34\n",
      "tensor_out[676] = \t 48\n",
      "tensor_out[677] = \t 0\n",
      "tensor_out[678] = \t 11\n",
      "tensor_out[679] = \t 0\n",
      "tensor_out[680] = \t 0\n",
      "tensor_out[681] = \t 58\n",
      "tensor_out[682] = \t 33\n",
      "tensor_out[683] = \t 0\n",
      "tensor_out[684] = \t 48\n",
      "tensor_out[685] = \t 13\n",
      "tensor_out[686] = \t 0\n",
      "tensor_out[687] = \t 0\n",
      "tensor_out[688] = \t 51\n",
      "tensor_out[689] = \t 1\n",
      "tensor_out[690] = \t 1\n",
      "tensor_out[691] = \t 67\n",
      "tensor_out[692] = \t 58\n",
      "tensor_out[693] = \t 0\n",
      "tensor_out[694] = \t 0\n",
      "tensor_out[695] = \t 1\n",
      "tensor_out[696] = \t 20\n",
      "tensor_out[697] = \t 0\n",
      "tensor_out[698] = \t 0\n",
      "tensor_out[699] = \t 0\n",
      "tensor_out[700] = \t 0\n",
      "tensor_out[701] = \t 0\n",
      "tensor_out[702] = \t 39\n",
      "tensor_out[703] = \t 0\n",
      "tensor_out[704] = \t 0\n",
      "tensor_out[705] = \t 0\n",
      "tensor_out[706] = \t 0\n",
      "tensor_out[707] = \t 0\n",
      "tensor_out[708] = \t 0\n",
      "tensor_out[709] = \t 0\n",
      "tensor_out[710] = \t 0\n",
      "tensor_out[711] = \t 56\n",
      "tensor_out[712] = \t 0\n",
      "tensor_out[713] = \t 65\n",
      "tensor_out[714] = \t 15\n",
      "tensor_out[715] = \t 9\n",
      "tensor_out[716] = \t 0\n",
      "tensor_out[717] = \t 0\n",
      "tensor_out[718] = \t 30\n",
      "tensor_out[719] = \t 43\n",
      "tensor_out[720] = \t 0\n",
      "tensor_out[721] = \t 55\n",
      "tensor_out[722] = \t 0\n",
      "tensor_out[723] = \t 48\n",
      "tensor_out[724] = \t 10\n",
      "tensor_out[725] = \t 0\n",
      "tensor_out[726] = \t 34\n",
      "tensor_out[727] = \t 9\n",
      "tensor_out[728] = \t 63\n",
      "tensor_out[729] = \t 66\n",
      "tensor_out[730] = \t 0\n",
      "tensor_out[731] = \t 1\n",
      "tensor_out[732] = \t 0\n",
      "tensor_out[733] = \t 0\n",
      "tensor_out[734] = \t 54\n",
      "tensor_out[735] = \t 33\n",
      "tensor_out[736] = \t 0\n",
      "tensor_out[737] = \t 0\n",
      "tensor_out[738] = \t 0\n",
      "tensor_out[739] = \t 0\n",
      "tensor_out[740] = \t 15\n",
      "tensor_out[741] = \t 52\n",
      "tensor_out[742] = \t 0\n",
      "tensor_out[743] = \t 0\n",
      "tensor_out[744] = \t 0\n",
      "tensor_out[745] = \t 0\n",
      "tensor_out[746] = \t 0\n",
      "tensor_out[747] = \t 0\n",
      "tensor_out[748] = \t 6\n",
      "tensor_out[749] = \t 36\n",
      "tensor_out[750] = \t 0\n",
      "tensor_out[751] = \t 0\n",
      "tensor_out[752] = \t 0\n",
      "tensor_out[753] = \t 0\n",
      "tensor_out[754] = \t 0\n",
      "tensor_out[755] = \t 0\n",
      "tensor_out[756] = \t 1\n",
      "tensor_out[757] = \t 0\n",
      "tensor_out[758] = \t 0\n",
      "tensor_out[759] = \t 0\n",
      "tensor_out[760] = \t 0\n",
      "tensor_out[761] = \t 7\n",
      "tensor_out[762] = \t 0\n",
      "tensor_out[763] = \t 0\n",
      "tensor_out[764] = \t 0\n",
      "tensor_out[765] = \t 1\n",
      "tensor_out[766] = \t 37\n",
      "tensor_out[767] = \t 56\n",
      "tensor_out[768] = \t 0\n",
      "tensor_out[769] = \t 74\n",
      "tensor_out[770] = \t 0\n",
      "tensor_out[771] = \t 0\n",
      "tensor_out[772] = \t 14\n",
      "tensor_out[773] = \t 24\n",
      "tensor_out[774] = \t 41\n",
      "tensor_out[775] = \t 0\n",
      "tensor_out[776] = \t 0\n",
      "tensor_out[777] = \t 8\n",
      "tensor_out[778] = \t 0\n",
      "tensor_out[779] = \t 41\n",
      "tensor_out[780] = \t 0\n",
      "tensor_out[781] = \t 9\n",
      "tensor_out[782] = \t 52\n",
      "tensor_out[783] = \t 0\n",
      "tensor_out[784] = \t 50\n",
      "tensor_out[785] = \t 26\n",
      "tensor_out[786] = \t 0\n",
      "tensor_out[787] = \t 75\n",
      "tensor_out[788] = \t 0\n",
      "tensor_out[789] = \t 5\n",
      "tensor_out[790] = \t 0\n",
      "tensor_out[791] = \t 61\n",
      "tensor_out[792] = \t 0\n",
      "tensor_out[793] = \t 1\n",
      "tensor_out[794] = \t 0\n",
      "tensor_out[795] = \t 1\n",
      "tensor_out[796] = \t 0\n",
      "tensor_out[797] = \t 50\n",
      "tensor_out[798] = \t 52\n",
      "tensor_out[799] = \t 0\n",
      "tensor_out[800] = \t 53\n",
      "tensor_out[801] = \t 0\n",
      "tensor_out[802] = \t 43\n",
      "tensor_out[803] = \t 0\n",
      "tensor_out[804] = \t 0\n",
      "tensor_out[805] = \t 42\n",
      "tensor_out[806] = \t 0\n",
      "tensor_out[807] = \t 47\n",
      "tensor_out[808] = \t 0\n",
      "tensor_out[809] = \t 2\n",
      "tensor_out[810] = \t 36\n",
      "tensor_out[811] = \t 0\n",
      "tensor_out[812] = \t 12\n",
      "tensor_out[813] = \t 0\n",
      "tensor_out[814] = \t 56\n",
      "tensor_out[815] = \t 42\n",
      "tensor_out[816] = \t 1\n",
      "tensor_out[817] = \t 0\n",
      "tensor_out[818] = \t 59\n",
      "tensor_out[819] = \t 33\n",
      "tensor_out[820] = \t 43\n",
      "tensor_out[821] = \t 45\n",
      "tensor_out[822] = \t 0\n",
      "tensor_out[823] = \t 0\n",
      "tensor_out[824] = \t 48\n",
      "tensor_out[825] = \t 48\n",
      "tensor_out[826] = \t 61\n",
      "tensor_out[827] = \t 0\n",
      "tensor_out[828] = \t 0\n",
      "tensor_out[829] = \t 0\n",
      "tensor_out[830] = \t 44\n",
      "tensor_out[831] = \t 42\n",
      "tensor_out[832] = \t 0\n",
      "tensor_out[833] = \t 51\n",
      "tensor_out[834] = \t 0\n",
      "tensor_out[835] = \t 44\n",
      "tensor_out[836] = \t 49\n",
      "tensor_out[837] = \t 47\n",
      "tensor_out[838] = \t 0\n",
      "tensor_out[839] = \t 0\n",
      "tensor_out[840] = \t 0\n",
      "tensor_out[841] = \t 0\n",
      "tensor_out[842] = \t 0\n",
      "tensor_out[843] = \t 35\n",
      "tensor_out[844] = \t 50\n",
      "tensor_out[845] = \t 0\n",
      "tensor_out[846] = \t 0\n",
      "tensor_out[847] = \t 53\n",
      "tensor_out[848] = \t 0\n",
      "tensor_out[849] = \t 0\n",
      "tensor_out[850] = \t 0\n",
      "tensor_out[851] = \t 19\n",
      "tensor_out[852] = \t 0\n",
      "tensor_out[853] = \t 45\n",
      "tensor_out[854] = \t 68\n",
      "tensor_out[855] = \t 0\n",
      "tensor_out[856] = \t 14\n",
      "tensor_out[857] = \t 0\n",
      "tensor_out[858] = \t 39\n",
      "tensor_out[859] = \t 0\n",
      "tensor_out[860] = \t 40\n",
      "tensor_out[861] = \t 15\n",
      "tensor_out[862] = \t 58\n",
      "tensor_out[863] = \t 1\n",
      "tensor_out[864] = \t 38\n",
      "tensor_out[865] = \t 0\n",
      "tensor_out[866] = \t 0\n",
      "tensor_out[867] = \t 1\n",
      "tensor_out[868] = \t 0\n",
      "tensor_out[869] = \t 0\n",
      "tensor_out[870] = \t 0\n",
      "tensor_out[871] = \t 0\n",
      "tensor_out[872] = \t 0\n",
      "tensor_out[873] = \t 0\n",
      "tensor_out[874] = \t 19\n",
      "tensor_out[875] = \t 11\n",
      "tensor_out[876] = \t 18\n",
      "tensor_out[877] = \t 36\n",
      "tensor_out[878] = \t 61\n",
      "tensor_out[879] = \t 45\n",
      "tensor_out[880] = \t 0\n",
      "tensor_out[881] = \t 50\n",
      "tensor_out[882] = \t 27\n",
      "tensor_out[883] = \t 0\n",
      "tensor_out[884] = \t 59\n",
      "tensor_out[885] = \t 0\n",
      "tensor_out[886] = \t 50\n",
      "tensor_out[887] = \t 46\n",
      "tensor_out[888] = \t 25\n",
      "tensor_out[889] = \t 39\n",
      "tensor_out[890] = \t 0\n",
      "tensor_out[891] = \t 0\n",
      "tensor_out[892] = \t 67\n",
      "tensor_out[893] = \t 42\n",
      "tensor_out[894] = \t 50\n",
      "tensor_out[895] = \t 0\n",
      "tensor_out[896] = \t 0\n",
      "tensor_out[897] = \t 0\n",
      "tensor_out[898] = \t 48\n",
      "tensor_out[899] = \t 0\n",
      "tensor_out[900] = \t 83\n",
      "tensor_out[901] = \t 67\n",
      "tensor_out[902] = \t 0\n",
      "tensor_out[903] = \t 0\n",
      "tensor_out[904] = \t 0\n",
      "tensor_out[905] = \t 47\n",
      "tensor_out[906] = \t 1\n",
      "tensor_out[907] = \t 0\n",
      "tensor_out[908] = \t 49\n",
      "tensor_out[909] = \t 40\n",
      "tensor_out[910] = \t 0\n",
      "tensor_out[911] = \t 0\n",
      "tensor_out[912] = \t 104\n",
      "tensor_out[913] = \t 0\n",
      "tensor_out[914] = \t 0\n",
      "tensor_out[915] = \t 0\n",
      "tensor_out[916] = \t 0\n",
      "tensor_out[917] = \t 0\n",
      "tensor_out[918] = \t 40\n",
      "tensor_out[919] = \t 0\n",
      "tensor_out[920] = \t 0\n",
      "tensor_out[921] = \t 0\n",
      "tensor_out[922] = \t 0\n",
      "tensor_out[923] = \t 0\n",
      "tensor_out[924] = \t 0\n",
      "tensor_out[925] = \t 0\n",
      "tensor_out[926] = \t 0\n",
      "tensor_out[927] = \t 25\n",
      "tensor_out[928] = \t 0\n",
      "tensor_out[929] = \t 0\n",
      "tensor_out[930] = \t 6\n",
      "tensor_out[931] = \t 0\n",
      "tensor_out[932] = \t 0\n",
      "tensor_out[933] = \t 48\n",
      "tensor_out[934] = \t 1\n",
      "tensor_out[935] = \t 0\n",
      "tensor_out[936] = \t 0\n",
      "tensor_out[937] = \t 38\n",
      "tensor_out[938] = \t 0\n",
      "tensor_out[939] = \t 28\n",
      "tensor_out[940] = \t 0\n",
      "tensor_out[941] = \t 0\n",
      "tensor_out[942] = \t 0\n",
      "tensor_out[943] = \t 0\n",
      "tensor_out[944] = \t 0\n",
      "tensor_out[945] = \t 25\n",
      "tensor_out[946] = \t 1\n",
      "tensor_out[947] = \t 48\n",
      "tensor_out[948] = \t 0\n",
      "tensor_out[949] = \t 23\n",
      "tensor_out[950] = \t 0\n",
      "tensor_out[951] = \t 0\n",
      "tensor_out[952] = \t 56\n",
      "tensor_out[953] = \t 0\n",
      "tensor_out[954] = \t 19\n",
      "tensor_out[955] = \t 45\n",
      "tensor_out[956] = \t 46\n",
      "tensor_out[957] = \t 0\n",
      "tensor_out[958] = \t 0\n",
      "tensor_out[959] = \t 0\n",
      "tensor_out[960] = \t 0\n",
      "tensor_out[961] = \t 0\n",
      "tensor_out[962] = \t 5\n",
      "tensor_out[963] = \t 0\n",
      "tensor_out[964] = \t 0\n",
      "tensor_out[965] = \t 9\n",
      "tensor_out[966] = \t 0\n",
      "tensor_out[967] = \t 0\n",
      "tensor_out[968] = \t 72\n",
      "tensor_out[969] = \t 0\n",
      "tensor_out[970] = \t 0\n",
      "tensor_out[971] = \t 0\n",
      "tensor_out[972] = \t 0\n",
      "tensor_out[973] = \t 0\n",
      "tensor_out[974] = \t 0\n",
      "tensor_out[975] = \t 0\n",
      "tensor_out[976] = \t 60\n",
      "tensor_out[977] = \t 0\n",
      "tensor_out[978] = \t 44\n",
      "tensor_out[979] = \t 38\n",
      "tensor_out[980] = \t 19\n",
      "tensor_out[981] = \t 1\n",
      "tensor_out[982] = \t 50\n",
      "tensor_out[983] = \t 0\n",
      "tensor_out[984] = \t 0\n",
      "tensor_out[985] = \t 0\n",
      "tensor_out[986] = \t 0\n",
      "tensor_out[987] = \t 0\n",
      "tensor_out[988] = \t 0\n",
      "tensor_out[989] = \t 13\n",
      "tensor_out[990] = \t 29\n",
      "tensor_out[991] = \t 9\n",
      "tensor_out[992] = \t 0\n",
      "tensor_out[993] = \t 0\n",
      "tensor_out[994] = \t 2\n",
      "tensor_out[995] = \t 1\n",
      "tensor_out[996] = \t 42\n",
      "tensor_out[997] = \t 0\n",
      "tensor_out[998] = \t 35\n",
      "tensor_out[999] = \t 0\n",
      "tensor_out[1000] = \t 0\n",
      "tensor_out[1001] = \t 0\n",
      "tensor_out[1002] = \t 0\n",
      "tensor_out[1003] = \t 1\n",
      "tensor_out[1004] = \t 51\n",
      "tensor_out[1005] = \t 2\n",
      "tensor_out[1006] = \t 1\n",
      "tensor_out[1007] = \t 24\n",
      "tensor_out[1008] = \t 0\n",
      "tensor_out[1009] = \t 42\n",
      "tensor_out[1010] = \t 0\n",
      "tensor_out[1011] = \t 46\n",
      "tensor_out[1012] = \t 0\n",
      "tensor_out[1013] = \t 0\n",
      "tensor_out[1014] = \t 1\n",
      "tensor_out[1015] = \t 0\n",
      "tensor_out[1016] = \t 9\n",
      "tensor_out[1017] = \t 0\n",
      "tensor_out[1018] = \t 53\n",
      "tensor_out[1019] = \t 48\n",
      "tensor_out[1020] = \t 44\n",
      "tensor_out[1021] = \t 44\n",
      "tensor_out[1022] = \t 52\n",
      "tensor_out[1023] = \t 0\n",
      " \n",
      "\n",
      "EXPECTED tensor_out layer27 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "tensor_out[0] = \t 0\n",
      "tensor_out[1] = \t 0\n",
      "tensor_out[2] = \t 0\n",
      "tensor_out[3] = \t 0\n",
      "tensor_out[4] = \t 0\n",
      "tensor_out[5] = \t 0\n",
      "tensor_out[6] = \t 0\n",
      "tensor_out[7] = \t 0\n",
      "tensor_out[8] = \t 0\n",
      "tensor_out[9] = \t 0\n",
      "tensor_out[10] = \t 0\n",
      "tensor_out[11] = \t 0\n",
      "tensor_out[12] = \t 0\n",
      "tensor_out[13] = \t 0\n",
      "tensor_out[14] = \t 0\n",
      "tensor_out[15] = \t 0\n",
      "tensor_out[16] = \t 0\n",
      "tensor_out[17] = \t 0\n",
      "tensor_out[18] = \t 0\n",
      "tensor_out[19] = \t 0\n",
      "tensor_out[20] = \t 0\n",
      "tensor_out[21] = \t 0\n",
      "tensor_out[22] = \t 0\n",
      "tensor_out[23] = \t 0\n",
      "tensor_out[24] = \t 0\n",
      "tensor_out[25] = \t 0\n",
      "tensor_out[26] = \t 0\n",
      "tensor_out[27] = \t 0\n",
      "tensor_out[28] = \t 0\n",
      "tensor_out[29] = \t 0\n",
      "tensor_out[30] = \t 0\n",
      "tensor_out[31] = \t 0\n",
      "tensor_out[32] = \t 0\n",
      "tensor_out[33] = \t 0\n",
      "tensor_out[34] = \t 0\n",
      "tensor_out[35] = \t 0\n",
      "tensor_out[36] = \t 0\n",
      "tensor_out[37] = \t 0\n",
      "tensor_out[38] = \t 0\n",
      "tensor_out[39] = \t 0\n",
      "tensor_out[40] = \t 0\n",
      "tensor_out[41] = \t 0\n",
      "tensor_out[42] = \t 0\n",
      "tensor_out[43] = \t 0\n",
      "tensor_out[44] = \t 0\n",
      "tensor_out[45] = \t 0\n",
      "tensor_out[46] = \t 0\n",
      "tensor_out[47] = \t 0\n",
      "tensor_out[48] = \t 0\n",
      "tensor_out[49] = \t 0\n",
      "tensor_out[50] = \t 0\n",
      "tensor_out[51] = \t 0\n",
      "tensor_out[52] = \t 0\n",
      "tensor_out[53] = \t 0\n",
      "tensor_out[54] = \t 0\n",
      "tensor_out[55] = \t 0\n",
      "tensor_out[56] = \t 0\n",
      "tensor_out[57] = \t 0\n",
      "tensor_out[58] = \t 0\n",
      "tensor_out[59] = \t 0\n",
      "tensor_out[60] = \t 0\n",
      "tensor_out[61] = \t 0\n",
      "tensor_out[62] = \t 0\n",
      "tensor_out[63] = \t 0\n",
      "tensor_out[64] = \t 0\n",
      "tensor_out[65] = \t 0\n",
      "tensor_out[66] = \t 0\n",
      "tensor_out[67] = \t 0\n",
      "tensor_out[68] = \t 0\n",
      "tensor_out[69] = \t 0\n",
      "tensor_out[70] = \t 0\n",
      "tensor_out[71] = \t 0\n",
      "tensor_out[72] = \t 0\n",
      "tensor_out[73] = \t 0\n",
      "tensor_out[74] = \t 0\n",
      "tensor_out[75] = \t 0\n",
      "tensor_out[76] = \t 0\n",
      "tensor_out[77] = \t 0\n",
      "tensor_out[78] = \t 0\n",
      "tensor_out[79] = \t 0\n",
      "tensor_out[80] = \t 0\n",
      "tensor_out[81] = \t 0\n",
      "tensor_out[82] = \t 0\n",
      "tensor_out[83] = \t 0\n",
      "tensor_out[84] = \t 0\n",
      "tensor_out[85] = \t 0\n",
      "tensor_out[86] = \t 0\n",
      "tensor_out[87] = \t 0\n",
      "tensor_out[88] = \t 0\n",
      "tensor_out[89] = \t 0\n",
      "tensor_out[90] = \t 0\n",
      "tensor_out[91] = \t 0\n",
      "tensor_out[92] = \t 0\n",
      "tensor_out[93] = \t 0\n",
      "tensor_out[94] = \t 0\n",
      "tensor_out[95] = \t 0\n",
      "tensor_out[96] = \t 0\n",
      "tensor_out[97] = \t 0\n",
      "tensor_out[98] = \t 0\n",
      "tensor_out[99] = \t 0\n",
      "tensor_out[100] = \t 0\n",
      "tensor_out[101] = \t 0\n",
      "tensor_out[102] = \t 0\n",
      "tensor_out[103] = \t 0\n",
      "tensor_out[104] = \t 0\n",
      "tensor_out[105] = \t 0\n",
      "tensor_out[106] = \t 0\n",
      "tensor_out[107] = \t 0\n",
      "tensor_out[108] = \t 0\n",
      "tensor_out[109] = \t 0\n",
      "tensor_out[110] = \t 0\n",
      "tensor_out[111] = \t 0\n",
      "tensor_out[112] = \t 0\n",
      "tensor_out[113] = \t 0\n",
      "tensor_out[114] = \t 0\n",
      "tensor_out[115] = \t 0\n",
      "tensor_out[116] = \t 0\n",
      "tensor_out[117] = \t 0\n",
      "tensor_out[118] = \t 0\n",
      "tensor_out[119] = \t 0\n",
      "tensor_out[120] = \t 0\n",
      "tensor_out[121] = \t 0\n",
      "tensor_out[122] = \t 0\n",
      "tensor_out[123] = \t 0\n",
      "tensor_out[124] = \t 0\n",
      "tensor_out[125] = \t 0\n",
      "tensor_out[126] = \t 0\n",
      "tensor_out[127] = \t 0\n",
      "tensor_out[128] = \t 0\n",
      "tensor_out[129] = \t 0\n",
      "tensor_out[130] = \t 0\n",
      "tensor_out[131] = \t 0\n",
      "tensor_out[132] = \t 0\n",
      "tensor_out[133] = \t 0\n",
      "tensor_out[134] = \t 0\n",
      "tensor_out[135] = \t 0\n",
      "tensor_out[136] = \t 0\n",
      "tensor_out[137] = \t 0\n",
      "tensor_out[138] = \t 0\n",
      "tensor_out[139] = \t 0\n",
      "tensor_out[140] = \t 0\n",
      "tensor_out[141] = \t 0\n",
      "tensor_out[142] = \t 0\n",
      "tensor_out[143] = \t 0\n",
      "tensor_out[144] = \t 0\n",
      "tensor_out[145] = \t 0\n",
      "tensor_out[146] = \t 0\n",
      "tensor_out[147] = \t 0\n",
      "tensor_out[148] = \t 0\n",
      "tensor_out[149] = \t 0\n",
      "tensor_out[150] = \t 0\n",
      "tensor_out[151] = \t 0\n",
      "tensor_out[152] = \t 0\n",
      "tensor_out[153] = \t 0\n",
      "tensor_out[154] = \t 0\n",
      "tensor_out[155] = \t 0\n",
      "tensor_out[156] = \t 0\n",
      "tensor_out[157] = \t 0\n",
      "tensor_out[158] = \t 0\n",
      "tensor_out[159] = \t 0\n",
      "tensor_out[160] = \t 0\n",
      "tensor_out[161] = \t 0\n",
      "tensor_out[162] = \t 0\n",
      "tensor_out[163] = \t 0\n",
      "tensor_out[164] = \t 0\n",
      "tensor_out[165] = \t 0\n",
      "tensor_out[166] = \t 0\n",
      "tensor_out[167] = \t 0\n",
      "tensor_out[168] = \t 0\n",
      "tensor_out[169] = \t 0\n",
      "tensor_out[170] = \t 0\n",
      "tensor_out[171] = \t 0\n",
      "tensor_out[172] = \t 0\n",
      "tensor_out[173] = \t 0\n",
      "tensor_out[174] = \t 0\n",
      "tensor_out[175] = \t 0\n",
      "tensor_out[176] = \t 0\n",
      "tensor_out[177] = \t 0\n",
      "tensor_out[178] = \t 0\n",
      "tensor_out[179] = \t 0\n",
      "tensor_out[180] = \t 0\n",
      "tensor_out[181] = \t 0\n",
      "tensor_out[182] = \t 0\n",
      "tensor_out[183] = \t 0\n",
      "tensor_out[184] = \t 0\n",
      "tensor_out[185] = \t 0\n",
      "tensor_out[186] = \t 0\n",
      "tensor_out[187] = \t 0\n",
      "tensor_out[188] = \t 0\n",
      "tensor_out[189] = \t 0\n",
      "tensor_out[190] = \t 0\n",
      "tensor_out[191] = \t 0\n",
      "tensor_out[192] = \t 0\n",
      "tensor_out[193] = \t 0\n",
      "tensor_out[194] = \t 0\n",
      "tensor_out[195] = \t 0\n",
      "tensor_out[196] = \t 0\n",
      "tensor_out[197] = \t 0\n",
      "tensor_out[198] = \t 0\n",
      "tensor_out[199] = \t 0\n",
      "tensor_out[200] = \t 0\n",
      "tensor_out[201] = \t 0\n",
      "tensor_out[202] = \t 0\n",
      "tensor_out[203] = \t 0\n",
      "tensor_out[204] = \t 0\n",
      "tensor_out[205] = \t 0\n",
      "tensor_out[206] = \t 0\n",
      "tensor_out[207] = \t 0\n",
      "tensor_out[208] = \t 0\n",
      "tensor_out[209] = \t 0\n",
      "tensor_out[210] = \t 0\n",
      "tensor_out[211] = \t 0\n",
      "tensor_out[212] = \t 0\n",
      "tensor_out[213] = \t 0\n",
      "tensor_out[214] = \t 0\n",
      "tensor_out[215] = \t 0\n",
      "tensor_out[216] = \t 0\n",
      "tensor_out[217] = \t 0\n",
      "tensor_out[218] = \t 0\n",
      "tensor_out[219] = \t 0\n",
      "tensor_out[220] = \t 0\n",
      "tensor_out[221] = \t 0\n",
      "tensor_out[222] = \t 0\n",
      "tensor_out[223] = \t 0\n",
      "tensor_out[224] = \t 0\n",
      "tensor_out[225] = \t 0\n",
      "tensor_out[226] = \t 0\n",
      "tensor_out[227] = \t 0\n",
      "tensor_out[228] = \t 0\n",
      "tensor_out[229] = \t 0\n",
      "tensor_out[230] = \t 0\n",
      "tensor_out[231] = \t 0\n",
      "tensor_out[232] = \t 0\n",
      "tensor_out[233] = \t 0\n",
      "tensor_out[234] = \t 0\n",
      "tensor_out[235] = \t 0\n",
      "tensor_out[236] = \t 0\n",
      "tensor_out[237] = \t 0\n",
      "tensor_out[238] = \t 0\n",
      "tensor_out[239] = \t 0\n",
      "tensor_out[240] = \t 0\n",
      "tensor_out[241] = \t 0\n",
      "tensor_out[242] = \t 0\n",
      "tensor_out[243] = \t 0\n",
      "tensor_out[244] = \t 0\n",
      "tensor_out[245] = \t 0\n",
      "tensor_out[246] = \t 0\n",
      "tensor_out[247] = \t 0\n",
      "tensor_out[248] = \t 0\n",
      "tensor_out[249] = \t 0\n",
      "tensor_out[250] = \t 0\n",
      "tensor_out[251] = \t 0\n",
      "tensor_out[252] = \t 0\n",
      "tensor_out[253] = \t 0\n",
      "tensor_out[254] = \t 0\n",
      "tensor_out[255] = \t 0\n",
      "tensor_out[256] = \t 0\n",
      "tensor_out[257] = \t 0\n",
      "tensor_out[258] = \t 0\n",
      "tensor_out[259] = \t 0\n",
      "tensor_out[260] = \t 0\n",
      "tensor_out[261] = \t 0\n",
      "tensor_out[262] = \t 0\n",
      "tensor_out[263] = \t 0\n",
      "tensor_out[264] = \t 0\n",
      "tensor_out[265] = \t 0\n",
      "tensor_out[266] = \t 0\n",
      "tensor_out[267] = \t 0\n",
      "tensor_out[268] = \t 0\n",
      "tensor_out[269] = \t 0\n",
      "tensor_out[270] = \t 0\n",
      "tensor_out[271] = \t 0\n",
      "tensor_out[272] = \t 0\n",
      "tensor_out[273] = \t 0\n",
      "tensor_out[274] = \t 0\n",
      "tensor_out[275] = \t 0\n",
      "tensor_out[276] = \t 0\n",
      "tensor_out[277] = \t 0\n",
      "tensor_out[278] = \t 0\n",
      "tensor_out[279] = \t 0\n",
      "tensor_out[280] = \t 0\n",
      "tensor_out[281] = \t 0\n",
      "tensor_out[282] = \t 0\n",
      "tensor_out[283] = \t 0\n",
      "tensor_out[284] = \t 0\n",
      "tensor_out[285] = \t 0\n",
      "tensor_out[286] = \t 0\n",
      "tensor_out[287] = \t 0\n",
      "tensor_out[288] = \t 0\n",
      "tensor_out[289] = \t 0\n",
      "tensor_out[290] = \t 0\n",
      "tensor_out[291] = \t 0\n",
      "tensor_out[292] = \t 0\n",
      "tensor_out[293] = \t 0\n",
      "tensor_out[294] = \t 0\n",
      "tensor_out[295] = \t 0\n",
      "tensor_out[296] = \t 0\n",
      "tensor_out[297] = \t 0\n",
      "tensor_out[298] = \t 0\n",
      "tensor_out[299] = \t 0\n",
      "tensor_out[300] = \t 0\n",
      "tensor_out[301] = \t 0\n",
      "tensor_out[302] = \t 0\n",
      "tensor_out[303] = \t 0\n",
      "tensor_out[304] = \t 0\n",
      "tensor_out[305] = \t 0\n",
      "tensor_out[306] = \t 0\n",
      "tensor_out[307] = \t 0\n",
      "tensor_out[308] = \t 0\n",
      "tensor_out[309] = \t 0\n",
      "tensor_out[310] = \t 0\n",
      "tensor_out[311] = \t 0\n",
      "tensor_out[312] = \t 0\n",
      "tensor_out[313] = \t 0\n",
      "tensor_out[314] = \t 0\n",
      "tensor_out[315] = \t 0\n",
      "tensor_out[316] = \t 0\n",
      "tensor_out[317] = \t 0\n",
      "tensor_out[318] = \t 0\n",
      "tensor_out[319] = \t 0\n",
      "tensor_out[320] = \t 0\n",
      "tensor_out[321] = \t 0\n",
      "tensor_out[322] = \t 0\n",
      "tensor_out[323] = \t 0\n",
      "tensor_out[324] = \t 0\n",
      "tensor_out[325] = \t 0\n",
      "tensor_out[326] = \t 0\n",
      "tensor_out[327] = \t 0\n",
      "tensor_out[328] = \t 0\n",
      "tensor_out[329] = \t 0\n",
      "tensor_out[330] = \t 0\n",
      "tensor_out[331] = \t 0\n",
      "tensor_out[332] = \t 0\n",
      "tensor_out[333] = \t 0\n",
      "tensor_out[334] = \t 0\n",
      "tensor_out[335] = \t 0\n",
      "tensor_out[336] = \t 0\n",
      "tensor_out[337] = \t 0\n",
      "tensor_out[338] = \t 0\n",
      "tensor_out[339] = \t 0\n",
      "tensor_out[340] = \t 0\n",
      "tensor_out[341] = \t 0\n",
      "tensor_out[342] = \t 0\n",
      "tensor_out[343] = \t 0\n",
      "tensor_out[344] = \t 0\n",
      "tensor_out[345] = \t 0\n",
      "tensor_out[346] = \t 0\n",
      "tensor_out[347] = \t 0\n",
      "tensor_out[348] = \t 0\n",
      "tensor_out[349] = \t 0\n",
      "tensor_out[350] = \t 0\n",
      "tensor_out[351] = \t 0\n",
      "tensor_out[352] = \t 0\n",
      "tensor_out[353] = \t 0\n",
      "tensor_out[354] = \t 0\n",
      "tensor_out[355] = \t 0\n",
      "tensor_out[356] = \t 0\n",
      "tensor_out[357] = \t 0\n",
      "tensor_out[358] = \t 0\n",
      "tensor_out[359] = \t 0\n",
      "tensor_out[360] = \t 0\n",
      "tensor_out[361] = \t 0\n",
      "tensor_out[362] = \t 0\n",
      "tensor_out[363] = \t 0\n",
      "tensor_out[364] = \t 0\n",
      "tensor_out[365] = \t 0\n",
      "tensor_out[366] = \t 0\n",
      "tensor_out[367] = \t 0\n",
      "tensor_out[368] = \t 0\n",
      "tensor_out[369] = \t 0\n",
      "tensor_out[370] = \t 0\n",
      "tensor_out[371] = \t 0\n",
      "tensor_out[372] = \t 0\n",
      "tensor_out[373] = \t 0\n",
      "tensor_out[374] = \t 0\n",
      "tensor_out[375] = \t 0\n",
      "tensor_out[376] = \t 0\n",
      "tensor_out[377] = \t 0\n",
      "tensor_out[378] = \t 0\n",
      "tensor_out[379] = \t 0\n",
      "tensor_out[380] = \t 0\n",
      "tensor_out[381] = \t 0\n",
      "tensor_out[382] = \t 0\n",
      "tensor_out[383] = \t 0\n",
      "tensor_out[384] = \t 0\n",
      "tensor_out[385] = \t 0\n",
      "tensor_out[386] = \t 0\n",
      "tensor_out[387] = \t 0\n",
      "tensor_out[388] = \t 0\n",
      "tensor_out[389] = \t 0\n",
      "tensor_out[390] = \t 0\n",
      "tensor_out[391] = \t 0\n",
      "tensor_out[392] = \t 0\n",
      "tensor_out[393] = \t 0\n",
      "tensor_out[394] = \t 0\n",
      "tensor_out[395] = \t 0\n",
      "tensor_out[396] = \t 0\n",
      "tensor_out[397] = \t 0\n",
      "tensor_out[398] = \t 0\n",
      "tensor_out[399] = \t 0\n",
      "tensor_out[400] = \t 0\n",
      "tensor_out[401] = \t 0\n",
      "tensor_out[402] = \t 0\n",
      "tensor_out[403] = \t 0\n",
      "tensor_out[404] = \t 0\n",
      "tensor_out[405] = \t 0\n",
      "tensor_out[406] = \t 0\n",
      "tensor_out[407] = \t 0\n",
      "tensor_out[408] = \t 0\n",
      "tensor_out[409] = \t 0\n",
      "tensor_out[410] = \t 0\n",
      "tensor_out[411] = \t 0\n",
      "tensor_out[412] = \t 0\n",
      "tensor_out[413] = \t 0\n",
      "tensor_out[414] = \t 0\n",
      "tensor_out[415] = \t 0\n",
      "tensor_out[416] = \t 0\n",
      "tensor_out[417] = \t 0\n",
      "tensor_out[418] = \t 0\n",
      "tensor_out[419] = \t 0\n",
      "tensor_out[420] = \t 0\n",
      "tensor_out[421] = \t 0\n",
      "tensor_out[422] = \t 0\n",
      "tensor_out[423] = \t 0\n",
      "tensor_out[424] = \t 0\n",
      "tensor_out[425] = \t 0\n",
      "tensor_out[426] = \t 0\n",
      "tensor_out[427] = \t 0\n",
      "tensor_out[428] = \t 0\n",
      "tensor_out[429] = \t 0\n",
      "tensor_out[430] = \t 0\n",
      "tensor_out[431] = \t 0\n",
      "tensor_out[432] = \t 0\n",
      "tensor_out[433] = \t 0\n",
      "tensor_out[434] = \t 0\n",
      "tensor_out[435] = \t 0\n",
      "tensor_out[436] = \t 0\n",
      "tensor_out[437] = \t 0\n",
      "tensor_out[438] = \t 0\n",
      "tensor_out[439] = \t 0\n",
      "tensor_out[440] = \t 0\n",
      "tensor_out[441] = \t 24\n",
      "tensor_out[442] = \t 0\n",
      "tensor_out[443] = \t 0\n",
      "tensor_out[444] = \t 0\n",
      "tensor_out[445] = \t 0\n",
      "tensor_out[446] = \t 0\n",
      "tensor_out[447] = \t 0\n",
      "tensor_out[448] = \t 0\n",
      "tensor_out[449] = \t 0\n",
      "tensor_out[450] = \t 0\n",
      "tensor_out[451] = \t 0\n",
      "tensor_out[452] = \t 0\n",
      "tensor_out[453] = \t 0\n",
      "tensor_out[454] = \t 0\n",
      "tensor_out[455] = \t 156\n",
      "tensor_out[456] = \t 0\n",
      "tensor_out[457] = \t 0\n",
      "tensor_out[458] = \t 0\n",
      "tensor_out[459] = \t 0\n",
      "tensor_out[460] = \t 0\n",
      "tensor_out[461] = \t 0\n",
      "tensor_out[462] = \t 0\n",
      "tensor_out[463] = \t 0\n",
      "tensor_out[464] = \t 0\n",
      "tensor_out[465] = \t 0\n",
      "tensor_out[466] = \t 0\n",
      "tensor_out[467] = \t 0\n",
      "tensor_out[468] = \t 0\n",
      "tensor_out[469] = \t 0\n",
      "tensor_out[470] = \t 0\n",
      "tensor_out[471] = \t 0\n",
      "tensor_out[472] = \t 43\n",
      "tensor_out[473] = \t 0\n",
      "tensor_out[474] = \t 0\n",
      "tensor_out[475] = \t 0\n",
      "tensor_out[476] = \t 0\n",
      "tensor_out[477] = \t 0\n",
      "tensor_out[478] = \t 0\n",
      "tensor_out[479] = \t 0\n",
      "tensor_out[480] = \t 0\n",
      "tensor_out[481] = \t 0\n",
      "tensor_out[482] = \t 0\n",
      "tensor_out[483] = \t 0\n",
      "tensor_out[484] = \t 0\n",
      "tensor_out[485] = \t 0\n",
      "tensor_out[486] = \t 0\n",
      "tensor_out[487] = \t 0\n",
      "tensor_out[488] = \t 0\n",
      "tensor_out[489] = \t 0\n",
      "tensor_out[490] = \t 0\n",
      "tensor_out[491] = \t 0\n",
      "tensor_out[492] = \t 0\n",
      "tensor_out[493] = \t 0\n",
      "tensor_out[494] = \t 0\n",
      "tensor_out[495] = \t 0\n",
      "tensor_out[496] = \t 0\n",
      "tensor_out[497] = \t 0\n",
      "tensor_out[498] = \t 0\n",
      "tensor_out[499] = \t 0\n",
      "tensor_out[500] = \t 0\n",
      "tensor_out[501] = \t 0\n",
      "tensor_out[502] = \t 0\n",
      "tensor_out[503] = \t 0\n",
      "tensor_out[504] = \t 0\n",
      "tensor_out[505] = \t 0\n",
      "tensor_out[506] = \t 0\n",
      "tensor_out[507] = \t 0\n",
      "tensor_out[508] = \t 0\n",
      "tensor_out[509] = \t 0\n",
      "tensor_out[510] = \t 0\n",
      "tensor_out[511] = \t 0\n",
      "tensor_out[512] = \t 0\n",
      "tensor_out[513] = \t 0\n",
      "tensor_out[514] = \t 0\n",
      "tensor_out[515] = \t 0\n",
      "tensor_out[516] = \t 0\n",
      "tensor_out[517] = \t 0\n",
      "tensor_out[518] = \t 0\n",
      "tensor_out[519] = \t 0\n",
      "tensor_out[520] = \t 0\n",
      "tensor_out[521] = \t 0\n",
      "tensor_out[522] = \t 0\n",
      "tensor_out[523] = \t 0\n",
      "tensor_out[524] = \t 0\n",
      "tensor_out[525] = \t 0\n",
      "tensor_out[526] = \t 0\n",
      "tensor_out[527] = \t 0\n",
      "tensor_out[528] = \t 0\n",
      "tensor_out[529] = \t 0\n",
      "tensor_out[530] = \t 0\n",
      "tensor_out[531] = \t 0\n",
      "tensor_out[532] = \t 0\n",
      "tensor_out[533] = \t 0\n",
      "tensor_out[534] = \t 0\n",
      "tensor_out[535] = \t 0\n",
      "tensor_out[536] = \t 0\n",
      "tensor_out[537] = \t 0\n",
      "tensor_out[538] = \t 0\n",
      "tensor_out[539] = \t 0\n",
      "tensor_out[540] = \t 0\n",
      "tensor_out[541] = \t 0\n",
      "tensor_out[542] = \t 0\n",
      "tensor_out[543] = \t 0\n",
      "tensor_out[544] = \t 0\n",
      "tensor_out[545] = \t 0\n",
      "tensor_out[546] = \t 0\n",
      "tensor_out[547] = \t 0\n",
      "tensor_out[548] = \t 0\n",
      "tensor_out[549] = \t 0\n",
      "tensor_out[550] = \t 0\n",
      "tensor_out[551] = \t 0\n",
      "tensor_out[552] = \t 0\n",
      "tensor_out[553] = \t 0\n",
      "tensor_out[554] = \t 0\n",
      "tensor_out[555] = \t 0\n",
      "tensor_out[556] = \t 0\n",
      "tensor_out[557] = \t 0\n",
      "tensor_out[558] = \t 0\n",
      "tensor_out[559] = \t 0\n",
      "tensor_out[560] = \t 0\n",
      "tensor_out[561] = \t 0\n",
      "tensor_out[562] = \t 0\n",
      "tensor_out[563] = \t 0\n",
      "tensor_out[564] = \t 0\n",
      "tensor_out[565] = \t 0\n",
      "tensor_out[566] = \t 0\n",
      "tensor_out[567] = \t 0\n",
      "tensor_out[568] = \t 0\n",
      "tensor_out[569] = \t 0\n",
      "tensor_out[570] = \t 0\n",
      "tensor_out[571] = \t 0\n",
      "tensor_out[572] = \t 0\n",
      "tensor_out[573] = \t 0\n",
      "tensor_out[574] = \t 0\n",
      "tensor_out[575] = \t 0\n",
      "tensor_out[576] = \t 0\n",
      "tensor_out[577] = \t 0\n",
      "tensor_out[578] = \t 0\n",
      "tensor_out[579] = \t 0\n",
      "tensor_out[580] = \t 0\n",
      "tensor_out[581] = \t 0\n",
      "tensor_out[582] = \t 0\n",
      "tensor_out[583] = \t 0\n",
      "tensor_out[584] = \t 0\n",
      "tensor_out[585] = \t 0\n",
      "tensor_out[586] = \t 0\n",
      "tensor_out[587] = \t 0\n",
      "tensor_out[588] = \t 0\n",
      "tensor_out[589] = \t 0\n",
      "tensor_out[590] = \t 0\n",
      "tensor_out[591] = \t 0\n",
      "tensor_out[592] = \t 0\n",
      "tensor_out[593] = \t 0\n",
      "tensor_out[594] = \t 0\n",
      "tensor_out[595] = \t 0\n",
      "tensor_out[596] = \t 0\n",
      "tensor_out[597] = \t 0\n",
      "tensor_out[598] = \t 0\n",
      "tensor_out[599] = \t 0\n",
      "tensor_out[600] = \t 0\n",
      "tensor_out[601] = \t 0\n",
      "tensor_out[602] = \t 0\n",
      "tensor_out[603] = \t 0\n",
      "tensor_out[604] = \t 0\n",
      "tensor_out[605] = \t 0\n",
      "tensor_out[606] = \t 0\n",
      "tensor_out[607] = \t 0\n",
      "tensor_out[608] = \t 0\n",
      "tensor_out[609] = \t 0\n",
      "tensor_out[610] = \t 0\n",
      "tensor_out[611] = \t 0\n",
      "tensor_out[612] = \t 0\n",
      "tensor_out[613] = \t 0\n",
      "tensor_out[614] = \t 0\n",
      "tensor_out[615] = \t 0\n",
      "tensor_out[616] = \t 0\n",
      "tensor_out[617] = \t 0\n",
      "tensor_out[618] = \t 0\n",
      "tensor_out[619] = \t 0\n",
      "tensor_out[620] = \t 0\n",
      "tensor_out[621] = \t 0\n",
      "tensor_out[622] = \t 0\n",
      "tensor_out[623] = \t 0\n",
      "tensor_out[624] = \t 0\n",
      "tensor_out[625] = \t 0\n",
      "tensor_out[626] = \t 0\n",
      "tensor_out[627] = \t 0\n",
      "tensor_out[628] = \t 0\n",
      "tensor_out[629] = \t 0\n",
      "tensor_out[630] = \t 0\n",
      "tensor_out[631] = \t 0\n",
      "tensor_out[632] = \t 0\n",
      "tensor_out[633] = \t 0\n",
      "tensor_out[634] = \t 0\n",
      "tensor_out[635] = \t 0\n",
      "tensor_out[636] = \t 0\n",
      "tensor_out[637] = \t 0\n",
      "tensor_out[638] = \t 0\n",
      "tensor_out[639] = \t 0\n",
      "tensor_out[640] = \t 0\n",
      "tensor_out[641] = \t 0\n",
      "tensor_out[642] = \t 0\n",
      "tensor_out[643] = \t 0\n",
      "tensor_out[644] = \t 0\n",
      "tensor_out[645] = \t 0\n",
      "tensor_out[646] = \t 0\n",
      "tensor_out[647] = \t 0\n",
      "tensor_out[648] = \t 0\n",
      "tensor_out[649] = \t 0\n",
      "tensor_out[650] = \t 0\n",
      "tensor_out[651] = \t 0\n",
      "tensor_out[652] = \t 0\n",
      "tensor_out[653] = \t 0\n",
      "tensor_out[654] = \t 0\n",
      "tensor_out[655] = \t 0\n",
      "tensor_out[656] = \t 0\n",
      "tensor_out[657] = \t 0\n",
      "tensor_out[658] = \t 0\n",
      "tensor_out[659] = \t 0\n",
      "tensor_out[660] = \t 0\n",
      "tensor_out[661] = \t 0\n",
      "tensor_out[662] = \t 0\n",
      "tensor_out[663] = \t 0\n",
      "tensor_out[664] = \t 0\n",
      "tensor_out[665] = \t 0\n",
      "tensor_out[666] = \t 0\n",
      "tensor_out[667] = \t 0\n",
      "tensor_out[668] = \t 0\n",
      "tensor_out[669] = \t 0\n",
      "tensor_out[670] = \t 0\n",
      "tensor_out[671] = \t 0\n",
      "tensor_out[672] = \t 0\n",
      "tensor_out[673] = \t 0\n",
      "tensor_out[674] = \t 0\n",
      "tensor_out[675] = \t 0\n",
      "tensor_out[676] = \t 0\n",
      "tensor_out[677] = \t 0\n",
      "tensor_out[678] = \t 0\n",
      "tensor_out[679] = \t 0\n",
      "tensor_out[680] = \t 0\n",
      "tensor_out[681] = \t 0\n",
      "tensor_out[682] = \t 0\n",
      "tensor_out[683] = \t 0\n",
      "tensor_out[684] = \t 0\n",
      "tensor_out[685] = \t 0\n",
      "tensor_out[686] = \t 0\n",
      "tensor_out[687] = \t 0\n",
      "tensor_out[688] = \t 0\n",
      "tensor_out[689] = \t 0\n",
      "tensor_out[690] = \t 0\n",
      "tensor_out[691] = \t 0\n",
      "tensor_out[692] = \t 0\n",
      "tensor_out[693] = \t 0\n",
      "tensor_out[694] = \t 0\n",
      "tensor_out[695] = \t 0\n",
      "tensor_out[696] = \t 0\n",
      "tensor_out[697] = \t 0\n",
      "tensor_out[698] = \t 0\n",
      "tensor_out[699] = \t 0\n",
      "tensor_out[700] = \t 0\n",
      "tensor_out[701] = \t 0\n",
      "tensor_out[702] = \t 0\n",
      "tensor_out[703] = \t 0\n",
      "tensor_out[704] = \t 0\n",
      "tensor_out[705] = \t 0\n",
      "tensor_out[706] = \t 0\n",
      "tensor_out[707] = \t 0\n",
      "tensor_out[708] = \t 0\n",
      "tensor_out[709] = \t 0\n",
      "tensor_out[710] = \t 0\n",
      "tensor_out[711] = \t 0\n",
      "tensor_out[712] = \t 0\n",
      "tensor_out[713] = \t 0\n",
      "tensor_out[714] = \t 0\n",
      "tensor_out[715] = \t 0\n",
      "tensor_out[716] = \t 0\n",
      "tensor_out[717] = \t 0\n",
      "tensor_out[718] = \t 0\n",
      "tensor_out[719] = \t 0\n",
      "tensor_out[720] = \t 0\n",
      "tensor_out[721] = \t 0\n",
      "tensor_out[722] = \t 0\n",
      "tensor_out[723] = \t 0\n",
      "tensor_out[724] = \t 0\n",
      "tensor_out[725] = \t 0\n",
      "tensor_out[726] = \t 0\n",
      "tensor_out[727] = \t 0\n",
      "tensor_out[728] = \t 0\n",
      "tensor_out[729] = \t 0\n",
      "tensor_out[730] = \t 0\n",
      "tensor_out[731] = \t 0\n",
      "tensor_out[732] = \t 0\n",
      "tensor_out[733] = \t 0\n",
      "tensor_out[734] = \t 0\n",
      "tensor_out[735] = \t 0\n",
      "tensor_out[736] = \t 0\n",
      "tensor_out[737] = \t 0\n",
      "tensor_out[738] = \t 0\n",
      "tensor_out[739] = \t 0\n",
      "tensor_out[740] = \t 0\n",
      "tensor_out[741] = \t 0\n",
      "tensor_out[742] = \t 0\n",
      "tensor_out[743] = \t 0\n",
      "tensor_out[744] = \t 0\n",
      "tensor_out[745] = \t 0\n",
      "tensor_out[746] = \t 0\n",
      "tensor_out[747] = \t 0\n",
      "tensor_out[748] = \t 0\n",
      "tensor_out[749] = \t 0\n",
      "tensor_out[750] = \t 0\n",
      "tensor_out[751] = \t 0\n",
      "tensor_out[752] = \t 0\n",
      "tensor_out[753] = \t 0\n",
      "tensor_out[754] = \t 0\n",
      "tensor_out[755] = \t 0\n",
      "tensor_out[756] = \t 0\n",
      "tensor_out[757] = \t 0\n",
      "tensor_out[758] = \t 0\n",
      "tensor_out[759] = \t 0\n",
      "tensor_out[760] = \t 0\n",
      "tensor_out[761] = \t 0\n",
      "tensor_out[762] = \t 0\n",
      "tensor_out[763] = \t 0\n",
      "tensor_out[764] = \t 0\n",
      "tensor_out[765] = \t 0\n",
      "tensor_out[766] = \t 0\n",
      "tensor_out[767] = \t 0\n",
      "tensor_out[768] = \t 0\n",
      "tensor_out[769] = \t 0\n",
      "tensor_out[770] = \t 0\n",
      "tensor_out[771] = \t 0\n",
      "tensor_out[772] = \t 0\n",
      "tensor_out[773] = \t 0\n",
      "tensor_out[774] = \t 0\n",
      "tensor_out[775] = \t 0\n",
      "tensor_out[776] = \t 0\n",
      "tensor_out[777] = \t 0\n",
      "tensor_out[778] = \t 0\n",
      "tensor_out[779] = \t 0\n",
      "tensor_out[780] = \t 0\n",
      "tensor_out[781] = \t 0\n",
      "tensor_out[782] = \t 0\n",
      "tensor_out[783] = \t 0\n",
      "tensor_out[784] = \t 0\n",
      "tensor_out[785] = \t 0\n",
      "tensor_out[786] = \t 0\n",
      "tensor_out[787] = \t 0\n",
      "tensor_out[788] = \t 0\n",
      "tensor_out[789] = \t 0\n",
      "tensor_out[790] = \t 0\n",
      "tensor_out[791] = \t 0\n",
      "tensor_out[792] = \t 0\n",
      "tensor_out[793] = \t 0\n",
      "tensor_out[794] = \t 0\n",
      "tensor_out[795] = \t 0\n",
      "tensor_out[796] = \t 0\n",
      "tensor_out[797] = \t 0\n",
      "tensor_out[798] = \t 0\n",
      "tensor_out[799] = \t 0\n",
      "tensor_out[800] = \t 0\n",
      "tensor_out[801] = \t 0\n",
      "tensor_out[802] = \t 0\n",
      "tensor_out[803] = \t 0\n",
      "tensor_out[804] = \t 0\n",
      "tensor_out[805] = \t 0\n",
      "tensor_out[806] = \t 0\n",
      "tensor_out[807] = \t 0\n",
      "tensor_out[808] = \t 0\n",
      "tensor_out[809] = \t 0\n",
      "tensor_out[810] = \t 0\n",
      "tensor_out[811] = \t 0\n",
      "tensor_out[812] = \t 0\n",
      "tensor_out[813] = \t 0\n",
      "tensor_out[814] = \t 0\n",
      "tensor_out[815] = \t 0\n",
      "tensor_out[816] = \t 0\n",
      "tensor_out[817] = \t 0\n",
      "tensor_out[818] = \t 0\n",
      "tensor_out[819] = \t 0\n",
      "tensor_out[820] = \t 0\n",
      "tensor_out[821] = \t 0\n",
      "tensor_out[822] = \t 0\n",
      "tensor_out[823] = \t 0\n",
      "tensor_out[824] = \t 0\n",
      "tensor_out[825] = \t 0\n",
      "tensor_out[826] = \t 0\n",
      "tensor_out[827] = \t 0\n",
      "tensor_out[828] = \t 0\n",
      "tensor_out[829] = \t 0\n",
      "tensor_out[830] = \t 0\n",
      "tensor_out[831] = \t 0\n",
      "tensor_out[832] = \t 0\n",
      "tensor_out[833] = \t 0\n",
      "tensor_out[834] = \t 0\n",
      "tensor_out[835] = \t 0\n",
      "tensor_out[836] = \t 0\n",
      "tensor_out[837] = \t 0\n",
      "tensor_out[838] = \t 0\n",
      "tensor_out[839] = \t 0\n",
      "tensor_out[840] = \t 0\n",
      "tensor_out[841] = \t 0\n",
      "tensor_out[842] = \t 0\n",
      "tensor_out[843] = \t 0\n",
      "tensor_out[844] = \t 0\n",
      "tensor_out[845] = \t 0\n",
      "tensor_out[846] = \t 0\n",
      "tensor_out[847] = \t 0\n",
      "tensor_out[848] = \t 0\n",
      "tensor_out[849] = \t 0\n",
      "tensor_out[850] = \t 0\n",
      "tensor_out[851] = \t 0\n",
      "tensor_out[852] = \t 0\n",
      "tensor_out[853] = \t 0\n",
      "tensor_out[854] = \t 0\n",
      "tensor_out[855] = \t 0\n",
      "tensor_out[856] = \t 0\n",
      "tensor_out[857] = \t 0\n",
      "tensor_out[858] = \t 0\n",
      "tensor_out[859] = \t 0\n",
      "tensor_out[860] = \t 0\n",
      "tensor_out[861] = \t 0\n",
      "tensor_out[862] = \t 0\n",
      "tensor_out[863] = \t 0\n",
      "tensor_out[864] = \t 0\n",
      "tensor_out[865] = \t 0\n",
      "tensor_out[866] = \t 0\n",
      "tensor_out[867] = \t 0\n",
      "tensor_out[868] = \t 0\n",
      "tensor_out[869] = \t 0\n",
      "tensor_out[870] = \t 0\n",
      "tensor_out[871] = \t 0\n",
      "tensor_out[872] = \t 0\n",
      "tensor_out[873] = \t 0\n",
      "tensor_out[874] = \t 0\n",
      "tensor_out[875] = \t 0\n",
      "tensor_out[876] = \t 0\n",
      "tensor_out[877] = \t 0\n",
      "tensor_out[878] = \t 0\n",
      "tensor_out[879] = \t 0\n",
      "tensor_out[880] = \t 0\n",
      "tensor_out[881] = \t 0\n",
      "tensor_out[882] = \t 0\n",
      "tensor_out[883] = \t 0\n",
      "tensor_out[884] = \t 0\n",
      "tensor_out[885] = \t 0\n",
      "tensor_out[886] = \t 0\n",
      "tensor_out[887] = \t 0\n",
      "tensor_out[888] = \t 0\n",
      "tensor_out[889] = \t 0\n",
      "tensor_out[890] = \t 0\n",
      "tensor_out[891] = \t 0\n",
      "tensor_out[892] = \t 0\n",
      "tensor_out[893] = \t 0\n",
      "tensor_out[894] = \t 0\n",
      "tensor_out[895] = \t 0\n",
      "tensor_out[896] = \t 0\n",
      "tensor_out[897] = \t 0\n",
      "tensor_out[898] = \t 0\n",
      "tensor_out[899] = \t 0\n",
      "tensor_out[900] = \t 0\n",
      "tensor_out[901] = \t 0\n",
      "tensor_out[902] = \t 0\n",
      "tensor_out[903] = \t 0\n",
      "tensor_out[904] = \t 0\n",
      "tensor_out[905] = \t 0\n",
      "tensor_out[906] = \t 0\n",
      "tensor_out[907] = \t 0\n",
      "tensor_out[908] = \t 0\n",
      "tensor_out[909] = \t 0\n",
      "tensor_out[910] = \t 0\n",
      "tensor_out[911] = \t 0\n",
      "tensor_out[912] = \t 0\n",
      "tensor_out[913] = \t 0\n",
      "tensor_out[914] = \t 0\n",
      "tensor_out[915] = \t 0\n",
      "tensor_out[916] = \t 0\n",
      "tensor_out[917] = \t 0\n",
      "tensor_out[918] = \t 0\n",
      "tensor_out[919] = \t 0\n",
      "tensor_out[920] = \t 0\n",
      "tensor_out[921] = \t 0\n",
      "tensor_out[922] = \t 0\n",
      "tensor_out[923] = \t 0\n",
      "tensor_out[924] = \t 0\n",
      "tensor_out[925] = \t 0\n",
      "tensor_out[926] = \t 0\n",
      "tensor_out[927] = \t 0\n",
      "tensor_out[928] = \t 0\n",
      "tensor_out[929] = \t 0\n",
      "tensor_out[930] = \t 0\n",
      "tensor_out[931] = \t 0\n",
      "tensor_out[932] = \t 0\n",
      "tensor_out[933] = \t 0\n",
      "tensor_out[934] = \t 0\n",
      "tensor_out[935] = \t 0\n",
      "tensor_out[936] = \t 0\n",
      "tensor_out[937] = \t 0\n",
      "tensor_out[938] = \t 0\n",
      "tensor_out[939] = \t 0\n",
      "tensor_out[940] = \t 0\n",
      "tensor_out[941] = \t 0\n",
      "tensor_out[942] = \t 0\n",
      "tensor_out[943] = \t 0\n",
      "tensor_out[944] = \t 0\n",
      "tensor_out[945] = \t 0\n",
      "tensor_out[946] = \t 0\n",
      "tensor_out[947] = \t 0\n",
      "tensor_out[948] = \t 0\n",
      "tensor_out[949] = \t 0\n",
      "tensor_out[950] = \t 0\n",
      "tensor_out[951] = \t 0\n",
      "tensor_out[952] = \t 0\n",
      "tensor_out[953] = \t 0\n",
      "tensor_out[954] = \t 0\n",
      "tensor_out[955] = \t 0\n",
      "tensor_out[956] = \t 0\n",
      "tensor_out[957] = \t 0\n",
      "tensor_out[958] = \t 0\n",
      "tensor_out[959] = \t 0\n",
      "tensor_out[960] = \t 0\n",
      "tensor_out[961] = \t 0\n",
      "tensor_out[962] = \t 0\n",
      "tensor_out[963] = \t 0\n",
      "tensor_out[964] = \t 0\n",
      "tensor_out[965] = \t 0\n",
      "tensor_out[966] = \t 0\n",
      "tensor_out[967] = \t 0\n",
      "tensor_out[968] = \t 0\n",
      "tensor_out[969] = \t 0\n",
      "tensor_out[970] = \t 0\n",
      "tensor_out[971] = \t 0\n",
      "tensor_out[972] = \t 0\n",
      "tensor_out[973] = \t 0\n",
      "tensor_out[974] = \t 0\n",
      "tensor_out[975] = \t 0\n",
      "tensor_out[976] = \t 0\n",
      "tensor_out[977] = \t 0\n",
      "tensor_out[978] = \t 0\n",
      "tensor_out[979] = \t 0\n",
      "tensor_out[980] = \t 0\n",
      "tensor_out[981] = \t 0\n",
      "tensor_out[982] = \t 0\n",
      "tensor_out[983] = \t 0\n",
      "tensor_out[984] = \t 0\n",
      "tensor_out[985] = \t 0\n",
      "tensor_out[986] = \t 0\n",
      "tensor_out[987] = \t 0\n",
      "tensor_out[988] = \t 0\n",
      "tensor_out[989] = \t 0\n",
      "tensor_out[990] = \t 0\n",
      "tensor_out[991] = \t 0\n",
      "tensor_out[992] = \t 0\n",
      "tensor_out[993] = \t 0\n",
      "tensor_out[994] = \t 0\n",
      "tensor_out[995] = \t 0\n",
      "tensor_out[996] = \t 0\n",
      "tensor_out[997] = \t 0\n",
      "tensor_out[998] = \t 0\n",
      "tensor_out[999] = \t 0\n",
      "tensor_out[1000] = \t 0\n",
      "tensor_out[1001] = \t 0\n",
      "tensor_out[1002] = \t 0\n",
      "tensor_out[1003] = \t 0\n",
      "tensor_out[1004] = \t 0\n",
      "tensor_out[1005] = \t 0\n",
      "tensor_out[1006] = \t 0\n",
      "tensor_out[1007] = \t 0\n",
      "tensor_out[1008] = \t 0\n",
      "tensor_out[1009] = \t 0\n",
      "tensor_out[1010] = \t 0\n",
      "tensor_out[1011] = \t 0\n",
      "tensor_out[1012] = \t 0\n",
      "tensor_out[1013] = \t 0\n",
      "tensor_out[1014] = \t 0\n",
      "tensor_out[1015] = \t 93\n",
      "tensor_out[1016] = \t 0\n",
      "tensor_out[1017] = \t 0\n",
      "tensor_out[1018] = \t 0\n",
      "tensor_out[1019] = \t 0\n",
      "tensor_out[1020] = \t 0\n",
      "tensor_out[1021] = \t 0\n",
      "tensor_out[1022] = \t 0\n",
      "tensor_out[1023] = \t 0\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print of the output aggregated as they appear in the deployed network\n",
    "for i in range(nlayer):\n",
    "    \n",
    "    y=out_list[i]\n",
    "    output_array_aggregate = torch.Tensor((y.size(1))*(y.size(3)))\n",
    "    index_arr = 0\n",
    "    print('EXPECTED tensor_out layer{} RESULTS AFTER QUANTIZATION: \\n' .format(i+1))\n",
    "   # for k in range(y.size(3)):\n",
    "    k = 0\n",
    "    for p in range(y.size(1)):\n",
    "        output_array_aggregate[index_arr] = y[0][p][0][k]\n",
    "        if (param_list[i]['act_o_bits'] == 8):\n",
    "            print('tensor_out[{}] = \\t' .format(int(index_arr)), end=' ')\n",
    "            print(int(output_array_aggregate[index_arr].item())) \n",
    "        if (param_list[i]['act_o_bits'] == 4):\n",
    "            if (index_arr % 2):\n",
    "                print('tensor_out[{}] = \\t' .format(int(index_arr/2)), end=' ')\n",
    "                print(int(output_array_aggregate[index_arr].item()),'-',int(output_array_aggregate[index_arr-1].item())) \n",
    "        elif (param_list[i]['act_o_bits'] == 2):\n",
    "            if not(index_arr % 4) and (index_arr != 0):\n",
    "                print('tensor_out[{}] = \\t' .format(int(index_arr/4)), end=' ')\n",
    "                print(int(output_array_aggregate[index_arr-1].item()),'-',int(output_array_aggregate[index_arr-2].item()),'-',int(output_array_aggregate[index_arr-3].item()),'-',int(output_array_aggregate[index_arr-4].item())) \n",
    "        index_arr += 1\n",
    "    print(' \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPECTED tensor_out layer8 RESULTS AFTER QUANTIZATION: \n",
      "\n",
      "static const GOLDEN_MODEL8[] = {255, 158, 0, 0, 61, 144, 104, 16, 0, 102, 7, 0, 57, 0, 168, 51, 0, 0, 35, 37, 21, 68, 171, 24, 205, 33, 106, 113, 5, 54, 0, 11, 0, 101, 4, 86, 62, 90, 255, 255, 0, 23, 16, 11, 151, 37, 93, 0, 5, 235, 100, 22, 107, 27, 23, 0, 255, 0, 55, 69, 40, 26, 76, 38, 0, 7, 0, 23, 77, 95, 48, 64, 3, 153, 0, 23, 156, 0, 111, 219, 0, 74, 0, 255, 220, 0, 140, 12, 96, 0, 82, 89, 31, 255, 0, 4, 2, 1, 0, 0, 16, 54, 35, 36, 158, 31, 0, 0, 0, 0, 89, 29, 102, 119, 144, 0, 0, 96, 142, 55, 176, 63, 124, 22, 0, 164, 0, 142, 121, 150, 0, 0, 89, 25, 93, 10, 0, 72, 21, 0, 69, 22, 150, 60, 0, 7, 63, 66, 40, 68, 187, 25, 155, 7, 71, 49, 78, 118, 8, 11, 0, 27, 4, 37, 129, 158, 229, 212, 0, 23, 0, 52, 169, 0, 93, 0, 5, 220, 130, 2, 144, 27, 59, 0, 226, 0, 20, 42, 0, 1, 126, 22, 7, 7, 0, 19, 117, 169, 53, 74, 20, 154, 31, 28, 141, 58, 58, 242, 0, 30, 0, 255, 207, 74, 101, 12, 1, 0, 113, 74, 51, 165, 0, 0, 5, 59, 34, 40, 32, 18, 51, 9, 144, 13, 0, 0, 0, 0, 86, 71, 81, 177, 1, 0, 0, 28, 163, 0, 157, 2, 104, 0, 9, 112, 19, 237, 93, 112, 0, 0, 62, 97, 114, 17, 0, 68, 5, 0, 69, 0, 140, 66, 20, 0, 40, 67, 55, 55, 150, 21, 201, 22, 161, 79, 50, 55, 0, 18, 0, 41, 4, 19, 76, 169, 220, 255, 0, 0, 0, 0, 169, 0, 89, 16, 28, 220, 131, 15, 168, 27, 0, 0, 255, 0, 26, 26, 22, 13, 195, 31, 0, 7, 0, 40, 85, 137, 71, 73, 3, 173, 4, 34, 246, 0, 81, 172, 14, 29, 0, 255, 232, 74, 103, 12, 1, 0, 136, 69, 58, 172, 0, 0, 0, 18, 18, 28, 55, 1, 55, 2, 137, 13, 0, 0, 0, 1, 111, 27, 117, 234, 90, 0, 0, 53, 160, 0, 181, 29, 176, 0, 0, 128, 46, 237, 171, 182, 0, 0, 91, 96, 87, 15, 0, 89, 5, 0, 62, 0, 128, 12, 0, 0, 43, 52, 20, 42, 172, 21, 194, 28, 137, 42, 78, 93, 16, 11, 0, 65, 4, 123, 72, 96, 241, 255, 16, 5, 0, 89, 94, 115, 76, 17, 60, 236, 45, 21, 151, 27, 27, 0, 255, 20, 61, 18, 0, 19, 86, 36, 0, 7, 6, 126, 89, 138, 86, 71, 3, 158, 25, 12, 35, 9, 74, 246, 21, 33, 0, 255, 209, 57, 126, 12, 1, 0, 148, 97, 7, 191, 97, 45, 0, 12, 0, 0, 19, 28, 7, 57, 168, 13, 0, 105, 0, 0, 130, 0, 98, 151, 104, 0, 0, 57, 165, 7, 187, 51, 211, 0, 0, 145, 3, 237, 214, 140, 0, 0, 72, 144, 95, 22, 0, 77, 5, 0, 42, 6, 154, 74, 0, 0, 20, 0, 17, 30, 67, 46, 175, 46, 75, 53, 78, 90, 32, 11, 0, 55, 4, 66, 69, 9, 255, 152, 29, 0, 0, 56, 134, 0, 93, 0, 61, 235, 44, 35, 148, 27, 0, 32, 223, 0, 98, 87, 0, 51, 119, 22, 16, 7, 0, 76, 105, 173, 0, 71, 3, 135, 163, 25, 16, 0, 36, 255, 77, 44, 24, 235, 185, 44, 113, 12, 49, 0, 201, 76, 0, 176, 0, 0, 0, 26, 0, 0, 20, 68, 8, 172, 190, 13, 0, 16, 10, 1, 52, 0, 107, 78, 88, 0, 0, 38, 70, 0, 185, 58, 160, 0, 39, 89, 35, 237, 224, 149, 0, 0, 66, 108, 134, 20, 0, 74, 21, 45, 30, 14, 145, 47, 0, 0, 32, 44, 27, 0, 112, 148, 197, 34, 53, 24, 78, 101, 61, 11, 0, 54, 4, 75, 58, 84, 255, 202, 3, 48, 0, 44, 141, 81, 86, 0, 67, 145, 76, 39, 183, 27, 0, 63, 255, 40, 30, 94, 0, 50, 71, 27, 59, 7, 0, 61, 83, 159, 0, 69, 3, 115, 29, 56, 16, 0, 29, 207, 3, 41, 0, 255, 143, 67, 113, 12, 1, 0, 120, 76, 0, 150, 0, 46, 0, 21, 0, 0, 22, 48, 11, 32, 200, 13, 0, 0, 0, 0, 91, 0, 102, 118, 77, 0, 0, 28, 90, 0, 199, 70, 210, 38, 36, 168, 6, 237, 185, 163, 0, 43, 80, 81, 81, 33, 0, 96, 20, 32, 59, 0, 159, 162, 0, 0, 50, 0, 57, 55, 154, 52, 207, 30, 36, 48, 71, 65, 52, 27, 0, 29, 4, 122, 28, 46, 255, 159, 0, 37, 0, 0, 169, 18, 82, 59, 36, 255, 128, 24, 183, 27, 72, 19, 236, 50, 129, 49, 0, 3, 149, 73, 0, 7, 0, 47, 111, 172, 52, 67, 3, 176, 1, 62, 110, 38, 19, 180, 47, 65, 0, 211, 131, 35, 140, 12, 10, 0, 103, 39, 0, 105, 0, 0, 0, 24, 67, 26, 4, 66, 35, 10, 156, 13, 0, 0, 0, 0, 65, 28, 71, 141, 1, 0, 0, 28, 44, 0, 199, 70, 141, 0, 0, 195, 37, 211, 160, 186, 0, 0, 76, 96, 99, 34, 0, 75, 27, 0, 69, 2, 166, 113, 4, 0, 75, 0, 33, 54, 174, 31, 190, 50, 72, 81, 70, 122, 38, 38, 0, 79, 4, 101, 54, 152, 249, 255, 14, 0, 0, 59, 163, 0, 83, 0, 43, 255, 120, 12, 183, 27, 43, 0, 226, 10, 65, 30, 0, 15, 97, 26, 0, 7, 0, 71, 100, 117, 0, 79, 3, 162, 1, 42, 113, 0, 24, 207, 21, 31, 0, 255, 181, 15, 120, 12, 11, 0, 104, 60, 0, 154, 0, 0, 0, 81, 6, 5, 16, 6, 26, 21, 96, 13, 0, 44, 0, 17, 88, 26, 95, 196, 68, 0, 0, 97, 132, 0, 199, 15, 119, 1, 10, 120, 27, 237, 119, 141, 0, 0, 78, 86, 82, 4, 0, 78, 5, 0, 69, 5, 151, 88, 0, 0, 7, 0, 52, 68, 177, 41, 195, 34, 65, 33, 70, 46, 0, 11, 0, 53, 4, 15, 28, 91, 239, 255, 0, 2, 27, 156, 155, 103, 93, 0, 5, 242, 37, 33, 176, 27, 50, 65, 246, 0, 124, 42, 20, 32, 83, 41, 24, 7, 0, 94, 73, 111, 63, 79, 3, 169, 17, 50, 48, 0, 7, 164, 33, 21, 0, 234, 137, 57, 96, 12, 1, 0, 44, 74, 14, 169, 24, 0, 0, 33, 28, 69, 4, 41, 24, 30, 147, 13, 0, 69, 0, 0, 95, 49, 51, 201, 110, 0, 0, 41, 134, 0, 199, 27, 113, 7, 19, 137, 30, 237, 134, 143, 0, 0, 81, 99, 158, 13, 0, 78, 5, 11, 69, 0, 154, 68, 40, 0, 88, 57, 44, 58, 141, 29, 207, 5, 126, 83, 63, 119, 12, 39, 0, 23, 4, 47, 45, 52, 248, 253, 26, 0, 0, 0, 145, 0, 76, 49, 16, 255, 96, 33, 175, 27, 56, 0, 245, 18, 53, 13, 39, 14, 166, 40, 0, 7, 0, 45, 116, 150, 44, 44, 3, 155, 0, 61, 76, 0, 19, 217, 79, 35, 15, 223, 136, 44, 99, 12, 1, 0, 113, 53, 24, 196, 0, 41, 0, 12, 0, 27, 4, 71, 30, 38, 147, 13, 0, 32, 0, 14, 102, 53, 85, 130, 37, 0, 0, 72, 48, 0, 199, 52, 169, 1, 22, 134, 10, 237, 175, 141, 0, 0, 40, 113, 84, 16, 0, 101, 5, 0, 69, 0, 156, 25, 0, 0, 43, 64, 36, 43, 167, 34, 200, 30, 159, 44, 70, 80, 61, 30, 0, 82, 4, 62, 83, 105, 255, 250, 26, 100, 0, 9, 153, 0, 74, 15, 34, 255, 123, 26, 143, 27, 36, 0, 255, 0, 38, 0, 37, 30, 124, 35, 0, 7, 0, 52, 110, 148, 12, 38, 3, 115, 2, 44, 112, 28, 51, 203, 16, 47, 0, 255, 201, 28, 121, 12, 1, 0, 129, 33, 0, 170, 0, 25, 0, 73, 0, 0, 7, 12, 15, 16, 172, 13, 0, 0, 12, 0, 149, 35, 91, 174, 1, 0, 0, 40, 86, 20, 199, 65, 184, 0, 59, 117, 11, 237, 164, 232, 0, 0, 74, 85, 122, 40, 0, 80, 5, 0, 69, 2, 169, 6, 0, 0, 43, 67, 34, 68, 174, 39, 200, 0, 61, 76, 78, 116, 0, 40, 19, 41, 4, 86, 128, 145, 255, 255, 0, 0, 53, 0, 157, 10, 89, 47, 38, 255, 122, 35, 170, 27, 18, 0, 230, 19, 66, 45, 0, 4, 88, 28, 0, 7, 0, 52, 97, 131, 100, 75, 3, 103, 0, 15, 172, 0, 49, 161, 7, 36, 45, 255, 178, 58, 135, 12, 14, 0, 82, 45, 0, 161, 0, 41, 0, 35, 42, 0, 12, 18, 50, 9, 152, 13, 0, 0, 0, 0, 136, 0, 103, 224, 72, 0, 0, 34, 154, 20, 181, 16, 103, 0, 0, 142, 32, 237, 200, 142, 0, 0, 94, 176, 113, 21, 0, 81, 54, 56, 38, 37, 177, 53, 0, 0, 94, 0, 23, 24, 161, 25, 200, 8, 29, 55, 53, 69, 0, 24, 0, 72, 88, 107, 85, 147, 255, 210, 19, 9, 37, 72, 164, 0, 93, 0, 14, 222, 137, 22, 168, 27, 0, 62, 255, 0, 175, 0, 0, 9, 95, 101, 0, 7, 23, 34, 78, 137, 166, 72, 3, 107, 11, 22, 74, 0, 39, 233, 0, 30, 61, 253, 129, 54, 131, 12, 77, 0, 91, 81, 0, 150, 0, 0, 0, 16, 0, 7, 22, 32, 84, 10, 132, 31, 0, 103, 0, 0, 77, 75, 46, 152, 182, 0, 0, 43, 79, 0, 178, 36, 114, 5, 48, 205, 25, 185, 157, 152, 0, 0, 79, 157, 108, 29, 0, 93, 5, 0, 38, 48, 170, 89, 0, 23, 29, 0, 43, 68, 159, 60, 186, 16, 105, 67, 57, 62, 0, 38, 0, 28, 19, 104, 117, 21, 255, 255, 11, 0, 47, 53, 153, 40, 93, 28, 28, 250, 141, 37, 183, 27, 34, 0, 253, 0, 122, 109, 0, 34, 133, 17, 0, 7, 0, 37, 97, 105, 114, 61, 3, 102, 0, 57, 17, 0, 57, 212, 53, 35, 98, 255, 227, 74, 140, 12, 1, 0, 70, 103, 44, 83, 0, 0, 0, 20, 0, 18, 17, 58, 108, 7, 182, 13, 0, 0, 0, 0, 129, 0, 122, 98, 141, 0, 0, 38, 65, 0, 199, 71, 126, 10, 0, 197, 61, 231, 206, 172, 0, 0, 69, 113, 104, 54, 0, 63, 98, 0, 69, 0, 172, 55, 61, 0, 60, 40, 49, 68, 187, 34, 196, 16, 38, 10, 78, 126, 0, 59, 0, 5, 28, 107, 58, 134, 255, 232, 4, 127, 28, 35, 140, 29, 89, 6, 9, 255, 118, 6, 167, 27, 0, 0, 243, 49, 123, 41, 0, 22, 82, 60, 0, 7, 0, 42, 99, 144, 114, 79, 3, 192, 7, 51, 118, 0, 50, 179, 17, 23, 29, 255, 193, 58, 133, 12, 24, 0, 52, 23, 0, 137, 0, 0, 0, 45, 0, 66, 24, 23, 64, 19, 69, 13, 0, 0, 0, 0, 88, 0, 58, 230, 156, 0, 0, 54, 128, 7, 199, 13, 99, 0, 0, 138, 16, 213, 105, 190, 0, 0, 79, 118, 89, 78, 0, 52, 5, 0, 42, 17, 68, 54, 0, 0, 71, 22, 21, 68, 165, 88, 201, 37, 35, 86, 34, 167, 0, 11, 0, 21, 4, 79, 55, 142, 170, 230, 1, 0, 34, 121, 117, 0, 54, 0, 18, 210, 37, 62, 129, 31, 52, 0, 241, 64, 67, 20, 0, 38, 97, 51, 0, 7, 0, 65, 110, 169, 0, 69, 3, 210, 22, 42, 159, 54, 27, 255, 0, 56, 0, 255, 216, 8, 116, 12, 1, 0, 95, 130, 6, 174, 0, 0, 0, 33, 20, 9, 20, 4, 30, 20, 146, 13, 0, 3, 0, 0, 100, 0, 108, 103, 44, 0, 0, 87, 187, 48, 195, 79, 141, 0, 20, 117, 47, 237, 126, 124, 0, 0, 66, 155, 91, 20, 0, 75, 23, 0, 69, 0, 62, 30, 0, 44, 104, 51, 31, 0, 72, 21, 147, 31, 166, 45, 62, 155, 49, 87, 0, 17, 4, 26, 67, 147, 180, 255, 21, 0, 12, 0, 140, 37, 93, 11, 103, 209, 87, 57, 138, 32, 56, 0, 255, 44, 71, 0, 15, 1, 162, 40, 31, 7, 0, 39, 89, 176, 0, 76, 3, 163, 93, 63, 88, 40, 16, 224, 0, 24, 0, 255, 255, 68, 112, 54, 1, 0, 137, 51, 36, 178, 0, 21, 0, 31, 0, 12, 30, 1, 12, 62, 150, 13, 0, 4, 84, 0, 116, 0, 77, 152, 47, 0, 0, 96, 219, 60, 189, 44, 161, 0, 10, 116, 21, 237, 115, 144, 0, 0, 52, 106, 94, 25, 0, 61, 20, 0, 69, 0, 162, 60, 0, 65, 39, 23, 31, 0, 156, 21, 178, 25, 139, 75, 49, 112, 81, 11, 0, 71, 4, 92, 146, 160, 255, 255, 24, 79, 0, 117, 137, 61, 91, 0, 37, 217, 111, 64, 110, 27, 11, 0, 225, 11, 74, 65, 0, 5, 53, 70, 0, 7, 0, 98, 87, 144, 76, 66, 3, 133, 11, 40, 55, 0, 61, 195, 0, 29, 31, 255, 160, 46, 103, 12, 1, 0, 110, 61, 56, 139, 0, 0, 0, 31, 0, 0, 43, 1, 55, 57, 144, 13, 0, 0, 0, 0, 78, 0, 152, 186, 91, 0, 0, 36, 136, 71, 167, 34, 149, 0, 55, 108, 39, 237, 147, 178, 0, 0, 78, 72, 128, 18, 0, 82, 47, 0, 58, 30, 154, 61, 0, 35, 12, 21, 53, 0, 134, 27, 170, 69, 40, 42, 64, 110, 23, 17, 0, 13, 4, 79, 79, 108, 248, 242, 39, 0, 0, 0, 145, 80, 81, 0, 76, 255, 131, 52, 142, 27, 0, 0, 235, 0, 69, 18, 0, 10, 89, 43, 50, 7, 0, 72, 80, 164, 34, 68, 5, 157, 28, 31, 134, 0, 54, 185, 24, 53, 51, 244, 219, 25, 135, 12, 1, 0, 177, 44, 0, 163, 0, 1, 0, 32, 51, 11, 53, 23, 51, 58, 179, 13, 0, 35, 0, 0, 42, 5, 47, 151, 48, 0, 0, 44, 110, 0, 199, 59, 182, 0, 0, 133, 39, 237, 175, 147, 0, 0, 83, 92, 92, 4, 0, 88, 5, 0, 13, 36, 149, 27, 0, 18, 8, 71, 22, 0, 166, 21, 168, 49, 47, 68, 20, 118, 43, 21, 0, 55, 4, 120, 123, 103, 255, 212, 18, 2, 0, 0, 124, 0, 70, 0, 60, 216, 93, 123, 170, 27, 0, 1, 249, 0, 72, 25, 0, 10, 81, 50, 78, 7, 0, 67, 115, 114, 54, 73, 5, 121, 7, 39, 90, 0, 68, 224, 27, 65, 0, 232, 174, 0, 64, 12, 35, 0, 61, 50, 0, 105, 0, 48, 0, 60, 77, 0, 4, 37, 32, 66, 221, 13, 0, 0, 7, 11, 84, 0, 125, 168, 116, 0, 0, 61, 126, 0, 169, 57, 157, 22, 25, 127, 0, 237, 114, 96, 0, 0, 47, 132, 64, 4, 0, 68, 23, 0, 45, 0, 169, 34, 0, 0, 19, 85, 23, 68, 178, 51, 197, 47, 75, 93, 78, 70, 65, 36, 3, 150, 4, 56, 92, 108, 255, 195, 9, 56, 31, 0, 169, 57, 89, 17, 5, 220, 69, 39, 141, 31, 0, 0, 217, 0, 57, 89, 0, 13, 136, 17, 54, 7, 0, 63, 109, 126, 6, 63, 3, 100, 0, 52, 144, 0, 15, 153, 31, 39, 0, 232, 161, 22, 127, 12, 1, 0, 82, 57, 27, 168, 0, 17, 0, 21, 0, 9, 15, 22, 25, 2, 165, 13, 0, 0, 40, 5, 86, 0, 135, 167, 85, 0, 0, 28, 38, 0, 176, 79, 179, 0, 132, 146, 50, 220, 129, 193, 0, 0, 67, 42, 102, 4, 0, 52, 25, 0, 63, 33, 184, 43, 0, 7, 38, 69, 13, 68, 184, 33, 189, 30, 97, 94, 43, 84, 0, 47, 60, 26, 4, 96, 93, 175, 255, 184, 0, 0, 33, 169, 158, 0, 85, 0, 5, 255, 103, 69, 90, 34, 98, 8, 180, 23, 27, 32, 32, 2, 56, 21, 0, 7, 0, 44, 86, 142, 105, 71, 3, 67, 0, 16, 87, 21, 63, 171, 1, 30, 0, 213, 148, 16, 130, 12, 1, 0, 41, 73, 85, 165, 0, 35, 0, 46, 76, 0, 86, 17, 116, 2, 101, 13, 0, 0, 0, 0, 69, 36, 80, 158, 1, 0, 0, 60, 70, 0, 199, 13, 143, 24, 0, 180, 14, 231, 123, 177, 0, 10, 88, 113, 116, 4, 0, 65, 35, 8, 48, 44, 183, 23, 37, 3, 30, 64, 30, 45, 184, 21, 194, 46, 88, 90, 75, 65, 0, 18, 0, 52, 4, 28, 68, 170, 245, 166, 1, 0, 0, 0, 150, 22, 85, 0, 5, 254, 83, 98, 26, 27, 0, 21, 216, 69, 53, 43, 20, 12, 166, 79, 0, 7, 0, 79, 105, 145, 63, 79, 3, 110, 16, 8, 200, 0, 29, 217, 0, 33, 68, 190, 153, 58, 133, 12, 53, 0, 147, 16, 75, 167, 0, 29, 0, 4, 0, 27, 96, 7, 150, 2, 80, 13, 0, 8, 0, 0, 18, 32, 85, 132, 130, 0, 0, 56, 68, 0, 187, 33, 143, 27, 0, 171, 3, 226, 154, 145, 0, 0, 68, 84, 74, 4, 0, 63, 25, 0, 69, 6, 168, 57, 0, 0, 51, 60, 40, 43, 187, 63, 194, 79, 131, 100, 71, 108, 0, 11, 0, 57, 4, 65, 63, 136, 255, 255, 0, 0, 32, 141, 169, 0, 75, 0, 17, 255, 55, 63, 139, 27, 56, 28, 222, 37, 100, 59, 0, 14, 44, 72, 21, 7, 0, 55, 88, 145, 18, 79, 4, 146, 23, 15, 71, 0, 26, 241, 0, 41, 0, 255, 169, 56, 128, 12, 71, 0, 56, 72, 0, 174, 0, 0, 0, 41, 0, 59, 66, 7, 59, 2, 92, 13, 0, 73, 0, 37, 83, 41, 79, 165, 106, 0, 0, 52, 104, 13, 189, 54, 132, 8, 0, 170, 5, 237, 168, 180, 0, 0, 94, 33, 113, 4, 0, 75, 51, 0, 69, 4, 166, 79, 0, 15, 7, 8, 57, 68, 178, 21, 190, 14, 109, 58, 78, 84, 28, 11, 2, 72, 4, 77, 120, 196, 255, 255, 0, 6, 2, 0, 169, 35, 89, 0, 11, 236, 65, 72, 0, 34, 112, 56, 220, 4, 45, 72, 0, 5, 118, 110, 31, 7, 0, 69, 78, 170, 185, 79, 5, 140, 7, 77, 158, 0, 25, 165, 0, 46, 37, 245, 184, 14, 104, 12, 18, 0, 101, 80, 0, 118, 0, 0, 0, 20, 82, 0, 27, 13, 93, 35, 115, 13, 0, 0, 14, 0, 60, 0, 128, 134, 16, 0, 0, 37, 144, 25, 184, 14, 163, 22, 0, 176, 6, 237, 126, 117, 0, 15, 70, 112, 140, 19, 0, 77, 66, 36, 69, 20, 182, 50, 0, 0, 48, 36, 44, 0, 150, 73, 174, 40, 71, 61, 75, 90, 82, 11, 28, 58, 4, 43, 37, 129, 255, 209, 11, 0, 19, 31, 151, 0, 93, 0, 7, 255, 65, 18, 76, 36, 0, 21, 255, 0, 95, 45, 0, 40, 125, 61, 46, 7, 0, 40, 96, 163, 67, 79, 3, 119, 0, 48, 104, 5, 18, 167, 3, 21, 0, 252, 145, 42, 121, 12, 24, 0, 130, 42, 0, 152, 0, 40, 0, 11, 0, 0, 4, 45, 43, 2, 130, 13, 0, 42, 0, 0, 77, 0, 72, 146, 97, 0, 0, 39, 60, 0, 195, 28, 138, 14, 0, 139, 9, 237, 158, 102, 0, 0, 46, 37, 103, 4, 0, 38, 16, 0, 69, 21, 166, 82, 0, 9, 39, 29, 84, 68, 166, 24, 207, 43, 48, 78, 59, 118, 21, 18, 73, 7, 4, 45, 74, 125, 255, 255, 1, 0, 24, 0, 115, 0, 93, 0, 75, 235, 50, 27, 86, 27, 48, 2, 240, 0, 32, 39, 19, 8, 82, 36, 0, 7, 0, 148, 106, 143, 89, 79, 3, 136, 0, 10, 255, 0, 46, 226, 0, 39, 0, 253, 164, 50, 103, 12, 27, 0, 136, 101, 99, 173, 0, 29, 0, 62, 44, 0, 7, 39, 34, 30, 163, 13, 0, 25, 39, 0, 110, 28, 61, 188, 23, 0, 0, 40, 129, 0, 167, 16, 157, 0, 0, 165, 0, 237, 232, 40, 0, 0, 19, 60, 82, 4, 0, 35, 45, 0, 69, 0, 135, 66, 0, 0, 43, 0, 66, 29, 110, 25, 191, 0, 39, 49, 78, 192, 65, 25, 94, 18, 4, 15, 77, 130, 199, 255, 0, 0, 39, 0, 156, 137, 93, 43, 93, 214, 32, 2, 154, 27, 91, 0, 199, 0, 50, 72, 49, 15, 42, 42, 5, 7, 0, 15, 117, 170, 110, 54, 3, 100, 11, 34, 235, 38, 35, 208, 0, 34, 0, 236, 226, 74, 101, 12, 36, 0, 14, 84, 0, 255, 0, 54, 0, 1, 57, 95, 21, 20, 7, 9, 126, 47, 0, 137, 0, 127, 90, 71, 98, 131, 1, 0, 0, 28, 87, 53, 122, 80, 101, 0, 16, 166, 0, 237, 211, 88, 0, 0, 36, 153, 109, 4, 0, 58, 24, 0, 69, 38, 186, 31, 0, 30, 38, 0, 12, 68, 181, 21, 177, 23, 58, 92, 54, 97, 61, 17, 0, 5, 4, 39, 52, 128, 255, 246, 22, 76, 19, 101, 169, 122, 93, 0, 9, 244, 116, 47, 142, 27, 35, 0, 244, 22, 53, 49, 34, 11, 86, 37, 0, 7, 0, 53, 30, 156, 0, 46, 3, 106, 10, 15, 40, 0, 35, 193, 34, 38, 79, 253, 226, 37, 100, 12, 29, 0, 61, 70, 17, 236, 0, 43, 0, 47, 0, 0, 39, 23, 57, 38, 160, 16, 0, 0, 0, 0, 81, 52, 51, 255, 154, 0, 0, 39, 108, 0, 180, 31, 122, 0, 61, 138, 29, 203, 72, 152, 0, 0, 41, 14, 127, 42, 0, 92, 29, 0, 50, 0, 178, 135, 0, 0, 68, 0, 63, 7, 184, 21, 186, 27, 44, 75, 54, 143, 40, 22, 0, 25, 4, 45, 103, 138, 240, 170, 14, 0, 0, 69, 156, 0, 93, 0, 9, 255, 107, 9, 165, 27, 38, 0, 225, 6, 47, 64, 0, 35, 116, 41, 81, 7, 0, 24, 111, 184, 0, 79, 3, 90, 30, 4, 116, 0, 7, 210, 0, 34, 10, 255, 164, 55, 135, 12, 1, 0, 74, 20, 37, 46, 0, 0, 0, 33, 67, 44, 25, 16, 32, 15, 187, 13, 0, 42, 0, 0, 106, 0, 19, 166, 1, 0, 0, 39, 33, 0, 191, 43, 68, 7, 7, 157, 62, 237, 102, 127, 0, 0, 78, 99, 116, 21, 0, 78, 5, 0, 7, 44, 172, 65, 30, 83, 4, 0, 52, 39, 184, 21, 196, 10, 151, 85, 42, 112, 27, 21, 13, 12, 4, 33, 28, 92, 225, 255, 50, 0, 0, 0, 169, 0, 86, 3, 5, 236, 141, 46, 143, 27, 0, 0, 245, 55, 98, 27, 66, 11, 113, 17, 0, 7, 0, 68, 92, 139, 0, 79, 3, 202, 26, 4, 166, 0, 54, 227, 0, 59, 12, 255, 219, 10, 122, 12, 1, 0, 147, 91, 53, 12, 0, 9, 0, 8, 29, 46, 38, 1, 101, 11, 165, 13, 0, 0, 0, 43, 77, 0, 131, 124, 133, 0, 0, 51, 104, 0, 199, 15, 103, 9, 0, 116, 11, 237, 112, 122, 0, 0, 89, 66, 100, 23, 0, 110, 14, 0, 48, 19, 164, 115, 0, 5, 63, 0, 57, 37, 177, 52, 177, 83, 44, 81, 57, 171, 64, 11, 0, 37, 6, 74, 42, 174, 255, 255, 10, 24, 2, 0, 169, 13, 82, 0, 14, 208, 114, 68, 139, 27, 47, 22, 255, 0, 75, 55, 0, 12, 83, 115, 0, 7, 22, 66, 115, 170, 29, 79, 3, 153, 0, 4, 255, 0, 32, 224, 0, 77, 21, 255, 201, 41, 116, 12, 18, 0, 93, 50, 0, 17, 0, 0, 0, 19, 20, 0, 27, 7, 31, 11, 169, 13, 0, 0, 0, 0, 83, 0, 91, 198, 51, 0, 0, 95, 120, 82, 171, 43, 60, 62, 0, 152, 63, 237, 138, 167, 0, 0, 73, 102, 100, 4, 0, 53, 24, 17, 40, 32, 164, 29, 0, 0, 23, 12, 57, 68, 178, 21, 171, 18, 35, 101, 71, 116, 0, 11, 0, 66, 8, 88, 41, 123, 246, 248, 94, 0, 0, 11, 169, 52, 93, 0, 25, 219, 145, 46, 74, 27, 8, 0, 255, 4, 63, 43, 40, 2, 92, 32, 0, 7, 0, 57, 86, 157, 0, 79, 3, 162, 0, 4, 156, 25, 11, 200, 0, 32, 29, 255, 215, 30, 91, 12, 24, 0, 116, 57, 0, 17, 26, 45, 0, 46, 0, 15, 17, 9, 57, 16, 190, 13, 0, 74, 0, 0, 111, 0, 95, 189, 37, 0, 0, 64, 98, 0, 181, 44, 103, 6, 0, 198, 0, 237, 87, 64, 0, 0, 74, 168, 113, 4, 0, 122, 28, 12, 65, 0, 160, 43, 0, 0, 48, 6, 42, 0, 165, 21, 187, 5, 40, 52, 78, 116, 92, 11, 0, 47, 4, 29, 61, 147, 255, 213, 10, 53, 13, 12, 125, 0, 93, 0, 24, 195, 139, 14, 166, 27, 0, 0, 255, 70, 90, 77, 0, 3, 136, 38, 13, 7, 0, 97, 101, 189, 39, 79, 3, 158, 7, 20, 42, 14, 12, 166, 0, 36, 0, 241, 160, 74, 105, 12, 1, 0, 98, 66, 16, 31, 0, 13, 0, 36, 11, 51, 22, 1, 117, 5, 166, 13, 0, 0, 0, 0, 86, 0, 94, 186, 140, 0, 0, 28, 109, 0, 178, 0, 65, 0, 48, 153, 0, 237, 88, 123, 0, 0, 94, 130, 101, 7, 0, 86, 9, 0, 29, 0, 160, 12, 0, 31, 38, 66, 20, 24, 187, 33, 124, 33, 50, 73, 63, 84, 92, 35, 42, 48, 4, 50, 54, 116, 248, 216, 49, 0, 0, 0, 106, 40, 93, 34, 37, 253, 123, 23, 183, 27, 32, 0, 255, 40, 24, 48, 0, 2, 167, 17, 20, 7, 0, 74, 96, 148, 0, 79, 3, 146, 1, 12, 114, 0, 12, 213, 20, 29, 28, 242, 225, 74, 140, 12, 14, 0, 129, 66, 64, 5, 22, 54, 0, 26, 8, 19, 43, 5, 122, 24, 181, 13, 0, 0, 0, 21, 110, 0, 107, 192, 78, 0, 0, 28, 28, 0, 199, 3, 131, 0, 0, 123, 20, 237, 123, 170, 0, 34, 94, 47, 106, 20, 0, 82, 11, 0, 36, 3, 178, 14, 67, 19, 66, 18, 20, 15, 187, 21, 192, 92, 47, 64, 43, 189, 0, 57, 0, 58, 4, 110, 63, 148, 243, 206, 51, 1, 0, 27, 146, 0, 93, 0, 16, 242, 119, 23, 183, 27, 69, 6, 240, 30, 62, 56, 0, 2, 106, 49, 61, 7, 0, 30, 104, 185, 0, 70, 3, 108, 3, 4, 65, 0, 7, 255, 0, 31, 40, 255, 165, 66, 137, 12, 37, 0, 114, 39, 0, 21, 0, 36, 0, 7, 47, 0, 14, 5, 52, 44, 151, 13, 0, 0, 2, 0, 93, 0, 56, 211, 12, 0, 0, 71, 59, 38, 199, 51, 115, 0, 0, 118, 22, 237, 83, 118, 0, 0, 94, 99, 103, 4, 0, 106, 45, 0, 69, 5, 175, 123, 44, 3, 17, 0, 42, 68, 174, 21, 207, 26, 50, 42, 70, 78, 0, 14, 0, 43, 4, 65, 31, 134, 246, 233, 1, 26, 0, 0, 136, 0, 93, 1, 5, 255, 99, 30, 183, 27, 45, 52, 252, 0, 55, 8, 0, 5, 126, 85, 2, 7, 0, 57, 95, 152, 0, 68, 3, 95, 0, 4, 79, 70, 9, 191, 0, 50, 0, 255, 189, 53, 128, 12, 24, 0, 124, 64, 35, 53, 0, 0, 0, 1, 0, 10, 21, 9, 79, 17, 151, 13, 0, 52, 0, 12, 62, 6, 54, 150, 38, 0, 0, 90, 33, 0, 188, 31, 114, 56, 0, 111, 32, 237, 95, 168, 0, 0, 94, 38, 102, 13, 0, 95, 5, 0, 69, 0, 132, 75, 32, 39, 28, 40, 30, 16, 151, 21, 183, 25, 159, 40, 70, 85, 36, 29, 0, 10, 4, 63, 89, 99, 213, 255, 42, 0, 0, 16, 101, 80, 93, 25, 52, 244, 123, 82, 183, 27, 72, 0, 238, 0, 78, 39, 0, 8, 111, 60, 0, 7, 0, 14, 99, 121, 0, 79, 3, 117, 20, 28, 34, 0, 64, 159, 32, 30, 34, 255, 224, 54, 113, 12, 1, 0, 48, 114, 0, 9, 0, 0, 0, 19, 30, 42, 18, 26, 47, 54, 208, 13, 0, 20, 63, 0, 108, 0, 118, 93, 39, 0, 0, 86, 59, 0, 184, 50, 158, 2, 59, 144, 21, 237, 133, 194, 0, 0, 94, 47, 89, 34, 0, 77, 62, 0, 69, 0, 147, 84, 0, 0, 79, 36, 37, 44, 155, 21, 159, 67, 40, 92, 78, 155, 10, 47, 0, 53, 4, 72, 86, 123, 235, 228, 75, 37, 0, 132, 118, 0, 67, 0, 57, 249, 128, 11, 183, 42, 69, 20, 225, 0, 86, 38, 0, 21, 159, 55, 0, 7, 0, 22, 87, 176, 0, 67, 3, 121, 31, 31, 86, 27, 7, 255, 0, 38, 0, 255, 211, 61, 117, 12, 9, 0, 165, 19, 2, 12, 0, 0, 0, 29, 0, 30, 75, 13, 32, 39, 190, 13, 0, 0, 14, 9, 92, 0, 69, 182, 35, 0, 0, 62, 85, 0, 167, 68, 135, 0, 41, 208, 75, 237, 68, 151, 0, 0, 83, 120, 94, 31, 0, 99, 5, 0, 36, 21, 169, 119, 0, 0, 14, 0, 62, 68, 177, 153, 185, 22, 35, 89, 30, 72, 26, 37, 0, 21, 4, 45, 67, 127, 255, 239, 4, 0, 38, 0, 169, 0, 45, 28, 11, 214, 109, 41, 183, 27, 59, 23, 234, 0, 103, 86, 0, 39, 102, 38, 14, 7, 0, 74, 88, 141, 0, 56, 3, 134, 23, 49, 255, 78, 16, 222, 21, 66, 0, 244, 213, 52, 103, 12, 12, 0, 76, 70, 0, 37, 0, 0, 0, 35, 0, 0, 11, 10, 97, 9, 205, 13, 0, 0, 0, 33, 89, 0, 138, 190, 92, 0, 0, 68, 120, 0, 179, 9, 31, 8, 0, 114, 83, 237, 126, 145, 0, 0, 94, 115, 85, 24, 0, 88, 41, 0, 55, 18, 175, 113, 0, 0, 67, 18, 87, 68, 155, 57, 194, 37, 35, 103, 65, 131, 14, 41, 0, 116, 30, 73, 135, 171, 255, 166, 1, 50, 0, 36, 169, 23, 84, 0, 16, 199, 78, 34, 98, 34, 0, 0, 226, 0, 156, 68, 0, 44, 133, 58, 0, 7, 0, 47, 100, 163, 0, 26, 5, 103, 18, 4, 42, 58, 38, 231, 0, 28, 0, 240, 195, 74, 136, 12, 33, 0, 131, 55, 0, 22, 0, 7, 0, 66, 0, 0, 26, 1, 63, 8, 174, 13, 0, 59, 0, 0, 53, 0, 89, 219, 148, 0, 0, 37, 74, 0, 199, 31, 107, 22, 56, 67, 17, 237, 111, 181, 0, 0, 94, 94, 127, 28, 0, 79, 40, 0, 22, 55, 178, 247, 0, 19, 65, 0, 109, 68, 152, 61, 170, 52, 100, 35, 3, 72, 0, 37, 13, 106, 30, 90, 51, 185, 255, 185, 18, 38, 2, 0, 148, 20, 76, 0, 19, 178, 79, 12, 170, 27, 41, 1, 248, 0, 202, 0, 0, 78, 154, 55, 45, 7, 0, 39, 86, 108, 0, 17, 3, 61, 0, 27, 149, 50, 35, 173, 0, 37, 0, 255, 177, 43, 125, 12, 1, 0, 55, 26, 0, 23, 0, 0, 86, 33, 43, 42, 21, 9, 106, 5, 157, 13, 0, 0, 0, 0, 60, 0, 106, 155, 25, 0, 0, 40, 69, 0, 199, 6, 107, 0, 77, 95, 65, 237, 125, 122, 0, 0, 66, 98, 79, 19, 0, 76, 34, 0, 0, 6, 181, 76, 58, 22, 75, 0, 44, 68, 184, 105, 185, 10, 37, 82, 66, 132, 5, 11, 0, 32, 4, 48, 28, 182, 255, 226, 9, 14, 0, 0, 145, 0, 93, 0, 16, 241, 110, 52, 159, 27, 0, 0, 255, 46, 91, 4, 5, 12, 91, 89, 0, 7, 0, 32, 91, 177, 0, 76, 3, 134, 5, 19, 183, 30, 36, 210, 0, 30, 0, 255, 195, 58, 91, 12, 1, 0, 87, 78, 0, 10, 0, 4, 0, 22, 22, 45, 28, 11, 65, 5, 137, 13, 0, 99, 0, 44, 30, 0, 92, 175, 66, 0, 0, 102, 121, 8, 188, 0, 114, 3, 0, 142, 22, 237, 120, 97, 0, 0, 67, 145, 104, 25, 0, 94, 9, 0, 40, 20, 135, 75, 0, 0, 45, 0, 35, 68, 163, 69, 207, 23, 89, 79, 36, 95, 34, 11, 0, 18, 4, 18, 33, 99, 206, 218, 0, 0, 0, 0, 157, 0, 93, 20, 24, 248, 99, 33, 160, 31, 6, 0, 255, 0, 66, 0, 26, 24, 174, 89, 0, 7, 0, 29, 94, 99, 79, 75, 3, 113, 29, 16, 177, 0, 12, 180, 0, 31, 5, 255, 200, 53, 118, 12, 1, 0, 75, 81, 0, 23, 0, 5, 51, 6, 0, 31, 24, 64, 56, 7, 193, 13, 0, 33, 0, 0, 118, 32, 67, 115, 60, 0, 0, 169, 47, 45, 182, 44, 145, 0, 0, 98, 46, 237, 100, 122, 0, 0, 73, 56, 131, 26, 0, 67, 28, 0, 69, 21, 109, 94, 0, 0, 65, 6, 75, 21, 162, 31, 158, 90, 53, 67, 78, 147, 87, 21, 17, 75, 4, 64, 42, 68, 243, 215, 4, 33, 21, 24, 169, 32, 83, 15, 43, 204, 133, 62, 171, 34, 62, 0, 216, 29, 54, 53, 0, 9, 107, 66, 0, 7, 0, 18, 98, 116, 128, 44, 3, 153, 33, 7, 180, 0, 7, 207, 14, 80, 0, 255, 204, 59, 83, 12, 17, 0, 110, 75, 0, 34, 0, 0, 0, 7, 2, 0, 37, 37, 14, 14, 184, 13, 0, 0, 27, 5, 81, 0, 36, 147, 18, 0, 0, 34, 81, 0, 193, 80, 100, 0, 62, 89, 31, 237, 79, 104, 0, 10, 94, 55, 83, 13, 0, 82, 14, 0, 36, 45, 140, 72, 0, 16, 27, 0, 41, 5, 177, 21, 196, 43, 15, 51, 78, 147, 79, 11, 0, 45, 4, 48, 82, 99, 255, 216, 10, 0, 0, 155, 119, 0, 90, 0, 21, 234, 131, 25, 114, 27, 12, 18, 206, 38, 30, 0, 0, 2, 124, 20, 37, 7, 0, 59, 72, 143, 45, 59, 3, 190, 5, 11, 75, 39, 7, 202, 0, 34, 0, 221, 150, 67, 79, 12, 1, 0, 77, 59, 0, 11, 0, 11, 0, 19, 40, 79, 15, 6, 34, 8, 142, 13, 0, 99, 0, 47, 39, 12, 128, 183, 28, 0, 0, 28, 92, 0, 199, 31, 139, 0, 0, 157, 13, 229, 77, 115, 0, 0, 69, 37, 87, 16, 0, 97, 5, 0, 69, 7, 178, 105, 0, 42, 46, 0, 87, 58, 167, 21, 173, 31, 20, 75, 78, 142, 55, 37, 0, 54, 4, 44, 93, 101, 255, 255, 11, 36, 10, 0, 164, 19, 88, 28, 53, 255, 90, 70, 166, 27, 99, 0, 215, 0, 10, 46, 0, 5, 131, 42, 0, 7, 0, 34, 106, 175, 0, 61, 3, 158, 6, 5, 255, 0, 7, 167, 0, 56, 0, 241, 188, 61, 135, 12, 27, 0, 37, 99, 0, 22, 0, 0, 0, 117, 64, 83, 41, 31, 37, 21, 129, 13, 0, 0, 9, 0, 79, 0, 74, 205, 1, 0, 0, 36, 74, 0, 173, 0, 55, 0, 0, 135, 55, 225, 124, 95, 0, 0, 89, 84, 151, 13, 0, 103, 39, 0, 69, 22, 135, 38, 0, 29, 28, 43, 40, 0, 151, 36, 207, 23, 54, 52, 62, 99, 77, 32, 45, 79, 4, 36, 58, 92, 242, 215, 11, 0, 0, 9, 139, 0, 88, 96, 19, 224, 106, 22, 164, 27, 22, 0, 255, 11, 13, 0, 0, 9, 143, 44, 0, 7, 0, 61, 68, 186, 0, 69, 3, 114, 12, 17, 29, 0, 7, 174, 42, 21, 4, 231, 191, 52, 123, 12, 1, 0, 108, 49, 0, 67, 0, 0, 0, 31, 0, 53, 57, 43, 63, 14, 201, 13, 0, 55, 0, 36, 80, 0, 92, 217, 67, 0, 0, 60, 80, 0, 192, 28, 120, 1, 0, 134, 61, 237, 89, 138, 0, 0, 81, 84, 110, 4, 0, 77, 68, 0, 69, 16, 143, 148, 3, 0, 68, 0, 44, 0, 179, 44, 201, 33, 40, 54, 55, 116, 48, 11, 0, 18, 4, 45, 127, 135, 231, 231, 19, 17, 0, 244, 169, 5, 93, 20, 14, 248, 133, 63, 102, 27, 2, 0, 210, 50, 44, 0, 0, 6, 177, 51, 0, 7, 0, 95, 103, 180, 0, 67, 3, 147, 16, 20, 115, 0, 9, 179, 25, 24, 0, 236, 207, 74, 118, 12, 1, 0, 125, 36, 0, 43, 0, 0, 0, 24, 0, 8, 39, 1, 148, 2, 158, 13, 0, 0, 0, 0, 85, 0, 47, 255, 100, 0, 0, 47, 120, 0, 187, 0, 93, 4, 40, 112, 33, 237, 135, 108, 0, 0, 91, 90, 114, 4, 0, 87, 39, 0, 69, 38, 160, 145, 0, 11, 57, 0, 44, 0, 81, 21, 202, 82, 74, 48, 38, 71, 34, 11, 26, 39, 4, 61, 169, 129, 255, 156, 7, 0, 0, 0, 123, 43, 93, 0, 65, 239, 132, 63, 130, 27, 0, 0, 255, 0, 116, 0, 24, 34, 147, 35, 0, 7, 0, 17, 94, 135, 0, 79, 3, 91, 79, 11, 255, 0, 19, 224, 0, 23, 0, 237, 215, 65, 116, 12, 43, 0, 185, 84, 0, 40, 0, 0, 7, 3, 25, 7, 36, 7, 89, 74, 186, 13, 0, 0, 0, 0, 59, 0, 67, 235, 25, 0, 0, 89, 83, 0, 74, 34, 160, 4, 21, 28, 41, 233, 120, 141, 0, 0, 94, 120, 107, 29, 0, 83, 22, 0, 69, 55, 180, 44, 0, 24, 62, 44, 29, 0, 151, 21, 194, 62, 87, 80, 53, 152, 65, 21, 54, 142, 4, 58, 101, 127, 221, 201, 7, 0, 0, 255, 169, 0, 62, 0, 29, 241, 145, 23, 162, 27, 0, 0, 224, 67, 55, 28, 0, 2, 72, 61, 0, 7, 0, 48, 107, 156, 0, 79, 3, 123, 48, 12, 76, 100, 36, 214, 0, 56, 0, 255, 178, 41, 102, 12, 35, 0, 156, 31, 0, 37, 22, 0, 62, 6, 0, 0, 13, 19, 114, 29, 173, 13, 0, 0, 0, 0, 81, 0, 105, 158, 117, 0, 0, 82, 111, 0, 192, 0, 94, 0, 0, 101, 30, 231, 115, 55, 0, 0, 84, 92, 88, 22, 0, 81, 40, 7, 69, 24, 176, 27, 0, 0, 83, 45, 24, 16, 163, 21, 198, 38, 12, 58, 69, 134, 56, 11, 33, 72, 4, 29, 49, 137, 247, 253, 28, 0, 0, 0, 169, 24, 90, 0, 5, 255, 123, 5, 108, 27, 0, 22, 255, 14, 125, 33, 0, 2, 114, 76, 26, 7, 0, 56, 100, 153, 0, 79, 5, 151, 29, 20, 209, 0, 9, 196, 0, 51, 35, 255, 185, 25, 97, 12, 1, 0, 58, 16, 0, 18, 0, 34, 0, 10, 30, 74, 15, 5, 111, 2, 164, 13, 0, 90, 0, 0, 65, 0, 74, 173, 155, 0, 0, 60, 108, 0, 181, 19, 79, 1, 9, 99, 6, 237, 103, 146, 0, 0, 81, 59, 78, 28, 0, 70, 11, 0, 25, 5, 179, 41, 0, 20, 40, 0, 30, 0, 148, 52, 207, 49, 100, 79, 66, 86, 53, 11, 0, 64, 4, 70, 66, 136, 239, 255, 0, 0, 16, 33, 129, 5, 88, 12, 13, 249, 115, 30, 107, 27, 73, 13, 233, 20, 72, 38, 0, 2, 130, 59, 44, 7, 0, 52, 78, 108, 0, 79, 5, 186, 11, 13, 85, 79, 25, 173, 0, 29, 0, 234, 177, 25, 122, 12, 18, 0, 116, 41, 0, 19, 0, 49, 0, 1, 60, 0, 4, 26, 40, 63, 136, 13, 0, 0, 0, 0, 17, 0, 69, 179, 57, 0, 0, 37, 83, 0, 196, 21, 88, 10, 0, 70, 14, 237, 118, 170, 0, 0, 94, 102, 96, 4, 0, 77, 12, 22, 69, 41, 183, 65, 7, 0, 45, 44, 51, 49, 155, 21, 207, 19, 19, 85, 78, 110, 0, 11, 0, 51, 4, 57, 89, 101, 255, 255, 0, 0, 0, 17, 122, 0, 93, 0, 26, 255, 72, 11, 120, 27, 27, 48, 255, 0, 84, 42, 0, 5, 89, 82, 27, 7, 0, 94, 98, 151, 0, 79, 3, 139, 0, 13, 72, 0, 7, 155, 42, 21, 5, 255, 197, 74, 135, 12, 1, 0, 152, 19, 0, 38, 0, 35, 0, 3, 0, 0, 4, 68, 80, 2, 190, 13, 0, 21, 5, 9, 87, 0, 66, 206, 99, 0, 0, 56, 99, 0, 193, 0, 133, 38, 0, 98, 14, 237, 142, 54, 0, 0, 94, 137, 115, 4, 0, 126, 29, 0, 69, 19, 166, 44, 55, 0, 45, 54, 51, 41, 167, 21, 207, 89, 63, 77, 64, 87, 36, 28, 24, 10, 4, 45, 63, 68, 255, 249, 0, 0, 0, 0, 120, 20, 93, 0, 60, 255, 92, 7, 151, 27, 0, 0, 255, 0, 93, 0, 52, 9, 148, 53, 0, 7, 0, 37, 83, 128, 0, 79, 3, 151, 0, 4, 108, 3, 22, 181, 64, 21, 1, 253, 171, 74, 123, 12, 1, 0, 215, 16, 0, 19, 0, 55, 0, 17, 0, 16, 37, 28, 58, 39, 178, 13, 0, 0, 0, 14, 103, 39, 42, 209, 220, 0, 0, 37, 63, 0, 193, 20, 155, 32, 38, 103, 9, 237, 160, 97, 0, 0, 68, 95, 151, 4, 0, 51, 36, 26, 69, 46, 186, 12, 15, 4, 6, 48, 21, 21, 175, 21, 161, 6, 0, 43, 78, 187, 56, 25, 0, 15, 100, 36, 28, 136, 255, 255, 0, 68, 0, 0, 146, 60, 93, 0, 5, 242, 48, 12, 158, 27, 82, 0, 173, 61, 26, 104, 0, 2, 83, 27, 40, 7, 0, 72, 100, 189, 14, 13, 3, 16, 25, 4, 127, 78, 45, 166, 0, 64, 0, 200, 193, 74, 140, 12, 107, 0, 42, 22, 0, 129, 0, 0, 0, 100, 75, 66, 82, 4, 82, 33, 95, 22, 0, 106, 0, 141, 61, 67, 53, 199, 12, 0, 0, 28, 24, 60, 146, 20, 70, 0, 0, 145, 33, 237, 158, 159, 0, 0, 55, 40, 66, 4, 0, 115, 26, 11, 69, 24, 171, 48, 0, 0, 75, 0, 27, 0, 136, 21, 145, 35, 21, 15, 65, 97, 0, 21, 0, 12, 4, 65, 85, 102, 255, 255, 25, 148, 47, 0, 135, 28, 72, 0, 53, 204, 94, 3, 115, 27, 92, 27, 250, 66, 68, 51, 0, 7, 99, 105, 21, 7, 0, 34, 76, 167, 0, 18, 3, 103, 121, 17, 94, 0, 17, 149, 0, 45, 30, 255, 177, 14, 126, 12, 152, 0, 50, 27, 68, 200, 0, 34, 0, 73, 90, 2, 17, 20, 47, 127, 155, 21, 0, 0, 0, 41, 21, 0, 45, 233, 8, 0, 0, 69, 133, 0, 195, 30, 91, 7, 99, 135, 5, 223, 89, 166, 0, 0, 87, 57, 104, 21, 0, 100, 5, 0, 53, 14, 177, 132, 0, 11, 70, 0, 62, 21, 184, 21, 180, 27, 37, 124, 48, 160, 1, 27, 0, 40, 4, 84, 66, 182, 255, 247, 0, 0, 17, 0, 163, 0, 73, 0, 20, 255, 92, 46, 151, 27, 68, 0, 255, 56, 70, 42, 0, 5, 117, 80, 0, 7, 0, 40, 99, 116, 8, 51, 3, 91, 0, 22, 164, 3, 12, 186, 0, 54, 0, 255, 157, 66, 126, 12, 21, 0, 63, 58, 116, 57, 0, 0, 0, 51, 52, 0, 25, 1, 35, 27, 109, 13, 0, 22, 0, 0, 46, 0, 109, 169, 29, 0, 0, 62, 72, 0, 137, 62, 138, 11, 12, 142, 33, 237, 151, 104, 0, 0, 94, 148, 96, 4, 0, 81, 5, 0, 0, 76, 146, 60, 25, 0, 24, 0, 46, 47, 153, 52, 200, 10, 43, 79, 66, 119, 31, 16, 0, 10, 4, 60, 28, 138, 239, 233, 0, 0, 0, 0, 140, 10, 54, 52, 9, 247, 84, 28, 148, 27, 0, 0, 255, 47, 160, 0, 11, 55, 119, 69, 0, 7, 0, 23, 89, 155, 0, 48, 3, 191, 11, 4, 115, 0, 12, 190, 0, 63, 0, 255, 187, 43, 122, 12, 28, 0, 157, 52, 28, 58, 0, 30, 0, 1, 0, 0, 17, 1, 95, 16, 138, 13, 0, 0, 0, 0, 59, 53, 93, 197, 172, 0, 0, 102, 112, 10, 178, 52, 129, 11, 0, 137, 17, 237, 85, 117, 0, 0, 94, 127, 93, 4, 0, 109, 5, 0, 44, 27, 159, 24, 0, 29, 56, 59, 27, 0, 187, 21, 174, 7, 72, 46, 61, 115, 58, 11, 23, 18, 7, 49, 65, 143, 188, 219, 0, 0, 11, 60, 138, 79, 41, 13, 11, 187, 115, 10, 169, 27, 17, 0, 255, 64, 20, 27, 0, 5, 115, 44, 0, 7, 0, 52, 95, 182, 0, 69, 3, 240, 17, 4, 20, 0, 7, 174, 0, 21, 0, 255, 198, 0, 122, 12, 17, 0, 138, 19, 0, 32, 0, 40, 0, 18, 0, 24, 4, 1, 46, 29, 145, 13, 0, 43, 0, 1, 51, 44, 76, 162, 104, 0, 0, 79, 158, 28, 171, 92, 97, 0, 0, 109, 3, 237, 88, 87, 0, 0, 94, 136, 139, 4, 0, 90, 21, 26, 69, 25, 143, 36, 33, 0, 59, 8, 27, 58, 180, 21, 199, 10, 109, 90, 56, 92, 0, 11, 0, 22, 11, 13, 45, 180, 248, 241, 60, 0, 42, 17, 144, 0, 93, 0, 21, 191, 100, 13, 125, 27, 59, 26, 255, 38, 26, 0, 71, 28, 137, 45, 0, 7, 0, 99, 89, 178, 0, 79, 3, 157, 0, 8, 37, 0, 17, 141, 0, 23, 0, 254, 142, 74, 86, 12, 1, 0, 83, 20, 0, 68, 0, 57, 0, 37, 0, 64, 4, 1, 48, 10, 159, 13, 0, 34, 0, 0, 135, 16, 84, 216, 43, 0, 0, 68, 157, 11, 142, 52, 99, 0, 0, 102, 0, 237, 70, 101, 0, 0, 94, 130, 95, 14, 0, 100, 5, 0, 69, 0, 158, 26, 0, 21, 24, 32, 19, 43, 162, 21, 192, 3, 87, 65, 78, 153, 25, 11, 0, 18, 4, 33, 60, 150, 255, 255, 41, 72, 0, 200, 167, 0, 83, 25, 17, 230, 145, 58, 101, 27, 22, 0, 231, 19, 39, 92, 12, 2, 144, 44, 45, 7, 0, 89, 99, 172, 0, 79, 3, 192, 11, 18, 60, 0, 12, 193, 0, 40, 57, 255, 203, 3, 103, 12, 1, 0, 147, 106, 0, 17, 0, 12, 0, 104, 0, 0, 25, 1, 102, 5, 186, 13, 0, 0, 18, 37, 79, 0, 159, 145, 108, 0, 0, 51, 146, 44, 134, 32, 105, 0, 80, 148, 0, 237, 59, 135, 0, 0, 94, 120, 120, 31, 0, 73, 21, 0, 40, 0, 178, 9, 0, 34, 8, 28, 30, 24, 177, 45, 192, 38, 28, 74, 70, 120, 0, 11, 24, 15, 4, 17, 82, 158, 255, 255, 14, 0, 39, 0, 142, 7, 93, 0, 24, 232, 110, 62, 30, 27, 9, 5, 248, 17, 65, 61, 0, 2, 78, 21, 13, 7, 0, 64, 80, 127, 124, 79, 3, 187, 0, 17, 245, 0, 15, 185, 3, 41, 40, 255, 183, 58, 122, 12, 23, 0, 122, 116, 0, 20, 0, 84, 0, 42, 22, 0, 62, 7, 44, 23, 175, 13, 0, 0, 0, 0, 77, 0, 140, 156, 73, 0, 0, 34, 112, 0, 176, 23, 103, 0, 0, 164, 15, 237, 89, 66, 0, 0, 94, 117, 101, 19, 0, 89, 5, 0, 69, 11, 178, 135, 42, 25, 53, 0, 87, 5, 145, 21, 183, 41, 16, 108, 78, 151, 28, 31, 0, 72, 4, 28, 105, 133, 255, 225, 39, 0, 0, 0, 117, 0, 72, 0, 40, 235, 116, 92, 100, 27, 71, 0, 255, 54, 159, 17, 0, 2, 74, 41, 58, 7, 0, 83, 94, 119, 48, 65, 3, 120, 29, 4, 57, 13, 12, 117, 0, 55, 12, 243, 139, 20, 104, 34, 27, 0, 60, 89, 0, 75, 74, 0, 0, 21, 34, 63, 58, 21, 106, 69, 170, 13, 0, 54, 0, 15, 92, 0, 73, 219, 15, 0, 0, 62, 71, 0, 199, 21, 82, 0, 25, 158, 66, 237, 117, 164, 0, 0, 60, 49, 84, 4, 0, 133, 29, 0, 69, 21, 170, 119, 7, 0, 58, 0, 53, 15, 146, 21, 203, 32, 17, 47, 78, 158, 58, 36, 0, 27, 19, 143, 66, 110, 246, 255, 17, 55, 0, 0, 152, 87, 89, 12, 14, 250, 121, 81, 159, 27, 32, 0, 254, 28, 19, 35, 0, 2, 98, 94, 24, 7, 0, 31, 100, 127, 0, 61, 3, 193, 7, 4, 139, 6, 18, 209, 0, 75, 0, 255, 199, 50, 135, 12, 112, 0, 205, 94, 20, 36, 0, 0, 0, 29, 23, 0, 28, 27, 109, 23, 144, 13, 0, 30, 0, 0, 65, 21, 13, 255, 44, 0, 0, 79, 69, 63, 199, 0, 192, 0, 0, 120, 35, 237, 100, 188, 0, 0, 94, 60, 94, 4, 0, 55, 17, 0, 69, 0, 136, 62, 2, 0, 34, 27, 54, 0, 172, 21, 112, 12, 28, 47, 48, 132, 0, 28, 0, 5, 4, 28, 50, 111, 184, 251, 73, 0, 0, 0, 55, 0, 85, 31, 66, 255, 138, 2, 170, 27, 58, 0, 206, 28, 69, 25, 0, 2, 81, 47, 32, 7, 0, 38, 56, 174, 59, 79, 3, 118, 35, 4, 255, 18, 12, 205, 0, 38, 0, 255, 202, 39, 131, 12, 1, 0, 60, 23, 0, 10, 0, 0, 0, 59, 54, 83, 55, 30, 82, 71, 166, 13, 0, 0, 5, 0, 92, 0, 81, 99, 86, 0, 0, 96, 71, 14, 166, 14, 111, 8, 18, 162, 24, 237, 110, 101, 0, 0, 94, 118, 113, 10, 0, 76, 32, 0, 58, 2, 175, 61, 0, 6, 79, 0, 59, 25, 136, 21, 179, 82, 75, 21, 62, 96, 52, 11, 0, 29, 4, 17, 75, 164, 254, 252, 13, 50, 0, 42, 152, 8, 89, 0, 43, 255, 138, 17, 133, 62, 42, 7, 239, 0, 62, 0, 65, 2, 145, 98, 0, 7, 0, 30, 80, 149, 4, 76, 3, 87, 29, 4, 147, 0, 12, 180, 0, 49, 41, 255, 221, 74, 114, 12, 1, 0, 208, 36, 0, 62, 0, 0, 0, 34, 0, 57, 15, 13, 89, 21, 154, 13, 0, 0, 1, 0, 58, 64, 72, 180, 128, 0, 0, 167, 82, 81, 154, 40, 151, 0, 26, 77, 33, 237, 116, 167, 0, 0, 58, 97, 126, 11, 0, 81, 16, 0, 48, 25, 136, 143, 0, 19, 56, 0, 92, 24, 150, 21, 183, 26, 75, 105, 8, 97, 0, 14, 0, 5, 6, 49, 82, 152, 249, 255, 16, 0, 0, 0, 149, 0, 84, 75, 16, 229, 130, 47, 183, 27, 97, 0, 239, 0, 60, 46, 0, 2, 120, 64, 14, 7, 0, 70, 87, 125, 0, 61, 3, 155, 23, 23, 180, 0, 17, 148, 48, 67, 87, 255, 201, 74, 107, 12, 1, 0, 118, 87, 0, 21, 0, 0, 0, 47, 9, 0, 12, 8, 88, 26, 179, 13, 0, 32, 0, 16, 88, 25, 113, 206, 55, 0, 0, 100, 107, 17, 199, 16, 103, 8, 18, 100, 55, 237, 125, 219, 0, 0, 43, 79, 121, 17, 0, 127, 45, 0, 69, 2, 154, 97, 0, 0, 39, 6, 55, 3, 145, 21, 186, 58, 15, 61, 75, 84, 0, 23, 0, 21, 51, 84, 88, 106, 255, 173, 8, 112, 0, 0, 159, 65, 93, 16, 14, 255, 127, 86, 110, 37, 0, 0, 246, 31, 56, 11, 4, 6, 88, 87, 0, 7, 0, 57, 81, 146, 0, 61, 7, 104, 26, 4, 137, 0, 7, 114, 0, 29, 0, 241, 209, 65, 124, 12, 112, 0, 132, 80, 0, 21, 0, 2, 0, 40, 45, 0, 37, 2, 61, 20, 178, 13, 0, 0, 17, 0, 50, 75, 22, 211, 83, 0, 0, 58, 14, 0, 199, 102, 146, 1, 0, 132, 10, 237, 99, 174, 0, 0, 94, 71, 109, 19, 0, 110, 14, 0, 69, 0, 172, 92, 0, 42, 20, 21, 49, 55, 174, 21, 169, 46, 70, 107, 54, 150, 0, 21, 0, 43, 4, 114, 83, 121, 255, 255, 10, 0, 0, 28, 158, 0, 89, 0, 39, 255, 118, 43, 179, 27, 72, 0, 214, 0, 56, 82, 0, 7, 134, 34, 31, 7, 0, 120, 103, 127, 0, 58, 3, 121, 7, 12, 93, 0, 7, 160, 0, 43, 0, 255, 198, 9, 120, 12, 1, 0, 77, 57, 0, 20, 0, 0, 0, 43, 78, 0, 14, 32, 83, 8, 213, 13, 0, 0, 25, 0, 108, 21, 145, 200, 37, 0, 0, 53, 69, 11, 199, 32, 100, 0, 64, 130, 27, 237, 63, 79, 0, 0, 91, 92, 116, 13, 0, 119, 26, 0, 69, 32, 186, 62, 0, 0, 116, 0, 58, 54, 167, 21, 161, 46, 16, 57, 54, 105, 0, 11, 0, 64, 4, 18, 98, 135, 251, 215, 7, 1, 39, 0, 167, 0, 84, 0, 8, 255, 100, 26, 179, 27, 22, 0, 255, 0, 26, 17, 0, 19, 97, 100, 26, 7, 0, 56, 80, 144, 0, 74, 3, 139, 33, 17, 255, 69, 7, 148, 0, 35, 0, 252, 239, 26, 129, 12, 1, 0, 139, 36, 0, 9, 0, 0, 0, 43, 34, 0, 8, 18, 133, 21, 158, 13, 0, 44, 0, 18, 59, 0, 57, 255, 118, 0, 0, 79, 98, 7, 199, 0, 106, 21, 33, 70, 4, 237, 66, 106, 0, 0, 91, 143, 115, 4, 0, 115, 35, 0, 61, 69, 181, 20, 0, 14, 82, 0, 27, 68, 174, 27, 148, 6, 48, 9, 78, 110, 20, 29, 0, 65, 16, 23, 140, 124, 255, 170, 52, 88, 32, 0, 167, 51, 93, 55, 30, 255, 113, 5, 122, 27, 0, 0, 255, 6, 112, 0, 35, 5, 145, 43, 40, 7, 0, 64, 93, 123, 0, 73, 3, 115, 0, 36, 123, 0, 15, 163, 20, 40, 85, 229, 225, 53, 135, 12, 10, 0, 69, 35, 0, 7, 0, 20, 0, 11, 0, 65, 4, 10, 139, 8, 151, 13, 0, 0, 0, 0, 98, 55, 138, 224, 111, 0, 0, 96, 74, 0, 187, 3, 107, 0, 0, 151, 5, 237, 150, 120, 0, 0, 77, 87, 102, 16, 0, 75, 46, 0, 54, 46, 153, 35, 25, 11, 55, 0, 57, 27, 184, 29, 200, 22, 46, 71, 78, 153, 89, 42, 154, 61, 4, 63, 103, 103, 228, 255, 10, 0, 0, 122, 169, 0, 93, 21, 47, 255, 123, 71, 164, 27, 0, 0, 255, 0, 58, 6, 8, 19, 160, 21, 36, 7, 0, 49, 101, 163, 21, 79, 3, 143, 23, 21, 30, 0, 7, 245, 9, 41, 18, 255, 205, 49, 132, 12, 1, 0, 95, 114, 0, 5, 0, 75, 0, 12, 29, 6, 8, 17, 116, 2, 172, 13, 0, 0, 3, 0, 83, 75, 87, 166, 53, 0, 0, 85, 111, 0, 172, 14, 98, 0, 100, 149, 9, 237, 164, 112, 0, 12, 94, 70, 120, 19, 0, 88, 45, 46, 69, 42, 174, 108, 0, 1, 59, 0, 37, 25, 177, 21, 159, 45, 35, 90, 47, 142, 65, 18, 60, 39, 4, 71, 76, 157, 255, 141, 8, 0, 0, 0, 169, 0, 83, 0, 36, 254, 145, 46, 119, 27, 6, 0, 255, 0, 30, 16, 0, 2, 72, 60, 25, 7, 0, 46, 111, 182, 0, 69, 3, 177, 25, 16, 101, 1, 7, 187, 0, 21, 0, 220, 172, 21, 119, 12, 1, 0, 137, 41, 0, 41, 0, 0, 0, 14, 0, 30, 14, 1, 82, 11, 139, 13, 0, 87, 0, 5, 24, 0, 51, 225, 19, 0, 0, 81, 66, 0, 199, 6, 109, 1, 0, 115, 44, 225, 105, 126, 0, 0, 89, 58, 96, 4, 0, 92, 5, 0, 49, 12, 181, 110, 42, 25, 91, 0, 80, 43, 164, 21, 173, 18, 74, 37, 67, 159, 58, 18, 0, 15, 4, 74, 112, 138, 255, 248, 13, 0, 0, 20, 163, 0, 93, 10, 34, 240, 128, 66, 170, 27, 36, 0, 219, 34, 60, 26, 0, 2, 177, 64, 20, 7, 0, 33, 92, 158, 0, 79, 3, 148, 27, 18, 83, 15, 33, 206, 0, 34, 6, 255, 192, 38, 116, 12, 35, 0, 100, 75, 0, 7, 0, 0, 0, 29, 52, 24, 32, 11, 76, 38, 153, 13, 0, 19, 0, 0, 95, 35, 88, 227, 35, 0, 0, 78, 118, 13, 174, 0, 135, 0, 0, 131, 58, 214, 103, 156, 0, 0, 72, 71, 97, 13, 0, 106, 5, 0, 45, 0, 146, 68, 0, 19, 47, 38, 35, 57, 151, 85, 200, 6, 15, 11, 78, 179, 75, 11, 0, 116, 4, 90, 94, 97, 231, 255, 18, 0, 0, 0, 156, 29, 85, 36, 50, 209, 85, 44, 168, 27, 33, 0, 255, 34, 80, 67, 0, 27, 107, 58, 11, 7, 0, 76, 110, 189, 0, 79, 3, 202, 8, 24, 58, 28, 21, 194, 0, 26, 0, 255, 202, 74, 131, 12, 1, 0, 132, 60, 0, 16, 5, 8, 0, 35, 19, 0, 23, 19, 35, 2, 197, 13, 0, 68, 0, 0, 94, 33, 97, 168, 82, 0, 0, 38, 146, 0, 187, 34, 183, 4, 0, 132, 15, 220, 93, 128, 0, 0, 91, 113, 97, 16, 0, 78, 21, 0, 23, 2, 154, 60, 0, 22, 52, 11, 39, 27, 171, 147, 207, 23, 74, 98, 75, 122, 41, 11, 0, 38, 4, 51, 49, 75, 208, 254, 20, 0, 0, 0, 149, 0, 93, 0, 19, 239, 104, 17, 96, 27, 18, 16, 240, 0, 27, 0, 0, 25, 116, 85, 5, 7, 6, 34, 112, 146, 0, 79, 3, 219, 10, 35, 80, 58, 30, 255, 44, 30, 0, 255, 224, 74, 130, 12, 1, 0, 139, 58, 10, 39, 0, 5, 0, 1, 4, 4, 6, 35, 22, 2, 189, 13, 0, 10, 20, 11, 85, 0, 79, 157, 144, 0, 0, 46, 132, 0, 192, 32, 127, 6, 22, 78, 36, 237, 86, 142, 0, 0, 72, 41, 103, 14, 0, 98, 41, 0, 69, 10, 167, 103, 0, 13, 52, 15, 62, 0, 164, 23, 207, 30, 57, 45, 71, 99, 0, 24, 0, 78, 4, 55, 129, 79, 240, 255, 51, 0, 0, 0, 146, 23, 93, 0, 60, 216, 139, 2, 183, 27, 41, 0, 240, 0, 75, 13, 0, 6, 91, 41, 0, 7, 0, 107, 95, 157, 41, 69, 3, 148, 6, 12, 237, 0, 14, 173, 33, 28, 0, 255, 188, 65, 135, 12, 14, 0, 115, 42, 0, 22, 0, 0, 0, 43, 58, 43, 8, 29, 84, 2, 183, 13, 0, 0, 0, 0, 139, 5, 49, 206, 10, 0, 0, 52, 118, 0, 173, 9, 96, 6, 7, 200, 71, 231, 76, 120, 0, 0, 73, 124, 101, 48, 0, 110, 23, 10, 25, 9, 176, 54, 0, 0, 113, 31, 75, 51, 168, 26, 198, 95, 25, 67, 65, 155, 87, 33, 0, 104, 4, 30, 99, 127, 231, 159, 11, 46, 10, 0, 161, 0, 93, 0, 34, 255, 141, 2, 172, 27, 0, 0, 255, 16, 139, 4, 0, 2, 174, 85, 1, 7, 0, 55, 83, 151, 37, 64, 3, 148, 8, 5, 251, 0, 11, 194, 0, 63, 26, 241, 167, 53, 100, 12, 20, 0, 88, 67, 0, 0, 0, 0, 0, 58, 67, 65, 17, 19, 129, 2, 153, 13, 0, 0, 0, 63, 63, 0, 70, 179, 127, 0, 0, 65, 95, 34, 192, 46, 68, 1, 103, 111, 36, 226, 130, 125, 0, 0, 82, 118, 120, 14, 0, 90, 7, 0, 69, 26, 169, 52, 0, 18, 110, 0, 43, 0, 182, 33, 187, 30, 38, 55, 49, 113, 92, 11, 0, 52, 4, 26, 75, 102, 241, 177, 49, 0, 0, 0, 129, 0, 80, 8, 5, 255, 136, 20, 90, 27, 0, 4, 255, 2, 48, 24, 26, 2, 120, 27, 4, 7, 0, 88, 87, 144, 0, 79, 3, 80, 0, 14, 122, 24, 11, 209, 0, 37, 0, 249, 202, 39, 97, 12, 1, 0, 161, 40, 0, 59, 0, 57, 0, 35, 0, 48, 4, 26, 116, 2, 168, 13, 0, 124, 0, 0, 109, 0, 89, 132, 55, 0, 0, 79, 79, 0, 199, 62, 129, 2, 0, 110, 37, 237, 138, 123, 0, 0, 82, 97, 117, 14, 0, 96, 8, 6, 65, 24, 167, 86, 0, 0, 97, 0, 49, 0, 152, 50, 196, 5, 37, 23, 71, 118, 56, 11, 0, 140, 9, 32, 101, 89, 255, 188, 58, 82, 28, 141, 126, 33, 68, 0, 5, 212, 138, 13, 37, 27, 0, 58, 247, 0, 84, 41, 16, 2, 106, 36, 0, 7, 0, 132, 99, 174, 0, 79, 3, 124, 0, 4, 124, 0, 12, 156, 5, 49, 39, 200, 118, 74, 80, 12, 24, 0, 213, 52, 0, 37, 0, 12, 0, 1, 10, 0, 4, 23, 96, 26, 192, 13, 0, 0, 64, 0, 76, 0, 90, 180, 87, 0, 0, 57, 29, 0, 183, 75, 108, 1, 23, 182, 23, 220, 86, 158, 0, 0, 94, 68, 105, 4, 0, 111, 7, 0, 60, 52, 152, 119, 42, 27, 99, 0, 63, 26, 178, 21, 182, 7, 51, 26, 78, 167, 0, 25, 0, 32, 4, 40, 103, 110, 243, 227, 1, 0, 0, 0, 131, 0, 93, 0, 48, 246, 128, 10, 140, 27, 27, 0, 245, 0, 118, 0, 0, 6, 159, 59, 57, 7, 0, 75, 97, 138, 0, 79, 3, 182, 0, 10, 255, 0, 7, 255, 0, 37, 0, 250, 211, 74, 96, 12, 1, 0, 69, 22, 0, 22, 8, 0, 0, 53, 26, 89, 4, 58, 143, 7, 156, 13, 0, 92, 0, 54, 116, 66, 53, 219, 89, 0, 0, 95, 122, 0, 176, 0, 65, 20, 0, 141, 37, 237, 111, 142, 0, 0, 94, 97, 136, 4, 0, 98, 9, 0, 69, 12, 150, 64, 3, 31, 22, 24, 36, 63, 148, 21, 207, 11, 130, 73, 63, 97, 29, 21, 9, 38, 4, 82, 64, 73, 255, 248, 2, 64, 0, 72, 124, 10, 93, 0, 70, 238, 128, 43, 141, 27, 0, 0, 250, 0, 131, 0, 20, 7, 179, 30, 0, 7, 0, 80, 106, 144, 0, 79, 3, 169, 0, 17, 88, 0, 36, 217, 88, 39, 0, 242, 225, 72, 108, 12, 1, 0, 148, 60, 0, 34, 0, 5, 0, 6, 0, 13, 13, 45, 33, 2, 196, 13, 0, 0, 142, 0, 138, 0, 115, 208, 118, 0, 0, 42, 126, 0, 192, 17, 137, 2, 39, 138, 35, 237, 197, 70, 0, 0, 63, 100, 114, 4, 0, 83, 11, 12, 69, 26, 186, 82, 0, 0, 34, 0, 60, 68, 175, 21, 207, 51, 3, 90, 78, 220, 87, 11, 3, 13, 4, 26, 28, 111, 255, 255, 1, 5, 36, 67, 169, 109, 93, 0, 5, 244, 66, 43, 183, 27, 51, 0, 222, 1, 109, 74, 0, 2, 58, 40, 0, 7, 0, 22, 117, 186, 0, 0, 3, 74, 67, 4, 132, 19, 19, 239, 9, 97, 0, 219, 207, 69, 140, 12, 170, 0, 97, 24, 0, 104, 0, 0, 0, 10, 13, 34, 4, 1, 30, 32, 147, 13, 0, 144, 0, 204, 38, 14, 45, 233, 15, 0, 0, 32, 19, 45, 178, 37, 70, 11, 65, 127, 33, 237, 194, 135, 0, 0, 88, 86, 136, 4, 0, 62, 43, 30, 65, 26, 172, 60, 0, 22, 61, 0, 43, 0, 182, 42, 113, 68, 19, 42, 71, 83, 0, 32, 0, 61, 4, 46, 29, 79, 222, 255, 68, 0, 0, 46, 79, 66, 93, 0, 47, 244, 137, 6, 139, 27, 85, 0, 246, 59, 78, 7, 63, 2, 143, 27, 0, 7, 0, 49, 79, 109, 0, 33, 3, 168, 73, 11, 172, 25, 17, 157, 20, 45, 0, 239, 197, 29, 123, 12, 97, 0, 105, 19, 13, 255, 125, 55, 33, 48, 30, 62, 18, 12, 62, 14, 143, 27, 0, 0, 0, 0, 38, 8, 38, 212, 10, 0, 0, 70, 127, 30, 196, 0, 130, 18, 0, 137, 10, 237, 146, 119, 0, 48, 88, 85, 106, 4, 0, 56, 5, 0, 60, 16, 179, 113, 0, 31, 67, 0, 67, 55, 142, 21, 207, 12, 21, 0, 51, 186, 64, 16, 0, 21, 4, 74, 38, 117, 255, 156, 43, 15, 9, 107, 153, 2, 83, 6, 16, 255, 76, 7, 71, 27, 36, 0, 236, 47, 42, 39, 0, 9, 83, 65, 0, 7, 0, 20, 113, 124, 0, 69, 3, 146, 0, 14, 78, 26, 15, 255, 1, 42, 7, 238, 207, 59, 98, 12, 1, 0, 117, 16, 0, 26, 0, 0, 0, 3, 0, 35, 10, 6, 21, 2, 129, 13, 0, 0, 0, 0, 24, 26, 127, 183, 56, 0, 0, 92, 82, 0, 113, 47, 101, 3, 9, 129, 29, 237, 106, 112, 0, 0, 85, 94, 106, 4, 0, 101, 5, 0, 55, 23, 170, 51, 20, 46, 56, 0, 83, 41, 154, 21, 200, 22, 13, 40, 49, 168, 11, 17, 0, 9, 4, 10, 43, 113, 228, 255, 11, 0, 24, 143, 139, 7, 77, 0, 28, 255, 120, 20, 23, 27, 11, 0, 211, 18, 71, 0, 8, 20, 123, 48, 0, 7, 0, 61, 115, 120, 0, 79, 3, 216, 6, 13, 179, 8, 15, 204, 0, 41, 0, 255, 186, 61, 121, 12, 19, 0, 53, 57, 22, 62, 0, 0, 0, 8, 28, 89, 22, 11, 61, 2, 124, 13, 0, 46, 4, 0, 62, 0, 113, 159, 49, 0, 0, 134, 72, 14, 199, 44, 75, 17, 12, 120, 57, 237, 103, 108, 0, 0, 83, 58, 144, 4, 0, 77, 5, 0, 69, 23, 154, 18, 0, 30, 18, 40, 74, 0, 146, 51, 177, 15, 3, 44, 78, 121, 23, 11, 0, 12, 14, 19, 50, 107, 224, 215, 0, 0, 52, 74, 121, 5, 83, 53, 41, 255, 77, 12, 100, 27, 90, 36, 245, 14, 22, 11, 1, 11, 111, 62, 47, 7, 0, 107, 101, 166, 0, 79, 3, 143, 43, 8, 190, 0, 7, 197, 15, 46, 0, 255, 156, 18, 140, 12, 21, 0, 53, 41, 43, 53, 133, 53, 7, 38, 85, 53, 4, 9, 74, 26, 139, 13, 0, 0, 0, 0, 93, 49, 67, 128, 1, 0, 0, 59, 74, 0, 141, 46, 67, 12, 0, 153, 14, 237, 129, 164, 0, 0, 94, 67, 147, 22, 0, 88, 15, 0, 65, 23, 168, 58, 41, 0, 110, 0, 24, 43, 155, 66, 175, 99, 24, 34, 78, 145, 0, 11, 0, 13, 23, 62, 102, 140, 254, 233, 29, 0, 0, 0, 136, 0, 70, 12, 40, 218, 133, 20, 183, 27, 58, 3, 255, 43, 62, 4, 0, 7, 137, 70, 0, 7, 0, 83, 113, 173, 0, 38, 3, 164, 45, 12, 75, 0, 7, 176, 0, 22, 4, 246, 169, 72, 123, 12, 64, 0, 111, 35, 127, 15, 0, 34, 0, 63, 11, 19, 13, 1, 77, 43, 157, 13, 0, 11, 0, 31, 30, 0, 0, 218, 1, 0, 0, 28, 113, 0, 193, 6, 117, 0, 15, 120, 9, 237, 175, 123, 0, 0, 94, 135, 90, 16, 0, 114, 52, 0, 53, 25, 142, 73, 0, 48, 77, 0, 28, 31, 78, 21, 186, 32, 14, 31, 78, 162, 61, 11, 0, 23, 4, 24, 155, 115, 234, 232, 26, 35, 0, 0, 142, 58, 89, 73, 39, 222, 145, 55, 176, 27, 12, 0, 255, 36, 84, 0, 0, 2, 87, 68, 70, 7, 0, 31, 115, 167, 0, 79, 3, 179, 58, 16, 73, 35, 15, 185, 0, 28, 0, 255, 216, 69, 93, 12, 112, 0, 53, 97, 68, 50, 0, 25, 0, 24, 0, 48, 4, 1, 71, 65, 167, 13, 0, 13, 0, 0, 9, 0, 80, 181, 102, 0, 0, 64, 142, 17, 176, 63, 101, 16, 2, 133, 41, 237, 62, 98, 0, 0, 77, 185, 92, 24, 0, 103, 24, 0, 69, 22, 110, 91, 0, 40, 20, 0, 44, 0, 101, 34, 172, 0, 40, 42, 8, 111, 63, 11, 59, 51, 4, 40, 85, 45, 165, 190, 23, 0, 4, 91, 129, 0, 20, 52, 67, 190, 128, 5, 143, 27, 0, 0, 255, 37, 106, 40, 0, 2, 140, 28, 15, 7, 0, 70, 114, 175, 0, 74, 3, 193, 46, 13, 39, 30, 7, 159, 97, 39, 0, 248, 233, 48, 107, 12, 33, 0, 59, 72, 0, 28, 0, 0, 0, 59, 0, 0, 16, 39, 125, 53, 214, 13, 0, 21, 13, 55, 65, 0, 135, 139, 249, 0, 0, 103, 114, 0, 144, 23, 90, 30, 0, 71, 11, 237, 60, 140, 0, 0, 94, 39, 149, 21, 0, 88, 22, 0, 69, 9, 169, 71, 44, 18, 42, 0, 56, 0, 93, 21, 201, 38, 59, 79, 57, 94, 0, 11, 30, 22, 4, 10, 56, 83, 245, 168, 13, 0, 0, 145, 110, 0, 62, 0, 33, 254, 113, 22, 111, 27, 54, 9, 212, 0, 70, 5, 0, 2, 221, 83, 16, 7, 0, 55, 94, 107, 0, 60, 3, 127, 45, 36, 255, 30, 9, 199, 31, 39, 0, 255, 184, 43, 82, 51, 48, 0, 73, 39, 0, 13, 0, 0, 0, 18, 105, 19, 29, 48, 35, 39, 198, 13, 0, 0, 0, 42, 67, 40, 57, 129, 17, 0, 0, 85, 71, 0, 181, 26, 69, 1, 35, 151, 69, 237, 116, 143, 0, 0, 94, 106, 109, 4, 0, 79, 43, 12, 69, 41, 176, 63, 54, 10, 66, 0, 56, 0, 171, 21, 190, 31, 49, 75, 65, 106, 45, 20, 0, 87, 30, 63, 57, 195, 224, 180, 15, 5, 0, 0, 115, 0, 93, 0, 39, 230, 108, 52, 100, 41, 0, 40, 255, 84, 29, 19, 0, 2, 81, 123, 16, 7, 0, 112, 68, 175, 38, 56, 3, 77, 11, 35, 190, 0, 11, 221, 0, 50, 7, 243, 179, 74, 122, 12, 1, 0, 221, 35, 0, 6, 13, 0, 0, 9, 0, 20, 14, 5, 114, 11, 198, 13, 0, 29, 0, 24, 107, 58, 55, 255, 78, 0, 0, 105, 63, 31, 199, 0, 109, 5, 0, 169, 61, 237, 99, 122, 0, 0, 77, 113, 119, 26, 0, 89, 11, 0, 69, 31, 166, 98, 43, 29, 68, 0, 51, 0, 173, 33, 187, 33, 85, 42, 14, 101, 58, 27, 0, 30, 4, 10, 86, 153, 214, 209, 45, 7, 0, 116, 129, 0, 69, 0, 22, 249, 133, 12, 110, 27, 0, 0, 255, 63, 26, 0, 58, 2, 153, 80, 0, 7, 0, 68, 58, 162, 0, 67, 3, 113, 20, 29, 86, 0, 17, 218, 0, 31, 63, 255, 227, 74, 124, 12, 1, 0, 202, 55, 0, 69, 0, 0, 0, 12, 0, 47, 8, 14, 106, 7, 188, 13, 0, 17, 29, 0, 125, 0, 91, 229, 83, 0, 0, 168, 95, 46, 199, 4, 118, 20, 30, 153, 14, 237, 93, 121, 0, 0, 94, 101, 109, 53, 0, 114, 13, 0, 60, 2, 179, 62, 9, 20, 26, 0, 68, 0, 187, 21, 188, 14, 83, 102, 78, 109, 0, 11, 0, 5, 4, 54, 92, 86, 255, 255, 12, 15, 0, 0, 128, 0, 39, 3, 9, 255, 145, 16, 81, 27, 0, 0, 255, 0, 154, 32, 7, 44, 123, 39, 59, 7, 0, 85, 55, 155, 8, 59, 3, 167, 7, 32, 222, 0, 18, 207, 0, 43, 0, 252, 239, 74, 140, 12, 1, 0, 51, 51, 0, 14, 0, 0, 0, 30, 34, 71, 4, 44, 77, 10, 179, 13, 0, 0, 13, 0, 97, 0, 113, 194, 143, 0, 0, 60, 65, 0, 199, 1, 107, 0, 0, 112, 38, 237, 89, 173, 0, 0, 89, 41, 90, 31, 0, 113, 17, 0, 44, 0, 162, 163, 20, 0, 53, 0, 53, 30, 180, 21, 200, 41, 0, 55, 75, 129, 0, 16, 0, 38, 7, 76, 65, 81, 255, 255, 11, 44, 0, 0, 128, 42, 82, 0, 23, 255, 107, 17, 183, 27, 70, 37, 232, 0, 63, 72, 0, 9, 60, 79, 31, 7, 0, 44, 93, 170, 58, 73, 3, 179, 2, 4, 197, 5, 22, 197, 37, 70, 0, 240, 210, 67, 129, 12, 1, 0, 68, 20, 0, 20, 0, 0, 0, 16, 120, 33, 4, 32, 37, 2, 169, 13, 0, 0, 0, 0, 67, 75, 93, 176, 25, 0, 0, 84, 60, 0, 199, 35, 109, 73, 0, 177, 43, 237, 82, 100, 0, 0, 89, 149, 88, 39, 0, 120, 27, 0, 69, 3, 165, 95, 59, 0, 50, 0, 44, 0, 164, 21, 157, 42, 49, 118, 50, 127, 26, 17, 0, 105, 11, 11, 76, 148, 255, 225, 24, 0, 0, 79, 154, 0, 93, 0, 18, 252, 108, 94, 170, 27, 0, 0, 255, 39, 176, 0, 0, 14, 189, 40, 0, 7, 0, 54, 90, 167, 0, 76, 3, 99, 17, 12, 41, 67, 27, 173, 0, 21, 0, 243, 214, 74, 105, 12, 1, 0, 129, 79, 29, 22, 0, 0, 0, 34, 0, 17, 34, 38, 35, 4, 228, 13, 0, 73, 0, 54, 128, 29, 95, 255, 126, 0, 0, 89, 81, 66, 199, 21, 164, 2, 0, 133, 32, 237, 62, 118, 0, 48, 94, 63, 106, 40, 0, 87, 40, 0, 69, 14, 178, 105, 5, 0, 69, 0, 53, 55, 187, 21, 163, 83, 32, 74, 53, 99, 28, 11, 0, 17, 8, 37, 28, 113, 255, 252, 23, 65, 0, 48, 144, 35, 86, 0, 34, 248, 98, 65, 165, 31, 52, 6, 234, 0, 82, 0, 0, 18, 51, 98, 57, 7, 0, 77, 102, 174, 0, 75, 7, 114, 4, 18, 65, 0, 7, 135, 39, 28, 39, 255, 182, 35, 111, 12, 1, 0, 89, 65, 0, 12, 0, 0, 0, 3, 73, 47, 12, 32, 46, 15, 184, 13, 0, 0, 122, 0, 78, 0, 109, 175, 29, 0, 0, 50, 49, 45, 199, 43, 121, 11, 107, 94, 24, 237, 104, 98, 0, 0, 77, 92, 94, 9, 0, 71, 11, 14, 69, 5, 168, 82, 0, 0, 69, 3, 41, 68, 184, 23, 207, 82, 9, 106, 46, 87, 47, 11, 41, 16, 12, 50, 68, 108, 241, 195, 3, 0, 39, 79, 157, 47, 82, 0, 5, 224, 92, 46, 157, 32, 64, 1, 255, 16, 22, 53, 13, 5, 128, 77, 42, 7, 0, 26, 90, 158, 0, 79, 8, 135, 34, 26, 136, 0, 7, 237, 0, 24, 71, 255, 176, 50, 134, 12, 1, 0, 61, 35, 0, 48, 0, 10, 0, 6, 0, 42, 21, 30, 93, 17, 180, 13, 0, 94, 0, 0, 106, 0, 36, 167, 43, 0, 0, 43, 87, 0, 135, 48, 115, 20, 0, 138, 32, 237, 100, 110, 0, 0, 80, 75, 92, 10, 0, 84, 5, 24, 0, 0, 175, 109, 0, 0, 39, 6, 70, 68, 155, 26, 201, 3, 102, 86, 43, 89, 0, 11, 0, 37, 4, 51, 66, 73, 250, 217, 1, 24, 23, 0, 164, 4, 93, 0, 5, 255, 127, 25, 143, 27, 36, 41, 241, 9, 139, 62, 0, 2, 142, 73, 30, 7, 0, 24, 98, 152, 0, 79, 3, 169, 10, 41, 81, 42, 7, 203, 35, 47, 0, 237, 174, 70, 125, 12, 21, 0, 52, 55, 0, 12, 0, 0, 0, 18, 17, 57, 4, 31, 15, 6, 157, 13, 0, 0, 25, 0, 59, 0, 73, 197, 25, 0, 0, 41, 57, 0, 168, 23, 85, 0, 65, 68, 25, 237, 188, 135, 44, 0, 74, 133, 114, 9, 0, 74, 24, 0, 31, 39, 155, 80, 0, 30, 24, 0, 49, 0, 163, 21, 196, 17, 56, 66, 54, 68, 57, 11, 61, 5, 4, 65, 64, 69, 249, 255, 10, 0, 17, 10, 137, 0, 93, 0, 11, 244, 141, 23, 151, 27, 0, 17, 255, 0, 162, 0, 64, 15, 94, 29, 0, 7, 0, 37, 87, 126, 0, 79, 3, 213, 25, 16, 16, 0, 7, 246, 71, 22, 47, 241, 194, 49, 134, 12, 55, 0, 148, 66, 0, 20, 0, 40, 0, 18, 0, 19, 13, 35, 68, 13, 178, 13, 0, 26, 0, 0, 49, 0, 63, 176, 111, 0, 0, 78, 114, 0, 133, 17, 138, 56, 0, 148, 0, 237, 170, 141, 0, 17, 94, 105, 96, 57, 0, 119, 60, 0, 69, 43, 108, 108, 0, 0, 77, 6, 63, 63, 110, 21, 183, 65, 87, 58, 62, 107, 75, 21, 13, 63, 4, 37, 37, 39, 241, 169, 8, 0, 0, 119, 133, 0, 93, 8, 94, 231, 84, 49, 178, 27, 0, 0, 242, 28, 83, 0, 0, 99, 163, 65, 0, 7, 0, 34, 105, 156, 0, 79, 3, 118, 65, 12, 156, 0, 20, 177, 15, 46, 0, 255, 191, 64, 117, 68, 58, 0, 170, 34, 0, 63, 120, 0, 5, 13, 0, 4, 7, 115, 120, 26, 198, 13, 0, 23, 0, 83, 121, 0, 0, 119, 147, 0, 0, 108, 23, 0, 199, 42, 92, 18, 0, 54, 0, 237, 136, 200, 0, 0, 89, 68, 92, 18, 0, 86, 16, 0, 48, 12, 152, 77, 36, 22, 80, 48, 35, 55, 184, 21, 200, 57, 116, 86, 42, 166, 58, 21, 0, 83, 4, 84, 133, 155, 254, 202, 9, 6, 0, 36, 169, 12, 93, 0, 11, 255, 131, 79, 176, 27, 5, 0, 255, 0, 75, 0, 0, 2, 58, 63, 0, 7, 0, 39, 115, 143, 0, 79, 3, 130, 26, 16, 40, 63, 57, 217, 0, 28, 0, 255, 243, 67, 114, 12, 10, 0, 164, 60, 0, 28, 0, 0, 0, 1, 0, 0, 12, 19, 94, 25, 201, 13, 0, 49, 0, 0, 87, 0, 61, 201, 42, 0, 0, 28, 117, 40, 199, 47, 145, 1, 0, 136, 21, 237, 68, 168, 0, 0, 94, 55, 98, 28, 0, 82, 21, 0, 53, 10, 142, 90, 48, 0, 89, 13, 43, 68, 181, 21, 161, 6, 48, 85, 78, 160, 0, 11, 0, 19, 16, 91, 85, 141, 255, 196, 0, 35, 0, 69, 163, 0, 91, 0, 11, 217, 129, 2, 125, 27, 46, 0, 210, 23, 119, 32, 0, 1, 149, 96, 0, 7, 8, 27, 112, 179, 49, 76, 3, 173, 21, 4, 92, 32, 7, 142, 0, 30, 0, 255, 154, 74, 140, 12, 18, 0, 89, 26, 48, 72, 0, 0, 0, 21, 19, 23, 8, 16, 82, 2, 142, 13, 0, 28, 0, 0, 97, 0, 39, 145, 96, 0, 0, 37, 70, 0, 185, 23, 122, 9, 58, 160, 40, 237, 86, 113, 0, 0, 94, 153, 130, 8, 0, 87, 5, 0, 25, 16, 128, 48, 34, 23, 31, 51, 27, 63, 187, 53, 151, 15, 22, 69, 53, 115, 17, 14, 0, 74, 4, 49, 48, 53, 166, 232, 0, 31, 6, 0, 139, 0, 89, 29, 5, 234, 84, 7, 144, 27, 0, 0, 255, 20, 41, 10, 0, 23, 128, 39, 0, 7, 0, 23, 103, 124, 19, 75, 3, 218, 52, 13, 212, 0, 7, 168, 58, 45, 119, 255, 130, 65, 138, 12, 1, 0, 217, 93, 12, 23, 0, 1, 0, 15, 2, 67, 53, 50, 102, 20, 166, 13, 0, 33, 0, 17, 110, 0, 79, 104, 168, 0, 0, 64, 69, 0, 192, 31, 114, 39, 0, 174, 19, 237, 95, 148, 0, 0, 26, 69, 113, 16, 0, 83, 50, 0, 69, 32, 157, 80, 22, 21, 121, 11, 24, 55, 187, 24, 182, 81, 128, 62, 69, 159, 31, 36, 0, 30, 12, 67, 64, 79, 204, 191, 8, 0, 0, 0, 159, 66, 84, 0, 5, 244, 115, 14, 144, 36, 0, 0, 255, 0, 52, 0, 16, 16, 163, 32, 0, 7, 0, 27, 103, 56, 0, 44, 7, 190, 94, 8, 156, 0, 40, 212, 48, 33, 0, 233, 230, 38, 123, 12, 53, 0, 188, 50, 0, 42, 0, 8, 0, 29, 11, 0, 16, 24, 45, 16, 185, 13, 0, 0, 0, 0, 75, 0, 53, 255, 40, 0, 0, 69, 121, 0, 163, 0, 184, 7, 92, 71, 30, 237, 93, 115, 0, 0, 62, 141, 76, 30, 0, 90, 17, 0, 0, 25, 181, 38, 0, 1, 38, 0, 53, 27, 174, 83, 182, 27, 85, 96, 71, 99, 16, 37, 31, 58, 20, 76, 60, 90, 255, 252, 14, 0, 0, 0, 128, 0, 76, 8, 11, 231, 124, 62, 121, 37, 32, 0, 238, 1, 106, 44, 0, 50, 106, 56, 31, 7, 0, 78, 84, 164, 10, 16, 8, 143, 38, 35, 133, 33, 12, 190, 0, 52, 21, 208, 157, 64, 100, 12, 71, 0, 127, 119, 0, 20, 69, 0, 0, 42, 0, 50, 31, 54, 71, 20, 177, 13, 0, 108, 0, 37, 63, 0, 92, 203, 144, 0, 0, 37, 80, 0, 187, 0, 152, 0, 0, 186, 25, 220, 83, 77, 0, 0, 89, 59, 109, 18, 0, 100, 16, 0, 69, 0, 169, 73, 0, 3, 43, 0, 68, 0, 158, 40, 178, 0, 85, 84, 78, 103, 40, 11, 0, 51, 4, 67, 61, 149, 255, 255, 0, 0, 0, 0, 145, 0, 93, 25, 5, 255, 76, 18, 66, 27, 46, 0, 255, 0, 62, 0, 0, 2, 118, 59, 42, 7, 0, 42, 101, 175, 67, 79, 3, 136, 29, 8, 98, 22, 12, 149, 44, 26, 0, 255, 171, 62, 118, 12, 1, 0, 138, 41, 16, 77, 0, 34, 0, 58, 43, 16, 8, 23, 19, 2, 141, 13, 0, 0, 0, 0, 84, 70, 92, 224, 80, 0, 0, 74, 66, 23, 193, 20, 162, 0, 28, 104, 48, 237, 111, 179, 0, 0, 94, 97, 118, 22, 0, 94, 7, 0, 69, 13, 161, 90, 0, 35, 87, 0, 57, 0, 167, 70, 139, 7, 132, 28, 47, 86, 0, 52, 0, 23, 12, 28, 99, 168, 225, 239, 17, 0, 0, 0, 118, 75, 93, 82, 5, 253, 91, 2, 157, 27, 0, 0, 255, 37, 95, 0, 5, 12, 120, 61, 0, 7, 20, 49, 109, 189, 0, 79, 3, 124, 0, 12, 239, 0, 36, 168, 0, 31, 14, 255, 213, 74, 113, 12, 1, 0, 236, 33, 0, 53, 0, 0, 0, 51, 0, 0, 26, 13, 67, 2, 185, 13, 0, 0, 27, 0, 117, 0, 81, 195, 65, 0, 0, 90, 100, 70, 170, 0, 139, 0, 55, 114, 55, 237, 95, 82, 0, 0, 94, 90, 117, 16, 0, 86, 22, 0, 25, 31, 162, 75, 0, 13, 52, 0, 81, 13, 154, 58, 184, 40, 57, 13, 78, 110, 34, 11, 0, 59, 4, 60, 104, 148, 255, 249, 41, 0, 5, 39, 166, 0, 93, 13, 15, 214, 132, 12, 171, 27, 0, 6, 238, 23, 179, 0, 4, 2, 125, 40, 46, 7, 0, 75, 87, 151, 0, 79, 3, 155, 0, 27, 77, 0, 21, 102, 0, 70, 21, 255, 111, 30, 112, 12, 17, 0, 51, 16, 0, 20, 0, 0, 0, 1, 17, 95, 17, 28, 16, 12, 160, 13, 0, 33, 0, 0, 64, 0, 77, 206, 128, 0, 0, 42, 111, 0, 169, 64, 56, 7, 25, 120, 53, 237, 158, 95, 0, 45, 94, 114, 80, 4, 0, 107, 5, 0, 20, 20, 164, 47, 6, 48, 20, 0, 58, 0, 158, 42, 195, 0, 43, 64, 64, 103, 55, 11, 87, 64, 4, 49, 83, 59, 255, 255, 6, 0, 0, 0, 95, 11, 88, 51, 26, 255, 118, 11, 152, 27, 0, 0, 255, 11, 24, 65, 0, 5, 42, 37, 7, 7, 0, 91, 65, 138, 0, 79, 3, 250, 6, 8, 242, 37, 14, 209, 55, 74, 0, 221, 223, 34, 118, 12, 1, 0, 103, 69, 30, 26, 3, 0, 0, 6, 25, 56, 20, 78, 90, 27, 161, 13, 0, 0, 0, 0, 62, 0, 122, 223, 126, 0, 0, 55, 161, 0, 199, 0, 116, 16, 0, 149, 50, 237, 255, 61, 0, 0, 2, 80, 113, 4, 0, 56, 37, 0, 69, 22, 183, 18, 24, 7, 30, 23, 35, 68, 163, 27, 190, 16, 0, 61, 78, 201, 24, 24, 0, 5, 4, 62, 75, 131, 255, 255, 0, 15, 0, 0, 164, 54, 93, 18, 5, 244, 50, 2, 183, 27, 21, 0, 204, 0, 64, 60, 14, 2, 90, 33, 10, 7, 0, 47, 113, 186, 0, 0, 3, 116, 25, 4, 93, 29, 31, 238, 23, 21, 0, 197, 224, 70, 132, 12, 130, 0, 39, 23, 65, 166, 0, 75, 0, 40, 71, 89, 10, 1, 64, 26, 77, 13, 0, 125, 0, 181, 34, 75, 30, 255, 80, 0, 0, 55, 72, 78, 199, 0, 129, 24, 48, 132, 23, 203, 209, 99, 0, 0, 94, 78, 104, 4, 0, 68, 87, 13, 69, 22, 186, 71, 30, 4, 81, 0, 41, 0, 124, 21, 131, 53, 27, 71, 75, 82, 0, 52, 0, 15, 4, 63, 28, 110, 248, 255, 44, 0, 0, 22, 123, 36, 93, 0, 5, 255, 122, 2, 76, 27, 0, 9, 243, 15, 70, 0, 34, 2, 161, 55, 0, 7, 0, 51, 81, 110, 36, 61, 3, 80, 70, 8, 173, 0, 32, 154, 0, 59, 78, 255, 196, 47, 124, 12, 64, 0, 97, 27, 0, 248, 28, 53, 0, 27, 9, 69, 30, 19, 58, 2, 153, 54, 0, 0, 0, 0, 51, 0, 5, 198, 93, 0, 0, 124, 74, 83, 199, 21, 93, 48, 0, 147, 15, 237, 116, 133, 0, 0, 67, 130, 78, 19, 0, 84, 27, 0, 58, 0, 175, 63, 0, 17, 56, 0, 35, 43, 165, 36, 189, 76, 47, 50, 26, 191, 2, 17, 35, 76, 4, 76, 31, 121, 252, 172, 48, 0, 0, 40, 165, 0, 93, 0, 17, 247, 98, 22, 167, 27, 0, 0, 255, 0, 79, 3, 0, 12, 79, 25, 32, 7, 0, 14, 101, 158, 0, 62, 3, 146, 38, 17, 49, 0, 7, 224, 28, 22, 3, 255, 196, 74, 101, 12, 1, 0, 205, 28, 0, 36, 0, 48, 0, 3, 0, 0, 4, 16, 35, 2, 195, 13, 0, 59, 0, 54, 39, 42, 89, 177, 94, 0, 0, 45, 73, 0, 199, 44, 85, 18, 10, 99, 13, 237, 129, 98, 0, 0, 73, 50, 121, 48, 0, 81, 18, 0, 69, 0, 180, 68, 67, 0, 6, 0, 89, 68, 162, 44, 207, 26, 12, 113, 38, 132, 56, 21, 0, 30, 9, 30, 50, 120, 212, 252, 24, 0, 0, 37, 122, 0, 93, 0, 5, 224, 101, 65, 152, 27, 51, 0, 255, 0, 82, 44, 0, 22, 120, 26, 35, 7, 0, 37, 79, 152, 53, 79, 3, 150, 19, 23, 95, 17, 7, 102, 15, 30, 0, 255, 169, 74, 104, 12, 27, 0, 72, 61, 0, 77, 0, 9, 0, 11, 65, 47, 25, 31, 106, 2, 163, 13, 0, 20, 0, 0, 97, 36, 90, 217, 26, 0, 0, 49, 87, 0, 199, 0, 143, 28, 13, 190, 12, 237, 136, 195, 0, 0, 78, 54, 62, 4, 0, 90, 26, 0, 69, 46, 170, 38, 4, 0, 31, 20, 35, 68, 142, 62, 199, 18, 37, 55, 78, 91, 0, 11, 0, 5, 4, 95, 81, 149, 255, 255, 3, 0, 0, 0, 135, 28, 78, 0, 14, 234, 109, 75, 99, 27, 96, 0, 255, 0, 73, 82, 2, 4, 131, 50, 0, 7, 0, 73, 81, 172, 43, 79, 3, 181, 53, 10, 205, 5, 7, 187, 0, 53, 29, 255, 216, 41, 140, 12, 38, 0, 52, 44, 0, 9, 0, 0, 0, 18, 83, 26, 39, 22, 102, 37, 151, 13, 0, 2, 0, 0, 66, 53, 99, 216, 59, 0, 0, 28, 135, 4, 192, 2, 173, 0, 0, 161, 35, 237, 124, 75, 0, 0, 89, 139, 86, 24, 0, 88, 16, 0, 69, 0, 159, 68, 0, 9, 52, 0, 46, 55, 164, 21, 195, 19, 28, 116, 78, 113, 22, 11, 0, 18, 4, 39, 73, 137, 215, 255, 0, 0, 37, 45, 110, 18, 90, 0, 31, 206, 47, 72, 34, 27, 0, 46, 246, 76, 126, 26, 8, 11, 93, 66, 0, 7, 0, 37, 62, 174, 70, 79, 3, 215, 41, 12, 83, 19, 7, 174, 2, 26, 0, 247, 145, 71, 135, 12, 1, 0, 75, 29, 47, 57, 0, 38, 0, 6, 0, 54, 56, 20, 41, 19, 147, 13, 0, 21, 9, 0, 97, 0, 89, 190, 145, 0, 0, 28, 99, 0, 199, 18, 116, 6, 0, 150, 28, 237, 35, 92, 0, 13, 73, 89, 126, 9, 0, 107, 31, 0, 62, 16, 158, 106, 20, 0, 78, 0, 63, 0, 177, 21, 190, 61, 41, 75, 70, 134, 28, 11, 11, 27, 4, 23, 61, 96, 219, 206, 14, 0, 0, 0, 162, 0, 88, 0, 5, 255, 83, 7, 136, 27, 0, 0, 221, 66, 144, 19, 0, 16, 133, 69, 28, 7, 0, 28, 107, 176, 47, 69, 3, 183, 9, 28, 134, 0, 17, 172, 25, 40, 0, 255, 177, 70, 129, 12, 21, 0, 49, 20, 10, 47, 0, 0, 0, 1, 16, 79, 31, 25, 41, 10, 109, 13, 0, 0, 11, 0, 65, 0, 0, 192, 174, 0, 0, 34, 43, 0, 199, 51, 96, 9, 37, 87, 53, 237, 88, 121, 0, 17, 78, 72, 80, 27, 0, 60, 50, 0, 69, 4, 157, 25, 21, 0, 9, 42, 37, 0, 173, 21, 207, 18, 29, 100, 70, 106, 0, 11, 0, 43, 4, 53, 110, 107, 226, 156, 46, 0, 0, 0, 156, 12, 62, 0, 17, 250, 76, 10, 159, 27, 46, 19, 226, 64, 50, 33, 0, 2, 108, 68, 0, 7, 0, 19, 98, 181, 13, 68, 3, 125, 19, 28, 151, 26, 12, 153, 0, 29, 0, 255, 179, 67, 121, 12, 6, 0, 62, 26, 0, 35, 0, 56, 0, 1, 2, 32, 4, 28, 30, 33, 160, 13, 0, 70, 0, 0, 76, 0, 76, 113, 52, 0, 0, 37, 61, 0, 196, 49, 161, 21, 0, 178, 21, 237, 63, 141, 0, 0, 94, 104, 136, 14, 0, 84, 5, 0, 40, 24, 185, 37, 0, 18, 8, 64, 34, 68, 175, 105, 207, 9, 136, 107, 0, 83, 22, 11, 13, 13, 4, 59, 88, 106, 255, 208, 1, 0, 0, 30, 169, 0, 93, 0, 5, 255, 92, 34, 150, 27, 43, 0, 226, 0, 122, 14, 21, 2, 156, 37, 0, 7, 0, 60, 116, 135, 0, 56, 3, 156, 1, 42, 68, 28, 37, 179, 17, 29, 0, 255, 204, 13, 103, 12, 1, 0, 125, 96, 0, 23, 0, 32, 0, 1, 19, 18, 104, 2, 32, 47, 193, 13, 0, 0, 39, 0, 69, 0, 90, 120, 63, 0, 0, 73, 115, 0, 193, 70, 134, 3, 50, 129, 31, 237, 81, 168, 0, 3, 92, 68, 72, 4, 0, 77, 11, 0, 51, 50, 183, 63, 0, 0, 44, 27, 87, 55, 169, 21, 189, 49, 45, 59, 0, 161, 0, 26, 0, 75, 4, 91, 76, 147, 225, 254, 1, 64, 0, 0, 142, 135, 93, 0, 15, 255, 27, 48, 80, 27, 13, 6, 199, 47, 14, 25, 0, 12, 79, 116, 44, 7, 0, 59, 100, 163, 0, 55, 3, 182, 27, 12, 255, 14, 12, 241, 0, 61, 0, 255, 241, 0, 137, 12, 33, 0, 124, 67, 0, 23, 0, 0, 0, 9, 51, 8, 33, 1, 54, 32, 140, 13, 0, 0, 0, 0, 47, 0, 59, 180, 79, 0, 0, 98, 67, 29, 199, 48, 86, 22, 0, 78, 58, 237, 71, 32, 0, 0, 91, 122, 149, 7, 0, 71, 41, 0, 64, 31, 172, 105, 0, 5, 74, 0, 70, 0, 177, 34, 146, 44, 33, 43, 53, 78, 51, 30, 0, 45, 4, 14, 55, 164, 252, 140, 30, 0, 12, 0, 116, 7, 83, 0, 47, 230, 46, 21, 183, 27, 0, 25, 245, 74, 35, 0, 43, 10, 93, 85, 6, 7, 0, 53, 49, 165, 9, 60, 3, 78, 11, 8, 68, 15, 7, 174, 0, 30, 0, 250, 183, 74, 132, 12, 1, 0, 118, 29, 0, 71, 0, 0, 0, 31, 8, 59, 29, 1, 79, 27, 172, 13, 0, 11, 11, 0, 92, 0, 31, 180, 76, 0, 0, 119, 76, 0, 199, 0, 73, 23, 24, 152, 83, 237, 95, 122, 0, 0, 94, 106, 141, 11, 0, 86, 5, 0, 60, 34, 175, 29, 0, 0, 59, 0, 34, 13, 174, 21, 144, 49, 9, 32, 78, 96, 10, 11, 0, 75, 4, 35, 56, 132, 255, 255, 31, 0, 0, 38, 111, 6, 77, 0, 46, 249, 131, 73, 160, 27, 47, 23, 255, 28, 72, 0, 1, 2, 115, 48, 0, 7, 0, 158, 88, 150, 0, 55, 3, 105, 0, 4, 20, 5, 7, 146, 0, 33, 32, 255, 212, 74, 135, 12, 1, 0, 166, 57, 0, 14, 0, 0, 0, 81, 27, 7, 4, 26, 113, 11, 180, 13, 0, 12, 0, 0, 117, 0, 104, 166, 110, 0, 0, 100, 66, 6, 199, 0, 130, 14, 6, 122, 21, 237, 119, 51, 0, 0, 94, 132, 113, 15, 0, 81, 5, 0, 50, 72, 186, 16, 14, 0, 46, 0, 35, 10, 185, 21, 124, 0, 42, 37, 20, 84, 57, 39, 0, 32, 16, 31, 101, 111, 227, 214, 59, 0, 0, 24, 98, 0, 93, 65, 18, 255, 121, 17, 173, 27, 0, 0, 255, 30, 38, 0, 0, 2, 158, 46, 0, 7, 0, 41, 90, 186, 0, 79, 3, 141, 23, 4, 84, 0, 7, 174, 56, 21, 0, 209, 232, 67, 98, 12, 1, 0, 194, 29, 0, 21, 0, 79, 0, 1, 0, 71, 4, 12, 68, 2, 204, 13, 0, 0, 16, 51, 93, 59, 101, 239, 107, 0, 0, 77, 94, 32, 146, 0, 113, 46, 11, 163, 23, 237, 56, 151, 0, 33, 94, 106, 100, 26, 0, 64, 5, 0, 69, 0, 186, 20, 11, 6, 32, 57, 19, 55, 182, 21, 207, 44, 63, 116, 55, 103, 10, 11, 0, 77, 4, 34, 87, 152, 255, 180, 28, 0, 0, 0, 169, 0, 89, 0, 11, 255, 140, 61, 121, 27, 74, 0, 225, 16, 38, 50, 0, 1, 128, 28, 2, 7, 0, 71, 106, 147, 43, 48, 3, 73, 3, 4, 126, 0, 27, 203, 0, 29, 51, 249, 220, 74, 111, 12, 1, 0, 87, 108, 0, 38, 0, 57, 0, 1, 0, 19, 22, 33, 57, 7, 215, 13, 0, 39, 0, 0, 118, 0, 90, 230, 1, 0, 0, 51, 50, 0, 193, 18, 106, 0, 4, 106, 13, 237, 84, 91, 0, 30, 73, 182, 127, 39, 0, 92, 25, 0, 69, 16, 175, 33, 0, 16, 50, 50, 17, 63, 163, 21, 158, 19, 1, 59, 42, 108, 40, 37, 48, 79, 11, 35, 54, 105, 213, 199, 35, 6, 0, 0, 157, 78, 85, 18, 11, 241, 115, 33, 179, 31, 0, 0, 255, 38, 30, 0, 36, 9, 113, 49, 0, 7, 0, 57, 92, 124, 133, 43, 10, 162, 26, 4, 74, 3, 7, 192, 71, 39, 53, 240, 208, 65, 127, 12, 27, 0, 204, 33, 0, 9, 0, 58, 0, 34, 0, 15, 10, 47, 50, 18, 164, 13, 0, 0, 0, 0, 58, 0, 100, 195, 105, 0, 0, 128, 59, 0, 177, 56, 159, 39, 0, 214, 14, 237, 37, 141, 0, 0, 94, 55, 93, 4, 0, 99, 38, 0, 69, 9, 162, 112, 0, 1, 129, 0, 66, 68, 182, 24, 181, 32, 36, 14, 49, 174, 0, 21, 62, 64, 18, 72, 50, 150, 241, 174, 29, 0, 0, 0, 169, 2, 88, 0, 30, 255, 99, 20, 179, 34, 36, 0, 193, 25, 46, 38, 0, 2, 132, 78, 57, 7, 0, 81, 111, 169, 30, 74, 11, 150, 13, 8, 255, 0, 17, 171, 0, 35, 84, 245, 155, 31, 114, 12, 36, 0, 211, 16, 8, 74, 0, 0, 0, 57, 40, 0, 26, 13, 65, 20, 104, 13, 0, 40, 0, 0, 14, 49, 45, 206, 76, 0, 0, 52, 59, 0, 156, 54, 147, 10, 0, 162, 79, 237, 83, 174, 0, 46, 76, 56, 108, 4, 0, 99, 5, 0, 69, 0, 149, 80, 26, 0, 88, 47, 53, 68, 174, 25, 199, 45, 13, 93, 78, 150, 0, 11, 17, 5, 4, 86, 87, 92, 228, 241, 16, 0, 0, 95, 167, 0, 93, 0, 5, 255, 111, 2, 176, 27, 80, 3, 217, 37, 68, 37, 0, 2, 79, 79, 0, 7, 0, 77, 117, 153, 0, 72, 3, 212, 16, 11, 49, 27, 7, 180, 3, 33, 0, 252, 167, 64, 99, 12, 14, 0, 142, 42, 0, 32, 43, 0, 0, 34, 62, 20, 8, 65, 68, 9, 134, 13, 0, 54, 0, 9, 80, 65, 10, 219, 8, 0, 0, 74, 67, 0, 190, 21, 162, 7, 0, 191, 23, 237, 71, 118, 0, 0, 77, 111, 119, 36, 0, 76, 53, 51, 69, 118, 174, 129, 0, 3, 73, 17, 41, 0, 171, 21, 161, 42, 21, 30, 78, 103, 57, 29, 58, 5, 4, 35, 105, 161, 233, 144, 24, 0, 0, 0, 114, 0, 88, 0, 12, 238, 118, 43, 177, 56, 0, 22, 248, 31, 16, 0, 0, 27, 160, 80, 0, 7, 0, 58, 94, 189, 0, 66, 3, 112, 3, 4, 249, 31, 7, 191, 0, 57, 0, 255, 183, 41, 64, 12, 33, 0, 211, 98, 0, 18, 81, 0, 0, 28, 2, 37, 16, 8, 111, 13, 201, 13, 0, 0, 0, 0, 105, 0, 47, 164, 37, 0, 0, 139, 98, 0, 184, 16, 120, 30, 0, 120, 56, 237, 105, 172, 5, 54, 43, 94, 121, 14, 0, 88, 72, 28, 69, 49, 181, 18, 0, 0, 64, 73, 22, 0, 185, 21, 168, 9, 90, 40, 78, 175, 56, 42, 0, 53, 4, 57, 64, 104, 255, 166, 79, 0, 0, 0, 167, 6, 87, 38, 23, 255, 128, 8, 173, 27, 14, 0, 245, 86, 45, 0, 0, 2, 127, 31, 0, 7, 0, 106, 112, 142, 0, 31, 3, 118, 5, 4, 68, 0, 7, 197, 5, 63, 9, 212, 207, 0, 97, 12, 20, 0, 146, 43, 30, 42, 0, 28, 0, 12, 0, 0, 10, 75, 110, 9, 157, 13, 0, 12, 0, 0, 74, 39, 80, 177, 66, 0, 0, 64, 51, 0, 199, 33, 85, 0, 84, 169, 6, 237, 97, 105, 29, 0, 94, 99, 120, 12, 0, 99, 5, 12, 69, 19, 177, 79, 0, 0, 91, 49, 51, 19, 176, 21, 165, 20, 27, 53, 78, 122, 58, 11, 0, 5, 4, 31, 51, 113, 255, 236, 27, 29, 0, 0, 160, 0, 80, 0, 5, 255, 104, 23, 180, 27, 35, 73, 255, 33, 118, 4, 0, 2, 86, 132, 38, 7, 0, 42, 72, 159, 0, 23, 3, 141, 16, 8, 20, 0, 7, 176, 0, 63, 0, 255, 149, 22, 110, 12, 12, 0, 113, 37, 1, 31, 0, 0, 0, 1, 8, 0, 19, 1, 58, 47, 154, 13, 0, 38, 0, 40, 83, 0, 58, 216, 45, 0, 0, 45, 33, 0, 199, 7, 94, 3, 0, 131, 49, 237, 103, 125, 0, 0, 94, 59, 100, 16, 0, 113, 13, 0, 58, 33, 161, 99, 0, 64, 59, 0, 46, 53, 185, 21, 205, 35, 19, 122, 78, 142, 34, 11, 0, 13, 4, 44, 67, 154, 246, 239, 34, 25, 7, 0, 134, 0, 90, 0, 11, 223, 133, 21, 143, 27, 9, 0, 255, 45, 95, 15, 0, 6, 107, 38, 0, 7, 19, 21, 65, 152, 0, 67, 3, 141, 25, 11, 105, 0, 29, 213, 0, 55, 3, 255, 194, 31, 96, 12, 24, 0, 71, 111, 0, 28, 0, 0, 0, 1, 121, 28, 102, 1, 102, 2, 178, 13, 0, 0, 0, 0, 55, 70, 117, 185, 59, 0, 0, 55, 98, 0, 125, 20, 96, 6, 22, 126, 27, 237, 158, 186, 0, 0, 57, 82, 62, 13, 0, 75, 5, 2, 3, 15, 175, 40, 0, 0, 20, 9, 59, 17, 163, 36, 190, 61, 44, 65, 30, 102, 0, 16, 0, 53, 4, 106, 79, 103, 255, 208, 4, 26, 44, 0, 159, 33, 88, 0, 11, 201, 93, 2, 170, 27, 0, 0, 248, 39, 31, 38, 0, 83, 142, 35, 0, 7, 0, 36, 100, 144, 13, 65, 3, 149, 35, 16, 250, 28, 46, 205, 0, 70, 0, 255, 157, 0, 137, 12, 66, 0, 92, 111, 0, 12, 0, 49, 0, 21, 89, 34, 35, 34, 54, 2, 199, 13, 0, 56, 0, 10, 84, 67, 125, 170, 94, 0, 0, 57, 102, 0, 170, 41, 133, 16, 0, 81, 18, 237, 142, 139, 0, 0, 94, 116, 158, 4, 0, 71, 26, 0, 69, 19, 151, 6, 1, 60, 18, 9, 18, 0, 152, 96, 125, 55, 63, 45, 49, 72, 9, 45, 71, 30, 18, 48, 100, 0, 215, 253, 13, 0, 0, 0, 48, 0, 93, 13, 100, 255, 103, 18, 168, 36, 0, 0, 255, 0, 93, 0, 18, 2, 154, 39, 0, 7, 0, 76, 109, 165, 17, 79, 10, 132, 23, 10, 79, 0, 33, 218, 54, 29, 4, 255, 182, 26, 128, 26, 27, 0, 129, 26, 23, 55, 125, 50, 53, 53, 0, 14, 10, 104, 48, 2, 163, 13, 0, 0, 11, 0, 93, 32, 63, 134, 37, 0, 0, 111, 52, 0, 199, 0, 116, 27, 4, 131, 46, 237, 96, 180, 0, 0, 63, 85, 138, 12, 0, 78, 62, 0, 55, 30, 169, 91, 0, 6, 79, 24, 50, 42, 159, 84, 207, 156, 68, 90, 70, 182, 46, 11, 0, 99, 32, 60, 104, 130, 232, 133, 13, 0, 0, 18, 155, 0, 62, 0, 5, 225, 76, 38, 158, 42, 15, 0, 249, 61, 31, 24, 6, 21, 143, 52, 0, 7, 0, 116, 98, 180, 0, 46, 11, 143, 21, 8, 123, 0, 29, 255, 0, 51, 0, 245, 175, 45, 73, 12, 36, 0, 233, 20, 0, 26, 0, 0, 0, 23, 0, 0, 21, 9, 49, 2, 211, 13, 0, 108, 0, 0, 38, 37, 83, 213, 69, 0, 0, 39, 58, 0, 199, 83, 147, 4, 0, 109, 56, 237, 64, 163, 0, 0, 89, 87, 81, 24, 0, 131, 5, 0, 69, 0, 175, 70, 0, 41, 35, 38, 60, 68, 174, 21, 207, 0, 59, 30, 78, 144, 6, 14, 0, 51, 4, 124, 95, 77, 225, 255, 14, 0, 0, 74, 130, 40, 93, 0, 11, 255, 121, 23, 123, 27, 0, 0, 231, 45, 90, 77, 0, 2, 64, 28, 24, 7, 26, 56, 88, 113, 0, 41, 3, 179, 20, 11, 160, 9, 28, 249, 0, 71, 0, 252, 216, 56, 136, 12, 1, 0, 193, 26, 34, 38, 0, 0, 0, 23, 0, 0, 8, 59, 81, 38, 190, 13, 0, 31, 0, 0, 100, 60, 113, 212, 153, 0, 0, 50, 83, 0, 188, 16, 119, 0, 31, 118, 82, 237, 112, 147, 0, 0, 92, 20, 121, 4, 0, 65, 5, 0, 69, 5, 178, 63, 0, 0, 23, 21, 56, 68, 164, 21, 191, 7, 5, 0, 65, 131, 0, 16, 26, 24, 6, 46, 54, 94, 247, 255, 15, 12, 0, 0, 146, 3, 93, 91, 44, 255, 105, 2, 165, 36, 77, 29, 209, 13, 47, 68, 0, 5, 114, 77, 64, 7, 0, 78, 108, 187, 11, 58, 3, 146, 0, 4, 255, 40, 21, 201, 56, 30, 0, 255, 212, 53, 138, 12, 36, 0, 57, 22, 0, 27, 0, 0, 0, 19, 56, 5, 23, 17, 48, 35, 154, 13, 0, 0, 17, 0, 94, 32, 53, 238, 1, 0, 0, 80, 123, 0, 152, 0, 83, 13, 34, 127, 51, 237, 121, 160, 0, 0, 91, 109, 157, 18, 0, 100, 22, 0, 38, 66, 166, 79, 0, 4, 34, 0, 38, 0, 169, 25, 186, 17, 27, 41, 75, 56, 0, 31, 0, 51, 8, 31, 73, 72, 225, 202, 66, 0, 0, 0, 161, 0, 93, 80, 47, 229, 136, 2, 183, 37, 0, 0, 248, 49, 147, 0, 77, 9, 128, 69, 0, 7, 0, 53, 90, 128, 0, 79, 3, 181, 73, 4, 98, 25, 27, 172, 0, 23, 0, 246, 244, 37, 136, 12, 80, 0, 68, 50, 98, 53, 20, 12, 0, 16, 0, 0, 4, 22, 58, 50, 128, 13, 0, 101, 3, 0, 39, 47, 11, 209, 129, 0, 0, 68, 101, 0, 199, 41, 106, 19, 0, 112, 40, 237, 163, 116, 0, 0, 18, 91, 101, 42, 0, 122, 5, 12, 53, 23, 101, 96, 0, 45, 21, 0, 44, 0, 114, 31, 199, 12, 37, 18, 22, 111, 47, 24, 57, 65, 4, 116, 194, 26, 229, 222, 34, 0, 29, 0, 159, 35, 85, 57, 57, 180, 143, 22, 154, 27, 13, 24, 255, 29, 123, 65, 10, 15, 159, 46, 0, 7, 0, 67, 86, 70, 0, 74, 3, 209, 65, 4, 133, 0, 36, 255, 43, 43, 0, 248, 188, 5, 129, 23, 1, 0, 120, 45, 12, 18, 0, 0, 0, 16, 23, 26, 25, 36, 142, 47, 217, 13, 0, 0, 0, 0, 82, 0, 73, 147, 35, 0, 0, 71, 115, 0, 179, 62, 118, 37, 15, 51, 41, 237, 253, 85, 0, 0, 84, 107, 117, 4, 0, 31, 61, 11, 69, 25, 154, 111, 0, 42, 10, 0, 64, 68, 130, 27, 207, 17, 18, 72, 64, 181, 0, 31, 46, 24, 4, 46, 80, 134, 255, 205, 1, 0, 21, 54, 160, 87, 93, 4, 9, 198, 86, 18, 183, 27, 0, 0, 170, 0, 105, 66, 100, 9, 79, 31, 0, 7, 0, 34, 101, 189, 0, 7, 3, 20, 68, 4, 163, 82, 61, 212, 23, 54, 0, 218, 241, 74, 88, 12, 50, 0, 47, 29, 0, 175, 0, 35, 0, 25, 79, 13, 36, 1, 68, 30, 169, 24, 0, 91, 0, 183, 60, 14, 99, 217, 100, 0, 0, 57, 56, 0, 61, 44, 59, 21, 3, 129, 16, 237, 206, 130, 0, 0, 83, 66, 135, 13, 0, 85, 26, 39, 53, 0, 183, 48, 0, 0, 46, 56, 45, 43, 138, 46, 182, 75, 85, 47, 71, 62, 33, 11, 0, 12, 4, 77, 53, 91, 255, 255, 1, 26, 0, 80, 169, 55, 77, 1, 18, 255, 124, 23, 126, 27, 100, 14, 195, 72, 10, 65, 0, 10, 134, 58, 11, 7, 0, 18, 78, 173, 0, 52, 3, 91, 67, 15, 158, 63, 32, 220, 4, 86, 0, 255, 200, 70, 122, 12, 94, 0, 94, 59, 0, 255, 0, 24, 0, 33, 39, 35, 54, 25, 25, 38, 160, 13, 0, 0, 0, 0, 47, 0, 27, 221, 1, 0, 0, 33, 74, 0, 199, 15, 74, 0, 0, 78, 27, 225, 105, 63, 0, 0, 94, 130, 97, 21, 0, 83, 5, 0, 24, 2, 186, 25, 0, 0, 36, 0, 46, 57, 178, 58, 175, 25, 22, 95, 50, 101, 92, 11, 0, 12, 4, 13, 58, 105, 255, 170, 1, 2, 58, 0, 163, 0, 93, 0, 5, 253, 108, 20, 164, 27, 18, 43, 255, 0, 47, 64, 0, 2, 84, 51, 52, 7, 0, 27, 98, 168, 0, 79, 6, 155, 10, 4, 94, 48, 7, 147, 0, 23, 0, 238, 118, 0, 140, 12, 1, 0, 91, 16, 32, 34, 0, 58, 0, 30, 31, 47, 8, 13, 40, 11, 152, 13, 0, 0, 0, 25, 68, 0, 105, 128, 118, 0, 0, 66, 31, 0, 199, 60, 125, 25, 31, 141, 11, 237, 64, 130, 0, 0, 89, 56, 116, 9, 0, 99, 16, 0, 69, 6, 185, 6, 0, 0, 25, 66, 27, 68, 187, 21, 207, 14, 49, 57, 78, 83, 51, 18, 0, 58, 12, 29, 86, 176, 251, 116, 0, 0, 0, 0, 169, 0, 88, 2, 5, 251, 107, 8, 140, 31, 80, 0, 250, 5, 41, 57, 0, 10, 175, 38, 22, 7, 0, 44, 75, 189, 0, 79, 7, 104, 0, 5, 68, 0, 19, 148, 0, 25, 8, 199, 211, 67, 113, 12, 1, 0, 139, 39, 13, 15, 0, 44, 0, 41, 2, 0, 19, 5, 71, 2, 187, 13, 0, 0, 0, 0, 89, 32, 131, 237, 28, 0, 0, 48, 96, 32, 183, 2, 136, 6, 0, 123, 0, 237, 112, 157, 0, 0, 78, 104, 87, 18, 0, 68, 15, 0, 50, 17, 176, 63, 0, 39, 51, 0, 66, 0, 187, 65, 188, 22, 130, 84, 78, 58, 60, 24, 64, 15, 4, 71, 74, 148, 255, 255, 40, 0, 0, 0, 130, 0, 93, 0, 5, 236, 135, 35, 142, 32, 16, 6, 255, 0, 190, 10, 66, 2, 153, 34, 0, 7, 0, 51, 84, 102, 0, 79, 16, 101, 45, 8, 27, 3, 20, 205, 0, 21, 0, 255, 191, 31, 130, 12, 1, 0, 128, 62, 0, 16, 0, 0, 0, 44, 0, 0, 23, 26, 54, 7, 198, 13, 0, 0, 0, 0, 101, 75, 146, 190, 88, 0, 0, 50, 94, 0, 176, 37, 103, 6, 0, 112, 70, 237, 127, 255, 0, 0, 88, 58, 145, 29, 0, 75, 5, 0, 32, 0, 162, 6, 0, 32, 24, 34, 48, 55, 180, 44, 207, 12, 37, 63, 78, 87, 0, 23, 0, 16, 4, 125, 78, 136, 255, 245, 3, 0, 42, 1, 163, 25, 93, 0, 5, 226, 119, 25, 134, 27, 52, 40, 202, 37, 44, 103, 0, 20, 107, 60, 0, 7, 0, 104, 97, 146, 0, 79, 3, 189, 10, 23, 178, 0, 22, 241, 0, 47, 0, 255, 218, 0, 132, 12, 1, 0, 100, 23, 0, 9, 13, 67, 0, 50, 51, 0, 13, 37, 22, 8, 169, 13, 0, 62, 0, 0, 47, 0, 101, 156, 24, 0, 0, 28, 88, 0, 193, 59, 132, 1, 0, 110, 41, 237, 137, 120, 0, 0, 88, 131, 128, 10, 0, 73, 13, 0, 69, 13, 151, 29, 0, 37, 9, 50, 26, 68, 181, 25, 200, 19, 34, 72, 78, 96, 70, 32, 16, 69, 8, 63, 90, 126, 250, 196, 26, 26, 0, 116, 160, 41, 93, 43, 8, 252, 125, 35, 151, 27, 0, 0, 225, 0, 110, 84, 0, 2, 67, 26, 27, 7, 0, 71, 78, 141, 0, 79, 3, 157, 9, 4, 89, 0, 14, 229, 63, 63, 49, 221, 200, 67, 130, 12, 7, 0, 85, 35, 0, 20, 0, 0, 0, 43, 0, 62, 107, 9, 28, 33, 186, 13, 0, 0, 71, 0, 38, 0, 73, 225, 110, 0, 0, 28, 83, 0, 167, 15, 138, 18, 82, 171, 19, 237, 96, 174, 0, 0, 43, 61, 113, 19, 0, 79, 10, 3, 38, 30, 163, 27, 0, 16, 37, 52, 35, 29, 168, 31, 185, 29, 32, 39, 43, 145, 41, 11, 113, 44, 12, 40, 156, 169, 237, 239, 41, 0, 0, 49, 104, 4, 93, 48, 96, 211, 141, 14, 165, 27, 59, 5, 218, 4, 48, 44, 50, 2, 136, 42, 0, 7, 0, 130, 108, 146, 0, 74, 9, 162, 19, 16, 57, 0, 7, 220, 0, 21, 87, 249, 203, 23, 116, 12, 10, 0, 119, 16, 0, 36, 0, 58, 0, 36, 0, 15, 4, 12, 63, 25, 167, 13, 0, 0, 8, 0, 82, 0, 111, 168, 7, 0, 0, 28, 138, 0, 199, 48, 149, 22, 0, 119, 14, 237, 103, 154, 0, 0, 77, 94, 100, 16, 0, 101, 18, 1, 10, 35, 179, 44, 0, 0, 33, 77, 25, 68, 164, 21, 200, 57, 10, 123, 50, 103, 0, 20, 46, 56, 4, 62, 93, 135, 255, 173, 1, 0, 0, 0, 169, 20, 93, 0, 5, 255, 134, 32, 157, 27, 43, 12, 255, 31, 26, 34, 0, 38, 87, 55, 57, 7, 0, 97, 101, 170, 58, 68, 11, 134, 7, 34, 228, 0, 21, 178, 0, 23, 141, 218, 151, 56, 117, 12, 18, 0, 68, 49, 0, 38, 0, 31, 0, 47, 59, 45, 4, 4, 54, 11, 185, 13, 0, 40, 0, 0, 55, 0, 42, 150, 81, 0, 0, 63, 35, 0, 199, 77, 106, 9, 0, 129, 11, 237, 93, 153, 0, 0, 91, 107, 94, 9, 0, 116, 22, 0, 43, 82, 183, 18, 2, 49, 37, 48, 7, 68, 171, 21, 200, 64, 3, 31, 0, 131, 34, 27, 17, 22, 4, 129, 52, 121, 242, 212, 3, 0, 0, 0, 169, 1, 89, 0, 5, 226, 116, 29, 130, 27, 0, 59, 255, 42, 57, 13, 0, 37, 153, 64, 0, 7, 0, 102, 95, 158, 0, 79, 3, 79, 31, 7, 63, 0, 14, 225, 0, 54, 0, 255, 194, 48, 122, 12, 33, 0, 186, 28, 0, 16, 0, 61, 0, 29, 0, 0, 4, 1, 92, 78, 183, 13, 0, 0, 14, 0, 93, 0, 84, 214, 134, 0, 0, 122, 79, 1, 199, 34, 132, 10, 84, 147, 28, 237, 97, 157, 0, 16, 65, 86, 79, 7, 0, 106, 45, 0, 53, 27, 180, 73, 0, 7, 64, 0, 55, 0, 164, 27, 207, 86, 19, 75, 0, 110, 15, 14, 9, 30, 4, 154, 69, 152, 250, 192, 0, 0, 33, 20, 169, 0, 74, 0, 19, 230, 44, 62, 183, 27, 0, 58, 255, 24, 103, 0, 0, 10, 78, 110, 0, 7, 0, 83, 85, 158, 0, 79, 3, 121, 36, 56, 162, 22, 7, 196, 0, 80, 25, 255, 177, 0, 122, 12, 12, 0, 152, 36, 0, 34, 0, 0, 0, 5, 15, 0, 4, 1, 32, 86, 197, 13, 0, 0, 0, 32, 17, 0, 75, 155, 88, 0, 0, 119, 94, 38, 199, 44, 126, 32, 0, 116, 42, 237, 50, 168, 18, 0, 77, 102, 65, 8, 0, 66, 13, 0, 69, 2, 186, 6, 45, 17, 30, 14, 13, 68, 173, 21, 183, 40, 65, 82, 41, 103, 23, 16, 0, 30, 4, 27, 58, 175, 242, 245, 0, 0, 0, 7, 157, 0, 93, 0, 35, 255, 91, 50, 132, 27, 0, 7, 226, 0, 129, 0, 7, 16, 172, 64, 0, 7, 0, 96, 79, 149, 23, 79, 3, 152, 0, 4, 16, 69, 7, 160, 0, 21, 0, 216, 179, 63, 130, 12, 1, 0, 130, 34, 0, 14, 40, 35, 0, 54, 0, 0, 38, 20, 69, 2, 157, 13, 0, 54, 0, 0, 60, 29, 103, 237, 98, 0, 0, 98, 114, 100, 183, 0, 148, 46, 46, 195, 16, 237, 94, 96, 0, 0, 77, 95, 166, 28, 0, 68, 13, 0, 65, 6, 186, 80, 17, 28, 59, 0, 91, 0, 138, 21, 180, 0, 59, 63, 73, 107, 0, 11, 0, 10, 4, 13, 45, 145, 216, 255, 29, 0, 0, 18, 112, 0, 93, 13, 17, 255, 93, 16, 137, 27, 0, 0, 230, 0, 142, 0, 25, 2, 145, 73, 32, 7, 0, 30, 87, 162, 0, 79, 5, 179, 47, 4, 149, 6, 7, 131, 85, 21, 46, 208, 177, 69, 134, 12, 1, 0, 28, 72, 0, 6, 0, 0, 0, 6, 0, 49, 36, 15, 54, 2, 118, 13, 0, 0, 0, 0, 42, 59, 65, 253, 57, 0, 0, 90, 149, 14, 175, 0, 53, 19, 0, 140, 57, 237, 89, 124, 0, 0, 94, 118, 100, 29, 0, 99, 5, 0, 60, 17, 164, 47, 0, 48, 106, 0, 46, 0, 99, 37, 186, 22, 94, 69, 78, 142, 48, 17, 0, 54, 4, 19, 55, 152, 249, 244, 66, 0, 28, 125, 149, 0, 93, 0, 5, 255, 123, 7, 143, 27, 15, 0, 255, 42, 53, 0, 41, 2, 65, 27, 0, 7, 0, 52, 67, 132, 0, 56, 3, 149, 39, 7, 81, 0, 7, 177, 0, 25, 0, 255, 192, 30, 134, 12, 1, 0, 78, 59, 0, 75, 0, 17, 0, 7, 0, 56, 74, 1, 94, 14, 170, 13, 0, 137, 7, 25, 41, 0, 79, 208, 70, 0, 0, 78, 67, 50, 185, 66, 101, 0, 9, 101, 51, 237, 98, 63, 0, 0, 73, 81, 64, 4, 0, 69, 24, 0, 57, 10, 153, 68, 0, 34, 58, 0, 58, 13, 152, 36, 163, 11, 30, 58, 78, 158, 80, 51, 4, 50, 4, 40, 160, 142, 218, 255, 19, 0, 57, 37, 161, 64, 93, 0, 19, 222, 80, 25, 150, 27, 21, 0, 244, 80, 91, 76, 0, 5, 156, 31, 0, 7, 0, 70, 91, 178, 0, 21, 3, 238, 7, 11, 38, 79, 31, 137, 0, 43, 0, 255, 154, 49, 118, 12, 1, 0, 48, 20, 0, 63, 0, 0, 0, 48, 0, 85, 59, 8, 61, 5, 111, 13, 0, 0, 0, 0, 38, 0, 101, 180, 59, 0, 0, 66, 123, 55, 148, 64, 44, 12, 71, 98, 98, 237, 83, 105, 0, 0, 89, 54, 140, 48, 0, 88, 32, 0, 45, 3, 164, 141, 0, 7, 108, 0, 69, 0, 164, 21, 183, 25, 27, 39, 78, 167, 39, 11, 0, 28, 4, 24, 93, 137, 230, 193, 40, 0, 48, 5, 161, 0, 81, 32, 47, 255, 130, 14, 180, 27, 16, 0, 230, 1, 109, 35, 0, 9, 115, 41, 30, 7, 0, 106, 96, 185, 0, 71, 3, 137, 0, 5, 188, 17, 14, 159, 0, 43, 0, 255, 154, 15, 92, 12, 1, 0, 74, 24, 0, 19, 0, 0, 4, 89, 95, 83, 20, 17, 60, 2, 145, 13, 0, 64, 0, 16, 61, 57, 123, 119, 59, 0, 0, 53, 59, 5, 157, 60, 59, 1, 0, 82, 48, 237, 80, 208, 0, 0, 89, 54, 107, 4, 0, 93, 5, 2, 69, 12, 178, 30, 0, 0, 50, 37, 34, 68, 187, 23, 186, 36, 14, 60, 78, 139, 0, 17, 0, 27, 4, 82, 97, 128, 234, 223, 27, 0, 2, 0, 157, 5, 84, 0, 5, 255, 120, 37, 172, 34, 30, 0, 224, 17, 26, 58, 52, 2, 147, 45, 34, 7, 0, 59, 100, 135, 70, 79, 3, 157, 0, 8, 158, 0, 12, 213, 0, 27, 25, 219, 202, 66, 97, 12, 1, 0, 186, 23, 0, 42, 0, 0, 0, 39, 55, 0, 24, 34, 64, 11, 167, 13, 0, 0, 0, 0, 68, 26, 55, 255, 10, 0, 0, 73, 95, 0, 193, 10, 126, 11, 0, 140, 11, 237, 45, 146, 0, 1, 94, 83, 59, 4, 0, 71, 20, 0, 69, 30, 174, 68, 0, 0, 66, 0, 44, 55, 178, 68, 198, 32, 54, 71, 78, 151, 0, 21, 7, 44, 4, 29, 61, 101, 255, 255, 33, 0, 0, 0, 162, 0, 44, 0, 26, 247, 84, 43, 176, 27, 3, 0, 203, 21, 22, 52, 0, 23, 143, 41, 10, 7, 0, 89, 99, 162, 0, 71, 3, 94, 1, 4, 175, 16, 14, 188, 9, 52, 0, 255, 200, 8, 95, 12, 5, 0, 234, 49, 0, 34, 0, 23, 0, 5, 0, 0, 11, 11, 39, 17, 202, 13, 0, 59, 0, 0, 106, 58, 93, 221, 39, 0, 0, 40, 71, 0, 190, 30, 139, 0, 13, 128, 26, 237, 132, 133, 0, 0, 94, 117, 111, 48, 0, 93, 70, 0, 69, 9, 158, 145, 0, 60, 20, 0, 67, 0, 179, 21, 198, 22, 121, 29, 66, 130, 0, 39, 0, 13, 4, 49, 74, 138, 248, 255, 34, 0, 0, 66, 164, 0, 65, 55, 52, 255, 128, 10, 175, 27, 0, 0, 251, 0, 187, 0, 0, 12, 101, 22, 44, 7, 0, 80, 71, 155, 0, 19, 3, 126, 7, 4, 120, 0, 27, 159, 29, 39, 0, 255, 223, 36, 119, 12, 6, 0, 41, 48, 0, 35, 0, 5, 0, 35, 0, 36, 12, 1, 30, 47, 170, 13, 0, 52, 0, 6, 77, 0, 105, 203, 125, 0, 0, 46, 104, 0, 195, 6, 76, 0, 35, 106, 5, 237, 71, 137, 0, 0, 94, 112, 134, 43, 0, 85, 41, 7, 69, 15, 151, 63, 14, 0, 29, 4, 37, 68, 175, 21, 197, 59, 17, 97, 70, 127, 0, 11, 0, 18, 4, 85, 113, 146, 237, 184, 1, 133, 0, 0, 164, 0, 43, 0, 46, 255, 96, 38, 96, 27, 29, 1, 241, 49, 81, 0, 34, 10, 136, 89, 0, 7, 0, 24, 82, 142, 0, 7, 3, 143, 5, 4, 123, 0, 7, 151, 0, 52, 0, 241, 153, 33, 121, 12, 18, 0, 86, 83, 0, 18, 0, 17, 0, 33, 10, 13, 20, 9, 62, 33, 153, 13, 0, 0, 8, 43, 67, 0, 55, 180, 87, 0, 0, 55, 45, 0, 195, 47, 107, 25, 0, 119, 2, 212, 73, 107, 58, 0, 94, 159, 97, 75, 0, 74, 25, 0, 46, 30, 164, 60, 40, 28, 29, 0, 45, 30, 174, 88, 205, 45, 67, 80, 57, 95, 53, 25, 0, 5, 4, 23, 124, 95, 228, 199, 11, 41, 0, 61, 146, 0, 77, 58, 18, 255, 120, 49, 179, 27, 0, 0, 252, 35, 85, 0, 0, 20, 152, 58, 0, 7, 0, 14, 88, 130, 0, 48, 3, 138, 0, 8, 16, 87, 7, 161, 68, 38, 0, 238, 146, 53, 118, 12, 1, 0, 142, 103, 0, 41, 0, 0, 0, 1, 0, 1, 55, 19, 84, 29, 170, 13, 0, 0, 0, 0, 71, 11, 81, 156, 127, 0, 0, 72, 39, 0, 199, 51, 108, 14, 12, 83, 18, 237, 151, 164, 0, 0, 58, 104, 104, 4, 0, 84, 20, 15, 44, 6, 172, 18, 0, 38, 62, 44, 17, 0, 173, 31, 144, 24, 39, 71, 73, 162, 8, 11, 0, 5, 16, 82, 81, 141, 255, 150, 103, 43, 62, 86, 155, 20, 93, 0, 25, 246, 134, 9, 40, 27, 0, 0, 238, 10, 19, 20, 5, 13, 103, 35, 0, 7, 0, 14, 91, 168, 0, 79, 9, 139, 0, 15, 128, 0, 7, 232, 0, 30, 22, 255, 202, 4, 122, 12, 24, 0, 186, 22, 88, 46, 19, 62, 0, 5, 0, 0, 55, 22, 82, 12, 163, 13, 0, 18, 0, 0, 54, 0, 127, 144, 99, 0, 0, 39, 104, 0, 179, 75, 102, 3, 0, 176, 26, 237, 116, 113, 0, 0, 94, 73, 56, 12, 0, 95, 5, 0, 69, 0, 136, 74, 0, 48, 48, 19, 60, 68, 154, 78, 207, 62, 26, 40, 54, 171, 26, 11, 49, 69, 4, 84, 100, 84, 254, 255, 22, 0, 70, 0, 162, 0, 89, 13, 21, 225, 126, 7, 150, 27, 0, 0, 255, 4, 141, 91, 50, 2, 94, 17, 30, 7, 0, 87, 102, 143, 0, 79, 3, 167, 36, 4, 137, 83, 25, 177, 23, 48, 0, 255, 189, 17, 102, 12, 1, 0, 97, 16, 0, 14, 0, 0, 0, 47, 31, 16, 18, 12, 72, 7, 162, 13, 0, 0, 0, 15, 57, 0, 76, 243, 96, 0, 0, 28, 118, 0, 177, 48, 69, 6, 0, 137, 57, 237, 53, 222, 0, 0, 94, 89, 102, 37, 0, 77, 39, 0, 69, 2, 186, 36, 0, 17, 55, 56, 16, 26, 185, 21, 203, 45, 26, 93, 71, 193, 0, 11, 0, 72, 4, 106, 95, 156, 255, 255, 26, 0, 22, 8, 164, 38, 84, 3, 5, 255, 114, 48, 183, 27, 34, 0, 236, 65, 118, 82, 0, 2, 93, 28, 47, 7, 0, 93, 106, 189, 9, 60, 3, 99, 1, 4, 125, 52, 12, 228, 0, 54, 0, 255, 212, 34, 132, 12, 1, 0, 83, 94, 0, 14, 0, 0, 0, 39, 48, 27, 118, 5, 84, 2, 164, 13, 0, 0, 13, 0, 80, 8, 94, 228, 88, 0, 0, 33, 111, 17, 169, 17, 103, 0, 0, 177, 24, 237, 113, 177, 0, 0, 94, 102, 92, 8, 0, 92, 13, 0, 67, 28, 164, 90, 0, 0, 79, 0, 44, 43, 144, 21, 198, 16, 7, 51, 55, 112, 0, 22, 0, 25, 4, 61, 114, 99, 250, 255, 62, 0, 0, 0, 169, 0, 93, 92, 5, 255, 133, 10, 183, 27, 0, 19, 255, 51, 169, 55, 0, 12, 107, 72, 10, 7, 0, 71, 95, 168, 0, 55, 3, 206, 3, 4, 253, 0, 23, 242, 57, 40, 59, 225, 235, 44, 119, 12, 1, 0, 177, 30, 0, 18, 0, 0, 0, 33, 0, 0, 17, 13, 87, 2, 170, 13, 0, 0, 0, 19, 35, 67, 94, 255, 171, 0, 0, 82, 78, 11, 199, 21, 121, 6, 0, 145, 39, 237, 189, 96, 0, 0, 91, 112, 84, 4, 0, 110, 8, 19, 57, 10, 186, 65, 0, 0, 68, 0, 43, 68, 136, 50, 184, 46, 56, 47, 33, 124, 62, 17, 0, 85, 7, 85, 104, 89, 255, 227, 22, 30, 0, 55, 151, 0, 93, 90, 29, 255, 119, 40, 170, 36, 0, 0, 255, 0, 167, 23, 0, 2, 160, 70, 0, 7, 0, 38, 91, 164, 0, 72, 3, 133, 79, 15, 63, 36, 30, 217, 1, 27, 0, 236, 179, 40, 125, 12, 1, 0, 180, 16, 58, 47, 0, 14, 0, 14, 0, 0, 28, 24, 125, 34, 191, 13, 0, 1, 0, 3, 52, 25, 36, 240, 148, 0, 0, 109, 53, 25, 193, 4, 111, 30, 50, 160, 40, 237, 124, 96, 0, 0, 91, 112, 125, 4, 0, 86, 10, 0, 60, 38, 176, 126, 0, 43, 65, 0, 73, 11, 144, 57, 196, 29, 55, 61, 45, 105, 78, 47, 0, 98, 11, 75, 88, 126, 236, 217, 5, 10, 52, 44, 157, 31, 83, 14, 15, 255, 74, 29, 162, 49, 73, 0, 253, 28, 58, 1, 0, 2, 55, 69, 0, 7, 0, 16, 117, 130, 0, 71, 3, 158, 4, 25, 200, 0, 7, 255, 0, 34, 0, 255, 255, 11, 137, 12, 1, 0, 168, 66, 53, 45, 0, 0, 0, 10, 0, 0, 13, 29, 78, 8, 110, 13, 0, 78, 0, 11, 49, 28, 81, 135, 39, 0, 0, 95, 119, 0, 191, 24, 98, 15, 0, 174, 36, 237, 51, 99, 0, 0, 54, 79, 115, 27, 0, 73, 8, 0, 63, 0, 111, 156, 0, 21, 47, 0, 68, 0, 151, 91, 188, 3, 36, 42, 62, 106, 81, 42, 0, 37, 4, 24, 87, 0, 189, 123, 85, 0, 25, 102, 163, 0, 93, 72, 25, 245, 120, 12, 183, 27, 27, 0, 255, 0, 111, 0, 0, 2, 172, 25, 39, 7, 0, 59, 100, 115, 20, 46, 3, 194, 49, 5, 133, 0, 7, 204, 44, 22, 41, 202, 190, 40, 102, 31, 1, 0, 98, 19, 0, 11, 0, 0, 0, 1, 58, 40, 4, 53, 58, 36, 189, 13, 0, 36, 35, 14, 95, 0, 84, 91, 15, 0, 0, 55, 80, 0, 188, 84, 115, 18, 0, 226, 61, 237, 197, 78, 0, 0, 25, 121, 119, 4, 0, 46, 139, 0, 69, 0, 151, 20, 0, 37, 0, 58, 11, 68, 156, 21, 165, 22, 4, 87, 63, 153, 65, 11, 0, 46, 4, 32, 76, 104, 234, 255, 0, 11, 44, 0, 151, 123, 93, 32, 24, 233, 53, 31, 183, 27, 0, 0, 232, 10, 30, 114, 34, 10, 108, 26, 0, 7, 0, 55, 81, 156, 0, 10, 3, 84, 69, 8, 89, 1, 78, 154, 75, 23, 0, 165, 180, 74, 140, 12, 88, 0, 56, 16, 0, 134, 0, 89, 0, 1, 30, 46, 4, 1, 26, 12, 125, 17, 0, 70, 0, 111, 51, 59, 68, 238, 39, 0, 0, 28, 80, 0, 47, 64, 128, 23, 40, 126, 0, 237, 194, 130, 0, 0, 79, 49, 102, 7, 0, 75, 18, 0, 55, 26, 160, 35, 0, 7, 80, 0, 53, 15, 168, 94, 178, 36, 45, 0, 64, 101, 17, 17, 0, 43, 16, 62, 81, 95, 255, 236, 52, 119, 0, 0, 163, 162, 70, 85, 119, 255, 142, 2, 157, 27, 48, 0, 223, 0, 55, 52, 29, 3, 74, 60, 44, 7, 0, 135, 90, 178, 0, 61, 3, 114, 0, 20, 255, 0, 24, 208, 0, 128, 50, 242, 255, 0, 107, 12, 24, 0, 67, 38, 127, 141, 0, 0, 0, 114, 116, 14, 8, 34, 88, 11, 137, 13, 0, 0, 63, 0, 33, 61, 99, 156, 29, 0, 0, 67, 124, 36, 199, 14, 80, 17, 71, 148, 12, 214, 71, 185, 0, 0, 89, 96, 105, 21, 0, 98, 34, 0, 17, 16, 180, 49, 0, 0, 49, 47, 27, 2, 187, 21, 207, 50, 18, 53, 48, 163, 4, 22, 59, 38, 4, 118, 49, 157, 253, 255, 5, 0, 44, 24, 169, 0, 93, 0, 85, 250, 133, 45, 67, 27, 0, 11, 237, 0, 105, 17, 0, 2, 135, 74, 8, 7, 28, 145, 85, 154, 48, 79, 3, 177, 0, 4, 102, 0, 11, 203, 0, 87, 68, 255, 182, 0, 124, 12, 1, 0, 122, 23, 59, 15, 0, 27, 0, 143, 7, 0, 12, 1, 39, 61, 111, 13, 0, 18, 0, 43, 73, 0, 96, 167, 60, 0, 0, 80, 123, 4, 182, 42, 138, 5, 0, 130, 13, 237, 98, 212, 0, 0, 61, 84, 113, 29, 0, 75, 40, 0, 60, 25, 179, 6, 0, 18, 32, 50, 8, 0, 147, 39, 193, 30, 63, 66, 62, 112, 0, 21, 49, 35, 4, 70, 47, 116, 221, 255, 15, 0, 0, 198, 120, 0, 85, 0, 8, 254, 122, 47, 106, 31, 92, 31, 237, 44, 11, 31, 0, 3, 89, 90, 0, 7, 0, 32, 76, 149, 0, 70, 3, 168, 20, 11, 65, 0, 29, 214, 18, 26, 0, 255, 189, 35, 140, 12, 62, 0, 130, 49, 0, 31, 0, 89, 0, 51, 0, 0, 25, 30, 33, 8, 159, 13, 0, 0, 7, 0, 22, 0, 115, 172, 48, 17, 0, 75, 143, 46, 171, 0, 155, 0, 0, 84, 0, 229, 160, 98, 0, 0, 63, 109, 103, 21, 0, 131, 32, 0, 36, 42, 161, 153, 0, 16, 50, 0, 67, 0, 111, 64, 170, 90, 38, 76, 62, 84, 31, 31, 0, 26, 4, 71, 69, 148, 232, 233, 22, 0, 0, 0, 150, 0, 93, 0, 5, 240, 93, 84, 152, 39, 0, 0, 255, 0, 82, 0, 12, 40, 129, 140, 0, 7, 35, 30, 75, 146, 89, 61, 24, 128, 42, 15, 255, 0, 21, 223, 0, 57, 6, 255, 186, 0, 128, 12, 131, 0, 158, 44, 0, 47, 0, 0, 26, 24, 46, 53, 34, 13, 99, 20, 160, 13, 0, 54, 0, 64, 32, 0, 74, 166, 115, 0, 0, 63, 72, 0, 184, 40, 124, 50, 0, 144, 60, 223, 92, 85, 0, 0, 94, 92, 94, 17, 0, 107, 7, 0, 25, 76, 165, 48, 0, 23, 35, 0, 67, 49, 184, 46, 187, 52, 36, 72, 78, 108, 58, 11, 0, 51, 4, 52, 107, 163, 255, 255, 20, 0, 0, 74, 169, 40, 93, 0, 5, 193, 130, 71, 141, 32, 0, 35, 248, 46, 64, 0, 29, 12, 106, 51, 0, 7, 0, 85, 55, 110, 57, 71, 3, 186, 6, 4, 35, 26, 16, 121, 0, 64, 0, 255, 122, 0, 118, 12, 50, 0, 102, 92, 0, 8, 0, 9, 0, 17, 7, 44, 32, 15, 93, 27, 179, 13, 0, 0, 0, 0, 112, 0, 73, 255, 64, 0, 0, 136, 148, 40, 192, 23, 92, 19, 0, 148, 60, 237, 136, 151, 0, 0, 94, 71, 56, 15, 0, 102, 29, 0, 69, 49, 109, 158, 0, 0, 42, 0, 72, 49, 185, 27, 169, 27, 30, 22, 43, 114, 0, 11, 25, 69, 11, 97, 60, 120, 255, 254, 1, 0, 0, 15, 161, 0, 93, 40, 14, 255, 95, 2, 167, 27, 73, 0, 234, 0, 132, 52, 0, 55, 145, 30, 39, 7, 0, 99, 115, 157, 0, 79, 3, 175, 12, 16, 67, 44, 14, 248, 15, 65, 0, 236, 203, 0, 99, 12, 67, 0, 44, 36, 0, 33, 0, 2, 0, 66, 35, 30, 8, 7, 68, 44, 175, 13, 0, 38, 0, 26, 79, 0, 49, 229, 6, 0, 0, 71, 131, 0, 183, 13, 87, 17, 0, 128, 17, 237, 100, 151, 0, 0, 94, 109, 132, 30, 0, 82, 43, 0, 44, 28, 111, 98, 48, 43, 74, 0, 61, 8, 152, 49, 147, 0, 69, 75, 78, 143, 1, 11, 5, 7, 18, 46, 66, 166, 203, 221, 41, 0, 0, 8, 80, 22, 93, 31, 76, 175, 108, 7, 140, 27, 40, 7, 255, 43, 86, 8, 0, 24, 107, 47, 0, 7, 0, 68, 108, 181, 0, 79, 3, 210, 52, 38, 73, 17, 9, 211, 0, 34, 0, 243, 205, 74, 96, 12, 1, 0, 108, 70, 0, 71, 13, 29, 0, 36, 0, 27, 25, 13, 86, 37, 153, 13, 0, 18, 0, 17, 63, 0, 140, 149, 14, 0, 0, 39, 182, 16, 176, 0, 96, 10, 16, 163, 20, 237, 107, 129, 0, 0, 94, 80, 145, 57, 0, 119, 26, 0, 0, 13, 170, 133, 0, 48, 37, 0, 114, 68, 181, 69, 200, 65, 38, 75, 53, 121, 26, 26, 17, 57, 4, 84, 87, 125, 255, 189, 0, 0, 27, 0, 169, 0, 51, 0, 5, 185, 128, 22, 159, 27, 66, 51, 228, 71, 119, 83, 0, 29, 156, 69, 62, 12, 0, 95, 91, 185, 0, 60, 3, 237, 0, 15, 150, 77, 11, 164, 0, 86, 0, 206, 135, 70, 122, 12, 71, 0, 96, 45, 0, 10, 0, 0, 0, 21, 14, 0, 23, 17, 91, 11, 184, 13, 0, 0, 27, 0, 31, 0, 111, 160, 94, 0, 0, 32, 66, 0, 199, 62, 56, 0, 30, 108, 47, 237, 96, 138, 0, 0, 94, 101, 117, 25, 0, 88, 33, 0, 20, 59, 179, 77, 0, 39, 39, 0, 71, 68, 178, 27, 193, 16, 33, 85, 24, 64, 0, 30, 0, 68, 4, 23, 90, 129, 254, 255, 0, 0, 0, 164, 131, 0, 93, 23, 7, 255, 132, 34, 176, 27, 73, 13, 255, 0, 135, 0, 0, 35, 111, 82, 0, 7, 0, 79, 59, 100, 0, 79, 3, 164, 0, 11, 83, 49, 7, 130, 1, 43, 0, 255, 189, 41, 119, 12, 48, 0, 47, 98, 0, 57, 48, 0, 0, 42, 3, 43, 22, 12, 154, 23, 143, 13, 0, 64, 0, 32, 86, 0, 99, 217, 81, 0, 0, 115, 72, 51, 199, 1, 98, 20, 1, 128, 90, 237, 104, 120, 0, 0, 73, 137, 107, 4, 0, 47, 37, 0, 57, 40, 165, 31, 0, 7, 48, 0, 66, 68, 187, 42, 207, 29, 47, 94, 35, 113, 45, 16, 0, 16, 4, 39, 56, 121, 222, 255, 6, 0, 0, 0, 129, 0, 93, 3, 31, 255, 137, 37, 183, 27, 0, 0, 255, 0, 96, 2, 0, 16, 112, 33, 0, 7, 0, 93, 78, 169, 0, 74, 3, 170, 0, 4, 81, 14, 16, 228, 14, 44, 0, 255, 225, 44, 117, 12, 17, 0, 103, 56, 0, 24, 0, 0, 0, 17, 0, 12, 8, 5, 104, 14, 182, 13, 0, 0, 0, 0, 108, 24, 31, 197, 156, 0, 0, 103, 88, 63, 199, 31, 98, 21, 0, 85, 14, 237, 79, 128, 0, 0, 79, 75, 91, 4, 0, 81, 17, 0, 69, 6, 167, 18, 0, 0, 74, 57, 39, 63, 139, 21, 191, 74, 11, 45, 65, 125, 24, 24, 0, 74, 4, 71, 79, 148, 255, 239, 5, 2, 19, 126, 142, 52, 93, 0, 38, 255, 130, 7, 133, 27, 51, 0, 249, 21, 72, 2, 0, 21, 121, 66, 0, 7, 0, 76, 77, 174, 0, 72, 3, 83, 20, 4, 171, 0, 7, 215, 0, 26, 0, 255, 186, 9, 90, 12, 61, 0, 163, 32, 0, 6, 0, 4, 0, 10, 0, 0, 4, 5, 78, 52, 171, 13, 0, 6, 55, 0, 76, 0, 25, 189, 118, 0, 0, 106, 92, 13, 178, 30, 102, 1, 43, 74, 39, 237, 125, 157, 0, 0, 80, 47, 89, 4, 0, 91, 21, 0, 69, 0, 186, 118, 0, 3, 63, 8, 85, 55, 156, 21, 185, 45, 27, 61, 64, 111, 36, 11, 0, 16, 4, 80, 74, 127, 255, 225, 2, 45, 47, 0, 163, 0, 93, 0, 32, 255, 56, 12, 14, 27, 47, 28, 206, 16, 141, 52, 0, 2, 148, 83, 0, 7, 0, 76, 91, 161, 0, 71, 7, 124, 43, 5, 159, 0, 7, 183, 0, 38, 18, 255, 158, 40, 136, 12, 60, 0, 120, 33, 0, 23, 0, 0, 0, 30, 58, 0, 43, 26, 45, 32, 126, 13, 0, 0, 0, 0, 31, 0, 44, 204, 102, 0, 0, 44, 101, 0, 159, 18, 85, 11, 10, 103, 36, 237, 91, 138, 0, 0, 89, 120, 118, 59, 0, 88, 16, 0, 69, 21, 132, 9, 45, 21, 47, 21, 21, 0, 185, 59, 148, 68, 36, 66, 71, 117, 12, 11, 11, 5, 4, 37, 35, 162, 188, 255, 46, 0, 0, 0, 138, 0, 93, 0, 16, 255, 92, 31, 111, 27, 0, 0, 255, 66, 57, 0, 0, 2, 72, 45, 31, 7, 0, 40, 39, 172, 0, 79, 3, 159, 30, 13, 181, 0, 7, 235, 0, 21, 11, 255, 250, 74, 122, 12, 1, 0, 170, 66, 0, 64, 0, 82, 0, 39, 15, 59, 62, 1, 79, 2, 142, 13, 0, 91, 0, 43, 71, 4, 60, 167, 126, 0, 0, 80, 95, 53, 167, 28, 161, 0, 0, 111, 21, 237, 111, 165, 0, 0, 94, 68, 91, 4, 0, 134, 17, 14, 67, 15, 160, 16, 4, 0, 105, 37, 22, 0, 93, 102, 182, 83, 0, 35, 24, 142, 14, 11, 0, 33, 4, 76, 79, 112, 213, 242, 10, 3, 0, 0, 157, 0, 93, 0, 29, 234, 132, 62, 147, 27, 12, 31, 255, 56, 73, 8, 58, 2, 126, 103, 0, 7, 0, 47, 94, 131, 1, 76, 3, 171, 17, 12, 50, 22, 7, 172, 0, 32, 20, 255, 193, 35, 123, 12, 69, 0, 157, 39, 0, 36, 0, 44, 0, 10, 0, 24, 35, 18, 50, 10, 149, 13, 0, 2, 0, 0, 67, 0, 35, 172, 73, 0, 0, 92, 108, 37, 126, 52, 142, 5, 13, 148, 11, 237, 110, 117, 0, 0, 94, 59, 112, 18, 0, 115, 19, 0, 63, 14, 170, 103, 0, 4, 27, 11, 63, 41, 153, 21, 191, 34, 11, 86, 14, 156, 20, 11, 0, 68, 4, 13, 93, 93, 239, 255, 18, 0, 0, 69, 127, 47, 86, 0, 8, 255, 126, 82, 173, 27, 19, 0, 236, 7, 34, 34, 0, 2, 134, 34, 0, 7, 0, 30, 117, 126, 34, 75, 3, 162, 0, 7, 153, 68, 7, 158, 11, 39, 0, 249, 212, 31, 124, 12, 19, 0, 126, 68, 0, 39, 0, 0, 0, 7, 26, 52, 15, 43, 58, 15, 170, 13, 0, 43, 0, 0, 82, 27, 89, 158, 73, 0, 0, 53, 107, 0, 123, 16, 96, 8, 0, 143, 17, 237, 96, 110, 0, 0, 94, 112, 94, 22, 0, 104, 7, 0, 69, 0, 167, 34, 0, 0, 72, 4, 36, 0, 142, 24, 176, 24, 63, 72, 78, 98, 3, 11, 0, 21, 4, 7, 37, 67, 254, 247, 31, 71, 0, 0, 127, 0, 82, 0, 35, 240, 134, 61, 179, 37, 0, 0, 255, 22, 68, 22, 21, 10, 136, 66, 0, 7, 0, 50, 91, 144, 0, 79, 3, 128, 7, 8, 75, 0, 11, 164, 25, 21, 22, 232, 199, 74, 108, 12, 16, 0, 108, 42, 0, 13, 0, 5, 0, 1, 0, 46, 11, 51, 32, 11, 213, 13, 0, 0, 25, 32, 98, 59, 68, 214, 139, 0, 0, 48, 120, 0, 196, 0, 124, 0, 29, 77, 10, 237, 82, 184, 0, 33, 77, 96, 83, 48, 0, 105, 11, 0, 69, 0, 162, 35, 0, 0, 67, 46, 25, 0, 170, 25, 189, 55, 148, 119, 54, 114, 0, 11, 0, 13, 4, 41, 57, 110, 255, 225, 10, 14, 0, 54, 162, 11, 91, 0, 5, 255, 118, 26, 169, 27, 0, 0, 249, 58, 6, 42, 0, 14, 123, 33, 0, 7, 0, 29, 75, 154, 13, 79, 5, 120, 20, 4, 125, 53, 33, 206, 19, 34, 0, 255, 226, 47, 99, 12, 7, 0, 227, 31, 0, 16, 0, 24, 0, 1, 0, 0, 31, 4, 11, 15, 187, 13, 0, 0, 0, 0, 80, 23, 85, 163, 101, 0, 0, 71, 49, 0, 189, 39, 145, 0, 53, 125, 3, 237, 36, 123, 0, 0, 89, 64, 112, 33, 0, 110, 16, 0, 34, 16, 164, 105, 0, 0, 71, 0, 91, 68, 165, 21, 186, 44, 11, 59, 56, 136, 0, 11, 0, 18, 4, 59, 69, 104, 255, 255, 15, 0, 28, 0, 156, 26, 89, 3, 17, 255, 121, 7, 159, 27, 69, 0, 202, 0, 94, 92, 0, 2, 69, 38, 71, 7, 0, 55, 70, 164, 0, 79, 3, 109, 13, 8, 253, 23, 7, 185, 0, 86, 0, 255, 190, 15, 140, 12, 10, 0, 48, 35, 9, 8, 0, 0, 0, 22, 107, 32, 24, 40, 61, 31, 161, 13, 0, 0, 0, 0, 73, 19, 57, 224, 46, 0, 0, 46, 61, 50, 182, 19, 61, 0, 0, 146, 56, 237, 125, 166, 0, 0, 60, 104, 118, 33, 0, 96, 37, 0, 1, 28, 151, 64, 0, 41, 36, 1, 46, 0, 167, 21, 187, 45, 11, 55, 70, 111, 12, 11, 0, 22, 28, 115, 54, 151, 245, 231, 15, 87, 0, 0, 123, 0, 93, 0, 42, 255, 140, 32, 114, 27, 33, 29, 255, 0, 149, 24, 39, 1, 138, 97, 0, 7, 0, 125, 117, 142, 0, 74, 3, 119, 11, 11, 16, 0, 7, 195, 0, 56, 53, 255, 218, 43, 135, 12, 24, 0, 147, 33, 74, 42, 0, 0, 0, 95, 35, 0, 4, 23, 100, 15, 109, 13, 0, 27, 104, 36, 61, 0, 126, 132, 52, 0, 0, 38, 162, 0, 51, 42, 112, 9, 53, 100, 32, 198, 126, 125, 0, 0, 77, 96, 97, 46, 0, 113, 16, 0, 26, 8, 111, 188, 0, 0, 27, 9, 87, 0, 75, 27, 207, 36, 26, 112, 33, 109, 58, 11, 0, 51, 4, 94, 95, 81, 255, 210, 0, 0, 0, 124, 118, 41, 85, 4, 53, 203, 133, 87, 104, 34, 54, 0, 255, 0, 108, 8, 0, 7, 87, 44, 0, 7, 0, 40, 102, 144, 0, 0, 3, 135, 78, 12, 73, 0, 7, 112, 58, 31, 41, 255, 206, 38, 99, 12, 112, 0, 178, 68, 28, 37, 0, 0, 0, 3, 0, 0, 4, 29, 35, 38, 207, 13, 0, 0, 0, 0, 56, 23, 104, 140, 41, 0, 0, 74, 90, 0, 184, 57, 141, 15, 64, 151, 32, 237, 119, 168, 0, 0, 94, 74, 99, 7, 0, 71, 22, 0, 50, 2, 151, 109, 0, 11, 33, 45, 32, 53, 147, 41, 203, 0, 22, 87, 29, 60, 34, 11, 0, 38, 4, 15, 133, 99, 255, 208, 27, 0, 28, 0, 154, 38, 93, 40, 8, 255, 112, 7, 179, 27, 76, 0, 246, 11, 13, 0, 0, 3, 108, 72, 0, 7, 0, 25, 64, 143, 0, 79, 3, 122, 17, 16, 255, 0, 7, 244, 3, 21, 6, 255, 228, 67, 90, 12, 1, 0, 50, 19, 0, 39, 0, 0, 0, 10, 59, 61, 4, 43, 42, 14, 212, 13, 0, 0, 0, 0, 104, 12, 72, 174, 1, 0, 0, 30, 40, 0, 199, 4, 99, 21, 0, 85, 26, 237, 97, 108, 0, 0, 89, 107, 113, 50, 0, 86, 16, 0, 69, 69, 176, 121, 0, 8, 47, 0, 43, 0, 177, 21, 182, 33, 14, 78, 78, 160, 0, 11, 0, 23, 4, 10, 113, 84, 251, 255, 78, 3, 0, 0, 158, 48, 86, 8, 45, 248, 121, 78, 179, 27, 1, 0, 254, 14, 99, 14, 0, 2, 122, 24, 12, 7, 0, 14, 86, 143, 0, 60, 3, 138, 3, 4, 106, 0, 7, 159, 50, 21, 0, 217, 224, 74, 53, 12, 1, 0, 85, 74, 53, 20, 0, 53, 0, 14, 0, 52, 52, 29, 113, 8, 203, 13, 0, 0, 65, 0, 117, 0, 98, 222, 1, 0, 0, 45, 97, 0, 199, 35, 149, 0, 73, 142, 27, 237, 118, 107, 0, 0, 94, 169, 93, 20, 0, 93, 34, 0, 69, 15, 172, 109, 0, 4, 63, 0, 63, 0, 161, 33, 179, 48, 30, 54, 75, 132, 0, 11, 0, 33, 4, 14, 94, 134, 255, 212, 30, 38, 0, 0, 161, 0, 78, 14, 70, 255, 114, 23, 127, 27, 32, 0, 255, 15, 130, 19, 0, 12, 120, 67, 25, 7, 0, 62, 106, 189, 0, 64, 12, 44, 12, 4, 200, 0, 7, 185, 0, 23, 70, 255, 177, 74, 113, 12, 1, 0, 138, 29, 0, 22, 0, 0, 0, 23, 0, 44, 23, 12, 40, 2, 199, 13, 0, 0, 0, 0, 42, 0, 70, 200, 92, 0, 0, 36, 69, 27, 199, 52, 116, 0, 8, 88, 18, 237, 118, 216, 0, 0, 94, 124, 98, 20, 0, 101, 55, 0, 67, 32, 153, 167, 8, 18, 48, 0, 55, 0, 169, 50, 201, 47, 91, 51, 48, 83, 4, 21, 0, 29, 4, 23, 59, 149, 243, 209, 10, 0, 0, 105, 162, 0, 84, 0, 18, 230, 80, 23, 118, 27, 0, 7, 213, 0, 96, 0, 15, 48, 184, 57, 0, 7, 0, 56, 93, 174, 0, 79, 15, 201, 7, 23, 70, 25, 29, 236, 0, 29, 0, 238, 244, 34, 94, 12, 18, 0, 189, 53, 0, 40, 0, 0, 0, 14, 0, 0, 33, 11, 84, 2, 147, 13, 0, 2, 0, 42, 17, 12, 109, 201, 104, 0, 0, 90, 172, 47, 193, 38, 142, 0, 14, 105, 30, 220, 108, 105, 0, 0, 94, 60, 128, 33, 0, 80, 20, 0, 56, 3, 170, 158, 0, 63, 63, 0, 111, 68, 168, 84, 202, 75, 76, 76, 41, 75, 52, 11, 0, 37, 4, 29, 121, 186, 255, 171, 49, 18, 0, 43, 169, 37, 93, 0, 10, 192, 122, 66, 166, 27, 112, 0, 202, 0, 92, 12, 19, 11, 112, 36, 47, 7, 0, 22, 50, 159, 0, 79, 3, 133, 27, 4, 144, 0, 16, 165, 0, 36, 0, 242, 189, 26, 103, 12, 1, 0, 46, 66, 1, 62, 0, 0, 0, 1, 30, 59, 50, 1, 51, 11, 167, 13, 0, 26, 0, 0, 15, 0, 101, 187, 1, 0, 0, 64, 89, 39, 191, 57, 112, 0, 83, 99, 80, 237, 74, 129, 0, 0, 60, 73, 75, 28, 0, 106, 18, 0, 69, 10, 182, 34, 0, 25, 41, 0, 52, 9, 170, 21, 191, 50, 30, 91, 50, 122, 0, 24, 0, 31, 4, 56, 97, 157, 247, 255, 28, 31, 7, 0, 159, 42, 78, 0, 21, 255, 95, 47, 141, 37, 0, 0, 255, 45, 71, 0, 3, 2, 129, 48, 6, 7, 0, 24, 79, 155, 0, 79, 3, 139, 17, 4, 245, 63, 7, 135, 0, 38, 0, 255, 194, 74, 140, 12, 1, 0, 130, 48, 46, 57, 0, 15, 0, 24, 44, 0, 23, 9, 30, 16, 149, 13, 0, 9, 0, 0, 84, 0, 71, 212, 102, 0, 0, 115, 83, 59, 199, 22, 149, 28, 0, 88, 35, 237, 118, 137, 0, 0, 79, 64, 100, 9, 0, 85, 25, 0, 69, 9, 147, 6, 0, 11, 61, 0, 36, 0, 177, 40, 185, 12, 117, 63, 61, 152, 0, 29, 0, 26, 4, 39, 68, 133, 206, 243, 61, 0, 0, 0, 128, 36, 93, 0, 8, 205, 110, 2, 141, 27, 0, 0, 255, 44, 109, 0, 5, 2, 96, 29, 0, 7, 0, 25, 77, 161, 11, 45, 3, 107, 7, 7, 38, 26, 36, 133, 0, 48, 0, 255, 192, 67, 100, 12, 45, 0, 77, 16, 0, 18, 0, 44, 0, 10, 9, 0, 30, 12, 26, 39, 201, 13, 0, 21, 0, 0, 131, 0, 57, 227, 60, 0, 0, 65, 104, 14, 166, 15, 121, 10, 0, 174, 1, 237, 215, 96, 0, 0, 9, 128, 120, 10, 0, 48, 34, 27, 42, 16, 183, 6, 0, 64, 7, 0, 11, 10, 167, 33, 196, 6, 3, 92, 78, 160, 55, 11, 42, 45, 4, 93, 70, 96, 255, 255, 5, 0, 42, 66, 134, 0, 93, 0, 39, 175, 69, 12, 183, 27, 18, 11, 228, 12, 114, 115, 0, 3, 73, 17, 64, 7, 0, 35, 117, 181, 0, 0, 3, 70, 36, 8, 43, 24, 91, 240, 81, 45, 0, 175, 177, 74, 134, 12, 78, 0, 53, 45, 31, 143, 0, 71, 0, 1, 0, 8, 66, 4, 53, 2, 175, 55, 0, 87, 36, 162, 56, 75, 39, 255, 100, 0, 0, 28, 108, 0, 91, 32, 90, 30, 41, 152, 5, 237, 185, 133, 0, 0, 84, 114, 149, 62, 0, 91, 99, 18, 56, 38, 163, 137, 0, 0, 128, 0, 80, 0, 152, 21, 183, 63, 94, 50, 63, 98, 0, 59, 0, 30, 4, 67, 91, 95, 255, 173, 52, 48, 0, 96, 169, 0, 73, 10, 18, 196, 121, 2, 168, 27, 0, 0, 255, 0, 161, 0, 40, 88, 204, 44, 0, 7, 0, 19, 73, 189, 0, 72, 5, 113, 41, 10, 84, 69, 55, 176, 0, 32, 0, 234, 247, 38, 79, 12, 47, 0, 171, 34, 0, 255, 26, 0, 0, 26, 0, 0, 30, 15, 39, 36, 163, 13, 0, 0, 0, 0, 81, 0, 23, 255, 43, 0, 0, 86, 136, 0, 199, 0, 109, 0, 56, 94, 51, 225, 86, 122, 0, 33, 85, 137, 83, 97, 0, 117, 36, 0, 69, 14, 180, 126, 0, 17, 55, 0, 44, 38, 174, 21, 207, 52, 94, 101, 55, 148, 65, 24, 0, 60, 4, 72, 29, 139, 255, 242, 36, 0, 7, 0, 133, 0, 50, 7, 5, 255, 145, 14, 175, 27, 0, 0, 255, 0, 155, 11, 0, 7, 159, 29, 0, 7, 0, 14, 79, 105, 0, 57, 3, 111, 10, 4, 153, 44, 20, 185, 9, 39, 0, 251, 190, 66, 116, 12, 1, 0, 161, 139, 5, 20, 0, 0, 0, 10, 0, 0, 30, 32, 82, 18, 204, 13, 0, 76, 0, 28, 75, 0, 77, 205, 114, 0, 0, 84, 54, 0, 171, 5, 142, 4, 0, 73, 26, 237, 113, 89, 0, 0, 73, 139, 88, 54, 0, 107, 10, 0, 60, 29, 156, 68, 0, 4, 21, 35, 57, 0, 161, 49, 177, 16, 65, 70, 62, 84, 34, 24, 90, 18, 4, 56, 47, 84, 220, 217, 16, 0, 0, 0, 112, 73, 85, 7, 11, 224, 122, 18, 169, 27, 0, 0, 255, 0, 162, 46, 0, 40, 109, 32, 0, 7, 0, 26, 110, 139, 2, 50, 3, 200, 34, 4, 79, 75, 14, 185, 63, 37, 0, 255, 201, 54, 117, 12, 1, 0, 94, 65, 0, 7, 13, 0, 0, 33, 17, 34, 7, 35, 33, 20, 181, 13, 0, 0, 0, 0, 100, 0, 66, 137, 165, 0, 0, 92, 148, 0, 151, 42, 100, 0, 0, 120, 28, 237, 131, 86, 0, 0, 35, 120, 112, 22, 0, 87, 10, 0, 63, 21, 186, 57, 0, 24, 40, 31, 42, 0, 176, 58, 162, 19, 29, 66, 65, 113, 34, 11, 90, 13, 4, 27, 68, 149, 255, 255, 28, 0, 0, 164, 155, 0, 93, 8, 5, 250, 118, 81, 152, 31, 0, 0, 240, 61, 62, 19, 0, 14, 73, 60, 0, 7, 0, 85, 94, 80, 0, 56, 3, 145, 38, 4, 43, 0, 7, 185, 0, 31, 0, 242, 252, 23, 115, 12, 1, 0, 159, 99, 0, 18, 0, 0, 0, 60, 0, 39, 69, 9, 173, 23, 203, 13, 0, 0, 0, 0, 86, 0, 114, 219, 121, 0, 0, 75, 86, 0, 199, 0, 140, 15, 72, 129, 33, 237, 104, 129, 0, 0, 94, 82, 174, 16, 0, 84, 43, 6, 69, 38, 137, 202, 0, 7, 94, 0, 115, 32, 146, 21, 172, 36, 37, 70, 62, 102, 9, 11, 81, 5, 4, 41, 82, 150, 242, 176, 28, 0, 0, 23, 154, 0, 93, 0, 19, 193, 110, 18, 143, 34, 0, 0, 241, 0, 193, 0, 0, 17, 163, 46, 0, 7, 0, 53, 98, 123, 0, 79, 3, 159, 8, 4, 129, 0, 18, 207, 0, 43, 35, 245, 200, 0, 56, 12, 72, 0, 91, 26, 0, 26, 0, 0, 0, 45, 12, 30, 42, 1, 122, 37, 191, 13, 0, 10, 0, 0, 84, 0, 82, 233, 81, 0, 0, 99, 126, 0, 188, 1, 85, 24, 0, 87, 107, 237, 90, 187, 0, 0, 85, 52, 96, 15, 0, 92, 17, 0, 65, 56, 119, 44, 0, 26, 47, 22, 33, 55, 171, 63, 195, 15, 27, 37, 70, 140, 0, 11, 0, 5, 4, 114, 84, 130, 250, 255, 46, 0, 0, 0, 161, 14, 91, 59, 43, 240, 117, 43, 164, 27, 90, 0, 202, 56, 71, 82, 0, 5, 91, 17, 29, 7, 0, 75, 78, 173, 0, 48, 3, 169, 15, 21, 137, 0, 19, 189, 0, 53, 32, 255, 217, 0, 76, 12, 152, 0, 54, 57, 0, 20, 0, 1, 0, 14, 86, 0, 4, 1, 102, 50, 155, 13, 0, 67, 0, 29, 54, 0, 109, 201, 66, 0, 0, 77, 155, 0, 184, 53, 108, 0, 2, 160, 45, 237, 114, 162, 0, 0, 83, 118, 91, 21, 0, 57, 37, 0, 38, 0, 149, 20, 0, 29, 29, 55, 24, 0, 145, 53, 116, 6, 53, 51, 70, 114, 0, 11, 0, 18, 40, 64, 76, 157, 225, 239, 11, 107, 0, 32, 144, 40, 89, 12, 71, 231, 102, 20, 128, 34, 48, 0, 242, 51, 87, 0, 0, 5, 119, 55, 0, 7, 0, 101, 70, 181, 54, 79, 3, 113, 78, 31, 64, 0, 26, 197, 0, 34, 68, 255, 205, 64, 132, 12, 1, 0, 176, 30, 0, 24, 0, 65, 0, 33, 0, 0, 48, 1, 67, 8, 178, 13, 0, 13, 0, 0, 62, 0, 80, 153, 77, 0, 0, 83, 78, 38, 199, 50, 180, 18, 0, 167, 40, 237, 123, 162, 0, 0, 94, 109, 107, 37, 0, 83, 5, 48, 1, 11, 150, 54, 57, 29, 43, 0, 47, 41, 187, 62, 207, 47, 72, 118, 19, 131, 0, 11, 0, 32, 4, 103, 119, 179, 234, 252, 12, 0, 0, 192, 160, 0, 89, 0, 7, 199, 116, 45, 183, 27, 47, 0, 251, 50, 126, 15, 0, 9, 123, 28, 27, 18, 0, 22, 111, 155, 0, 62, 3, 165, 0, 11, 91, 0, 12, 173, 0, 42, 45, 255, 149, 74, 119, 12, 47, 0, 139, 46, 0, 35, 0, 0, 0, 15, 0, 0, 62, 1, 85, 16, 175, 13, 0, 0, 66, 0, 85, 0, 51, 161, 60, 0, 0, 58, 77, 10, 165, 52, 91, 1, 67, 161, 28, 237, 70, 171, 0, 0, 94, 100, 82, 60, 0, 88, 5, 0, 69, 12, 181, 48, 8, 25, 67, 32, 42, 68, 155, 31, 207, 81, 1, 63, 12, 152, 2, 11, 0, 44, 4, 56, 105, 171, 255, 209, 8, 0, 0, 0, 149, 98, 84, 0, 7, 255, 93, 20, 183, 27, 18, 12, 229, 48, 96, 18, 0, 2, 99, 49, 11, 7, 0, 90, 108, 145, 0, 79, 3, 118, 0, 4, 82, 0, 7, 180, 0, 33, 0, 255, 182, 47, 122, 12, 23, 0, 143, 95, 25, 33, 0, 10, 0, 45, 18, 12, 71, 31, 182, 4, 140, 13, 0, 40, 0, 0, 54, 0, 78, 246, 84, 0, 0, 100, 100, 54, 184, 10, 122, 24, 18, 83, 32, 237, 105, 126, 0, 0, 94, 113, 122, 18, 0, 86, 5, 0, 69, 8, 172, 143, 0, 0, 56, 0, 65, 46, 161, 33, 207, 11, 56, 66, 29, 107, 56, 11, 59, 91, 4, 62, 41, 92, 231, 158, 0, 0, 3, 3, 60, 8, 89, 17, 26, 255, 88, 12, 175, 27, 16, 1, 238, 0, 203, 0, 0, 20, 143, 81, 18, 7, 0, 56, 97, 142, 0, 56, 3, 119, 0, 4, 103, 3, 22, 224, 56, 25, 0, 222, 169, 74, 122, 12, 35, 0, 137, 23, 0, 23, 0, 0, 0, 35, 0, 16, 18, 39, 99, 5, 179, 13, 0, 0, 53, 25, 108, 0, 67, 165, 52, 0, 0, 129, 61, 0, 188, 0, 108, 23, 31, 133, 30, 237, 43, 164, 0, 0, 94, 81, 136, 20, 0, 92, 17, 0, 64, 57, 161, 41, 26, 0, 64, 37, 70, 29, 187, 21, 186, 32, 36, 85, 52, 142, 27, 21, 10, 48, 9, 80, 28, 169, 255, 255, 23, 0, 25, 0, 169, 0, 84, 0, 11, 255, 114, 32, 169, 34, 0, 5, 238, 10, 34, 17, 0, 2, 160, 88, 22, 7, 0, 53, 89, 156, 80, 60, 3, 94, 0, 4, 255, 5, 7, 140, 0, 50, 0, 255, 150, 21, 64, 12, 44, 0, 147, 39, 50, 38, 0, 9, 0, 1, 104, 9, 59, 1, 127, 70, 152, 13, 0, 33, 0, 0, 85, 0, 54, 194, 125, 0, 0, 134, 111, 43, 135, 11, 84, 2, 0, 159, 26, 237, 88, 84, 0, 0, 94, 136, 103, 26, 0, 89, 25, 0, 59, 22, 173, 91, 0, 21, 51, 0, 59, 49, 187, 21, 178, 0, 53, 65, 71, 111, 84, 14, 0, 19, 4, 29, 38, 137, 255, 255, 6, 0, 48, 67, 161, 0, 93, 40, 18, 255, 127, 20, 157, 27, 0, 23, 255, 0, 171, 17, 24, 10, 104, 52, 0, 7, 0, 70, 109, 113, 0, 79, 3, 129, 7, 7, 131, 75, 7, 190, 2, 26, 0, 255, 159, 59, 110, 12, 1, 0, 153, 64, 2, 47, 0, 0, 6, 34, 0, 54, 4, 7, 82, 20, 161, 13, 0, 1, 0, 0, 88, 0, 107, 135, 182, 0, 0, 63, 72, 20, 181, 19, 84, 1, 0, 125, 32, 237, 116, 122, 0, 0, 8, 77, 108, 4, 0, 97, 24, 0, 69, 27, 175, 15, 0, 35, 43, 60, 32, 0, 174, 21, 168, 10, 42, 21, 28, 168, 10, 23, 22, 13, 4, 69, 42, 158, 234, 142, 18, 0, 0, 0, 162, 9, 93, 0, 100, 244, 100, 39, 147, 27, 0, 0, 255, 36, 47, 0, 0, 11, 157, 72, 0, 7, 0, 150, 114, 179, 0, 79, 3, 84, 10, 8, 153, 0, 12, 213, 0, 35, 0, 239, 235, 60, 122, 12, 1, 0, 129, 22, 71, 66, 0, 9, 0, 120, 0, 77, 7, 9, 103, 2, 198, 13, 0, 0, 45, 0, 96, 14, 71, 182, 111, 0, 0, 79, 82, 0, 136, 35, 119, 4, 0, 118, 14, 237, 69, 64, 0, 0, 89, 78, 99, 4, 0, 98, 16, 23, 55, 15, 160, 47, 0, 15, 98, 32, 60, 0, 153, 109, 115, 40, 91, 105, 71, 121, 51, 11, 18, 44, 4, 24, 80, 182, 200, 171, 22, 0, 51, 152, 167, 0, 93, 0, 5, 196, 125, 37, 176, 27, 0, 0, 235, 14, 96, 0, 2, 10, 194, 80, 43, 7, 0, 17, 107, 147, 0, 74, 3, 175, 2, 27, 92, 0, 17, 209, 0, 45, 18, 255, 157, 25, 61, 12, 7, 0, 36, 54, 0, 9, 0, 0, 0, 1, 4, 61, 46, 1, 69, 15, 143, 13, 0, 29, 0, 30, 78, 0, 100, 156, 114, 0, 0, 86, 111, 62, 172, 20, 59, 0, 0, 133, 30, 237, 120, 91, 0, 0, 94, 105, 118, 66, 0, 105, 36, 0, 40, 0, 161, 93, 25, 1, 51, 0, 57, 0, 187, 21, 183, 59, 64, 116, 62, 128, 10, 11, 24, 38, 4, 25, 52, 122, 226, 218, 8, 0, 0, 0, 167, 0, 67, 0, 5, 242, 109, 93, 167, 27, 0, 12, 243, 49, 67, 9, 0, 10, 148, 53, 0, 7, 0, 25, 89, 148, 0, 73, 3, 230, 3, 11, 215, 0, 19, 183, 3, 60, 40, 242, 195, 50, 111, 12, 38, 0, 113, 57, 0, 14, 0, 13, 0, 1, 28, 24, 26, 42, 7, 20, 168, 13, 0, 0, 0, 0, 59, 0, 91, 207, 89, 0, 0, 52, 100, 48, 199, 33, 120, 0, 0, 125, 30, 237, 106, 139, 0, 0, 89, 91, 110, 15, 0, 76, 38, 0, 69, 12, 181, 24, 0, 40, 70, 0, 28, 0, 173, 21, 194, 14, 94, 80, 24, 99, 46, 11, 0, 83, 4, 47, 73, 160, 255, 255, 0, 0, 0, 147, 131, 0, 51, 11, 5, 255, 127, 84, 130, 27, 28, 16, 252, 61, 45, 27, 8, 1, 90, 57, 0, 7, 0, 75, 91, 156, 97, 79, 3, 158, 4, 4, 116, 0, 47, 202, 0, 43, 0, 255, 198, 74, 113, 12, 41, 0, 139, 113, 0, 14, 0, 2, 0, 10, 0, 0, 39, 23, 60, 2, 141, 13, 0, 59, 0, 0, 89, 32, 86, 225, 13, 0, 0, 39, 148, 25, 132, 14, 102, 4, 3, 166, 23, 237, 118, 157, 0, 0, 87, 117, 100, 21, 0, 116, 7, 0, 64, 26, 160, 14, 0, 0, 82, 71, 31, 0, 153, 23, 190, 43, 62, 93, 41, 136, 0, 18, 0, 5, 4, 55, 65, 152, 255, 231, 28, 0, 0, 73, 96, 76, 90, 0, 30, 198, 115, 9, 121, 27, 21, 0, 255, 12, 60, 0, 0, 7, 75, 62, 0, 7, 0, 36, 113, 166, 0, 79, 7, 92, 31, 4, 142, 0, 21, 227, 0, 63, 0, 255, 224, 65, 123, 12, 1, 0, 174, 48, 0, 39, 0, 12, 0, 28, 0, 10, 16, 1, 62, 21, 214, 13, 0, 0, 0, 0, 113, 17, 51, 162, 23, 0, 0, 81, 51, 0, 167, 50, 107, 1, 0, 124, 0, 237, 110, 171, 0, 63, 89, 102, 111, 16, 0, 105, 14, 25, 38, 0, 161, 38, 0, 0, 55, 30, 38, 68, 160, 26, 207, 31, 29, 112, 39, 97, 30, 18, 35, 24, 4, 34, 55, 146, 255, 175, 4, 0, 0, 0, 155, 0, 88, 0, 5, 210, 119, 9, 156, 27, 80, 8, 255, 10, 48, 43, 34, 2, 151, 67, 0, 7, 53, 40, 115, 138, 23, 79, 3, 72, 6, 4, 69, 0, 18, 157, 0, 26, 18, 255, 137, 0, 133, 12, 1, 0, 160, 45, 0, 9, 0, 0, 0, 5, 26, 20, 25, 1, 71, 9, 196, 13, 0, 0, 0, 0, 79, 0, 26, 155, 29, 0, 0, 49, 28, 0, 182, 50, 123, 0, 0, 142, 15, 237, 155, 82, 0, 0, 94, 139, 94, 19, 0, 74, 30, 0, 69, 0, 145, 93, 0, 6, 55, 0, 39, 0, 187, 32, 140, 20, 59, 75, 78, 137, 9, 11, 0, 13, 4, 15, 54, 140, 255, 255, 45, 0, 0, 65, 108, 0, 93, 0, 5, 205, 135, 16, 145, 27, 0, 0, 255, 51, 85, 48, 26, 2, 130, 36, 0, 7, 0, 48, 110, 162, 0, 71, 3, 148, 18, 4, 71, 21, 7, 208, 0, 36, 0, 245, 171, 74, 117, 12, 1, 0, 133, 47, 15, 29, 22, 0, 0, 1, 0, 85, 20, 1, 47, 9, 190, 13, 0, 65, 0, 11, 79, 0, 83, 201, 106, 0, 0, 59, 152, 0, 158, 11, 136, 0, 0, 134, 36, 186, 75, 126, 0, 23, 92, 91, 120, 13, 0, 83, 44, 0, 61, 13, 162, 149, 0, 43, 45, 17, 75, 68, 165, 21, 207, 80, 18, 67, 50, 109, 43, 11, 0, 47, 4, 10, 53, 131, 255, 226, 12, 15, 54, 118, 150, 90, 93, 0, 37, 237, 116, 107, 67, 37, 0, 0, 212, 19, 190, 66, 0, 19, 130, 66, 0, 7, 0, 73, 107, 177, 0, 79, 3, 68, 1, 4, 178, 0, 12, 126, 2, 55, 0, 255, 172, 48, 123, 12, 5, 0, 91, 71, 142, 31, 0, 0, 0, 8, 71, 31, 38, 11, 77, 16, 160, 13, 0, 0, 24, 0, 67, 0, 82, 153, 167, 0, 0, 84, 41, 0, 199, 52, 39, 13, 94, 121, 82, 237, 127, 225, 0, 8, 86, 40, 126, 7, 0, 58, 71, 0, 54, 8, 142, 109, 0, 81, 46, 40, 74, 2, 182, 50, 202, 34, 40, 50, 55, 108, 0, 11, 100, 50, 4, 31, 104, 166, 255, 190, 0, 0, 0, 0, 139, 0, 93, 3, 45, 221, 121, 14, 149, 27, 21, 0, 241, 83, 133, 26, 16, 2, 159, 54, 0, 7, 0, 92, 110, 164, 1, 79, 3, 88, 1, 4, 46, 45, 16, 239, 0, 66, 0, 255, 206, 15, 119, 12, 15, 0, 102, 30, 0, 41, 37, 0, 0, 26, 78, 10, 12, 17, 152, 25, 165, 13, 0, 44, 0, 15, 101, 0, 82, 192, 35, 0, 0, 32, 95, 0, 199, 13, 117, 10, 0, 153, 108, 237, 66, 181, 0, 4, 82, 63, 108, 8, 0, 70, 52, 31, 69, 0, 161, 34, 0, 0, 53, 75, 48, 0, 150, 59, 198, 17, 50, 94, 54, 135, 0, 11, 73, 27, 4, 33, 82, 69, 255, 171, 0, 0, 2, 14, 146, 1, 93, 0, 23, 250, 79, 58, 143, 27, 103, 0, 209, 19, 56, 47, 0, 10, 160, 73, 0, 7, 0, 46, 109, 132, 0, 71, 3, 95, 3, 36, 102, 12, 19, 163, 0, 27, 0, 255, 162, 47, 86, 12, 12, 0, 80, 36, 0, 44, 0, 26, 0, 1, 43, 28, 8, 54, 25, 22, 194, 13, 0, 54, 0, 0, 129, 0, 47, 159, 14, 0, 0, 34, 34, 0, 199, 40, 131, 0, 0, 144, 3, 237, 52, 186, 0, 0, 87, 82, 113, 56, 0, 88, 16, 3, 69, 0, 153, 61, 0, 0, 73, 76, 30, 17, 130, 40, 207, 51, 56, 44, 71, 132, 61, 11, 24, 65, 4, 56, 70, 84, 222, 226, 22, 0, 0, 39, 136, 0, 73, 39, 40, 255, 128, 2, 125, 42, 63, 0, 235, 56, 18, 53, 19, 18, 148, 74, 0, 7, 0, 66, 111, 123, 43, 79, 3, 94, 15, 11, 116, 18, 45, 160, 70, 34, 0, 255, 198, 64, 108, 12, 1, 0, 172, 62, 0, 21, 0, 8, 0, 9, 25, 34, 8, 32, 51, 20, 204, 13, 0, 43, 0, 0, 128, 5, 44, 227, 15, 0, 0, 39, 65, 0, 195, 22, 114, 0, 10, 137, 10, 237, 135, 168, 0, 0, 84, 129, 130, 59, 0, 117, 25, 0, 69, 6, 130, 123, 0, 3, 58, 23, 47, 42, 171, 46, 207, 37, 67, 84, 41, 107, 44, 11, 0, 84, 4, 35, 85, 142, 250, 254, 0, 11, 0, 0, 145, 39, 57, 10, 37, 237, 132, 16, 155, 46, 2, 0, 255, 0, 107, 7, 41, 18, 83, 65, 0, 7, 0, 39, 100, 130, 62, 79, 3, 118, 21, 4, 137, 0, 54, 170, 0, 41, 0, 255, 234, 48, 82, 12, 24, 0, 143, 83, 0, 44, 0, 0, 0, 10, 0, 0, 16, 4, 151, 7, 198, 13, 0, 0, 6, 0, 113, 0, 78, 181, 53, 0, 0, 103, 134, 5, 195, 18, 123, 6, 0, 221, 16, 211, 66, 156, 0, 6, 68, 37, 107, 79, 0, 95, 10, 0, 69, 37, 151, 6, 0, 0, 16, 41, 56, 43, 180, 21, 190, 20, 9, 80, 78, 111, 10, 18, 0, 16, 4, 21, 74, 81, 223, 255, 30, 0, 0, 0, 139, 64, 93, 21, 26, 209, 134, 12, 183, 27, 79, 0, 232, 7, 15, 30, 0, 7, 97, 19, 26, 7, 0, 56, 92, 161, 59, 79, 3, 173, 9, 4, 199, 0, 12, 126, 0, 21, 0, 244, 181, 34, 73, 12, 1, 0, 122, 48, 0, 10, 13, 79, 0, 1, 72, 0, 23, 1, 73, 16, 164, 13, 0, 12, 0, 0, 96, 0, 85, 253, 1, 0, 0, 40, 90, 10, 199, 0, 183, 0, 0, 134, 31, 237, 103, 107, 0, 0, 89, 105, 100, 22, 0, 91, 31, 16, 69, 27, 169, 24, 0, 0, 81, 30, 75, 0, 168, 84, 207, 23, 17, 80, 56, 87, 57, 19, 0, 10, 4, 49, 97, 104, 255, 255, 2, 88, 0, 11, 153, 0, 93, 4, 27, 244, 137, 9, 152, 34, 0, 28, 244, 13, 122, 0, 22, 2, 147, 84, 4, 7, 0, 57, 72, 136, 0, 58, 3, 190, 19, 4, 103, 0, 7, 106, 34, 23, 0, 236, 103, 0, 124, 12, 1, 0, 89, 24, 137, 10, 0, 50, 0, 19, 1, 71, 4, 6, 60, 30, 97, 13, 0, 0, 0, 0, 62, 0, 73, 184, 178, 0, 0, 75, 68, 0, 199, 9, 88, 6, 30, 116, 57, 237, 148, 123, 0, 0, 87, 105, 114, 4, 0, 90, 91, 0, 69, 62, 161, 11, 0, 18, 52, 46, 18, 46, 187, 21, 207, 63, 14, 82, 52, 147, 44, 23, 0, 37, 112, 79, 68, 140, 255, 228, 51, 93, 0, 0, 169, 35, 93, 0, 26, 226, 107, 87, 105, 41, 0, 0, 255, 20, 56, 14, 0, 2, 119, 64, 0, 7, 0, 50, 81, 127, 36, 35, 9, 152, 0, 4, 152, 0, 7, 236, 0, 60, 44, 222, 162, 38, 87, 12, 58, 0, 151, 52, 0, 30, 0, 0, 0, 31, 37, 69, 4, 45, 104, 26, 183, 13, 0, 20, 0, 80, 74, 0, 96, 211, 25, 0, 0, 89, 80, 0, 174, 0, 80, 5, 0, 98, 17, 237, 193, 40, 0, 0, 73, 91, 110, 4, 0, 63, 45, 0, 40, 18, 144, 13, 0, 53, 28, 0, 48, 68, 180, 34, 207, 2, 0, 85, 73, 175, 77, 11, 0, 10, 9, 70, 48, 88, 243, 255, 12, 45, 66, 0, 169, 117, 93, 1, 24, 201, 85, 11, 143, 27, 59, 0, 208, 0, 122, 123, 0, 2, 75, 35, 66, 7, 0, 23, 117, 137, 0, 0, 3, 119, 17, 4, 37, 73, 68, 231, 87, 38, 0, 177, 176, 74, 128, 12, 108, 0, 31, 35, 44, 78, 0, 8, 0, 1, 23, 84, 17, 25, 31, 2, 113, 13, 0, 80, 0, 139, 53, 65, 50, 218, 21, 0, 0, 28, 95, 0, 155, 83, 62, 14, 70, 102, 30, 237, 171, 155, 0, 0, 71, 61, 114, 29, 0, 138, 42, 0, 55, 33, 167, 32, 2, 21, 40, 46, 65, 24, 165, 39, 176, 60, 47, 52, 69, 90, 0, 24, 0, 13, 4, 144, 71, 103, 255, 255, 22, 0, 9, 0, 169, 4, 90, 0, 20, 215, 94, 7, 173, 27, 95, 0, 234, 0, 6, 80, 0, 18, 72, 17, 0, 7, 0, 73, 84, 156, 0, 79, 7, 111, 7, 11, 211, 0, 67, 177, 0, 80, 94, 251, 255, 42, 131, 12, 109, 0, 112, 114, 70, 196, 0, 96, 0, 102, 77, 0, 64, 36, 70, 56, 168, 13, 0, 0, 0, 35, 78, 0, 96, 255, 49, 0, 0, 36, 133, 0, 165, 0, 122, 0, 0, 170, 0, 214, 84, 132, 0, 0, 53, 67, 102, 58, 0, 118, 34, 0, 69, 14, 142, 59, 0, 3, 39, 28, 45, 52, 184, 21, 207, 39, 94, 90, 68, 118, 41, 11, 8, 5, 4, 90, 44, 86, 253, 255, 2, 0, 40, 201, 167, 9, 65, 16, 5, 247, 121, 5, 173, 31, 86, 0, 254, 0, 49, 37, 34, 5, 96, 22, 4, 7, 0, 73, 76, 79, 0, 72, 3, 170, 0, 13, 75, 0, 12, 223, 0, 47, 0, 255, 224, 0, 126, 12, 18, 0, 96, 105, 0, 20, 0, 2, 0, 1, 0, 0, 20, 34, 54, 12, 209, 13, 0, 17, 49, 6, 160, 0, 106, 180, 82, 0, 0, 68, 103, 0, 161, 65, 152, 0, 0, 123, 3, 237, 127, 199, 0, 0, 81, 59, 117, 37, 0, 96, 17, 0, 65, 9, 152, 21, 0, 0, 86, 41, 47, 0, 181, 27, 196, 76, 9, 71, 59, 151, 0, 37, 0, 84, 4, 112, 50, 156, 255, 179, 17, 23, 0, 0, 155, 73, 51, 0, 5, 198, 85, 30, 171, 32, 65, 0, 255, 36, 83, 61, 0, 14, 105, 52, 0, 7, 0, 79, 90, 176, 0, 71, 3, 142, 0, 4, 210, 2, 16, 233, 0, 47, 0, 255, 175, 7, 123, 12, 72, 0, 136, 38, 0, 43, 0, 42, 0, 14, 0, 0, 10, 8, 63, 2, 185, 13, 0, 0, 0, 0, 90, 0, 29, 255, 7, 0, 0, 71, 72, 0, 163, 47, 168, 20, 0, 174, 23, 237, 87, 104, 0, 0, 91, 78, 113, 31, 0, 88, 56, 0, 60, 93, 155, 17, 0, 20, 84, 24, 50, 0, 151, 64, 180, 5, 21, 72, 21, 149, 7, 25, 0, 66, 28, 20, 72, 155, 245, 229, 1, 29, 0, 0, 142, 0, 93, 4, 8, 255, 84, 30, 109, 27, 0, 0, 251, 9, 143, 0, 0, 31, 128, 68, 64, 7, 0, 93, 102, 162, 0, 71, 3, 91, 0, 8, 67, 27, 21, 150, 0, 33, 0, 255, 175, 61, 88, 12, 1, 0, 98, 28, 0, 33, 0, 54, 0, 40, 25, 41, 38, 6, 112, 9, 176, 13, 0, 0, 12, 40, 99, 0, 96, 190, 46, 0, 0, 147, 127, 37, 170, 10, 116, 23, 60, 80, 18, 237, 116, 164, 0, 0, 94, 149, 106, 4, 0, 61, 9, 21, 69, 25, 148, 50, 30, 18, 30, 34, 34, 0, 168, 61, 188, 14, 57, 50, 0, 114, 9, 11, 28, 81, 6, 39, 83, 142, 213, 255, 14, 0, 0, 0, 151, 0, 93, 0, 35, 229, 74, 56, 105, 49, 0, 52, 248, 5, 61, 26, 54, 48, 129, 45, 0, 7, 0, 87, 74, 137, 0, 79, 3, 173, 7, 4, 151, 57, 19, 255, 0, 36, 0, 255, 195, 59, 125, 12, 1, 0, 233, 60, 0, 7, 0, 2, 0, 12, 0, 57, 74, 4, 71, 5, 176, 13, 0, 0, 0, 53, 96, 0, 92, 186, 120, 0, 0, 72, 126, 0, 199, 13, 153, 14, 16, 160, 32, 237, 89, 74, 0, 9, 94, 106, 104, 4, 0, 78, 22, 0, 56, 61, 171, 53, 0, 52, 102, 0, 70, 0, 181, 139, 205, 16, 51, 100, 64, 169, 67, 11, 0, 5, 8, 19, 90, 161, 221, 196, 19, 0, 21, 0, 153, 3, 27, 2, 18, 249, 117, 65, 164, 27, 0, 0, 201, 14, 67, 17, 0, 10, 181, 46, 5, 7, 0, 28, 102, 141, 0, 28, 3, 122, 0, 4, 168, 62, 18, 195, 0, 21, 0, 233, 175, 62, 125, 12, 56, 0, 112, 55, 57, 32, 0, 0, 0, 7, 2, 6, 78, 4, 178, 30, 149, 13, 0, 35, 31, 0, 72, 0, 66, 195, 155, 0, 0, 71, 52, 18, 199, 6, 52, 17, 35, 139, 56, 237, 73, 109, 0, 0, 94, 92, 99, 15, 0, 88, 5, 0, 69, 67, 181, 105, 0, 81, 85, 0, 72, 12, 178, 21, 207, 19, 69, 24, 78, 133, 5, 11, 0, 33, 4, 61, 59, 159, 255, 208, 8, 71, 0, 14, 143, 0, 88, 0, 20, 255, 121, 71, 114, 37, 34, 5, 250, 0, 195, 10, 41, 11, 137, 87, 0, 7, 0, 132, 114, 165, 0, 79, 3, 95, 3, 4, 45, 2, 42, 145, 0, 28, 0, 255, 130, 37, 134, 12, 11, 0, 48, 73, 10, 21, 0, 0, 0, 10, 44, 0, 12, 2, 95, 9, 122, 13, 0, 11, 0, 0, 59, 3, 104, 173, 147, 0, 0, 65, 40, 28, 183, 66, 62, 0, 17, 116, 84, 237, 87, 201, 0, 0, 83, 110, 146, 68, 0, 103, 13, 0, 69, 4, 172, 65, 27, 7, 26, 9, 49, 27, 167, 21, 203, 17, 62, 88, 78, 159, 0, 11, 0, 23, 4, 85, 47, 141, 255, 255, 8, 0, 7, 0, 150, 10, 86, 13, 32, 255, 126, 14, 136, 27, 60, 0, 255, 80, 53, 51, 0, 6, 121, 31, 4, 7, 0, 56, 97, 167, 13, 64, 3, 149, 4, 4, 232, 0, 14, 161, 0, 59, 0, 251, 181, 74, 127, 12, 1, 0, 61, 119, 0, 25, 0, 26, 0, 36, 59, 22, 70, 9, 70, 20, 163, 13, 0, 0, 0, 32, 97, 0, 133, 206, 57, 0, 0, 50, 117, 0, 165, 0, 114, 8, 36, 88, 15, 237, 143, 153, 0, 0, 43, 115, 108, 34, 0, 112, 18, 0, 69, 0, 178, 41, 0, 0, 4, 24, 8, 0, 161, 21, 202, 42, 114, 109, 34, 94, 79, 11, 0, 29, 4, 128, 113, 124, 255, 212, 0, 6, 29, 148, 169, 29, 82, 46, 5, 255, 109, 2, 183, 27, 0, 20, 255, 20, 43, 68, 1, 10, 137, 56, 12, 7, 0, 84, 87, 114, 0, 79, 3, 114, 15, 4, 103, 0, 12, 218, 0, 34, 15, 255, 183, 4, 122, 12, 86, 0, 171, 48, 0, 7, 0, 11, 0, 30, 0, 0, 20, 9, 52, 35, 171, 13, 0, 13, 8, 0, 33, 0, 73, 198, 183, 0, 0, 45, 55, 0, 199, 72, 148, 16, 0, 171, 36, 237, 122, 137, 0, 0, 94, 137, 90, 62, 0, 86, 16, 21, 60, 0, 181, 48, 0, 0, 28, 45, 42, 54, 165, 21, 207, 3, 19, 83, 50, 134, 57, 18, 0, 53, 4, 40, 55, 41, 255, 182, 0, 0, 0, 0, 99, 0, 73, 37, 7, 255, 96, 7, 173, 27, 0, 0, 255, 16, 72, 35, 0, 23, 95, 27, 0, 7, 0, 39, 111, 109, 0, 55, 3, 132, 24, 4, 87, 0, 14, 223, 0, 45, 2, 215, 186, 43, 128, 12, 116, 0, 157, 36, 0, 7, 5, 0, 0, 36, 0, 53, 39, 86, 38, 49, 184, 13, 0, 0, 0, 53, 81, 0, 61, 244, 43, 0, 0, 38, 42, 0, 146, 17, 156, 41, 18, 140, 26, 237, 89, 88, 0, 0, 94, 125, 106, 31, 0, 104, 22, 0, 45, 56, 136, 133, 1, 53, 63, 0, 102, 37, 180, 21, 199, 25, 128, 63, 68, 79, 63, 11, 0, 13, 12, 30, 47, 123, 255, 255, 6, 0, 0, 0, 169, 0, 53, 67, 7, 246, 127, 40, 161, 37, 0, 0, 246, 38, 60, 46, 12, 10, 143, 57, 30, 7, 0, 66, 77, 148, 0, 40, 3, 157, 4, 15, 96, 14, 7, 156, 0, 42, 0, 255, 211, 65, 88, 12, 1, 0, 125, 69, 0, 45, 0, 0, 0, 31, 0, 61, 22, 5, 84, 2, 155, 13, 0, 87, 0, 0, 68, 0, 81, 195, 94, 0, 0, 99, 113, 0, 199, 37, 148, 0, 0, 57, 45, 229, 111, 149, 0, 0, 94, 83, 79, 26, 0, 119, 30, 0, 50, 12, 109, 29, 0, 0, 5, 0, 34, 68, 184, 21, 196, 33, 45, 79, 56, 59, 0, 16, 0, 18, 4, 76, 102, 93, 255, 254, 12, 0, 0, 0, 151, 58, 82, 0, 8, 217, 67, 21, 154, 27, 8, 37, 255, 4, 93, 82, 0, 64, 91, 38, 0, 7, 0, 64, 74, 166, 26, 68, 3, 140, 64, 25, 183, 0, 7, 215, 0, 23, 0, 255, 178, 59, 114, 12, 49, 0, 92, 61, 0, 25, 0, 15, 0, 60, 67, 46, 8, 7, 30, 2, 189, 13, 0, 0, 0, 0, 133, 0, 74, 133, 69, 0, 0, 28, 94, 0, 193, 109, 143, 6, 0, 145, 9, 186, 116, 140, 0, 0, 94, 105, 128, 41, 0, 87, 36, 24, 56, 0, 169, 37, 0, 0, 42, 25, 22, 0, 156, 27, 134, 16, 66, 77, 71, 109, 0, 27, 0, 43, 4, 42, 123, 175, 226, 202, 57, 30, 0, 71, 155, 0, 89, 0, 16, 197, 89, 21, 169, 27, 0, 0, 235, 8, 127, 0, 0, 33, 131, 71, 42, 7, 0, 37, 92, 170, 77, 79, 3, 108, 25, 17, 54, 78, 15, 237, 0, 38, 0, 255, 195, 62, 114, 12, 18, 0, 79, 38, 0, 74, 0, 0, 0, 34, 0, 92, 90, 3, 45, 83, 200, 13, 0, 0, 0, 0, 109, 0, 56, 255, 194, 0, 0, 28, 134, 22, 142, 0, 97, 17, 49, 150, 16, 237, 119, 170, 0, 0, 43, 136, 85, 22, 0, 79, 21, 0, 58, 21, 179, 21, 0, 0, 22, 72, 21, 0, 164, 36, 192, 26, 61, 105, 47, 106, 0, 11, 0, 70, 4, 48, 84, 135, 255, 249, 50, 0, 0, 0, 161, 0, 58, 0, 11, 241, 125, 52, 183, 27, 0, 0, 255, 69, 27, 55, 0, 3, 62, 17, 12, 7, 0, 27, 86, 173, 0, 79, 3, 97, 9, 4, 64, 67, 25, 187, 0, 40, 0, 255, 230, 36, 132, 12, 1, 0, 131, 59, 45, 14, 0, 34, 0, 5, 0, 0, 24, 1, 43, 30, 212, 13, 0, 49, 0, 3, 86, 0, 128, 136, 181, 0, 0, 41, 85, 0, 158, 20, 131, 0, 0, 128, 0, 237, 87, 96, 0, 0, 87, 102, 101, 11, 0, 115, 18, 0, 0, 28, 174, 122, 0, 11, 51, 0, 49, 0, 179, 21, 163, 49, 25, 75, 15, 97, 22, 11, 40, 46, 4, 59, 136, 96, 250, 255, 16, 0, 0, 14, 159, 23, 45, 0, 11, 255, 141, 47, 176, 27, 36, 0, 247, 36, 38, 56, 7, 3, 107, 71, 0, 7, 0, 28, 82, 154, 0, 79, 3, 225, 19, 4, 163, 4, 12, 147, 1, 59, 0, 255, 234, 0, 119, 12, 9, 0, 115, 80, 45, 4, 0, 0, 0, 8, 29, 0, 68, 31, 56, 44, 134, 13, 0, 0, 0, 9, 7, 0, 120, 134, 136, 0, 0, 59, 93, 4, 199, 38, 113, 5, 0, 141, 51, 237, 113, 126, 0, 0, 78, 84, 92, 13, 0, 111, 43, 17, 0, 17, 161, 39, 0, 3, 83, 0, 32, 23, 154, 84, 189, 52, 101, 116, 48, 113, 11, 11, 43, 13, 4, 71, 47, 143, 242, 193, 6, 0, 0, 0, 169, 0, 91, 0, 5, 221, 112, 37, 100, 27, 65, 0, 255, 24, 92, 24, 13, 37, 176, 106, 0, 7, 0, 33, 97, 147, 0, 79, 3, 215, 31, 8, 212, 37, 17, 255, 0, 46, 0, 255, 205, 24, 94, 12, 12, 0, 63, 44, 0, 65, 0, 21, 0, 7, 56, 89, 62, 9, 48, 2, 135, 13, 0, 1, 0, 85, 12, 0, 64, 142, 66, 0, 0, 40, 123, 11, 142, 54, 137, 28, 57, 91, 30, 237, 81, 66, 0, 0, 88, 211, 114, 10, 0, 98, 32, 0, 59, 26, 157, 44, 0, 33, 108, 0, 31, 0, 166, 24, 174, 28, 120, 47, 35, 99, 92, 24, 0, 5, 4, 13, 34, 170, 211, 214, 83, 0, 47, 9, 128, 0, 89, 0, 47, 227, 106, 46, 20, 27, 0, 0, 255, 6, 169, 0, 10, 57, 160, 71, 54, 7, 0, 35, 83, 168, 0, 79, 3, 113, 19, 33, 22, 0, 18, 200, 0, 27, 0, 255, 175, 1, 75, 12, 18, 0, 37, 69, 6, 41, 0, 18, 21, 20, 0, 68, 89, 1, 76, 31, 150, 13, 0, 40, 0, 0, 70, 0, 69, 143, 115, 0, 0, 90, 65, 71, 199, 45, 33, 3, 18, 147, 5, 237, 89, 171, 0, 0, 75, 130, 109, 14, 0, 87, 13, 0, 42, 0, 171, 74, 0, 55, 42, 0, 30, 0, 171, 25, 181, 16, 50, 26, 78, 108, 33, 26, 130, 25, 4, 84, 131, 163, 248, 255, 0, 0, 0, 0, 113, 115, 83, 9, 19, 251, 109, 81, 163, 27, 59, 0, 241, 4, 120, 44, 0, 2, 72, 57, 35, 7, 0, 80, 103, 166, 0, 79, 3, 67, 23, 4, 37, 11, 24, 172, 0, 35, 0, 245, 173, 0, 100, 12, 45, 0, 93, 110, 88, 46, 0, 0, 0, 13, 0, 0, 78, 11, 100, 4, 163, 13, 0, 0, 26, 0, 18, 39, 105, 237, 68, 0, 0, 39, 90, 0, 154, 32, 148, 4, 40, 115, 36, 237, 136, 136, 0, 0, 72, 96, 96, 20, 0, 72, 15, 18, 69, 10, 163, 101, 0, 3, 69, 0, 48, 0, 148, 21, 199, 35, 21, 47, 78, 142, 84, 11, 36, 28, 4, 54, 43, 142, 255, 199, 12, 0, 0, 0, 148, 0, 93, 59, 77, 249, 131, 34, 171, 27, 61, 0, 255, 21, 21, 48, 0, 2, 125, 56, 21, 7, 0, 89, 94, 168, 0, 76, 3, 180, 32, 4, 172, 0, 7, 210, 0, 21, 3, 246, 194, 0, 109, 12, 60, 0, 175, 59, 110, 80, 0, 44, 0, 50, 0, 0, 18, 5, 35, 60, 128, 13, 0, 100, 0, 56, 0, 0, 47, 222, 71, 0, 0, 81, 134, 0, 133, 33, 132, 0, 0, 112, 27, 237, 99, 76, 0, 8, 76, 88, 92, 21, 0, 90, 16, 0, 62, 3, 177, 146, 0, 0, 27, 0, 97, 55, 119, 21, 199, 35, 17, 61, 36, 123, 0, 11, 0, 10, 4, 12, 76, 60, 255, 191, 18, 0, 0, 88, 149, 0, 93, 7, 58, 198, 102, 81, 164, 27, 32, 0, 228, 0, 155, 62, 0, 5, 180, 45, 25, 7, 0, 93, 108, 116, 21, 75, 3, 175, 1, 21, 129, 10, 13, 136, 99, 40, 0, 200, 180, 44, 121, 12, 7, 0, 200, 99, 0, 22, 0, 0, 0, 17, 0, 30, 31, 49, 44, 2, 183, 13, 0, 88, 0, 92, 50, 0, 59, 223, 60, 0, 0, 28, 41, 0, 199, 4, 110, 25, 42, 173, 54, 237, 91, 154, 0, 0, 80, 72, 81, 4, 0, 119, 31, 0, 51, 5, 165, 34, 0, 30, 60, 36, 44, 68, 181, 46, 207, 52, 74, 119, 26, 172, 53, 11, 0, 9, 4, 82, 167, 134, 255, 255, 14, 0, 29, 0, 137, 44, 93, 0, 5, 169, 129, 61, 86, 27, 0, 0, 255, 0, 97, 30, 0, 2, 124, 38, 34, 7, 0, 36, 117, 86, 27, 58, 3, 148, 11, 12, 62, 29, 31, 130, 0, 37, 0, 255, 157, 74, 128, 12, 37, 0, 146, 103, 1, 47, 0, 51, 0, 1, 25, 0, 23, 19, 27, 2, 204, 13, 0, 0, 0, 0, 130, 0, 76, 185, 80, 0, 0, 28, 81, 0, 178, 9, 189, 0, 0, 111, 30, 237, 104, 179, 0, 0, 79, 68, 99, 27, 0, 82, 54, 0, 69, 10, 163, 43, 0, 55, 29, 35, 37, 0, 168, 54, 207, 3, 35, 50, 65, 149, 11, 11, 0, 57, 4, 74, 103, 122, 255, 255, 8, 0, 11, 0, 116, 5, 93, 0, 29, 238, 119, 45, 68, 27, 80, 0, 255, 62, 16, 88, 0, 11, 70, 34, 26, 7, 0, 55, 108, 155, 32, 56, 3, 169, 10, 8, 144, 0, 55, 148, 0, 57, 0, 255, 142, 47, 140, 12, 12, 0, 32, 29, 0, 22, 0, 30, 0, 1, 99, 56, 9, 37, 34, 20, 161, 13, 0, 0, 0, 0, 78, 0, 107, 165, 19, 0, 0, 50, 128, 0, 199, 35, 75, 23, 0, 114, 22, 237, 94, 113, 0, 0, 74, 124, 102, 9, 0, 80, 26, 0, 69, 12, 152, 121, 0, 0, 42, 0, 56, 0, 173, 21, 203, 20, 40, 50, 70, 98, 47, 11, 48, 93, 4, 37, 74, 140, 255, 196, 6, 0, 0, 0, 121, 36, 77, 0, 18, 254, 96, 14, 176, 27, 7, 31, 255, 50, 39, 69, 0, 7, 108, 54, 0, 7, 0, 76, 93, 94, 0, 53, 3, 73, 15, 4, 95, 85, 12, 140, 16, 50, 0, 255, 186, 58, 104, 12, 1, 0, 131, 75, 18, 16, 0, 0, 0, 9, 0, 66, 14, 34, 71, 2, 195, 13, 0, 0, 0, 0, 114, 0, 138, 231, 53, 0, 0, 54, 100, 0, 196, 42, 135, 11, 22, 139, 42, 237, 85, 104, 0, 0, 85, 132, 108, 4, 0, 138, 56, 0, 69, 2, 173, 170, 0, 10, 82, 0, 54, 0, 168, 21, 193, 14, 33, 62, 71, 108, 46, 11, 20, 37, 4, 39, 110, 178, 255, 238, 0, 0, 0, 0, 145, 12, 51, 51, 5, 255, 125, 2, 158, 27, 0, 0, 255, 36, 47, 26, 0, 2, 89, 42, 0, 7, 0, 96, 75, 176, 0, 64, 3, 111, 16, 4, 192, 34, 14, 149, 0, 69, 4, 232, 213, 54, 123, 12, 1, 0, 162, 16, 0, 57, 0, 0, 0, 37, 0, 24, 4, 9, 99, 2, 142, 13, 0, 0, 0, 0, 77, 53, 105, 255, 93, 0, 0, 75, 108, 60, 193, 0, 138, 10, 11, 85, 49, 237, 79, 65, 0, 0, 45, 113, 103, 15, 0, 100, 63, 0, 69, 0, 183, 40, 0, 0, 45, 2, 27, 0, 163, 21, 165, 25, 51, 10, 30, 83, 84, 21, 0, 38, 9, 15, 74, 140, 255, 191, 47, 14, 18, 218, 128, 0, 93, 2, 15, 255, 141, 2, 183, 27, 0, 37, 233, 32, 16, 1, 38, 2, 165, 40, 0, 7, 0, 67, 77, 139, 0, 79, 3, 116, 9, 8, 49, 0, 9, 164, 0, 30, 25, 197, 157, 57, 130, 12, 1, 0, 193, 28, 0, 39, 0, 25, 0, 33, 0, 12, 4, 16, 53, 25, 155, 13, 0, 86, 0, 0, 67, 3, 100, 255, 189, 0, 0, 109, 96, 12, 193, 0, 113, 4, 0, 121, 15, 237, 114, 183, 0, 0, 50, 84, 96, 47, 0, 135, 66, 0, 50, 13, 129, 74, 0, 5, 53, 55, 48, 0, 160, 52, 207, 74, 78, 84, 20, 112, 0, 29, 0, 55, 4, 61, 100, 119, 234, 255, 4, 0, 0, 0, 136, 0, 93, 0, 38, 232, 128, 6, 183, 37, 6, 0, 255, 0, 85, 9, 34, 34, 151, 79, 0, 7, 0, 61, 117, 119, 0, 79, 3, 171, 30, 15, 222, 0, 15, 163, 0, 45, 0, 255, 201, 23, 99, 12, 9, 0, 136, 35, 0, 30, 0, 0, 0, 43, 47, 0, 4, 11, 97, 43, 158, 13, 0, 54, 0, 11, 62, 3, 95, 119, 93, 0, 0, 80, 105, 0, 199, 55, 124, 0, 0, 169, 26, 220, 86, 123, 0, 0, 54, 126, 84, 31, 0, 138, 11, 0, 54, 34, 164, 71, 0, 38, 45, 47, 45, 0, 184, 21, 207, 15, 11, 103, 39, 116, 27, 17, 0, 62, 4, 12, 94, 106, 255, 255, 1, 112, 33, 0, 120, 26, 91, 0, 12, 226, 125, 31, 165, 27, 51, 0, 255, 0, 61, 29, 0, 26, 114, 50, 0, 7, 0, 49, 107, 130, 0, 71, 3, 208, 11, 4, 169, 0, 19, 158, 0, 36, 0, 220, 175, 24, 78, 12, 12, 0, 83, 77, 0, 27, 0, 0, 0, 20, 0, 34, 4, 29, 78, 14, 151, 13, 0, 10, 0, 0, 75, 32, 98, 255, 13, 0, 0, 32, 129, 0, 185, 0, 113, 25, 0, 97, 21, 237, 183, 67, 12, 0, 43, 148, 98, 17, 0, 87, 13, 21, 69, 38, 154, 6, 0, 30, 12, 86, 0, 0, 164, 67, 203, 13, 8, 62, 43, 136, 51, 18, 0, 34, 12, 99, 42, 99, 248, 255, 0, 80, 0, 6, 111, 177, 89, 0, 7, 172, 56, 26, 158, 27, 28, 0, 218, 19, 34, 117, 0, 37, 76, 17, 0, 7, 0, 31, 95, 138, 5, 0, 3, 92, 74, 4, 16, 0, 56, 235, 60, 56, 0, 204, 193, 16, 130, 12, 162, 0, 117, 34, 61, 137, 0, 52, 0, 10, 0, 41, 23, 19, 28, 56, 157, 57, 0, 36, 9, 121, 66, 0, 92, 192, 33, 13, 0, 28, 22, 28, 125, 83, 179, 9, 73, 140, 0, 212, 218, 150, 0, 0, 83, 92, 106, 8, 0, 96, 14, 0, 64, 71, 176, 60, 27, 31, 60, 0, 34, 50, 146, 40, 160, 18, 88, 129, 67, 86, 45, 14, 44, 5, 4, 104, 52, 108, 255, 255, 16, 0, 0, 0, 123, 11, 93, 0, 33, 255, 138, 10, 64, 27, 28, 0, 255, 0, 64, 29, 0, 15, 110, 27, 0, 7, 0, 47, 81, 163, 16, 71, 3, 110, 34, 19, 192, 12, 95, 198, 63, 38, 15, 255, 255, 74, 111, 12, 71, 0, 176, 90, 0, 251, 0, 54, 0, 54, 0, 0, 57, 30, 69, 22, 146, 13, 0, 0, 0, 0, 74, 0, 46, 170, 45, 0, 0, 32, 105, 0, 187, 10, 144, 0, 31, 84, 23, 237, 136, 153, 0, 0, 85, 139, 64, 78, 0, 125, 25, 0, 31, 49, 180, 129, 51, 25, 26, 0, 46, 29, 187, 21, 200, 104, 73, 82, 64, 172, 0, 29, 9, 72, 4, 90, 78, 131, 255, 255, 3, 0, 0, 0, 142, 0, 93, 0, 21, 255, 88, 19, 183, 31, 0, 0, 255, 0, 165, 13, 0, 16, 104, 17, 0, 7, 0, 39, 105, 124, 45, 79, 3, 159, 0, 4, 112, 0, 39, 255, 0, 41, 0, 255, 255, 74, 132, 12, 48, 0, 140, 100, 0, 11, 0, 0, 0, 43, 0, 3, 8, 41, 84, 14, 192, 13, 0, 87, 0, 53, 99, 59, 87, 134, 139, 0, 0, 58, 94, 0, 191, 35, 134, 20, 0, 144, 21, 237, 126, 107, 0, 0, 80, 165, 145, 4, 0, 112, 22, 0, 64, 19, 149, 159, 28, 58, 63, 0, 58, 68, 178, 65, 182, 25, 111, 67, 43, 95, 32, 56, 0, 21, 64, 34, 68, 166, 233, 190, 15, 68, 23, 0, 115, 27, 93, 0, 37, 215, 68, 22, 183, 34, 0, 0, 255, 0, 207, 0, 64, 14, 173, 57, 0, 7, 0, 16, 111, 163, 81, 79, 33, 84, 0, 13, 52, 0, 36, 148, 0, 24, 0, 205, 188, 54, 110, 12, 1, 0, 187, 32, 0, 39, 0, 0, 0, 83, 0, 5, 10, 8, 120, 12, 220, 13, 0, 0, 39, 0, 92, 75, 63, 166, 255, 0, 0, 132, 81, 0, 0, 8, 147, 31, 65, 133, 30, 237, 105, 168, 0, 0, 91, 49, 102, 39, 0, 55, 27, 0, 59, 8, 116, 32, 0, 0, 2, 30, 43, 68, 181, 126, 198, 55, 50, 98, 56, 106, 6, 11, 0, 51, 4, 66, 66, 142, 255, 213, 12, 0, 16, 29, 157, 0, 67, 0, 5, 179, 123, 21, 155, 27, 29, 21, 211, 32, 69, 110, 0, 12, 126, 36, 52, 7, 0, 49, 98, 177, 86, 53, 3, 131, 0, 10, 119, 0, 31, 188, 0, 43, 24, 255, 128, 64, 140, 12, 1, 0, 56, 34, 0, 55, 0, 12, 0, 9, 64, 50, 54, 5, 44, 11, 204, 13, 0, 0, 0, 0, 89, 0, 53, 137, 93, 0, 0, 28, 75, 0, 102, 67, 119, 0, 0, 156, 30, 237, 104, 117, 0, 8, 77, 83, 105, 16, 0, 98, 19, 0, 69, 33, 173, 112, 21, 34, 19, 0, 82, 30, 173, 21, 191, 68, 37, 116, 13, 73, 0, 11, 37, 10, 7, 31, 43, 179, 246, 233, 5, 0, 28, 0, 169, 32, 93, 0, 5, 242, 108, 91, 140, 57, 79, 16, 227, 0, 58, 17, 34, 12, 98, 77, 18, 7, 0, 52, 97, 156, 55, 79, 3, 185, 0, 13, 255, 0, 7, 174, 0, 61, 0, 255, 211, 62, 140, 12, 1, 0, 58, 80, 0, 44, 0, 0, 0, 17, 3, 22, 97, 1, 108, 5, 128, 13, 0, 0, 24, 0, 33, 0, 91, 167, 75, 0, 0, 82, 93, 61, 199, 39, 102, 14, 34, 109, 55, 237, 105, 89, 0, 0, 94, 87, 121, 4, 0, 78, 32, 0, 34, 5, 181, 39, 37, 17, 33, 0, 78, 29, 149, 94, 161, 74, 78, 78, 35, 54, 6, 11, 56, 5, 11, 24, 57, 163, 251, 195, 65, 0, 73, 0, 88, 22, 93, 0, 23, 255, 131, 78, 171, 27, 62, 12, 221, 0, 47, 15, 0, 1, 134, 29, 12, 7, 0, 31, 61, 162, 0, 79, 3, 143, 0, 42, 222, 0, 7, 177, 0, 29, 0, 231, 152, 45, 140, 12, 14, 0, 110, 37, 42, 64, 0, 30, 0, 22, 44, 3, 47, 5, 52, 27, 162, 13, 0, 45, 0, 0, 52, 42, 67, 150, 19, 0, 0, 62, 37, 0, 199, 43, 38, 9, 0, 107, 48, 237, 147, 131, 0, 0, 94, 111, 81, 15, 0, 105, 17, 0, 69, 0, 179, 63, 0, 0, 35, 0, 76, 29, 177, 21, 207, 59, 53, 79, 78, 161, 0, 11, 15, 10, 4, 67, 112, 136, 255, 255, 29, 0, 72, 136, 135, 0, 93, 0, 40, 221, 93, 37, 164, 27, 4, 0, 255, 0, 236, 82, 24, 49, 115, 17, 0, 7, 0, 52, 111, 117, 0, 76, 3, 135, 3, 4, 35, 72, 21, 154, 0, 30, 0, 255, 223, 18, 111, 12, 20, 0, 123, 99, 8, 32, 0, 5, 0, 19, 0, 0, 39, 24, 56, 9, 192, 13, 0, 76, 0, 15, 91, 25, 136, 201, 130, 0, 0, 28, 107, 0, 188, 71, 117, 3, 0, 127, 6, 237, 130, 154, 0, 0, 84, 91, 97, 35, 0, 105, 12, 0, 64, 2, 134, 80, 0, 0, 105, 0, 75, 0, 136, 23, 180, 47, 35, 28, 78, 164, 0, 18, 0, 41, 4, 62, 104, 73, 255, 178, 0, 75, 56, 0, 163, 66, 62, 0, 56, 217, 99, 18, 165, 27, 52, 0, 234, 43, 85, 42, 0, 2, 86, 38, 0, 7, 0, 74, 95, 189, 0, 51, 3, 122, 44, 4, 229, 0, 17, 200, 0, 68, 50, 252, 192, 40, 104, 12, 1, 0, 63, 59, 0, 26, 0, 0, 0, 50, 57, 28, 22, 33, 36, 2, 210, 13, 0, 4, 0, 11, 90, 0, 83, 190, 51, 0, 0, 45, 79, 0, 199, 52, 134, 27, 0, 123, 13, 237, 129, 68, 0, 0, 94, 147, 100, 31, 0, 102, 29, 0, 59, 18, 129, 49, 0, 0, 52, 0, 39, 0, 119, 26, 173, 42, 60, 42, 78, 138, 74, 11, 0, 35, 4, 15, 64, 117, 255, 255, 16, 2, 0, 0, 123, 0, 93, 18, 30, 235, 106, 46, 107, 27, 0, 0, 255, 0, 109, 6, 33, 5, 106, 42, 0, 7, 0, 43, 83, 163, 0, 60, 3, 166, 31, 4, 255, 47, 9, 198, 14, 63, 0, 255, 173, 31, 100, 12, 27, 0, 144, 65, 0, 24, 0, 0, 0, 26, 2, 88, 25, 14, 94, 49, 180, 13, 0, 0, 83, 51, 71, 0, 97, 198, 159, 0, 0, 113, 137, 0, 172, 46, 113, 14, 39, 104, 36, 237, 93, 155, 0, 2, 94, 116, 115, 4, 0, 60, 5, 0, 69, 4, 161, 40, 0, 54, 54, 51, 18, 68, 167, 21, 207, 82, 28, 95, 70, 121, 92, 21, 0, 32, 4, 96, 81, 151, 255, 148, 19, 0, 0, 38, 138, 0, 93, 0, 7, 229, 108, 102, 139, 27, 0, 2, 224, 0, 86, 33, 0, 3, 127, 49, 37, 7, 28, 122, 109, 148, 0, 79, 3, 101, 12, 4, 54, 88, 11, 204, 0, 44, 0, 245, 213, 33, 103, 12, 38, 0, 128, 117, 0, 46, 0, 7, 0, 43, 0, 0, 38, 31, 104, 22, 179, 13, 0, 60, 0, 41, 107, 0, 84, 183, 100, 0, 0, 53, 107, 0, 161, 58, 138, 25, 0, 189, 19, 237, 93, 205, 0, 0, 83, 92, 97, 10, 0, 85, 5, 0, 69, 0, 154, 53, 0, 0, 2, 5, 21, 0, 184, 21, 207, 5, 54, 85, 78, 67, 32, 11, 0, 30, 4, 62, 78, 123, 255, 230, 8, 0, 0, 0, 109, 14, 86, 68, 17, 246, 134, 16, 162, 27, 24, 17, 255, 31, 39, 49, 7, 17, 120, 43, 81, 7, 0, 67, 72, 140, 0, 65, 3, 74, 6, 4, 95, 2, 7, 207, 0, 21, 0, 255, 189, 74, 123, 12, 1, 0, 147, 61, 43, 53, 12, 41, 0, 21, 54, 34, 16, 38, 79, 2, 207, 13, 0, 47, 0, 0, 120, 44, 32, 158, 88, 0, 0, 44, 39, 0, 161, 66, 192, 0, 0, 140, 38, 224, 100, 183, 0, 0, 94, 119, 107, 42, 0, 71, 33, 19, 62, 62, 160, 90, 0, 21, 57, 0, 58, 4, 168, 21, 207, 15, 5, 76, 78, 97, 0, 17, 0, 5, 64, 43, 46, 155, 244, 213, 37, 58, 0, 0, 150, 72, 79, 0, 8, 212, 122, 22, 135, 27, 0, 56, 255, 0, 109, 50, 0, 32, 107, 78, 35, 7, 0, 69, 107, 161, 35, 61, 3, 174, 15, 4, 255, 0, 11, 205, 0, 21, 0, 224, 227, 74, 110, 12, 1, 0, 127, 95, 0, 8, 0, 0, 0, 50, 51, 0, 29, 14, 146, 2, 157, 13, 0, 0, 26, 0, 69, 0, 97, 176, 178, 0, 0, 90, 165, 0, 132, 0, 83, 52, 15, 134, 25, 237, 90, 107, 9, 0, 63, 134, 92, 26, 0, 113, 11, 0, 55, 0, 177, 45, 0, 10, 27, 44, 39, 27, 130, 31, 199, 35, 123, 81, 47, 106, 50, 22, 0, 25, 4, 55, 86, 150, 255, 214, 18, 29, 12, 0, 144, 0, 83, 0, 49, 204, 103, 27, 141, 27, 0, 0, 255, 0, 157, 13, 0, 10, 184, 65, 16, 7, 0, 74, 88, 150, 0, 64, 3, 121, 38, 22, 16, 0, 32, 231, 30, 35, 149, 222, 174, 71, 64, 12, 24, 0, 139, 41, 0, 33, 0, 31, 0, 60, 0, 0, 9, 15, 31, 2, 214, 13, 0, 0, 64, 24, 87, 55, 114, 255, 209, 0, 0, 28, 97, 0, 153, 8, 152, 6, 69, 131, 7, 237, 102, 111, 0, 50, 94, 119, 100, 46, 0, 94, 28, 0, 55, 25, 162, 46, 21, 0, 44, 0, 41, 0, 166, 38, 200, 88, 32, 97, 62, 174, 34, 11, 0, 107, 4, 31, 79, 167, 233, 183, 14, 0, 0, 130, 157, 26, 55, 0, 10, 214, 106, 84, 133, 27, 0, 0, 255, 54, 51, 0, 0, 2, 100, 22, 0, 7, 2, 17, 94, 153, 27, 69, 3, 171, 0, 4, 73, 0, 7, 253, 0, 42, 42, 249, 186, 40, 75, 12, 1, 0, 149, 51, 0, 33, 0, 37, 0, 1, 0, 78, 16, 18, 60, 2, 178, 13, 0, 68, 0, 0, 85, 36, 71, 255, 137, 0, 0, 54, 113, 14, 187, 3, 129, 5, 0, 114, 3, 237, 86, 135, 0, 0, 64, 81, 95, 13, 0, 70, 9, 0, 0, 30, 146, 29, 0, 55, 30, 29, 52, 10, 164, 52, 186, 7, 16, 35, 5, 81, 31, 11, 0, 42, 4, 84, 114, 145, 211, 255, 5, 26, 34, 0, 154, 85, 82, 0, 45, 179, 140, 72, 126, 27, 56, 0, 228, 0, 43, 2, 62, 15, 142, 24, 0, 7, 0, 94, 93, 170, 0, 79, 3, 255, 20, 13, 68, 0, 33, 133, 0, 41, 40, 255, 212, 0, 131, 12, 59, 0, 22, 46, 11, 11, 0, 67, 0, 31, 42, 9, 83, 6, 83, 16, 78, 13, 0, 0, 69, 0, 74, 27, 110, 151, 26, 0, 0, 54, 225, 36, 166, 33, 143, 10, 0, 93, 0, 237, 135, 149, 0, 0, 85, 95, 157, 14, 0, 79, 86, 0, 60, 63, 176, 21, 0, 41, 70, 35, 18, 0, 183, 21, 176, 103, 63, 45, 42, 140, 13, 25, 0, 16, 4, 56, 54, 201, 235, 200, 67, 0, 0, 0, 158, 95, 90, 0, 37, 213, 124, 48, 95, 27, 0, 0, 255, 10, 35, 0, 56, 13, 127, 28, 5, 7, 0, 48, 101, 93, 0, 65, 3, 142, 20, 10, 173, 0, 25, 200, 0, 33, 0, 255, 193, 0, 130, 12, 16, 0, 124, 100, 74, 54, 0, 78, 0, 34, 4, 25, 22, 1, 58, 25, 176, 13, 0, 0, 0, 0, 66, 0, 81, 206, 80, 0, 0, 53, 108, 24, 179, 16, 161, 0, 53, 94, 3, 237, 117, 118, 0, 0, 89, 97, 114, 4, 0, 89, 9, 0, 63, 22, 186, 35, 0, 0, 117, 10, 41, 68, 177, 21, 169, 33, 27, 32, 78, 137, 0, 16, 0, 15, 4, 46, 68, 127, 255, 240, 36, 0, 58, 79, 163, 31, 88, 0, 43, 254, 117, 2, 167, 27, 39, 40, 215, 44, 112, 11, 0, 24, 100, 64, 53, 7, 0, 140, 93, 178, 151, 36, 3, 123, 42, 4, 116, 0, 15, 237, 0, 38, 92, 252, 199, 0, 79, 12, 24, 0, 101, 19, 0, 22, 0, 10, 0, 11, 0, 5, 4, 40, 83, 18, 131, 13, 0, 0, 0, 0, 57, 0, 107, 255, 75, 0, 0, 103, 114, 41, 199, 0, 112, 5, 42, 179, 17, 237, 145, 92, 0, 40, 88, 159, 81, 16, 0, 98, 13, 15, 51, 2, 176, 102, 38, 0, 115, 0, 44, 0, 162, 31, 172, 62, 41, 48, 15, 113, 77, 30, 38, 48, 4, 26, 28, 84, 239, 182, 8, 0, 0, 69, 20, 0, 93, 27, 32, 213, 92, 2, 149, 27, 0, 19, 255, 0, 159, 0, 0, 21, 132, 93, 0, 7, 0, 26, 103, 128, 0, 48, 3, 112, 1, 4, 52, 43, 12, 182, 0, 34, 0, 255, 140, 74, 119, 12, 1, 0, 148, 16, 0, 27, 57, 0, 0, 22, 0, 34, 4, 58, 65, 22, 163, 13, 0, 62, 0, 66, 83, 0, 58, 206, 151, 0, 0, 87, 88, 22, 199, 0, 94, 46, 0, 126, 28, 237, 74, 115, 0, 18, 88, 108, 141, 10, 0, 84, 43, 0, 30, 18, 141, 93, 19, 36, 87, 0, 75, 55, 158, 36, 207, 60, 103, 79, 0, 140, 89, 32, 18, 103, 4, 49, 28, 135, 230, 210, 2, 0, 0, 127, 152, 64, 93, 38, 11, 215, 125, 32, 164, 27, 9, 0, 231, 0, 98, 1, 0, 9, 147, 39, 8, 7, 0, 25, 102, 154, 0, 0, 3, 155, 34, 8, 102, 0, 19, 199, 2, 28, 10, 255, 203, 74, 127, 12, 1, 0, 115, 125, 0, 30, 0, 0, 0, 1, 0, 16, 46, 10, 42, 5, 164, 13, 0, 44, 0, 12, 59, 0, 59, 214, 98, 0, 0, 102, 69, 0, 195, 30, 127, 0, 12, 117, 36, 237, 80, 173, 0, 0, 33, 31, 89, 66, 0, 135, 23, 0, 69, 0, 148, 75, 0, 34, 5, 0, 53, 49, 87, 44, 201, 64, 34, 118, 43, 118, 0, 11, 0, 47, 4, 106, 133, 39, 234, 255, 1, 0, 0, 0, 96, 21, 72, 0, 96, 247, 126, 98, 165, 27, 73, 0, 221, 24, 41, 69, 0, 41, 87, 29, 36, 7, 0, 24, 98, 130, 0, 73, 3, 164, 118, 27, 173, 0, 21, 191, 0, 52, 0, 244, 218, 65, 131, 12, 1, 0, 64, 112, 0, 29, 0, 23, 0, 6, 123, 0, 36, 98, 36, 115, 188, 13, 0, 75, 0, 0, 67, 0, 124, 132, 1, 0, 0, 32, 92, 0, 195, 19, 176, 4, 29, 49, 18, 237, 73, 174, 0, 0, 94, 143, 148, 44, 0, 111, 49, 29, 69, 25, 160, 17, 67, 0, 134, 21, 14, 54, 161, 115, 195, 73, 13, 89, 64, 169, 0, 23, 0, 5, 4, 61, 45, 74, 223, 181, 10, 152, 0, 0, 90, 1, 84, 0, 16, 244, 134, 23, 139, 42, 0, 13, 255, 78, 44, 0, 0, 16, 92, 108, 0, 7, 0, 29, 109, 189, 0, 79, 3, 217, 58, 7, 100, 0, 12, 224, 69, 32, 120, 255, 144, 74, 121, 12, 87, 0, 189, 29, 0, 37, 0, 68, 0, 7, 0, 0, 23, 34, 61, 32, 156, 13, 0, 0, 83, 14, 76, 0, 0, 255, 109, 0, 0, 35, 115, 0, 199, 16, 98, 16, 58, 175, 8, 237, 97, 74, 0, 0, 94, 128, 112, 51, 0, 88, 5, 0, 69, 57, 152, 67, 15, 39, 87, 0, 56, 0, 148, 21, 202, 31, 112, 40, 71, 108, 24, 32, 0, 5, 4, 30, 86, 122, 177, 240, 13, 0, 39, 0, 133, 0, 76, 13, 47, 206, 136, 32, 44, 46, 0, 0, 251, 0, 151, 15, 0, 15, 215, 80, 57, 7, 0, 38, 90, 160, 0, 74, 3, 205, 10, 8, 167, 14, 13, 139, 0, 41, 0, 255, 203, 74, 140, 12, 1, 0, 73, 65, 0, 17, 15, 12, 0, 5, 0, 36, 16, 32, 96, 2, 143, 13, 0, 89, 0, 0, 117, 40, 71, 161, 213, 0, 0, 119, 156, 25, 199, 43, 115, 53, 0, 137, 10, 237, 92, 183, 0, 0, 94, 80, 139, 18, 0, 95, 5, 0, 0, 16, 148, 84, 32, 18, 30, 0, 38, 0, 160, 21, 203, 49, 99, 120, 64, 96, 0, 18, 0, 21, 4, 44, 81, 127, 255, 226, 17, 101, 0, 64, 155, 87, 70, 0, 30, 255, 142, 106, 127, 27, 31, 0, 191, 62, 39, 39, 0, 9, 94, 55, 0, 7, 0, 32, 83, 147, 80, 38, 3, 127, 14, 4, 97, 0, 41, 216, 13, 31, 89, 255, 218, 70, 80, 12, 1, 0, 93, 51, 26, 37, 0, 19, 0, 11, 14, 45, 14, 8, 48, 6, 143, 13, 0, 0, 0, 0, 84, 42, 87, 166, 42, 0, 0, 39, 53, 0, 175, 47, 191, 1, 57, 130, 30, 237, 65, 145, 0, 0, 80, 122, 94, 4, 0, 87, 5, 6, 40, 3, 178, 123, 0, 0, 60, 0, 65, 68, 155, 21, 202, 21, 38, 63, 78, 78, 0, 11, 0, 10, 4, 31, 101, 104, 255, 252, 13, 0, 12, 0, 150, 0, 78, 0, 5, 255, 113, 2, 183, 27, 62, 46, 242, 53, 46, 5, 0, 2, 149, 47, 0, 7, 0, 40, 74, 144, 19, 71, 3, 160, 25, 4, 255, 0, 7, 181, 1, 26, 15, 240, 153, 0, 113, 12, 1, 0, 104, 20, 0, 32, 0, 23, 0, 4, 0, 37, 19, 2, 46, 18, 121, 13, 0, 0, 0, 0, 73, 62, 77, 164, 54, 0, 0, 61, 75, 0, 199, 38, 75, 16, 0, 82, 44, 237, 66, 96, 0, 0, 46, 157, 117, 11, 0, 103, 91, 0, 69, 9, 186, 37, 0, 0, 64, 58, 19, 0, 112, 21, 171, 27, 113, 50, 78, 103, 31, 17, 0, 22, 12, 39, 93, 137, 255, 222, 85, 48, 0, 155, 154, 0, 93, 0, 5, 255, 84, 2, 172, 27, 0, 7, 255, 42, 6, 0, 12, 2, 154, 41, 0, 12, 0, 42, 82, 144, 38, 79, 3, 110, 0, 12, 23, 0, 11, 175, 0, 27, 0, 255, 209, 3, 100, 12, 1, 0, 255, 20, 0, 14, 0, 70, 0, 42, 0, 0, 10, 6, 47, 14, 179, 13, 0, 75, 0, 27, 97, 26, 108, 247, 142, 0, 0, 81, 103, 0, 192, 0, 167, 0, 17, 134, 5, 237, 67, 63, 0, 0, 73, 84, 124, 61, 0, 88, 36, 0, 55, 23, 175, 97, 0, 68, 36, 9, 49, 0, 187, 21, 207, 56, 65, 111, 64, 151, 46, 41, 0, 66, 4, 14, 50, 181, 255, 247, 55, 0, 10, 0, 169, 10, 72, 0, 5, 233, 103, 63, 87, 27, 66, 0, 251, 0, 37, 15, 0, 2, 155, 17, 26, 7, 0, 34, 111, 136, 0, 69, 3, 68, 0, 23, 183, 0, 17, 162, 0, 68, 0, 255, 184, 74, 100, 12, 1, 0, 73, 132, 30, 24, 8, 0, 0, 28, 88, 81, 39, 4, 111, 2, 215, 13, 0, 0, 0, 34, 134, 0, 97, 188, 67, 0, 0, 93, 50, 0, 188, 0, 71, 0, 0, 158, 29, 237, 97, 159, 0, 0, 94, 94, 108, 42, 0, 113, 50, 0, 51, 19, 173, 29, 36, 11, 71, 21, 11, 0, 168, 33, 165, 50, 48, 74, 78, 169, 0, 25, 0, 59, 4, 93, 56, 152, 255, 220, 8, 36, 0, 76, 164, 20, 90, 0, 10, 244, 125, 22, 61, 27, 62, 0, 255, 0, 59, 14, 0, 40, 69, 20, 0, 7, 0, 26, 101, 189, 16, 79, 3, 155, 18, 18, 39, 0, 34, 255, 0, 23, 0, 255, 203, 74, 77, 12, 17, 0, 171, 55, 0, 31, 0, 48, 0, 1, 0, 0, 21, 29, 48, 11, 201, 13, 0, 47, 0, 0, 92, 26, 97, 255, 114, 0, 0, 34, 104, 0, 128, 17, 216, 3, 0, 109, 16, 237, 244, 123, 0, 0, 49, 85, 116, 30, 0, 42, 25, 19, 69, 53, 145, 40, 0, 25, 40, 0, 21, 68, 170, 71, 180, 7, 0, 12, 55, 190, 0, 35, 0, 5, 4, 40, 56, 90, 200, 255, 0, 15, 0, 0, 163, 83, 88, 46, 17, 215, 103, 26, 175, 27, 74, 1, 199, 22, 106, 106, 65, 2, 107, 17, 0, 7, 0, 30, 111, 189, 64, 52, 3, 194, 17, 4, 188, 10, 56, 255, 69, 26, 0, 162, 242, 74, 122, 12, 59, 0, 77, 26, 0, 165, 0, 52, 0, 78, 14, 69, 13, 20, 11, 2, 72, 17, 0, 71, 0, 185, 41, 75, 70, 193, 29, 0, 0, 28, 90, 55, 117, 73, 119, 11, 19, 173, 5, 198, 220, 100, 0, 0, 84, 113, 107, 17, 0, 83, 43, 0, 62, 35, 182, 55, 12, 23, 48, 0, 43, 41, 166, 25, 189, 44, 94, 14, 78, 104, 21, 16, 0, 10, 4, 57, 151, 97, 255, 240, 29, 0, 34, 28, 169, 5, 89, 0, 18, 244, 128, 2, 138, 27, 0, 0, 251, 4, 154, 32, 5, 12, 116, 50, 1, 7, 0, 64, 69, 162, 4, 38, 3, 105, 49, 8, 86, 0, 38, 199, 0, 73, 32, 255, 205, 0, 123, 12, 18, 0, 117, 43, 0, 255, 0, 0, 0, 72, 0, 32, 89, 43, 76, 38, 130, 13, 0, 12, 0, 0, 35, 0, 104, 216, 63, 0, 0, 28, 67, 0, 199, 40, 109, 0, 40, 167, 35, 237, 136, 75, 0, 0, 62, 98, 155, 19, 0, 79, 44, 0, 22, 51, 184, 105, 26, 23, 28, 0, 62, 16, 171, 31, 180, 44, 27, 117, 43, 83, 40, 17, 0, 119, 4, 40, 44, 136, 255, 231, 16, 22, 0, 0, 169, 3, 85, 0, 5, 234, 116, 47, 163, 27, 13, 2, 237, 0, 139, 14, 0, 23, 87, 26, 76, 7, 0, 42, 86, 108, 0, 33, 3, 130, 1, 13, 124, 0, 7, 241, 2, 58, 35, 255, 196, 74, 135, 12, 1, 0, 62, 53, 0, 24, 0, 0, 0, 34, 17, 65, 58, 5, 47, 8, 149, 13, 0, 19, 0, 2, 58, 0, 82, 161, 35, 0, 0, 49, 41, 0, 194, 12, 109, 17, 0, 120, 19, 237, 94, 167, 0, 0, 94, 63, 92, 24, 0, 100, 19, 0, 65, 31, 183, 48, 55, 29, 64, 12, 40, 55, 146, 37, 178, 61, 41, 52, 39, 133, 0, 23, 0, 47, 4, 63, 71, 137, 237, 244, 53, 22, 0, 0, 138, 31, 93, 0, 41, 225, 139, 75, 102, 41, 43, 0, 255, 0, 50, 32, 32, 2, 123, 17, 9, 7, 0, 36, 101, 164, 0, 79, 3, 115, 17, 5, 51, 49, 32, 146, 0, 27, 0, 255, 211, 41, 130, 12, 1, 0, 97, 62, 0, 29, 0, 11, 0, 43, 53, 14, 62, 6, 88, 24, 193, 13, 0, 0, 0, 0, 99, 0, 71, 220, 68, 0, 0, 68, 101, 0, 115, 31, 175, 0, 58, 129, 14, 237, 58, 124, 0, 0, 94, 88, 101, 37, 0, 104, 5, 0, 57, 15, 186, 108, 0, 0, 44, 34, 71, 68, 165, 42, 180, 41, 15, 102, 71, 213, 45, 32, 0, 70, 4, 78, 87, 142, 255, 250, 28, 0, 78, 73, 169, 0, 44, 0, 56, 245, 145, 20, 152, 27, 31, 0, 255, 0, 148, 71, 0, 14, 104, 35, 75, 7, 0, 29, 113, 178, 0, 38, 3, 110, 0, 8, 129, 0, 22, 190, 0, 67, 0, 255, 195, 74, 128, 12, 9, 0, 66, 66, 62, 53, 0, 0, 0, 6, 52, 6, 59, 10, 92, 21, 213, 13, 0, 0, 0, 23, 98, 0, 72, 201, 134, 0, 0, 28, 56, 0, 199, 27, 42, 0, 37, 148, 24, 237, 86, 162, 0, 0, 94, 87, 84, 16, 0, 135, 5, 0, 56, 15, 160, 10, 52, 0, 36, 93, 33, 55, 142, 21, 198, 17, 79, 114, 64, 85, 7, 11, 5, 127, 4, 121, 112, 98, 253, 253, 0, 0, 29, 89, 116, 0, 93, 0, 21, 252, 111, 14, 175, 27, 4, 0, 255, 27, 79, 21, 0, 20, 139, 59, 0, 7, 0, 56, 86, 140, 0, 79, 3, 135, 2, 4, 43, 52, 13, 232, 0, 35, 0, 255, 214, 74, 130, 12, 12, 0, 162, 47, 0, 41, 0, 75, 0, 1, 0, 0, 45, 35, 63, 53, 185, 13, 0, 61, 0, 29, 100, 0, 71, 186, 130, 0, 0, 67, 47, 3, 199, 27, 206, 5, 0, 34, 14, 220, 42, 144, 0, 0, 85, 78, 95, 7, 0, 52, 5, 0, 34, 16, 156, 6, 23, 0, 54, 63, 73, 54, 167, 26, 191, 49, 39, 78, 78, 103, 0, 11, 9, 12, 4, 16, 32, 165, 247, 255, 31, 0, 0, 0, 161, 66, 93, 0, 25, 188, 99, 66, 169, 27, 22, 44, 223, 0, 59, 12, 9, 36, 118, 33, 39, 7, 0, 70, 88, 144, 0, 76, 3, 135, 2, 23, 255, 41, 11, 104, 0, 47, 0, 255, 156, 71, 103, 12, 10, 0, 125, 118, 2, 28, 0, 52, 0, 46, 85, 2, 64, 1, 60, 2, 175, 13, 0, 0, 41, 0, 97, 0, 71, 186, 86, 0, 0, 52, 139, 11, 157, 21, 96, 8, 50, 86, 9, 237, 128, 93, 0, 0, 83, 113, 146, 14, 0, 94, 22, 0, 69, 28, 174, 29, 0, 18, 33, 12, 32, 21, 140, 29, 185, 98, 37, 88, 78, 104, 64, 11, 0, 32, 4, 65, 51, 151, 247, 255, 40, 0, 34, 57, 156, 18, 93, 0, 47, 255, 110, 26, 104, 27, 0, 0, 255, 0, 155, 17, 43, 31, 128, 17, 48, 7, 0, 103, 79, 82, 0, 70, 3, 100, 0, 4, 35, 0, 7, 199, 13, 43, 81, 255, 248, 66, 101, 12, 1, 0, 164, 83, 38, 23, 0, 77, 0, 77, 0, 0, 22, 12, 85, 2, 177, 13, 0, 2, 0, 0, 86, 20, 112, 178, 124, 0, 0, 37, 93, 4, 199, 64, 142, 11, 0, 115, 16, 237, 99, 255, 0, 42, 94, 64, 114, 10, 0, 99, 58, 0, 65, 26, 178, 6, 0, 0, 80, 10, 15, 68, 153, 24, 171, 64, 24, 83, 71, 114, 0, 21, 0, 54, 19, 118, 52, 141, 255, 147, 0, 137, 0, 0, 157, 0, 83, 61, 27, 255, 114, 69, 141, 34, 68, 0, 225, 83, 38, 43, 0, 28, 151, 75, 0, 7, 0, 78, 79, 165, 0, 59, 3, 40, 7, 13, 76, 10, 7, 132, 0, 42, 0, 210, 201, 53, 108, 12, 1, 0, 242, 36, 17, 46, 0, 45, 0, 28, 29, 0, 25, 44, 120, 20, 162, 13, 0, 0, 0, 8, 16, 0, 25, 210, 66, 0, 0, 65, 22, 0, 166, 48, 148, 17, 33, 149, 13, 237, 139, 136, 0, 56, 94, 128, 115, 24, 0, 60, 8, 5, 57, 58, 174, 145, 0, 17, 29, 0, 81, 42, 182, 67, 207, 6, 46, 82, 31, 107, 0, 11, 0, 27, 4, 50, 28, 141, 255, 255, 7, 18, 0, 48, 164, 9, 93, 74, 26, 255, 104, 111, 159, 27, 47, 1, 228, 0, 30, 74, 58, 21, 133, 32, 0, 7, 0, 76, 79, 124, 0, 61, 4, 112, 0, 5, 255, 19, 11, 149, 6, 46, 0, 255, 202, 36, 129, 12, 1, 0, 115, 83, 0, 37, 0, 0, 0, 1, 0, 23, 61, 26, 128, 30, 182, 13, 0, 0, 0, 0, 73, 34, 120, 108, 79, 0, 0, 28, 78, 7, 169, 81, 101, 0, 0, 137, 15, 237, 79, 109, 0, 0, 94, 96, 106, 4, 0, 106, 7, 0, 49, 6, 153, 181, 0, 12, 36, 2, 51, 68, 149, 27, 207, 15, 36, 55, 50, 156, 92, 18, 0, 48, 4, 23, 57, 150, 247, 226, 13, 18, 0, 36, 163, 32, 51, 71, 5, 235, 86, 2, 141, 27, 0, 0, 235, 52, 28, 41, 0, 2, 106, 110, 0, 7, 0, 59, 89, 172, 0, 48, 5, 112, 0, 49, 73, 0, 12, 144, 0, 54, 0, 255, 184, 0, 119, 12, 17, 0, 112, 23, 0, 49, 0, 0, 0, 70, 0, 52, 11, 6, 25, 47, 135, 13, 0, 65, 0, 0, 43, 0, 101, 214, 103, 0, 0, 58, 81, 42, 191, 72, 104, 0, 46, 106, 79, 237, 94, 127, 0, 0, 84, 107, 113, 19, 0, 70, 10, 0, 69, 14, 165, 133, 0, 26, 42, 0, 75, 4, 175, 21, 207, 34, 48, 24, 66, 75, 23, 39, 0, 52, 4, 38, 135, 146, 231, 235, 1, 34, 0, 27, 163, 18, 93, 47, 17, 255, 129, 22, 152, 27, 6, 18, 223, 0, 103, 27, 63, 9, 112, 49, 37, 7, 0, 33, 76, 90, 46, 79, 3, 102, 6, 25, 64, 0, 7, 137, 0, 29, 0, 255, 176, 45, 133, 12, 10, 0, 26, 26, 0, 53, 0, 0, 0, 21, 9, 83, 21, 9, 136, 2, 158, 13, 0, 0, 13, 0, 93, 42, 4, 236, 113, 0, 0, 80, 75, 0, 192, 28, 102, 9, 22, 169, 60, 237, 118, 98, 0, 0, 68, 122, 93, 51, 0, 71, 64, 0, 69, 0, 173, 21, 0, 23, 48, 0, 48, 13, 184, 50, 205, 2, 11, 0, 57, 129, 18, 27, 37, 63, 4, 47, 63, 102, 204, 255, 2, 0, 0, 158, 152, 0, 93, 49, 87, 250, 84, 23, 173, 27, 0, 0, 220, 0, 108, 0, 0, 16, 123, 41, 41, 7, 0, 95, 97, 172, 0, 79, 3, 195, 12, 7, 60, 42, 14, 199, 53, 49, 0, 235, 187, 32, 96, 12, 11, 0, 148, 61, 12, 19, 0, 56, 0, 10, 0, 0, 17, 20, 149, 2, 150, 13, 0, 1, 0, 0, 44, 16, 54, 255, 205, 0, 0, 28, 178, 0, 148, 0, 92, 12, 0, 140, 24, 237, 67, 143, 0, 0, 89, 104, 148, 68, 0, 68, 89, 0, 69, 0, 173, 45, 0, 0, 90, 0, 40, 0, 187, 21, 189, 31, 60, 61, 78, 131, 33, 34, 47, 13, 4, 76, 69, 98, 246, 227, 77, 0, 60, 0, 144, 0, 85, 0, 43, 238, 121, 2, 183, 27, 0, 0, 242, 0, 99, 0, 0, 7, 110, 56, 14, 7, 0, 97, 115, 189, 0, 54, 3, 160, 7, 11, 133, 70, 32, 215, 26, 45, 0, 185, 166, 32, 96, 12, 1, 0, 121, 28, 16, 115, 0, 33, 0, 55, 0, 0, 26, 28, 11, 2, 186, 13, 0, 32, 0, 0, 88, 11, 52, 255, 93, 0, 0, 34, 110, 0, 177, 0, 69, 0, 54, 118, 51, 237, 69, 144, 0, 0, 18, 79, 111, 29, 0, 78, 7, 0, 69, 0, 173, 30, 0, 52, 15, 40, 28, 55, 150, 21, 178, 18, 9, 35, 66, 167, 38, 11, 7, 15, 4, 78, 132, 140, 226, 255, 12, 0, 0, 0, 155, 0, 81, 0, 29, 220, 111, 2, 56, 27, 31, 49, 215, 3, 75, 81, 0, 2, 165, 32, 57, 7, 0, 158, 110, 177, 29, 73, 3, 160, 7, 8, 111, 0, 24, 211, 7, 24, 90, 212, 212, 46, 132, 12, 1, 0, 119, 61, 0, 16, 0, 60, 0, 75, 0, 0, 34, 20, 28, 10, 158, 13, 0, 0, 0, 19, 61, 0, 125, 220, 37, 0, 0, 36, 148, 0, 175, 4, 90, 0, 73, 158, 28, 237, 125, 148, 0, 0, 73, 134, 100, 37, 0, 112, 78, 0, 55, 2, 103, 83, 0, 0, 31, 0, 67, 0, 155, 64, 110, 0, 17, 0, 78, 111, 83, 11, 0, 28, 4, 114, 102, 104, 196, 227, 2, 6, 67, 0, 126, 0, 80, 61, 33, 235, 110, 5, 62, 27, 25, 39, 255, 25, 156, 0, 0, 7, 91, 44, 7, 7, 0, 58, 104, 170, 83, 71, 3, 161, 31, 32, 68, 21, 21, 160, 0, 37, 0, 255, 188, 0, 114, 12, 77, 0, 62, 54, 0, 17, 14, 23, 0, 36, 30, 0, 4, 42, 90, 38, 123, 13, 0, 93, 0, 0, 48, 0, 96, 163, 102, 0, 0, 39, 134, 0, 151, 37, 90, 6, 0, 152, 5, 237, 133, 111, 0, 12, 60, 165, 91, 20, 0, 83, 95, 0, 29, 4, 151, 95, 0, 7, 45, 0, 37, 0, 161, 64, 203, 38, 52, 62, 78, 84, 86, 11, 101, 61, 4, 100, 78, 63, 255, 255, 23, 0, 57, 63, 155, 83, 76, 0, 40, 251, 116, 10, 122, 27, 0, 56, 255, 0, 168, 0, 23, 8, 162, 50, 16, 7, 0, 107, 99, 157, 0, 79, 3, 152, 13, 4, 52, 43, 34, 255, 74, 49, 0, 254, 206, 0, 115, 12, 73, 0, 89, 94, 0, 11, 0, 30, 0, 63, 0, 5, 34, 32, 51, 25, 195, 13, 0, 0, 59, 0, 78, 23, 92, 166, 132, 0, 0, 34, 58, 0, 175, 59, 71, 1, 38, 86, 16, 237, 109, 125, 0, 0, 89, 126, 107, 67, 0, 105, 80, 0, 39, 5, 175, 110, 0, 19, 110, 0, 34, 0, 171, 21, 201, 49, 34, 30, 54, 105, 92, 11, 54, 39, 4, 77, 118, 147, 255, 200, 12, 0, 32, 89, 164, 0, 72, 0, 13, 222, 116, 7, 167, 27, 1, 41, 255, 0, 162, 0, 0, 41, 107, 76, 49, 7, 0, 70, 100, 168, 0, 69, 3, 122, 34, 4, 71, 90, 7, 200, 0, 28, 0, 255, 155, 19, 116, 12, 60, 0, 190, 50, 0, 58, 59, 0, 0, 21, 0, 0, 16, 8, 56, 19, 152, 13, 0, 74, 0, 98, 48, 0, 17, 173, 102, 0, 0, 43, 71, 0, 199, 23, 79, 14, 17, 55, 62, 237, 74, 130, 0, 0, 89, 78, 129, 31, 0, 70, 10, 0, 20, 10, 165, 90, 33, 52, 78, 60, 55, 55, 169, 21, 207, 59, 92, 113, 54, 114, 64, 11, 0, 61, 4, 54, 71, 150, 255, 171, 15, 0, 0, 169, 147, 0, 87, 0, 14, 255, 145, 98, 130, 27, 60, 0, 217, 0, 44, 18, 28, 2, 166, 53, 0, 7, 0, 60, 104, 117, 59, 61, 3, 93, 10, 4, 109, 0, 41, 201, 0, 47, 43, 255, 198, 53, 131, 12, 5, 0, 133, 161, 0, 36, 0, 0, 0, 55, 0, 30, 69, 13, 100, 21, 176, 13, 0, 97, 0, 29, 117, 57, 63, 100, 70, 0, 0, 55, 46, 0, 199, 55, 153, 0, 0, 150, 32, 237, 114, 193, 0, 0, 94, 114, 105, 41, 0, 68, 37, 0, 69, 21, 157, 55, 0, 34, 24, 42, 49, 54, 171, 21, 196, 58, 15, 66, 59, 103, 0, 11, 38, 105, 4, 119, 72, 115, 255, 222, 0, 62, 0, 115, 103, 0, 80, 0, 30, 231, 115, 16, 127, 27, 33, 0, 224, 62, 6, 40, 0, 44, 70, 41, 28, 7, 0, 25, 87, 140, 0, 24, 3, 110, 2, 10, 153, 18, 34, 242, 0, 42, 0, 255, 182, 53, 130, 12, 6, 0, 156, 114, 0, 34, 0, 0, 0, 4, 0, 0, 75, 24, 113, 10, 189, 13, 0, 0, 49, 0, 94, 0, 69, 183, 36, 0, 0, 111, 78, 0, 196, 0, 157, 17, 30, 104, 28, 237, 72, 181, 0, 22, 94, 111, 108, 50, 0, 64, 66, 7, 69, 26, 183, 28, 0, 0, 89, 4, 59, 68, 180, 67, 202, 87, 70, 52, 32, 96, 0, 11, 0, 5, 4, 47, 79, 116, 251, 131, 74, 0, 0, 0, 127, 0, 93, 0, 14, 255, 107, 9, 147, 27, 52, 5, 205, 68, 55, 0, 0, 3, 164, 60, 0, 7, 0, 23, 104, 162, 0, 45, 3, 125, 0, 10, 203, 0, 28, 255, 0, 44, 84, 219, 194, 74, 128, 12, 1, 0, 62, 29, 0, 34, 0, 26, 0, 6, 6, 87, 32, 25, 64, 2, 154, 13, 0, 20, 0, 1, 8, 0, 51, 173, 43, 0, 0, 96, 38, 0, 193, 19, 80, 12, 41, 160, 26, 237, 101, 118, 0, 0, 94, 75, 79, 4, 0, 65, 22, 8, 69, 0, 184, 6, 0, 0, 9, 18, 56, 68, 177, 21, 190, 53, 51, 124, 71, 92, 0, 21, 22, 12, 4, 54, 71, 148, 255, 224, 1, 0, 7, 0, 130, 0, 93, 0, 31, 227, 93, 35, 112, 27, 84, 42, 211, 22, 96, 58, 0, 2, 134, 57, 47, 7, 0, 39, 77, 166, 0, 48, 3, 146, 0, 40, 121, 125, 20, 137, 7, 34, 0, 245, 99, 43, 129, 12, 18, 0, 50, 33, 16, 11, 0, 23, 9, 12, 25, 59, 22, 14, 57, 2, 138, 13, 0, 0, 0, 3, 67, 0, 50, 202, 55, 0, 0, 40, 95, 0, 199, 7, 105, 0, 0, 124, 63, 237, 45, 186, 11, 0, 94, 102, 141, 15, 0, 115, 5, 0, 69, 2, 180, 53, 0, 14, 50, 0, 19, 0, 169, 21, 176, 5, 63, 71, 71, 96, 52, 24, 0, 13, 4, 53, 84, 130, 236, 255, 8, 0, 31, 6, 129, 0, 93, 16, 5, 255, 100, 15, 183, 27, 38, 25, 253, 31, 97, 24, 0, 41, 125, 55, 0, 7, 0, 62, 84, 130, 31, 71, 3, 131, 27, 37, 16, 0, 11, 182, 0, 24, 8, 255, 170, 0, 140, 12, 1, 0, 136, 81, 0, 28, 0, 20, 0, 1, 0, 0, 37, 54, 69, 21, 134, 13, 0, 94, 0, 0, 110, 0, 100, 171, 64, 0, 0, 59, 90, 0, 199, 63, 101, 8, 0, 107, 41, 237, 89, 171, 0, 5, 94, 93, 50, 41, 0, 99, 5, 0, 53, 3, 177, 131, 25, 0, 27, 0, 35, 34, 139, 21, 202, 97, 33, 66, 64, 176, 92, 21, 0, 18, 4, 98, 100, 84, 255, 161, 20, 58, 0, 210, 164, 26, 84, 0, 5, 255, 93, 33, 123, 27, 44, 17, 214, 11, 59, 76, 0, 30, 116, 56, 0, 7, 0, 26, 110, 109, 0, 74, 3, 132, 17, 12, 43, 89, 20, 244, 30, 47, 0, 255, 156, 15, 94, 12, 71, 0, 104, 57, 0, 11, 0, 0, 0, 3, 19, 0, 20, 40, 57, 34, 194, 13, 0, 0, 81, 0, 63, 0, 98, 214, 99, 0, 0, 37, 58, 0, 162, 64, 84, 1, 131, 139, 13, 237, 99, 180, 0, 13, 94, 79, 162, 22, 0, 89, 5, 0, 35, 56, 122, 94, 0, 3, 21, 0, 68, 68, 176, 57, 173, 12, 40, 26, 78, 77, 0, 20, 117, 32, 4, 79, 62, 111, 226, 241, 67, 0, 0, 0, 110, 0, 76, 60, 5, 255, 111, 9, 173, 31, 96, 0, 235, 20, 29, 8, 7, 14, 161, 75, 0, 7, 7, 84, 89, 130, 0, 79, 3, 165, 0, 22, 175, 52, 11, 131, 16, 54, 0, 208, 176, 0, 91, 12, 27, 0, 120, 78, 0, 31, 149, 12, 0, 6, 61, 7, 18, 51, 114, 24, 137, 13, 0, 14, 0, 49, 60, 55, 91, 204, 19, 0, 0, 118, 100, 30, 184, 30, 127, 0, 0, 133, 28, 237, 127, 121, 23, 0, 60, 132, 77, 78, 0, 105, 21, 0, 69, 36, 175, 127, 0, 35, 42, 0, 62, 2, 187, 49, 207, 50, 66, 78, 31, 185, 45, 27, 0, 27, 4, 76, 94, 143, 255, 219, 29, 0, 11, 8, 169, 0, 75, 7, 5, 255, 107, 2, 170, 32, 17, 0, 255, 0, 145, 14, 14, 19, 163, 40, 0, 18, 0, 42, 117, 110, 0, 57, 3, 186, 6, 4, 88, 20, 27, 141, 0, 39, 0, 239, 199, 11, 115, 12, 1, 0, 158, 94, 0, 25, 0, 22, 0, 21, 0, 0, 21, 11, 46, 2, 150, 13, 0, 107, 0, 98, 95, 0, 118, 208, 118, 0, 0, 48, 118, 0, 199, 28, 93, 0, 35, 71, 15, 237, 28, 100, 0, 7, 89, 134, 111, 58, 0, 106, 70, 0, 57, 0, 183, 103, 17, 13, 32, 0, 57, 68, 161, 21, 199, 56, 21, 125, 29, 134, 0, 11, 0, 33, 4, 69, 28, 127, 249, 250, 4, 65, 30, 0, 169, 25, 59, 0, 16, 255, 123, 17, 137, 27, 68, 0, 231, 30, 30, 22, 0, 27, 87, 56, 10, 7, 0, 29, 102, 107, 0, 53, 3, 192, 16, 19, 211, 0, 28, 153, 0, 66, 19, 255, 220, 48, 138, 12, 1, 0, 77, 161, 19, 36, 0, 0, 0, 1, 41, 0, 83, 31, 63, 29, 152, 13, 0, 0, 0, 0, 89, 0, 106, 193, 88, 0, 0, 28, 49, 0, 199, 11, 96, 3, 0, 104, 55, 237, 52, 120, 0, 13, 94, 117, 84, 4, 0, 110, 5, 0, 59, 0, 186, 17, 17, 5, 27, 8, 32, 68, 181, 34, 192, 34, 9, 83, 43, 167, 0, 11, 0, 23, 4, 72, 38, 116, 255, 225, 5, 113, 72, 70, 169, 48, 93, 0, 5, 255, 83, 20, 145, 27, 6, 0, 244, 0, 40, 109, 0, 6, 82, 41, 0, 7, 8, 31, 95, 156, 0, 79, 3, 179, 0, 23, 175, 15, 7, 200, 0, 21, 0, 255, 182, 24, 136, 12, 1, 0, 103, 37, 0, 24, 0, 25, 0, 3, 2, 0, 14, 20, 65, 26, 136, 13, 0, 22, 0, 0, 56, 10, 122, 167, 162, 0, 0, 46, 61, 37, 156, 53, 86, 15, 14, 92, 22, 237, 210, 118, 17, 0, 83, 107, 127, 4, 0, 71, 65, 57, 0, 0, 186, 51, 0, 0, 0, 0, 30, 68, 153, 52, 185, 45, 0, 53, 43, 198, 0, 52, 0, 10, 4, 69, 47, 113, 255, 224, 0, 0, 19, 21, 169, 73, 93, 86, 8, 241, 90, 17, 145, 27, 18, 0, 199, 0, 170, 86, 66, 7, 99, 17, 0, 7, 0, 14, 117, 189, 0, 33, 3, 64, 27, 15, 56, 12, 55, 217, 40, 42, 0, 178, 212, 59, 130, 12, 195, 0, 94, 16, 0, 145, 0, 11, 0, 25, 0, 27, 16, 8, 18, 22, 105, 41, 0, 80, 0, 164, 44, 61, 72, 242, 63, 0, 0, 55, 5, 4, 164, 57, 101, 4, 74, 118, 34, 229, 217, 122, 0, 0, 94, 137, 99, 22, 0, 78, 51, 3, 56, 5, 181, 24, 27, 0, 42, 0, 62, 32, 184, 27, 188, 26, 86, 71, 46, 87, 10, 17, 0, 36, 4, 59, 46, 81, 255, 255, 40, 14, 0, 150, 160, 97, 93, 0, 5, 246, 126, 21, 145, 27, 0, 9, 255, 0, 44, 61, 0, 16, 65, 29, 0, 7, 0, 46, 23, 181, 0, 79, 3, 71, 58, 12, 61, 17, 36, 209, 37, 31, 0, 255, 224, 74, 135, 12, 43, 0, 106, 22, 0, 246, 0, 42, 0, 37, 0, 75, 23, 39, 28, 20, 200, 13, 0, 13, 0, 4, 89, 0, 27, 220, 111, 0, 0, 40, 64, 0, 199, 7, 137, 3, 57, 73, 9, 213, 91, 164, 0, 0, 63, 66, 75, 4, 0, 90, 30, 0, 49, 10, 183, 53, 39, 0, 62, 40, 29, 55, 187, 29, 179, 19, 4, 51, 46, 150, 16, 21, 41, 105, 4, 30, 104, 117, 255, 226, 8, 0, 0, 93, 154, 0, 93, 0, 69, 215, 26, 12, 183, 49, 36, 28, 239, 21, 64, 32, 6, 2, 171, 38, 48, 7, 0, 96, 107, 154, 72, 79, 12, 88, 13, 4, 46, 0, 18, 210, 3, 23, 0, 255, 152, 57, 120, 12, 1, 0, 114, 16, 0, 51, 0, 0, 0, 29, 60, 44, 14, 7, 26, 39, 208, 13, 0, 0, 0, 2, 105, 47, 50, 185, 56, 0, 0, 55, 67, 0, 199, 19, 110, 41, 13, 133, 26, 237, 66, 143, 0, 9, 94, 82, 137, 30, 0, 90, 26, 0, 6, 12, 182, 64, 15, 0, 35, 29, 49, 63, 175, 94, 180, 43, 63, 109, 78, 159, 0, 11, 65, 62, 4, 68, 59, 164, 252, 255, 16, 0, 0, 0, 149, 0, 93, 0, 23, 227, 109, 24, 95, 56, 31, 20, 213, 43, 73, 37, 0, 2, 136, 40, 0, 7, 0, 52, 103, 157, 47, 79, 15, 145, 4, 11, 83, 1, 40, 111, 0, 47, 0, 237, 136, 40, 115, 12, 5, 0, 112, 103, 0, 53, 0, 0, 0, 12, 0, 0, 76, 16, 32, 12, 175, 13, 0, 1, 0, 0, 81, 0, 132, 163, 83, 0, 0, 46, 83, 40, 199, 32, 124, 0, 0, 109, 14, 237, 89, 180, 0, 0, 94, 90, 110, 17, 0, 95, 45, 0, 69, 49, 183, 18, 47, 0, 87, 55, 47, 68, 174, 50, 195, 64, 63, 96, 75, 106, 0, 11, 0, 13, 4, 71, 49, 193, 255, 158, 23, 44, 27, 0, 138, 0, 93, 0, 5, 255, 139, 33, 154, 27, 24, 42, 247, 25, 143, 0, 0, 26, 132, 93, 17, 7, 0, 33, 88, 161, 18, 79, 3, 94, 0, 8, 124, 0, 7, 244, 0, 34, 41, 255, 196, 72, 95, 12, 6, 0, 47, 30, 0, 47, 0, 39, 0, 10, 33, 36, 51, 1, 95, 12, 170, 13, 0, 18, 0, 0, 92, 0, 46, 226, 70, 0, 0, 116, 65, 8, 199, 3, 86, 0, 0, 142, 35, 237, 106, 140, 0, 0, 94, 121, 89, 22, 0, 115, 5, 0, 32, 18, 164, 22, 0, 29, 42, 55, 27, 68, 173, 21, 196, 45, 59, 106, 58, 113, 9, 14, 48, 7, 10, 78, 98, 167, 255, 255, 27, 0, 39, 0, 166, 4, 93, 0, 11, 212, 117, 60, 108, 27, 0, 19, 255, 0, 179, 44, 3, 9, 87, 49, 0, 7, 0, 56, 80, 124, 36, 79, 5, 125, 5, 8, 110, 0, 18, 212, 0, 38, 80, 255, 186, 69, 140, 12, 1, 0, 96, 173, 23, 23, 0, 0, 0, 23, 0, 41, 10, 7, 81, 2, 160, 13, 0, 17, 0, 10, 96, 0, 117, 130, 231, 0, 0, 71, 136, 0, 177, 73, 131, 11, 10, 227, 40, 237, 62, 96, 0, 0, 94, 71, 130, 45, 0, 102, 13, 9, 12, 36, 171, 12, 0, 0, 116, 0, 51, 33, 164, 31, 203, 40, 9, 1, 73, 172, 23, 16, 0, 26, 16, 47, 28, 109, 235, 173, 34, 0, 37, 0, 164, 0, 89, 0, 26, 223, 96, 13, 147, 27, 0, 17, 219, 0, 80, 11, 0, 6, 126, 40, 81, 28, 0, 58, 104, 189, 38, 67, 18, 149, 5, 11, 255, 0, 12, 209, 0, 47, 31, 229, 165, 74, 24, 12, 17, 0, 113, 36, 0, 47, 0, 41, 0, 45, 75, 0, 19, 37, 154, 2, 151, 13, 0, 17, 0, 0, 79, 0, 30, 207, 109, 0, 0, 41, 98, 0, 169, 31, 95, 30, 0, 160, 3, 237, 87, 71, 0, 0, 94, 85, 103, 17, 0, 93, 5, 0, 54, 30, 167, 28, 15, 25, 43, 4, 39, 68, 157, 31, 201, 0, 27, 28, 78, 177, 54, 11, 0, 23, 4, 42, 28, 140, 231, 255, 54, 51, 78, 24, 120, 0, 85, 53, 42, 230, 108, 17, 25, 27, 23, 21, 239, 16, 97, 18, 24, 8, 102, 49, 39, 50, 0, 126, 98, 162, 17, 65, 3, 177, 0, 4, 57, 0, 7, 138, 0, 49, 36, 255, 167, 67, 98, 12, 6, 0, 74, 16, 6, 17, 0, 31, 0, 28, 29, 0, 17, 20, 144, 2, 134, 13, 0, 67, 0, 0, 101, 0, 133, 188, 68, 0, 0, 80, 139, 28, 191, 19, 64, 0, 0, 160, 20, 237, 115, 101, 0, 0, 94, 157, 73, 17, 0, 126, 27, 0, 69, 6, 183, 40, 0, 22, 65, 0, 23, 68, 158, 21, 207, 23, 32, 70, 59, 154, 30, 11, 0, 65, 30, 47, 77, 107, 255, 239, 8, 124, 0, 0, 137, 11, 78, 79, 36, 255, 120, 44, 150, 37, 0, 0, 255, 0, 168, 0, 0, 5, 119, 66, 25, 7, 0, 46, 108, 158, 0, 51, 3, 160, 27, 16, 93, 12, 12, 209, 25, 43, 0, 244, 238, 67, 107, 12, 1, 0, 95, 23, 0, 50, 0, 0, 0, 26, 0, 12, 19, 25, 68, 2, 185, 13, 0, 0, 40, 0, 11, 65, 96, 195, 220, 0, 0, 113, 78, 0, 179, 0, 81, 11, 96, 138, 19, 237, 93, 101, 0, 0, 94, 121, 105, 33, 0, 138, 7, 0, 69, 0, 173, 92, 0, 0, 36, 0, 58, 0, 169, 25, 207, 0, 46, 72, 78, 107, 51, 11, 0, 79, 4, 51, 123, 108, 255, 255, 0, 0, 0, 0, 169, 0, 93, 83, 11, 255, 114, 6, 176, 27, 0, 0, 255, 22, 87, 66, 0, 12, 90, 71, 31, 7, 0, 56, 86, 162, 0, 48, 5, 145, 0, 8, 202, 0, 14, 157, 0, 43, 0, 234, 213, 74, 120, 12, 1, 0, 142, 36, 134, 7, 0, 0, 0, 31, 39, 6, 23, 37, 96, 2, 158, 13, 0, 0, 0, 50, 52, 42, 138, 166, 156, 0, 0, 30, 66, 0, 153, 18, 108, 0, 0, 136, 10, 237, 156, 90, 0, 0, 94, 91, 127, 8, 0, 69, 23, 0, 48, 19, 159, 38, 0, 0, 7, 41, 30, 2, 171, 37, 207, 7, 47, 20, 48, 118, 0, 21, 0, 13, 4, 48, 82, 153, 227, 213, 38, 0, 0, 65, 169, 25, 93, 4, 12, 242, 121, 14, 108, 27, 0, 0, 242, 70, 62, 30, 11, 9, 143, 43, 0, 7, 0, 77, 63, 165, 0, 79, 5, 108, 0, 17, 135, 0, 7, 200, 0, 39, 52, 255, 203, 28, 93, 12, 1, 0, 144, 20, 0, 11, 0, 49, 0, 25, 0, 26, 27, 1, 63, 2, 175, 13, 0, 25, 6, 0, 78, 36, 135, 112, 83, 0, 0, 50, 73, 60, 179, 66, 119, 0, 12, 125, 16, 237, 109, 102, 0, 0, 82, 92, 80, 4, 0, 127, 38, 0, 65, 0, 186, 111, 0, 10, 71, 0, 100, 25, 157, 65, 191, 35, 44, 3, 78, 88, 0, 25, 0, 31, 4, 55, 46, 159, 255, 255, 23, 34, 10, 40, 169, 26, 93, 0, 32, 220, 128, 2, 91, 27, 0, 0, 240, 0, 150, 42, 24, 21, 109, 17, 22, 7, 0, 51, 105, 147, 14, 79, 3, 135, 0, 30, 87, 13, 7, 174, 0, 36, 0, 255, 171, 49, 131, 12, 49, 0, 63, 24, 4, 24, 0, 25, 0, 45, 41, 66, 4, 1, 34, 7, 114, 13, 0, 0, 0, 0, 38, 9, 95, 200, 175, 0, 0, 28, 97, 0, 171, 32, 132, 6, 16, 117, 27, 237, 103, 132, 0, 0, 52, 62, 96, 7, 0, 100, 28, 0, 46, 0, 151, 152, 0, 4, 85, 0, 84, 68, 187, 120, 167, 48, 19, 78, 55, 135, 5, 25, 0, 71, 4, 59, 67, 144, 238, 254, 1, 0, 0, 35, 161, 0, 93, 0, 26, 170, 105, 25, 175, 27, 32, 0, 225, 0, 82, 54, 0, 25, 132, 17, 13, 7, 0, 34, 115, 138, 0, 72, 3, 159, 0, 10, 194, 0, 7, 184, 0, 37, 38, 255, 182, 74, 130, 12, 20, 0, 110, 24, 0, 38, 0, 0, 0, 13, 96, 6, 13, 1, 29, 7, 188, 13, 0, 128, 0, 80, 74, 0, 24, 230, 95, 0, 0, 28, 168, 0, 130, 7, 77, 0, 0, 138, 53, 237, 110, 114, 8, 0, 94, 144, 116, 23, 0, 70, 15, 0, 24, 4, 131, 98, 32, 49, 26, 0, 59, 29, 174, 25, 180, 53, 60, 93, 54, 83, 26, 11, 0, 35, 4, 80, 40, 141, 245, 238, 1, 56, 5, 0, 131, 14, 93, 0, 36, 176, 79, 57, 143, 27, 0, 0, 255, 0, 194, 0, 0, 42, 123, 47, 0, 7, 18, 36, 112, 170, 10, 71, 3, 114, 0, 50, 194, 0, 16, 142, 2, 40, 0, 255, 174, 41, 107, 12, 1, 0, 114, 75, 83, 40, 0, 0, 0, 11, 0, 0, 30, 7, 19, 4, 195, 13, 0, 0, 142, 0, 107, 5, 92, 177, 219, 0, 0, 37, 135, 3, 181, 38, 77, 7, 39, 93, 68, 237, 66, 163, 0, 38, 94, 62, 97, 22, 0, 96, 15, 0, 11, 4, 162, 141, 0, 50, 48, 0, 88, 13, 161, 31, 207, 37, 50, 63, 78, 121, 1, 11, 67, 35, 4, 70, 45, 136, 255, 255, 8, 0, 40, 144, 169, 0, 79, 0, 37, 173, 132, 6, 118, 27, 10, 0, 242, 0, 158, 38, 0, 16, 102, 32, 61, 7, 0, 84, 105, 162, 0, 69, 3, 102, 11, 17, 123, 87, 37, 104, 0, 33, 0, 255, 139, 41, 128, 12, 1, 0, 86, 55, 0, 46, 3, 0, 0, 9, 64, 0, 30, 10, 78, 23, 177, 13, 0, 7, 0, 1, 62, 0, 96, 160, 95, 0, 0, 28, 109, 0, 179, 39, 71, 0, 62, 80, 136, 237, 109, 118, 0, 0, 69, 150, 86, 27, 0, 93, 17, 0, 26, 9, 166, 40, 0, 16, 47, 3, 47, 0, 168, 49, 207, 25, 8, 65, 31, 144, 82, 53, 0, 33, 4, 48, 143, 137, 251, 170, 7, 15, 22, 74, 144, 0, 74, 2, 14, 249, 78, 42, 70, 27, 45, 4, 255, 0, 81, 0, 35, 15, 74, 40, 0, 7, 0, 91, 84, 161, 0, 67, 3, 120, 31, 23, 59, 5, 7, 185, 0, 39, 0, 250, 220, 28, 93, 12, 1, 0, 166, 45, 0, 34, 0, 5, 0, 37, 0, 0, 25, 20, 74, 19, 195, 13, 0, 45, 0, 0, 40, 0, 127, 225, 85, 0, 0, 52, 98, 0, 176, 51, 97, 8, 0, 123, 16, 237, 90, 116, 0, 0, 94, 92, 125, 37, 0, 106, 15, 14, 60, 22, 135, 45, 0, 33, 63, 44, 57, 46, 136, 58, 195, 79, 47, 41, 78, 128, 84, 85, 3, 69, 4, 91, 70, 125, 204, 255, 25, 0, 0, 0, 135, 60, 68, 12, 33, 209, 119, 23, 135, 27, 0, 0, 241, 1, 74, 35, 0, 5, 149, 56, 23, 7, 0, 94, 104, 177, 0, 79, 3, 155, 2, 13, 40, 23, 7, 191, 0, 40, 15, 255, 179, 20, 137, 12, 14, 0, 204, 36, 0, 33, 0, 26, 0, 43, 0, 0, 17, 31, 49, 9, 168, 13, 0, 0, 76, 0, 71, 42, 29, 207, 64, 0, 0, 76, 141, 70, 193, 42, 100, 8, 29, 162, 26, 237, 82, 116, 0, 0, 89, 72, 94, 72, 0, 107, 14, 0, 62, 4, 186, 114, 61, 0, 36, 0, 39, 0, 160, 63, 190, 72, 31, 116, 78, 62, 43, 11, 0, 31, 4, 42, 28, 102, 242, 255, 5, 0, 0, 0, 158, 0, 93, 0, 13, 255, 90, 57, 159, 27, 53, 73, 255, 33, 54, 0, 0, 27, 84, 68, 32, 7, 0, 69, 77, 145, 0, 79, 3, 131, 15, 4, 194, 0, 7, 201, 0, 29, 33, 255, 181, 62, 135, 12, 16, 0, 114, 97, 0, 38, 0, 14, 0, 16, 0, 26, 15, 21, 7, 2, 181, 13, 0, 48, 0, 0, 70, 45, 80, 181, 23, 0, 0, 28, 59, 31, 199, 20, 159, 7, 0, 168, 41, 237, 146, 19, 0, 0, 84, 97, 123, 4, 0, 86, 48, 0, 69, 35, 161, 111, 0, 28, 45, 0, 128, 0, 142, 21, 207, 0, 19, 0, 78, 143, 92, 11, 0, 40, 10, 21, 61, 111, 237, 255, 39, 43, 60, 2, 105, 8, 83, 1, 14, 236, 81, 24, 145, 27, 0, 0, 255, 29, 125, 0, 65, 28, 96, 76, 12, 7, 0, 77, 66, 184, 0, 51, 3, 161, 16, 4, 77, 11, 7, 174, 0, 81, 0, 255, 187, 27, 135, 12, 7, 0, 67, 20, 0, 23, 0, 0, 0, 49, 0, 67, 17, 4, 98, 69, 160, 13, 0, 15, 1, 0, 62, 0, 81, 191, 94, 0, 0, 94, 110, 0, 199, 19, 69, 10, 64, 91, 38, 237, 154, 161, 0, 0, 52, 41, 101, 4, 0, 70, 15, 0, 39, 59, 137, 64, 0, 13, 9, 31, 69, 0, 153, 21, 189, 0, 39, 85, 37, 107, 4, 38, 0, 95, 16, 102, 89, 70, 247, 242, 39, 0, 0, 0, 145, 23, 77, 56, 30, 255, 137, 66, 139, 27, 24, 23, 195, 19, 62, 61, 0, 16, 111, 36, 0, 7, 0, 24, 93, 151, 0, 38, 3, 177, 5, 31, 183, 0, 11, 209, 0, 91, 80, 255, 203, 21, 132, 12, 10, 0, 85, 69, 47, 21, 0, 0, 0, 26, 113, 30, 4, 41, 51, 11, 167, 13, 0, 26, 0, 4, 65, 26, 86, 142, 47, 0, 0, 62, 64, 0, 199, 78, 112, 0, 0, 221, 48, 237, 146, 220, 0, 0, 85, 28, 92, 39, 0, 55, 10, 0, 0, 30, 129, 116, 0, 32, 26, 31, 48, 0, 108, 21, 187, 15, 68, 16, 78, 99, 0, 11, 6, 96, 4, 74, 39, 58, 223, 193, 26, 0, 0, 0, 136, 0, 62, 13, 64, 196, 120, 46, 148, 27, 112, 17, 192, 47, 68, 67, 0, 24, 100, 49, 0, 7, 0, 38, 99, 180, 0, 72, 3, 152, 60, 54, 108, 0, 34, 242, 48, 63, 0, 255, 189, 74, 95, 12, 1, 0, 43, 39, 37, 9, 0, 0, 0, 7, 137, 66, 16, 14, 39, 16, 195, 13, 0, 0, 57, 0, 28, 0, 44, 142, 1, 0, 0, 28, 99, 0, 178, 4, 122, 6, 23, 50, 26, 237, 164, 146, 0, 0, 83, 86, 114, 4, 0, 112, 45, 0, 17, 0, 179, 145, 0, 8, 27, 0, 72, 27, 184, 21, 199, 100, 18, 86, 78, 116, 29, 11, 53, 21, 4, 83, 51, 134, 255, 249, 35, 0, 0, 0, 162, 44, 93, 0, 50, 225, 122, 27, 83, 36, 34, 23, 255, 17, 133, 51, 0, 2, 106, 57, 36, 7, 9, 56, 73, 153, 0, 47, 3, 175, 4, 12, 214, 0, 29, 167, 10, 80, 42, 255, 182, 41, 104, 12, 24, 0, 165, 66, 53, 6, 0, 0, 0, 1, 76, 0, 26, 4, 51, 20, 162, 13, 0, 0, 0, 29, 21, 0, 67, 189, 37, 0, 0, 34, 126, 0, 154, 36, 154, 12, 24, 105, 56, 237, 116, 119, 79, 0, 77, 126, 97, 4, 0, 62, 23, 0, 1, 41, 134, 21, 0, 55, 31, 0, 15, 25, 153, 41, 164, 37, 17, 55, 51, 117, 83, 11, 0, 18, 4, 55, 65, 184, 250, 213, 20, 0, 0, 0, 135, 5, 93, 12, 16, 216, 102, 64, 166, 37, 0, 41, 255, 56, 83, 0, 0, 19, 84, 28, 7, 7, 0, 157, 76, 158, 0, 79, 3, 97, 0, 18, 16, 0, 7, 177, 0, 26, 0, 255, 207, 66, 119, 12, 27, 0, 225, 51, 0, 51, 0, 71, 0, 27, 0, 0, 39, 5, 112, 12, 167, 13, 0, 129, 0, 0, 93, 0, 133, 175, 107, 0, 0, 48, 108, 19, 85, 74, 180, 16, 0, 130, 41, 237, 80, 220, 0, 0, 85, 74, 124, 37, 0, 62, 5, 0, 67, 16, 154, 28, 8, 19, 21, 23, 42, 68, 169, 51, 205, 30, 16, 71, 58, 85, 14, 11, 0, 74, 4, 101, 58, 124, 255, 178, 42, 0, 0, 106, 149, 32, 93, 0, 16, 255, 119, 34, 151, 27, 26, 15, 187, 0, 45, 35, 0, 2, 137, 50, 63, 7, 0, 99, 73, 153, 0, 71, 3, 62, 0, 12, 180, 0, 7, 244, 0, 55, 80, 255, 217, 35, 140, 12, 33, 0, 140, 80, 0, 42, 0, 0, 0, 13, 80, 0, 39, 4, 103, 19, 209, 13, 0, 0, 88, 0, 81, 0, 51, 141, 74, 0, 0, 44, 65, 0, 199, 39, 146, 12, 35, 138, 33, 237, 121, 204, 0, 3, 83, 46, 87, 4, 0, 99, 5, 0, 63, 6, 184, 96, 0, 0, 13, 0, 60, 46, 157, 21, 207, 27, 70, 55, 73, 84, 0, 26, 52, 48, 4, 129, 52, 56, 255, 229, 8, 0, 0, 0, 163, 0, 93, 55, 11, 255, 122, 16, 159, 31, 104, 31, 217, 0, 78, 47, 39, 5, 142, 89, 0, 7, 0, 32, 105, 118, 39, 79, 3, 138, 0, 34, 118, 1, 14, 178, 27, 39, 35, 237, 173, 21, 140, 12, 48, 0, 147, 44, 0, 32, 0, 0, 0, 20, 78, 0, 42, 74, 36, 64, 188, 13, 0, 9, 0, 0, 52, 0, 32, 204, 8, 0, 0, 59, 23, 66, 199, 28, 121, 0, 19, 126, 52, 237, 104, 203, 0, 0, 89, 64, 120, 75, 0, 109, 28, 0, 69, 26, 176, 96, 37, 0, 39, 0, 77, 0, 179, 21, 199, 41, 54, 88, 54, 158, 0, 43, 53, 20, 4, 110, 32, 122, 255, 255, 11, 0, 0, 81, 142, 6, 78, 34, 12, 255, 39, 2, 128, 34, 52, 4, 227, 0, 81, 7, 0, 12, 38, 56, 0, 7, 0, 46, 111, 179, 0, 55, 3, 161, 6, 33, 138, 0, 17, 242, 20, 86, 66, 255, 255, 18, 120, 12, 1, 0, 158, 81, 19, 20, 0, 0, 0, 13, 3, 0, 58, 1, 35, 24, 172, 13, 0, 35, 0, 18, 73, 0, 54, 216, 1, 0, 0, 66, 64, 0, 199, 18, 120, 15, 0, 116, 24, 237, 81, 110, 0, 0, 82, 126, 128, 50, 0, 120, 5, 0, 6, 32, 179, 171, 45, 0, 42, 0, 46, 68, 125, 21, 188, 34, 11, 88, 0, 91, 26, 32, 0, 26, 4, 63, 39, 124, 233, 147, 0, 0, 0, 0, 152, 0, 64, 0, 23, 208, 102, 32, 164, 27, 0, 53, 255, 16, 119, 0, 0, 32, 119, 120, 21, 7, 0, 33, 79, 152, 9, 49, 3, 186, 25, 41, 183, 0, 7, 171, 0, 41, 39, 255, 168, 72, 133, 12, 1, 0, 214, 58, 0, 66, 0, 0, 0, 8, 44, 29, 12, 35, 42, 9, 138, 13, 0, 0, 0, 0, 37, 22, 59, 158, 52, 0, 0, 28, 52, 0, 165, 41, 154, 41, 19, 146, 67, 237, 115, 76, 0, 0, 84, 128, 93, 19, 0, 84, 5, 2, 37, 2, 126, 96, 0, 15, 41, 0, 51, 2, 131, 21, 202, 20, 94, 48, 60, 123, 16, 11, 0, 74, 4, 38, 132, 79, 254, 120, 29, 0, 48, 105, 163, 19, 93, 0, 62, 255, 115, 16, 126, 27, 0, 28, 255, 4, 45, 33, 6, 24, 139, 23, 13, 7, 0, 22, 65, 170, 0, 38, 3, 145, 0, 37, 207, 15, 17, 255, 0, 42, 0, 255, 217, 59, 126, 12, 1, 0, 174, 30, 0, 4, 0, 29, 0, 56, 0, 38, 4, 32, 7, 22, 212, 13, 0, 1, 40, 0, 87, 28, 163, 82, 130, 0, 0, 28, 54, 0, 140, 131, 170, 16, 14, 122, 0, 237, 227, 82, 14, 0, 68, 75, 110, 4, 0, 40, 88, 41, 40, 4, 158, 6, 0, 23, 7, 2, 26, 68, 111, 37, 200, 82, 22, 74, 75, 148, 20, 11, 0, 94, 4, 89, 47, 139, 233, 255, 0, 0, 37, 146, 169, 102, 93, 0, 20, 196, 129, 43, 96, 27, 48, 0, 190, 0, 44, 81, 21, 5, 91, 17, 0, 7, 0, 14, 95, 189, 26, 24, 3, 94, 41, 16, 35, 22, 93, 230, 23, 24, 0, 203, 208, 62, 129, 12, 135, 0, 65, 91, 0, 138, 0, 59, 0, 79, 12, 54, 66, 1, 11, 22, 109, 50, 0, 107, 0, 142, 50, 33, 48, 222, 65, 0, 0, 28, 74, 0, 84, 78, 121, 0, 63, 172, 16, 224, 233, 152, 0, 0, 94, 99, 96, 12, 0, 85, 11, 0, 69, 24, 176, 88, 0, 0, 30, 0, 75, 0, 173, 21, 157, 52, 12, 51, 78, 38, 0, 11, 0, 5, 20, 135, 28, 65, 255, 240, 18, 4, 8, 0, 95, 44, 93, 0, 11, 255, 86, 32, 8, 27, 62, 67, 211, 0, 80, 62, 0, 32, 70, 54, 0, 7, 0, 78, 18, 172, 45, 79, 3, 73, 32, 8, 166, 0, 35, 255, 0, 73, 62, 255, 255, 67, 124, 12, 86, 0, 73, 23, 15, 181, 7, 3, 0, 23, 29, 68, 4, 68, 39, 2, 166, 13, 0, 73, 0, 17, 75, 0, 66, 222, 47, 0, 0, 46, 76, 0, 183, 28, 121, 34, 0, 171, 20, 149, 131, 104, 0, 0, 78, 164, 120, 15, 0, 107, 5, 0, 69, 41, 162, 108, 17, 23, 27, 0, 37, 68, 178, 21, 183, 12, 22, 99, 57, 104, 68, 11, 2, 78, 14, 20, 28, 107, 255, 204, 19, 0, 0, 0, 157, 0, 93, 0, 5, 255, 102, 12, 116, 27, 0, 22, 255, 18, 136, 61, 0, 9, 146, 31, 0, 7, 0, 33, 85, 136, 0, 79, 3, 205, 7, 4, 49, 20, 32, 207, 66, 30, 0, 217, 202, 61, 125, 12, 1, 0, 165, 65, 0, 15, 0, 0, 0, 1, 0, 79, 14, 35, 59, 2, 176, 13, 0, 0, 0, 46, 74, 0, 133, 152, 186, 0, 0, 99, 74, 0, 176, 44, 123, 23, 68, 136, 25, 237, 84, 102, 0, 0, 84, 93, 125, 10, 0, 100, 5, 0, 65, 36, 135, 173, 0, 14, 58, 9, 73, 68, 187, 21, 190, 20, 33, 69, 78, 131, 4, 11, 0, 10, 23, 75, 45, 183, 255, 203, 43, 0, 0, 187, 169, 27, 93, 0, 5, 255, 133, 26, 121, 27, 8, 0, 219, 35, 49, 35, 19, 2, 97, 51, 21, 7, 0, 130, 92, 182, 0, 79, 3, 126, 0, 10, 152, 0, 29, 214, 0, 76, 46, 255, 212, 41, 116, 12, 7, 0, 110, 38, 0, 64, 0, 0, 0, 50, 34, 32, 19, 16, 92, 2, 140, 13, 0, 48, 27, 0, 61, 1, 132, 145, 109, 0, 0, 90, 81, 11, 189, 64, 113, 0, 13, 147, 63, 237, 138, 200, 0, 0, 81, 77, 105, 4, 0, 95, 9, 31, 60, 9, 150, 56, 0, 0, 56, 8, 36, 42, 138, 53, 190, 33, 57, 43, 70, 97, 13, 11, 0, 18, 4, 62, 127, 159, 253, 228, 0, 4, 0, 0, 149, 0, 93, 15, 12, 255, 102, 16, 109, 27, 49, 17, 255, 0, 73, 0, 66, 6, 109, 104, 0, 7, 0, 92, 77, 134, 0, 79, 3, 103, 0, 8, 116, 0, 11, 188, 0, 27, 0, 255, 208, 71, 130, 12, 10, 0, 118, 49, 4, 49, 0, 0, 0, 33, 45, 0, 4, 27, 53, 19, 184, 13, 0, 94, 0, 19, 50, 65, 84, 140, 23, 0, 0, 77, 87, 0, 195, 40, 112, 3, 0, 33, 48, 237, 147, 87, 0, 0, 75, 133, 106, 13, 0, 103, 64, 0, 69, 39, 168, 43, 0, 0, 103, 0, 52, 0, 143, 21, 185, 60, 66, 55, 63, 150, 92, 48, 0, 10, 15, 34, 46, 136, 255, 213, 29, 40, 17, 0, 161, 48, 93, 28, 14, 247, 115, 7, 170, 27, 0, 0, 255, 0, 57, 0, 0, 2, 127, 49, 1, 7, 0, 61, 104, 141, 0, 79, 7, 200, 7, 4, 129, 43, 7, 143, 45, 33, 0, 237, 179, 57, 112, 12, 1, 0, 176, 36, 30, 11, 0, 17, 0, 65, 0, 0, 6, 17, 72, 2, 164, 13, 0, 0, 9, 0, 40, 35, 6, 255, 108, 0, 0, 132, 93, 0, 199, 16, 130, 13, 38, 76, 27, 237, 95, 73, 0, 0, 73, 72, 108, 39, 0, 59, 23, 4, 3, 30, 158, 25, 0, 19, 88, 22, 105, 0, 156, 57, 170, 56, 95, 87, 64, 120, 58, 38, 115, 21, 27, 32, 109, 120, 200, 148, 2, 0, 8, 36, 161, 0, 68, 0, 31, 194, 118, 23, 175, 27, 54, 0, 211, 0, 64, 24, 0, 2, 121, 80, 54, 7, 0, 71, 83, 181, 0, 79, 8, 123, 0, 7, 27, 0, 7, 145, 0, 25, 16, 254, 140, 52, 62, 12, 7, 0, 68, 74, 0, 45, 39, 13, 0, 99, 0, 74, 12, 11, 70, 2, 201, 13, 0, 82, 0, 28, 83, 0, 84, 172, 28, 0, 0, 87, 69, 0, 167, 20, 109, 15, 0, 115, 24, 237, 78, 87, 0, 0, 68, 54, 148, 9, 0, 82, 16, 0, 0, 47, 145, 94, 0, 6, 89, 0, 61, 43, 157, 71, 181, 62, 23, 37, 52, 103, 0, 17, 24, 55, 8, 11, 109, 170, 233, 236, 15, 0, 0, 87, 137, 0, 80, 0, 5, 217, 116, 29, 161, 43, 14, 0, 205, 31, 48, 0, 26, 2, 121, 74, 7, 7, 0, 32, 69, 138, 0, 74, 3, 146, 0, 11, 156, 41, 7, 154, 0, 46, 9, 255, 192, 67, 73, 12, 15, 0, 107, 27, 0, 23, 0, 0, 0, 22, 4, 91, 4, 1, 71, 8, 147, 13, 0, 0, 0, 0, 86, 0, 52, 250, 37, 0, 0, 77, 75, 11, 168, 35, 138, 4, 12, 156, 35, 237, 82, 102, 0, 6, 94, 102, 112, 10, 0, 86, 43, 0, 60, 28, 175, 46, 0, 51, 68, 27, 37, 68, 178, 52, 199, 25, 80, 108, 78, 128, 25, 21, 109, 32, 4, 54, 121, 136, 247, 255, 22, 0, 0, 0, 157, 39, 84, 0, 5, 255, 137, 14, 157, 47, 8, 0, 231, 18, 74, 31, 0, 2, 167, 37, 30, 7, 0, 58, 67, 130, 0, 69, 3, 140, 7, 4, 145, 2, 7, 200, 0, 42, 0, 255, 183, 62, 127, 12, 6, 0, 65, 60, 0, 65, 0, 14, 0, 1, 0, 77, 43, 11, 55, 12, 130, 13, 0, 13, 0, 8, 89, 36, 64, 162, 60, 0, 0, 43, 40, 0, 179, 84, 153, 0, 0, 102, 14, 237, 111, 197, 0, 4, 94, 75, 88, 36, 0, 102, 46, 0, 46, 62, 150, 76, 19, 18, 62, 0, 52, 43, 166, 42, 207, 30, 28, 89, 78, 149, 0, 11, 34, 17, 4, 70, 67, 123, 238, 182, 11, 0, 0, 80, 169, 0, 93, 13, 5, 255, 102, 14, 183, 27, 70, 0, 214, 40, 44, 32, 0, 15, 101, 70, 10, 7, 0, 41, 76, 153, 0, 79, 3, 134, 24, 7, 103, 47, 7, 203, 0, 40, 24, 255, 167, 0, 125, 12, 1, 0, 154, 27, 5, 14, 0, 0, 0, 9, 35, 33, 23, 36, 65, 2, 119, 13, 0, 0, 0, 0, 40, 75, 65, 147, 13, 0, 0, 82, 52, 0, 153, 74, 188, 0, 107, 123, 17, 237, 134, 65, 0, 17, 85, 165, 137, 13, 0, 95, 7, 22, 0, 65, 156, 136, 0, 32, 23, 0, 70, 57, 156, 114, 207, 7, 71, 17, 78, 75, 92, 11, 0, 5, 4, 33, 60, 120, 238, 194, 26, 0, 0, 18, 161, 0, 93, 91, 40, 248, 123, 45, 126, 27, 0, 0, 255, 31, 116, 24, 70, 44, 143, 72, 0, 7, 0, 53, 99, 155, 0, 79, 3, 135, 17, 8, 173, 1, 36, 162, 0, 45, 0, 233, 196, 33, 116, 12, 1, 0, 156, 45, 29, 30, 17, 0, 0, 53, 0, 24, 31, 21, 97, 16, 112, 13, 0, 143, 0, 0, 46, 18, 115, 93, 184, 0, 0, 39, 87, 0, 195, 23, 82, 0, 0, 112, 80, 237, 157, 101, 0, 0, 68, 122, 119, 21, 0, 100, 87, 0, 50, 0, 141, 33, 0, 16, 74, 0, 54, 0, 155, 21, 174, 51, 21, 75, 78, 88, 33, 11, 0, 39, 4, 46, 78, 134, 255, 216, 42, 142, 0, 0, 135, 38, 93, 27, 44, 245, 129, 25, 141, 27, 0, 0, 255, 46, 59, 0, 8, 9, 92, 31, 0, 7, 0, 44, 63, 123, 0, 79, 3, 111, 31, 4, 35, 0, 7, 198, 7, 42, 0, 255, 221, 74, 117, 12, 47, 0, 129, 58, 0, 31, 0, 5, 0, 5, 0, 63, 19, 14, 88, 17, 185, 13, 0, 0, 90, 4, 75, 34, 56, 186, 41, 0, 0, 86, 128, 0, 195, 23, 125, 0, 72, 128, 30, 237, 62, 114, 0, 0, 56, 89, 90, 23, 0, 125, 42, 0, 58, 0, 186, 58, 0, 11, 38, 0, 82, 17, 185, 21, 203, 73, 56, 98, 66, 113, 65, 11, 0, 71, 4, 70, 112, 180, 255, 230, 13, 34, 31, 116, 169, 33, 84, 0, 5, 211, 117, 43, 139, 27, 61, 3, 230, 13, 6, 35, 0, 5, 90, 19, 43, 7, 0, 30, 106, 131, 10, 70, 3, 68, 0, 14, 154, 0, 7, 184, 0, 68, 0, 255, 203, 54, 132, 12, 19, 0, 137, 27, 0, 9, 0, 0, 0, 6, 47, 50, 38, 11, 91, 18, 174, 15, 0, 39, 0, 0, 81, 0, 82, 206, 51, 0, 0, 67, 60, 0, 199, 0, 79, 13, 0, 134, 22, 237, 95, 75, 0, 0, 66, 117, 119, 21, 0, 93, 12, 0, 38, 11, 169, 13, 0, 0, 44, 17, 41, 21, 144, 27, 207, 17, 38, 49, 66, 120, 51, 11, 36, 18, 4, 45, 110, 102, 217, 255, 8, 0, 0, 0, 86, 57, 76, 3, 53, 207, 117, 15, 183, 27, 0, 8, 255, 78, 94, 47, 0, 7, 149, 45, 27, 7, 0, 40, 95, 117, 45, 65, 3, 177, 7, 19, 109, 0, 26, 150, 4, 44, 37, 255, 147, 23, 140, 12, 65, 0, 135, 85, 0, 60, 0, 33, 0, 9, 0, 57, 12, 26, 110, 15, 137, 16, 0, 0, 0, 0, 96, 69, 22, 252, 199, 0, 0, 74, 124, 43, 199, 18, 93, 17, 0, 179, 9, 237, 83, 154, 0, 0, 94, 57, 99, 22, 0, 71, 21, 0, 61, 14, 150, 68, 41, 0, 33, 86, 51, 68, 183, 29, 207, 32, 41, 136, 64, 66, 0, 11, 78, 18, 4, 89, 85, 150, 255, 255, 17, 0, 0, 49, 156, 0, 93, 0, 27, 239, 125, 94, 138, 27, 37, 33, 190, 10, 14, 55, 0, 6, 93, 45, 33, 7, 0, 89, 60, 140, 15, 75, 3, 100, 10, 4, 164, 0, 14, 186, 0, 51, 14, 255, 166, 27, 119, 12, 73, 0, 111, 128, 0, 18, 11, 0, 0, 28, 63, 33, 94, 14, 134, 27, 160, 13, 0, 16, 0, 6, 105, 0, 92, 137, 97, 0, 0, 34, 74, 39, 199, 52, 147, 0, 0, 131, 3, 237, 109, 159, 0, 0, 75, 84, 135, 13, 0, 70, 65, 0, 54, 87, 160, 19, 0, 0, 77, 8, 39, 42, 182, 21, 188, 39, 27, 25, 38, 103, 0, 17, 0, 28, 28, 22, 62, 178, 237, 168, 22, 22, 0, 0, 138, 0, 84, 14, 27, 222, 101, 48, 161, 27, 8, 0, 255, 9, 61, 0, 0, 17, 131, 85, 0, 7, 0, 42, 77, 134, 0, 79, 3, 122, 18, 4, 94, 0, 7, 229, 0, 36, 41, 255, 204, 28, 69, 12, 34, 0, 63, 19, 0, 34, 0, 15, 0, 41, 10, 8, 25, 1, 83, 49, 153, 13, 0, 31, 0, 0, 72, 1, 43, 206, 86, 0, 0, 115, 129, 35, 199, 13, 104, 12, 45, 99, 7, 237, 63, 69, 0, 0, 55, 102, 94, 16, 0, 86, 5, 0, 69, 4, 167, 47, 0, 44, 31, 0, 55, 68, 186, 33, 196, 21, 56, 82, 38, 107, 92, 28, 0, 46, 4, 64, 114, 110, 227, 255, 0, 27, 60, 0, 152, 17, 66, 42, 18, 255, 128, 51, 176, 27, 0, 0, 236, 10, 93, 22, 15, 23, 148, 57, 41, 7, 0, 85, 78, 132, 80, 79, 3, 140, 9, 16, 105, 0, 12, 168, 23, 66, 0, 235, 153, 50, 140, 12, 20, 0, 66, 28, 0, 19, 0, 0, 0, 44, 23, 27, 12, 1, 129, 9, 147, 13, 0, 0, 76, 0, 67, 61, 99, 247, 177, 0, 0, 90, 90, 115, 199, 0, 53, 10, 138, 170, 21, 237, 118, 177, 0, 0, 82, 66, 112, 14, 0, 112, 11, 0, 69, 0, 176, 73, 0, 0, 45, 14, 61, 42, 177, 36, 207, 63, 8, 83, 65, 131, 29, 11, 0, 22, 4, 127, 102, 178, 237, 255, 0, 0, 0, 0, 145, 0, 93, 0, 10, 255, 139, 43, 130, 27, 95, 58, 255, 28, 39, 91, 0, 0, 90, 74, 52, 7, 51, 50, 68, 173, 44, 73, 3, 149, 10, 4, 92, 0, 7, 232, 0, 33, 61, 255, 171, 74, 140, 12, 29, 0, 151, 29, 0, 36, 0, 31, 0, 1, 15, 0, 4, 13, 73, 12, 140, 13, 0, 37, 0, 0, 60, 26, 62, 255, 37, 0, 0, 34, 104, 75, 191, 12, 130, 11, 0, 97, 0, 237, 116, 118, 0, 0, 81, 132, 62, 10, 0, 92, 13, 12, 69, 40, 162, 104, 0, 6, 42, 0, 55, 43, 181, 44, 199, 31, 52, 28, 62, 112, 29, 11, 0, 40, 15, 45, 98, 165, 248, 246, 0, 2, 0, 96, 169, 38, 93, 0, 10, 231, 60, 57, 183, 27, 0, 0, 249, 57, 104, 5, 25, 31, 135, 51, 11, 7, 0, 64, 106, 170, 57, 71, 3, 117, 0, 52, 38, 69, 20, 208, 0, 23, 0, 255, 182, 18, 134, 12, 1, 0, 157, 35, 85, 48, 0, 8, 0, 44, 0, 58, 23, 10, 116, 2, 167, 13, 0, 0, 0, 0, 77, 0, 84, 220, 87, 0, 0, 97, 57, 8, 119, 72, 134, 26, 6, 188, 16, 237, 110, 77, 0, 0, 33, 107, 129, 19, 0, 72, 44, 13, 55, 70, 173, 76, 0, 20, 71, 0, 62, 0, 175, 21, 177, 10, 19, 73, 15, 115, 92, 51, 0, 35, 25, 34, 78, 192, 255, 140, 0, 0, 61, 16, 169, 0, 93, 4, 8, 228, 135, 16, 142, 27, 47, 0, 231, 69, 86, 18, 0, 2, 98, 68, 28, 44, 0, 46, 113, 174, 0, 50, 3, 144, 2, 17, 75, 28, 28, 131, 0, 33, 0, 224, 152, 0, 123, 12, 64, 0, 100, 37, 114, 78, 0, 0, 8, 72, 22, 0, 48, 1, 94, 83, 97, 13, 0, 42, 0, 0, 18, 0, 78, 189, 120, 0, 0, 76, 53, 0, 199, 37, 53, 3, 40, 114, 32, 237, 96, 150, 0, 0, 53, 53, 109, 58, 0, 91, 21, 0, 69, 16, 138, 50, 0, 0, 43, 6, 48, 0, 166, 21, 169, 0, 31, 88, 59, 85, 28, 11, 16, 45, 4, 42, 128, 52, 214, 201, 1, 0, 2, 0, 127, 0, 51, 0, 16, 255, 93, 7, 140, 27, 30, 64, 247, 86, 69, 35, 0, 12, 151, 50, 17, 7, 0, 72, 93, 124, 21, 79, 3, 162, 23, 13, 171, 0, 7, 184, 0, 33, 0, 230, 140, 74, 92, 26, 5, 0, 106, 44, 18, 13, 75, 0, 0, 12, 96, 60, 4, 102, 44, 2, 203, 13, 0, 28, 0, 0, 90, 0, 97, 110, 47, 0, 0, 38, 21, 0, 187, 40, 82, 38, 0, 157, 24, 237, 123, 92, 0, 0, 47, 110, 102, 29, 0, 89, 7, 0, 56, 0, 169, 64, 0, 38, 73, 0, 48, 0, 185, 39, 107, 24, 53, 59, 73, 95, 83, 11, 0, 17, 4, 48, 62, 96, 255, 255, 0, 0, 16, 0, 125, 0, 93, 0, 42, 217, 70, 20, 162, 36, 25, 60, 255, 38, 86, 80, 16, 2, 106, 61, 1, 7, 0, 116, 104, 168, 0, 79, 3, 133, 4, 21, 69, 0, 7, 192, 0, 39, 0, 255, 168, 57, 102, 12, 6, 0, 93, 45, 77, 13, 0, 0, 0, 23, 0, 3, 4, 3, 43, 2, 182, 13, 0, 43, 0, 23, 103, 45, 71, 191, 39, 0, 0, 28, 121, 0, 163, 41, 93, 6, 11, 125, 58, 237, 83, 125, 0, 0, 78, 81, 97, 7, 0, 108, 10, 0, 69, 17, 156, 22, 0, 0, 74, 4, 21, 63, 169, 21, 162, 58, 31, 51, 78, 125, 0, 31, 0, 31, 4, 76, 90, 128, 239, 235, 2, 25, 0, 25, 145, 0, 93, 14, 33, 210, 65, 12, 177, 42, 48, 25, 253, 71, 34, 34, 0, 5, 128, 67, 0, 7, 0, 99, 90, 175, 68, 79, 3, 126, 2, 11, 55, 1, 7, 159, 0, 21, 0, 254, 133, 44, 130, 12, 115, 0, 125, 20, 0, 37, 0, 0, 0, 40, 20, 0, 4, 1, 60, 14, 134, 13, 0, 0, 2, 0, 109, 0, 0, 255, 71, 0, 0, 77, 81, 0, 199, 56, 103, 20, 21, 131, 84, 237, 93, 156, 0, 0, 94, 132, 135, 18, 0, 81, 13, 0, 63, 6, 173, 84, 0, 5, 59, 0, 37, 6, 177, 39, 184, 23, 134, 62, 58, 84, 16, 11, 0, 73, 4, 51, 126, 134, 236, 255, 7, 34, 0, 0, 147, 0, 93, 0, 21, 235, 69, 33, 159, 49, 7, 5, 255, 0, 115, 0, 42, 2, 200, 58, 26, 7, 0, 47, 99, 139, 0, 79, 3, 116, 4, 4, 57, 1, 30, 206, 0, 21, 0, 255, 166, 68, 140, 12, 1, 0, 71, 75, 52, 20, 0, 0, 0, 6, 0, 0, 10, 6, 41, 2, 170, 13, 0, 75, 4, 0, 121, 51, 49, 175, 63, 0, 0, 65, 83, 0, 199, 0, 82, 17, 0, 142, 55, 237, 130, 224, 0, 0, 89, 128, 100, 4, 0, 72, 5, 0, 69, 23, 172, 9, 0, 8, 48, 77, 18, 19, 133, 21, 188, 5, 108, 71, 64, 130, 0, 17, 0, 45, 4, 102, 88, 97, 252, 236, 20, 79, 0, 98, 152, 0, 83, 31, 21, 255, 137, 31, 139, 27, 20, 0, 255, 0, 59, 23, 0, 2, 95, 63, 0, 7, 0, 52, 98, 138, 0, 79, 3, 180, 0, 28, 81, 0, 18, 162, 11, 21, 9, 255, 231, 39, 83, 12, 9, 0, 211, 103, 0, 19, 0, 98, 0, 19, 0, 0, 20, 27, 62, 21, 174, 13, 0, 0, 77, 0, 80, 0, 102, 158, 45, 0, 0, 98, 111, 0, 199, 0, 176, 6, 88, 130, 1, 237, 136, 204, 0, 0, 63, 38, 105, 77, 0, 112, 5, 0, 69, 16, 152, 157, 0, 23, 42, 18, 60, 0, 172, 21, 203, 13, 40, 87, 78, 138, 50, 21, 52, 5, 4, 77, 77, 111, 246, 197, 19, 0, 0, 99, 151, 0, 79, 0, 26, 240, 127, 45, 135, 27, 20, 0, 255, 54, 21, 46, 0, 5, 131, 53, 0, 7, 0, 43, 113, 173, 7, 79, 3, 77, 2, 22, 116, 0, 9, 214, 0, 54, 0, 255, 218, 40, 113, 12, 12, 0, 172, 131, 0, 17, 0, 0, 0, 30, 29, 0, 71, 22, 92, 32, 200, 13, 0, 8, 0, 28, 132, 0, 92, 91, 1, 0, 0, 51, 22, 0, 170, 42, 124, 9, 0, 136, 40, 237, 114, 130, 0, 0, 72, 130, 116, 17, 0, 77, 5, 0, 67, 0, 168, 188, 14, 0, 42, 0, 71, 68, 159, 21, 195, 11, 90, 98, 4, 92, 0, 11, 0, 5, 4, 68, 71, 137, 255, 237, 4, 0, 0, 0, 123, 0, 72, 0, 7, 255, 99, 47, 177, 27, 0, 0, 255, 0, 170, 0, 0, 7, 143, 62, 0, 7, 0, 54, 82, 148, 0, 79, 3, 112, 4, 22, 87, 53, 11, 193, 19, 29, 0, 255, 238, 71, 104, 12, 1, 0, 108, 46, 0, 46, 0, 0, 0, 10, 0, 13, 11, 4, 32, 9, 200, 13, 0, 38, 0, 17, 105, 0, 93, 142, 104, 0, 0, 46, 55, 0, 133, 41, 146, 0, 0, 102, 13, 237, 74, 179, 0, 0, 77, 100, 88, 21, 0, 72, 18, 0, 53, 2, 134, 50, 0, 10, 45, 4, 15, 0, 182, 21, 205, 63, 89, 81, 21, 106, 0, 11, 0, 18, 4, 102, 88, 151, 255, 231, 5, 6, 0, 15, 146, 10, 87, 0, 12, 220, 93, 18, 149, 27, 34, 0, 225, 24, 74, 6, 0, 11, 116, 17, 0, 7, 0, 134, 72, 168, 3, 76, 3, 199, 2, 16, 100, 0, 38, 255, 0, 30, 13, 255, 239, 0, 110, 12, 1, 0, 77, 72, 0, 19, 0, 36, 0, 95, 0, 0, 12, 15, 72, 7, 193, 13, 0, 14, 27, 0, 113, 0, 64, 173, 102, 0, 0, 28, 151, 0, 181, 58, 189, 0, 27, 184, 39, 237, 221, 80, 0, 0, 78, 50, 143, 4, 0, 29, 88, 0, 49, 0, 180, 6, 0, 68, 14, 53, 18, 68, 162, 27, 186, 30, 0, 47, 64, 162, 38, 24, 0, 37, 4, 26, 58, 134, 221, 255, 0, 47, 0, 0, 169, 141, 80, 0, 37, 223, 103, 2, 142, 27, 91, 0, 192, 12, 6, 107, 0, 2, 80, 17, 9, 7, 0, 38, 90, 189, 0, 37, 3, 197, 14, 4, 79, 87, 62, 208, 34, 26, 0, 224, 233, 21, 136, 12, 56, 0, 67, 27, 0, 150, 0, 49, 0, 60, 60, 31, 34, 4, 13, 26, 58, 13, 0, 93, 0, 135, 10, 55, 39, 238, 1, 0, 0, 28, 131, 72, 143, 53, 139, 3, 94, 106, 36, 237, 241, 104, 0, 33, 94, 97, 112, 21, 0, 89, 47, 0, 48, 27, 171, 58, 8, 21, 53, 0, 55, 30, 113, 21, 194, 81, 25, 70, 78, 61, 7, 11, 0, 50, 32, 73, 42, 96, 255, 255, 18, 0, 0, 30, 139, 10, 93, 0, 26, 255, 108, 10, 57, 27, 0, 50, 235, 14, 84, 75, 0, 52, 84, 46, 0, 7, 0, 42, 51, 149, 0, 64, 3, 135, 46, 15, 167, 0, 15, 236, 0, 72, 18, 255, 255, 27, 129, 12, 41, 0, 155, 49, 0, 229, 0, 56, 0, 22, 0, 37, 33, 23, 37, 20, 130, 13, 0, 107, 0, 0, 5, 0, 96, 191, 59, 0, 0, 45, 125, 20, 199, 35, 105, 8, 0, 72, 65, 237, 123, 109, 0, 0, 78, 88, 134, 35, 0, 102, 70, 0, 69, 37, 161, 103, 0, 0, 26, 10, 75, 68, 154, 21, 194, 50, 29, 61, 55, 90, 60, 11, 0, 76, 4, 35, 70, 70, 255, 184, 0, 68, 0, 0, 131, 0, 93, 0, 29, 215, 93, 13, 121, 27, 0, 1, 244, 4, 79, 63, 0, 67, 122, 56, 0, 7, 0, 30, 103, 121, 0, 79, 3, 166, 62, 4, 221, 0, 15, 185, 21, 54, 0, 255, 184, 32, 133, 12, 128, 0, 120, 43, 0, 29, 0, 0, 0, 16, 11, 52, 36, 17, 34, 93, 183, 13, 0, 0, 41, 7, 46, 0, 58, 134, 114, 0, 0, 55, 79, 0, 146, 81, 137, 16, 66, 121, 56, 237, 68, 138, 0, 0, 56, 73, 134, 11, 0, 128, 13, 0, 69, 30, 140, 48, 0, 10, 74, 29, 73, 12, 155, 21, 176, 17, 22, 56, 71, 138, 0, 58, 0, 71, 4, 7, 113, 141, 195, 255, 12, 0, 0, 0, 169, 59, 93, 36, 54, 246, 104, 10, 141, 27, 21, 0, 243, 26, 30, 9, 4, 2, 141, 56, 25, 7, 0, 30, 109, 169, 0, 79, 3, 207, 0, 5, 177, 8, 11, 173, 0, 53, 0, 255, 255, 52, 107, 12, 1, 0, 156, 40, 22, 45, 0, 7, 0, 10, 49, 36, 24, 20, 39, 26, 95, 13, 0, 88, 0, 0, 75, 0, 110, 192, 42, 0, 0, 42, 136, 0, 169, 30, 136, 0, 0, 44, 27, 237, 163, 91, 0, 0, 54, 118, 83, 6, 0, 76, 11, 36, 47, 2, 136, 96, 0, 0, 35, 0, 51, 0, 113, 21, 180, 28, 54, 70, 47, 138, 92, 11, 0, 30, 4, 38, 100, 84, 248, 165, 20, 124, 6, 107, 161, 0, 93, 0, 36, 228, 78, 26, 115, 27, 0, 40, 248, 0, 39, 53, 65, 2, 131, 31, 26, 7, 0, 78, 101, 159, 54, 79, 3, 167, 0, 32, 121, 12, 12, 224, 8, 42, 0, 255, 155, 28, 81, 12, 1, 0, 177, 51, 0, 65, 0, 0, 0, 33, 0, 0, 39, 7, 55, 8, 181, 13, 0, 0, 0, 1, 71, 0, 80, 93, 126, 0, 0, 28, 52, 0, 189, 98, 81, 11, 32, 162, 45, 237, 124, 119, 0, 0, 54, 131, 85, 22, 0, 90, 26, 0, 69, 18, 154, 63, 0, 24, 29, 0, 88, 4, 170, 21, 171, 50, 103, 88, 67, 134, 92, 41, 19, 51, 4, 69, 121, 141, 249, 255, 0, 0, 42, 0, 147, 12, 93, 0, 5, 237, 93, 83, 146, 27, 67, 0, 255, 0, 22, 44, 0, 12, 131, 26, 0, 7, 0, 48, 95, 148, 57, 79, 3, 147, 15, 31, 88, 26, 25, 211, 0, 28, 21, 255, 202, 42, 125, 12, 27, 0, 126, 70, 68, 63, 0, 0, 0, 40, 0, 0, 46, 15, 52, 2, 194, 13, 0, 0, 42, 0, 73, 0, 55, 204, 37, 0, 0, 73, 132, 10, 185, 18, 99, 4, 27, 160, 46, 220, 171, 117, 0, 0, 85, 74, 84, 15, 0, 115, 59, 0, 0, 31, 132, 86, 0, 21, 98, 45, 59, 0, 152, 34, 162, 53, 93, 50, 71, 125, 55, 18, 1, 15, 7, 96, 57, 96, 234, 204, 2, 0, 29, 187, 150, 0, 93, 0, 17, 230, 128, 54, 148, 27, 23, 0, 243, 0, 123, 42, 40, 36, 162, 55, 0, 7, 0, 32, 101, 149, 7, 79, 3, 148, 35, 24, 106, 53, 52, 197, 0, 28, 0, 255, 194, 37, 114, 12, 36, 0, 113, 20, 0, 19, 36, 0, 0, 40, 38, 25, 24, 42, 89, 8, 189, 13, 0, 80, 0, 12, 55, 21, 35, 111, 61, 0, 0, 45, 47, 0, 181, 49, 128, 12, 0, 121, 20, 209, 123, 158, 0, 0, 43, 52, 88, 22, 0, 82, 15, 14, 29, 7, 143, 88, 0, 34, 17, 3, 54, 0, 184, 38, 128, 57, 94, 86, 50, 100, 20, 11, 8, 50, 11, 107, 43, 121, 235, 255, 0, 0, 8, 0, 129, 0, 65, 0, 5, 208, 125, 31, 183, 36, 3, 15, 175, 47, 76, 124, 0, 2, 152, 45, 9, 7, 0, 90, 95, 105, 0, 35, 3, 172, 50, 21, 172, 51, 65, 198, 0, 50, 0, 217, 153, 65, 119, 12, 52, 0, 100, 92, 0, 35, 0, 0, 0, 119, 109, 8, 53, 7, 69, 51, 159, 13, 0, 15, 0, 0, 47, 0, 106, 191, 110, 0, 0, 28, 91, 0, 147, 25, 141, 0, 0, 63, 51, 237, 129, 126, 0, 8, 74, 43, 116, 4, 0, 71, 28, 0, 67, 0, 161, 129, 39, 0, 47, 32, 82, 24, 176, 39, 199, 22, 75, 111, 48, 95, 52, 11, 28, 63, 14, 83, 49, 143, 255, 255, 26, 0, 20, 1, 155, 9, 82, 0, 5, 255, 119, 47, 167, 42, 56, 12, 236, 0, 119, 75, 84, 6, 71, 73, 34, 7, 0, 55, 76, 91, 0, 79, 3, 173, 57, 4, 153, 34, 35, 142, 0, 63, 0, 255, 134, 46, 130, 12, 28, 0, 71, 84, 0, 27, 0, 0, 0, 1, 71, 19, 38, 21, 15, 17, 132, 13, 0, 0, 0, 0, 58, 40, 69, 238, 15, 0, 0, 34, 77, 0, 187, 50, 100, 11, 0, 139, 69, 237, 89, 204, 0, 0, 80, 44, 103, 86, 0, 91, 24, 0, 8, 40, 136, 49, 1, 17, 69, 46, 53, 68, 148, 90, 207, 36, 19, 97, 50, 194, 0, 39, 45, 35, 23, 74, 41, 100, 235, 166, 29, 0, 23, 0, 156, 7, 93, 21, 5, 234, 83, 31, 162, 27, 60, 0, 240, 30, 38, 37, 26, 8, 93, 65, 0, 7, 0, 59, 105, 133, 0, 79, 3, 111, 44, 4, 233, 14, 7, 239, 0, 50, 0, 255, 186, 0, 134, 12, 1, 0, 151, 26, 0, 53, 0, 0, 0, 11, 48, 6, 4, 50, 62, 11, 158, 13, 0, 45, 17, 4, 62, 53, 113, 161, 25, 0, 0, 37, 61, 0, 187, 37, 161, 15, 0, 78, 13, 237, 93, 133, 0, 0, 66, 52, 103, 82, 0, 90, 22, 0, 69, 37, 146, 100, 11, 14, 61, 0, 69, 68, 135, 36, 207, 38, 58, 70, 25, 149, 14, 39, 0, 5, 4, 34, 129, 130, 236, 224, 12, 0, 0, 0, 157, 25, 93, 3, 17, 216, 92, 47, 183, 27, 82, 0, 188, 32, 23, 37, 29, 14, 159, 83, 0, 7, 27, 38, 106, 134, 0, 72, 3, 202, 45, 10, 210, 35, 7, 173, 0, 31, 0, 243, 194, 45, 84, 12, 1, 0, 96, 91, 113, 110, 0, 1, 0, 34, 91, 28, 47, 2, 149, 32, 128, 13, 0, 3, 0, 6, 67, 0, 109, 255, 64, 0, 0, 37, 160, 12, 176, 0, 131, 23, 0, 117, 65, 237, 100, 145, 0, 0, 94, 93, 73, 41, 0, 68, 5, 0, 69, 4, 152, 15, 0, 0, 28, 0, 38, 17, 171, 21, 207, 20, 32, 53, 78, 114, 53, 11, 31, 59, 4, 102, 198, 155, 245, 255, 4, 0, 0, 7, 144, 48, 93, 0, 23, 255, 77, 57, 153, 27, 74, 0, 213, 56, 6, 41, 0, 7, 149, 49, 0, 7, 0, 80, 86, 164, 5, 50, 3, 74, 5, 17, 100, 14, 12, 180, 0, 31, 41, 255, 205, 74, 113, 12, 1, 0, 104, 88, 0, 29, 0, 40, 0, 10, 3, 20, 72, 35, 156, 2, 183, 13, 0, 26, 0, 0, 142, 25, 99, 202, 31, 0, 0, 85, 112, 5, 170, 8, 83, 0, 0, 106, 23, 237, 138, 66, 0, 21, 88, 146, 180, 17, 0, 103, 62, 2, 65, 32, 186, 74, 0, 0, 23, 0, 59, 0, 177, 75, 201, 36, 56, 95, 31, 77, 26, 11, 0, 56, 19, 30, 87, 146, 255, 183, 34, 0, 0, 38, 169, 0, 93, 0, 11, 255, 109, 51, 74, 27, 0, 3, 255, 21, 60, 0, 15, 2, 125, 53, 33, 7, 0, 34, 88, 153, 0, 39, 3, 90, 0, 21, 116, 0, 7, 162, 11, 51, 0, 228, 222, 42, 111, 12, 27, 0, 186, 44, 0, 20, 0, 19, 0, 9, 0, 63, 74, 14, 56, 14, 149, 16, 0, 0, 32, 2, 59, 0, 75, 195, 126, 0, 0, 111, 36, 0, 199, 0, 100, 0, 39, 156, 30, 237, 99, 171, 3, 0, 63, 144, 91, 22, 0, 96, 76, 25, 60, 37, 186, 82, 0, 0, 41, 0, 45, 0, 161, 103, 207, 44, 38, 116, 78, 112, 55, 11, 0, 30, 32, 72, 157, 105, 255, 198, 47, 0, 26, 29, 164, 0, 93, 0, 5, 255, 116, 24, 157, 27, 0, 14, 255, 0, 87, 40, 0, 12, 136, 45, 0, 7, 0, 21, 93, 162, 0, 56, 3, 103, 0, 27, 98, 42, 12, 195, 0, 33, 0, 228, 168, 35, 104, 12, 38, 0, 227, 43, 0, 15, 0, 35, 0, 12, 0, 0, 23, 76, 71, 2, 159, 21, 0, 34, 0, 26, 78, 37, 87, 166, 105, 0, 0, 100, 65, 0, 199, 24, 101, 0, 0, 123, 10, 237, 144, 98, 0, 0, 84, 107, 103, 13, 0, 69, 8, 0, 29, 70, 164, 156, 38, 78, 33, 8, 54, 0, 147, 21, 207, 5, 30, 39, 38, 137, 74, 25, 0, 38, 16, 49, 75, 138, 255, 159, 25, 0, 0, 21, 147, 43, 67, 0, 33, 213, 129, 36, 133, 27, 0, 0, 248, 0, 116, 54, 0, 2, 56, 45, 40, 7, 0, 58, 103, 149, 30, 73, 3, 120, 0, 18, 42, 27, 17, 174, 0, 59, 0, 255, 176, 52, 103, 12, 20, 0, 151, 46, 0, 46, 0, 0, 0, 35, 4, 48, 29, 13, 84, 28, 168, 13, 0, 45, 0, 0, 48, 0, 74, 127, 120, 0, 0, 45, 87, 0, 142, 22, 82, 6, 0, 156, 43, 237, 159, 148, 0, 0, 81, 63, 89, 16, 0, 81, 77, 0, 51, 6, 144, 28, 0, 25, 2, 0, 25, 0, 168, 21, 196, 38, 31, 73, 69, 76, 0, 11, 24, 60, 4, 39, 80, 121, 222, 255, 1, 0, 0, 42, 151, 0, 93, 5, 46, 255, 115, 27, 170, 27, 40, 15, 240, 28, 91, 44, 0, 5, 153, 39, 4, 7, 0, 50, 65, 102, 0, 79, 3, 202, 0, 4, 124, 36, 9, 213, 0, 35, 0, 255, 193, 69, 128, 12, 52, 0, 30, 30, 0, 7, 0, 62, 0, 61, 40, 49, 13, 55, 72, 51, 172, 13, 0, 0, 14, 38, 46, 0, 49, 163, 41, 0, 0, 91, 136, 37, 122, 20, 105, 14, 34, 150, 28, 237, 133, 115, 0, 0, 75, 136, 103, 35, 0, 78, 52, 0, 36, 0, 178, 58, 0, 18, 25, 0, 39, 31, 177, 36, 168, 36, 37, 46, 65, 145, 0, 21, 0, 29, 4, 44, 79, 109, 255, 255, 1, 0, 0, 76, 151, 0, 93, 15, 53, 255, 116, 53, 146, 31, 0, 0, 249, 0, 103, 0, 0, 2, 106, 26, 0, 7, 0, 21, 104, 176, 0, 79, 3, 182, 9, 8, 213, 29, 23, 194, 5, 51, 0, 226, 216, 74, 93, 12, 1, 0, 141, 23, 0, 11, 0, 54, 0, 1, 7, 55, 10, 9, 59, 2, 134, 13, 0, 41, 0, 25, 49, 0, 94, 253, 219, 0, 0, 28, 145, 0, 199, 0, 103, 9, 0, 104, 17, 237, 131, 169, 0, 0, 74, 113, 97, 40, 0, 88, 26, 0, 32, 16, 157, 96, 0, 17, 33, 0, 50, 0, 164, 26, 207, 20, 58, 46, 55, 152, 27, 17, 47, 5, 9, 93, 182, 139, 239, 255, 0, 0, 0, 0, 132, 11, 93, 0, 11, 234, 124, 31, 124, 32, 47, 0, 255, 0, 109, 10, 0, 2, 143, 38, 0, 7, 0, 36, 110, 184, 0, 59, 3, 127, 16, 11, 192, 47, 21, 201, 0, 40, 0, 255, 206, 74, 82, 12, 19, 0, 180, 32, 0, 23, 0, 0, 0, 20, 0, 0, 17, 12, 85, 16, 176, 13, 0, 0, 9, 26, 75, 0, 101, 190, 65, 0, 0, 68, 112, 0, 183, 25, 140, 34, 28, 141, 42, 237, 64, 96, 0, 0, 91, 83, 128, 26, 0, 97, 17, 0, 69, 37, 161, 78, 0, 0, 65, 0, 46, 68, 173, 21, 187, 22, 43, 52, 50, 83, 72, 25, 0, 62, 13, 27, 65, 134, 232, 188, 1, 0, 0, 0, 130, 0, 93, 42, 53, 213, 121, 17, 183, 27, 58, 23, 229, 1, 36, 17, 0, 2, 162, 81, 1, 7, 0, 36, 92, 137, 0, 67, 3, 165, 0, 4, 153, 0, 20, 155, 32, 37, 0, 210, 164, 61, 115, 12, 21, 0, 122, 46, 0, 27, 0, 28, 0, 14, 61, 53, 18, 16, 96, 5, 139, 13, 0, 35, 0, 39, 54, 0, 51, 244, 65, 0, 0, 86, 76, 0, 187, 0, 131, 17, 0, 66, 45, 237, 111, 117, 0, 9, 79, 71, 105, 21, 0, 95, 16, 0, 62, 7, 186, 81, 45, 0, 37, 8, 63, 52, 181, 46, 200, 24, 59, 91, 56, 83, 10, 16, 0, 34, 4, 59, 60, 155, 255, 197, 1, 17, 43, 0, 164, 24, 93, 54, 5, 255, 104, 48, 183, 27, 47, 34, 255, 37, 35, 72, 0, 2, 131, 35, 27, 7, 0, 96, 76, 128, 5, 79, 3, 62, 5, 22, 185, 0, 28, 195, 36, 48, 0, 254, 226, 59, 129, 12, 1, 0, 96, 114, 42, 27, 0, 11, 0, 1, 47, 43, 31, 11, 77, 2, 177, 13, 0, 0, 0, 0, 92, 57, 106, 129, 95, 0, 0, 32, 3, 0, 199, 19, 139, 9, 29, 129, 2, 237, 99, 91, 0, 0, 56, 129, 130, 28, 0, 87, 5, 0, 69, 16, 179, 60, 0, 0, 60, 15, 31, 9, 184, 76, 193, 30, 70, 103, 34, 87, 31, 25, 0, 12, 19, 61, 93, 148, 232, 205, 2, 0, 0, 149, 118, 0, 88, 8, 5, 255, 85, 29, 183, 27, 0, 0, 255, 5, 124, 0, 34, 3, 142, 47, 0, 7, 0, 47, 87, 118, 129, 79, 15, 153, 3, 16, 90, 0, 17, 228, 0, 28, 96, 248, 189, 38, 112, 12, 7, 0, 97, 26, 0, 38, 0, 19, 0, 9, 0, 66, 39, 15, 56, 4, 185, 13, 0, 25, 0, 0, 98, 0, 69, 195, 123, 0, 0, 43, 33, 12, 180, 11, 136, 10, 0, 186, 21, 237, 88, 83, 0, 0, 88, 90, 95, 57, 0, 110, 14, 9, 55, 30, 172, 48, 0, 0, 53, 0, 44, 39, 177, 60, 196, 51, 48, 50, 51, 100, 56, 11, 0, 47, 4, 29, 95, 124, 255, 199, 8, 0, 0, 0, 142, 39, 93, 38, 42, 255, 117, 39, 183, 27, 61, 0, 255, 36, 84, 10, 0, 2, 81, 82, 36, 7, 0, 51, 106, 157, 103, 79, 5, 90, 7, 16, 127, 0, 7, 144, 0, 31, 56, 255, 160, 28, 134, 12, 10, 0, 33, 45, 0, 23, 0, 25, 0, 22, 23, 58, 29, 10, 91, 24, 181, 13, 0, 53, 0, 0, 98, 0, 19, 144, 17, 0, 0, 45, 1, 0, 181, 20, 84, 30, 0, 144, 15, 237, 108, 116, 0, 0, 94, 134, 102, 7, 0, 96, 16, 0, 69, 24, 151, 24, 0, 0, 37, 3, 48, 55, 180, 51, 199, 43, 128, 99, 73, 133, 9, 41, 6, 38, 4, 30, 81, 89, 255, 244, 7, 0, 0, 178, 128, 0, 93, 0, 37, 255, 127, 36, 48, 27, 0, 0, 255, 0, 71, 21, 29, 2, 175, 39, 0, 7, 0, 25, 94, 148, 0, 76, 3, 167, 65, 16, 101, 25, 31, 202, 35, 22, 0, 255, 208, 74, 129, 12, 1, 0, 87, 37, 0, 22, 0, 25, 0, 6, 0, 24, 9, 23, 94, 2, 163, 13, 0, 0, 62, 0, 88, 0, 79, 179, 162, 0, 0, 114, 85, 0, 181, 0, 147, 6, 53, 109, 22, 237, 129, 195, 0, 0, 94, 64, 100, 8, 0, 106, 17, 0, 62, 14, 155, 103, 0, 8, 72, 0, 62, 63, 184, 40, 202, 57, 110, 96, 63, 87, 0, 11, 0, 40, 4, 62, 40, 140, 255, 255, 1, 0, 0, 0, 141, 0, 93, 8, 9, 255, 127, 109, 111, 65, 44, 18, 255, 0, 159, 0, 0, 2, 101, 90, 48, 7, 0, 47, 101, 149, 58, 75, 3, 137, 11, 68, 181, 44, 21, 190, 0, 29, 0, 255, 211, 64, 103, 12, 1, 0, 31, 115, 0, 42, 0, 9, 12, 1, 115, 85, 16, 49, 83, 10, 151, 13, 0, 92, 0, 0, 68, 44, 86, 163, 83, 0, 0, 75, 76, 0, 195, 51, 164, 5, 0, 68, 13, 237, 132, 199, 0, 19, 89, 88, 125, 10, 0, 67, 45, 0, 55, 46, 135, 34, 0, 87, 75, 37, 45, 17, 158, 21, 177, 25, 23, 82, 78, 144, 0, 52, 0, 13, 76, 58, 47, 119, 255, 243, 7, 136, 0, 0, 146, 50, 62, 13, 13, 234, 114, 54, 183, 27, 22, 6, 255, 13, 121, 0, 0, 2, 109, 74, 65, 7, 0, 136, 110, 139, 0, 79, 3, 144, 11, 4, 255, 48, 24, 205, 0, 31, 0, 233, 217, 62, 125, 12, 1, 0, 58, 104, 0, 28, 0, 7, 0, 24, 57, 0, 12, 30, 99, 44, 170, 13, 0, 0, 113, 3, 51, 0, 47, 255, 133, 0, 0, 53, 104, 0, 195, 15, 74, 31, 129, 83, 45, 237, 140, 109, 0, 0, 66, 118, 79, 8, 0, 99, 5, 0, 0, 0, 165, 33, 0, 26, 9, 3, 55, 46, 163, 33, 182, 28, 80, 61, 78, 107, 64, 11, 0, 75, 4, 40, 57, 79, 255, 255, 5, 0, 54, 29, 169, 0, 93, 0, 19, 235, 117, 33, 175, 27, 48, 0, 255, 0, 120, 44, 0, 2, 166, 43, 21, 7, 0, 82, 91, 160, 0, 79, 3, 130, 0, 28, 16, 19, 19, 190, 13, 50, 0, 253, 188, 28, 137, 12, 1, 0, 75, 40, 0, 9, 13, 49, 0, 45, 0, 0, 18, 49, 43, 12, 195, 13, 0, 71, 0, 0, 99, 46, 113, 170, 54, 0, 0, 28, 114, 0, 166, 34, 114, 10, 0, 107, 22, 237, 94, 160, 0, 0, 54, 55, 92, 4, 0, 91, 5, 0, 13, 0, 159, 101, 28, 0, 14, 61, 28, 68, 166, 167, 203, 0, 76, 89, 21, 90, 0, 14, 0, 5, 4, 82, 69, 107, 255, 239, 0, 52, 0, 125, 157, 5, 93, 4, 12, 250, 111, 12, 169, 27, 96, 0, 202, 27, 41, 74, 0, 2, 95, 35, 0, 7, 10, 59, 74, 167, 0, 79, 3, 205, 0, 17, 228, 0, 34, 204, 35, 55, 0, 255, 165, 0, 111, 12, 1, 0, 102, 57, 0, 8, 0, 0, 0, 1, 49, 0, 47, 6, 19, 2, 150, 13, 0, 7, 0, 0, 99, 23, 105, 161, 1, 0, 0, 28, 66, 0, 199, 57, 132, 0, 0, 129, 24, 237, 142, 106, 0, 0, 54, 137, 113, 39, 0, 98, 17, 0, 65, 3, 157, 86, 0, 14, 64, 0, 46, 0, 147, 21, 166, 48, 100, 78, 39, 161, 54, 16, 0, 30, 4, 52, 74, 135, 255, 224, 0, 2, 0, 0, 98, 0, 83, 4, 7, 255, 95, 39, 183, 27, 0, 0, 255, 0, 127, 0, 42, 21, 120, 17, 2, 7, 0, 39, 84, 172, 94, 74, 3, 115, 5, 4, 117, 0, 25, 255, 0, 33, 6, 255, 252, 74, 123, 12, 1, 0, 102, 61, 0, 49, 0, 0, 0, 6, 0, 28, 22, 18, 24, 2, 206, 13, 0, 0, 39, 0, 139, 0, 108, 165, 101, 0, 0, 103, 51, 0, 199, 32, 143, 0, 30, 114, 19, 237, 167, 69, 0, 0, 94, 60, 110, 4, 0, 72, 22, 16, 43, 14, 178, 32, 0, 58, 0, 23, 39, 68, 178, 21, 188, 0, 0, 65, 43, 188, 77, 31, 0, 5, 4, 96, 32, 115, 255, 255, 0, 54, 23, 3, 169, 128, 77, 0, 23, 195, 63, 22, 113, 27, 112, 0, 176, 35, 38, 126, 0, 2, 72, 32, 64, 7, 0, 33, 117, 189, 11, 0, 3, 59, 11, 4, 122, 24, 7, 246, 2, 91, 11, 228, 185, 21, 131, 12, 84, 0, 32, 35, 0, 101, 0, 0, 0, 31, 68, 79, 39, 13, 32, 2, 131, 35, 0, 92, 0, 136, 73, 0, 115, 147, 1, 0, 0, 28, 26, 0, 93, 81, 56, 41, 0, 182, 63, 237, 239, 117, 0, 0, 77, 149, 111, 4, 0, 105, 25, 0, 44, 22, 176, 6, 0, 19, 9, 51, 24, 51, 159, 153, 177, 22, 57, 39, 26, 69, 50, 76, 0, 39, 4, 43, 44, 68, 255, 255, 1, 41, 0, 31, 146, 67, 93, 12, 72, 239, 134, 2, 128, 27, 0, 23, 255, 0, 143, 27, 0, 8, 58, 38, 0, 7, 0, 72, 50, 176, 65, 71, 3, 109, 21, 16, 83, 0, 57, 197, 19, 68, 5, 252, 255, 33, 130, 12, 41, 0, 182, 35, 0, 243, 0, 81, 0, 69, 0, 0, 39, 15, 39, 32, 172, 24, 0, 0, 0, 0, 81, 26, 103, 160, 141, 0, 0, 30, 108, 0, 199, 27, 130, 11, 43, 146, 14, 237, 156, 117, 25, 0, 71, 180, 101, 62, 0, 97, 11, 0, 51, 18, 172, 86, 0, 0, 18, 0, 59, 0, 181, 26, 180, 48, 43, 100, 15, 90, 77, 47, 0, 55, 4, 43, 156, 114, 255, 163, 19, 0, 0, 4, 140, 0, 93, 0, 50, 187, 129, 25, 165, 27, 0, 0, 255, 0, 214, 5, 0, 56, 161, 17, 0, 7, 45, 67, 106, 119, 0, 79, 3, 112, 36, 4, 16, 0, 12, 196, 0, 21, 7, 248, 175, 57, 129, 12, 1, 0, 109, 61, 0, 43, 0, 18, 0, 53, 2, 0, 24, 29, 47, 2, 189, 13, 0, 17, 0, 94, 93, 0, 85, 166, 155, 0, 0, 52, 103, 0, 199, 35, 108, 16, 14, 123, 0, 237, 110, 145, 0, 0, 63, 92, 89, 16, 0, 83, 5, 0, 26, 32, 154, 93, 0, 0, 37, 0, 54, 0, 152, 29, 162, 32, 96, 57, 22, 92, 61, 80, 0, 65, 4, 65, 50, 71, 255, 167, 14, 51, 0, 214, 169, 68, 62, 8, 70, 193, 139, 17, 183, 27, 5, 0, 241, 0, 91, 31, 0, 44, 132, 23, 34, 7, 0, 14, 103, 150, 8, 58, 3, 143, 0, 11, 89, 79, 34, 198, 22, 26, 0, 230, 218, 23, 115, 12, 33, 0, 108, 51, 0, 33, 0, 0, 0, 34, 0, 0, 24, 40, 38, 2, 220, 13, 0, 13, 0, 0, 65, 0, 35, 202, 90, 0, 0, 48, 91, 0, 199, 18, 154, 3, 0, 207, 23, 237, 121, 64, 0, 0, 51, 125, 108, 17, 0, 66, 22, 16, 37, 27, 163, 72, 0, 2, 45, 0, 55, 0, 163, 32, 143, 17, 69, 66, 62, 89, 65, 11, 0, 81, 4, 51, 83, 82, 219, 255, 43, 0, 17, 0, 130, 0, 93, 0, 21, 222, 135, 41, 130, 27, 31, 12, 255, 0, 16, 31, 0, 10, 126, 46, 20, 7, 0, 24, 91, 147, 0, 79, 3, 198, 1, 37, 217, 41, 9, 149, 0, 31, 0, 255, 183, 39, 104, 12, 1, 0, 130, 94, 0, 49, 0, 42, 0, 6, 0, 0, 62, 10, 68, 7, 133, 13, 0, 30, 0, 10, 42, 0, 79, 131, 59, 0, 0, 45, 105, 0, 196, 56, 124, 14, 0, 133, 42, 237, 109, 116, 0, 0, 0, 96, 113, 19, 0, 85, 61, 18, 69, 22, 179, 61, 0, 20, 50, 6, 56, 0, 147, 21, 179, 7, 57, 91, 42, 147, 47, 11, 79, 58, 19, 21, 32, 86, 255, 201, 18, 2, 37, 182, 99, 0, 91, 0, 5, 251, 131, 32, 164, 34, 44, 0, 233, 2, 30, 74, 29, 11, 102, 66, 7, 7, 0, 97, 99, 58, 0, 79, 3, 110, 17, 4, 48, 13, 11, 174, 56, 23, 0, 250, 138, 58, 140, 12, 1, 0, 56, 54, 0, 25, 0, 7, 0, 12, 0, 83, 39, 39, 64, 15, 168, 13, 0, 0, 0, 0, 63, 0, 40, 218, 28, 0, 0, 37, 83, 0, 193, 37, 79, 19, 73, 137, 9, 237, 104, 54, 0, 0, 94, 161, 112, 15, 0, 116, 49, 0, 11, 35, 142, 70, 0, 3, 83, 43, 50, 0, 148, 26, 196, 44, 47, 68, 38, 107, 39, 36, 20, 22, 32, 31, 77, 131, 214, 165, 23, 0, 47, 83, 126, 0, 87, 27, 19, 196, 115, 43, 155, 36, 18, 0, 255, 0, 122, 45, 32, 11, 98, 84, 7, 7, 20, 47, 100, 161, 0, 79, 3, 202, 31, 4, 81, 0, 15, 165, 0, 27, 81, 253, 212, 59, 130, 12, 1, 0, 125, 91, 0, 2, 0, 0, 0, 63, 0, 0, 93, 16, 110, 21, 133, 13, 0, 155, 0, 3, 16, 0, 71, 166, 112, 0, 0, 88, 96, 40, 199, 11, 85, 17, 0, 153, 28, 237, 70, 86, 0, 0, 57, 130, 117, 60, 0, 58, 5, 5, 69, 33, 87, 50, 0, 0, 58, 0, 40, 0, 185, 29, 174, 44, 34, 48, 0, 133, 92, 44, 18, 101, 4, 53, 35, 93, 197, 255, 20, 0, 0, 53, 109, 34, 58, 0, 21, 188, 110, 32, 173, 27, 0, 0, 255, 0, 133, 0, 0, 9, 96, 54, 81, 7, 0, 63, 100, 117, 0, 18, 3, 167, 47, 16, 140, 0, 7, 200, 0, 28, 0, 255, 158, 74, 82, 12, 58, 0, 75, 72, 0, 14, 0, 0, 0, 63, 0, 87, 38, 15, 90, 48, 192, 13, 0, 1, 131, 0, 120, 0, 61, 170, 113, 0, 0, 115, 112, 0, 199, 14, 94, 47, 30, 142, 40, 237, 101, 108, 0, 0, 51, 95, 96, 12, 0, 69, 13, 0, 67, 76, 120, 24, 42, 0, 86, 50, 51, 68, 146, 65, 107, 16, 34, 48, 3, 89, 58, 21, 0, 26, 64, 15, 33, 151, 212, 184, 66, 35, 15, 44, 135, 0, 93, 20, 27, 209, 120, 6, 153, 27, 0, 0, 255, 0, 104, 0, 36, 2, 142, 84, 37, 7, 0, 127, 80, 96, 0, 79, 3, 145, 98, 4, 86, 0, 7, 124, 0, 23, 69, 255, 171, 74, 73, 12, 41, 0, 106, 27, 0, 55, 0, 44, 0, 110, 26, 57, 6, 17, 100, 8, 168, 13, 0, 32, 66, 40, 97, 49, 16, 224, 101, 0, 0, 99, 149, 0, 193, 29, 117, 40, 35, 160, 30, 237, 73, 197, 0, 0, 39, 102, 110, 28, 0, 92, 5, 0, 42, 0, 155, 61, 0, 7, 55, 56, 36, 55, 169, 68, 205, 29, 62, 114, 0, 167, 0, 11, 0, 59, 4, 57, 88, 135, 245, 255, 3, 10, 0, 0, 157, 33, 93, 0, 18, 158, 69, 49, 167, 27, 22, 0, 249, 0, 107, 38, 0, 17, 152, 27, 28, 7, 0, 26, 105, 119, 0, 73, 3, 163, 2, 4, 91, 0, 7, 171, 0, 25, 44, 255, 156, 74, 135, 12, 5, 0, 96, 75, 0, 5, 0, 13, 0, 3, 0, 0, 51, 11, 57, 6, 213, 13, 0, 0, 0, 0, 119, 0, 77, 173, 121, 0, 0, 58, 96, 0, 199, 10, 118, 10, 0, 125, 14, 237, 117, 153, 0, 12, 77, 73, 86, 34, 0, 62, 22, 0, 69, 12, 164, 107, 0, 15, 68, 0, 48, 0, 157, 33, 205, 6, 87, 22, 56, 129, 0, 11, 39, 12, 19, 84, 42, 98, 255, 217, 1, 0, 37, 103, 137, 24, 64, 0, 15, 255, 92, 32, 159, 27, 0, 0, 205, 23, 167, 33, 0, 40, 91, 55, 75, 7, 0, 79, 91, 176, 0, 71, 6, 112, 25, 16, 155, 0, 27, 255, 4, 36, 0, 255, 196, 65, 106, 23, 6, 0, 47, 57, 48, 68, 0, 0, 0, 31, 38, 13, 29, 59, 80, 9, 183, 13, 0, 0, 10, 0, 90, 0, 64, 169, 194, 0, 0, 79, 89, 0, 199, 16, 101, 1, 10, 74, 45, 237, 99, 86, 0, 0, 94, 63, 129, 13, 0, 85, 5, 0, 13, 20, 182, 16, 0, 13, 63, 0, 63, 46, 149, 21, 205, 0, 32, 76, 71, 95, 92, 17, 11, 13, 34, 42, 95, 54, 255, 211, 17, 0, 18, 0, 160, 9, 38, 0, 14, 240, 107, 11, 114, 27, 8, 0, 240, 12, 18, 62, 0, 14, 124, 62, 0, 7, 41, 93, 68, 184, 0, 32, 7, 179, 0, 15, 75, 0, 15, 154, 65, 52, 19, 195, 165, 59, 129, 12, 1, 0, 70, 40, 0, 31, 0, 49, 0, 22, 14, 58, 4, 50, 79, 2, 179, 13, 0, 65, 0, 0, 44, 0, 88, 212, 14, 0, 0, 37, 89, 0, 125, 39, 126, 4, 0, 166, 21, 237, 128, 142, 0, 0, 94, 74, 97, 4, 0, 92, 37, 0, 22, 5, 173, 90, 0, 87, 19, 0, 59, 0, 120, 21, 205, 47, 46, 41, 75, 97, 57, 25, 0, 25, 8, 75, 128, 128, 255, 245, 3, 46, 0, 0, 118, 14, 93, 0, 44, 255, 106, 84, 119, 27, 0, 0, 242, 23, 10, 37, 1, 6, 114, 35, 0, 7, 0, 79, 55, 103, 0, 22, 3, 83, 27, 4, 129, 0, 17, 167, 0, 58, 58, 255, 198, 0, 135, 12, 1, 0, 177, 71, 0, 43, 0, 0, 0, 32, 4, 0, 54, 2, 51, 7, 141, 13, 0, 0, 0, 0, 29, 16, 124, 104, 26, 0, 0, 55, 96, 0, 96, 73, 191, 22, 6, 55, 31, 237, 103, 139, 0, 0, 85, 116, 146, 4, 0, 104, 12, 0, 22, 28, 174, 54, 0, 39, 52, 10, 77, 3, 132, 21, 188, 75, 20, 56, 54, 131, 30, 16, 32, 54, 4, 80, 96, 126, 251, 156, 1, 0, 0, 51, 152, 0, 89, 12, 54, 255, 129, 81, 128, 27, 10, 0, 207, 9, 61, 67, 0, 7, 108, 83, 0, 7, 0, 42, 65, 118, 0, 72, 3, 159, 0, 4, 151, 0, 21, 138, 2, 41, 58, 255, 171, 0, 122, 12, 1, 0, 102, 45, 55, 13, 0, 0, 0, 16, 0, 29, 19, 17, 85, 18, 121, 13, 0, 0, 0, 0, 0, 0, 132, 157, 81, 0, 0, 28, 97, 4, 132, 99, 119, 14, 82, 93, 17, 237, 93, 98, 0, 0, 82, 86, 136, 4, 0, 94, 41, 0, 69, 22, 172, 176, 0, 11, 103, 60, 124, 51, 141, 21, 207, 25, 50, 85, 65, 85, 71, 11, 0, 48, 4, 26, 69, 184, 237, 132, 13, 0, 0, 3, 164, 0, 77, 20, 14, 245, 96, 67, 176, 27, 0, 0, 230, 0, 75, 28, 0, 2, 170, 141, 0, 7, 0, 31, 103, 122, 0, 71, 3, 177, 1, 4, 255, 0, 48, 181, 0, 48, 28, 230, 182, 56, 104, 12, 1, 0, 92, 43, 0, 22, 0, 0, 0, 19, 76, 63, 56, 1, 120, 34, 117, 13, 0, 99, 0, 42, 13, 0, 86, 201, 95, 0, 0, 74, 110, 0, 107, 15, 85, 29, 0, 117, 59, 237, 104, 122, 0, 0, 82, 87, 103, 4, 0, 84, 35, 0, 64, 19, 158, 44, 0, 44, 42, 52, 32, 68, 157, 47, 207, 41, 56, 106, 49, 101, 50, 11, 20, 9, 4, 98, 105, 152, 235, 255, 27, 53, 1, 0, 155, 86, 93, 7, 29, 255, 145, 111, 171, 27, 0, 0, 221, 0, 70, 58, 0, 2, 119, 66, 0, 7, 0, 72, 63, 102, 0, 79, 3, 193, 37, 4, 131, 0, 65, 179, 5, 50, 46, 255, 220, 64, 137, 12, 36, 0, 72, 146, 46, 10, 0, 25, 0, 29, 0, 14, 102, 4, 159, 86, 140, 13, 0, 0, 0, 0, 39, 0, 90, 154, 149, 0, 0, 83, 153, 0, 193, 7, 124, 6, 32, 101, 27, 237, 122, 114, 0, 1, 83, 99, 78, 10, 0, 112, 64, 19, 28, 0, 156, 14, 0, 17, 56, 30, 27, 43, 90, 58, 207, 83, 30, 66, 71, 99, 92, 11, 0, 69, 4, 109, 63, 70, 255, 194, 6, 0, 36, 255, 151, 0, 73, 14, 9, 255, 139, 32, 97, 31, 0, 25, 197, 18, 94, 50, 0, 27, 148, 79, 0, 7, 0, 47, 96, 141, 0, 67, 3, 214, 33, 4, 87, 0, 27, 205, 0, 59, 11, 255, 142, 0, 137, 12, 48, 0, 84, 41, 51, 21, 0, 0, 0, 1, 1, 56, 12, 28, 30, 68, 131, 13, 0, 68, 0, 0, 0, 0, 96, 102, 189, 0, 0, 28, 64, 0, 199, 86, 94, 2, 35, 80, 21, 237, 122, 180, 0, 0, 94, 87, 133, 43, 0, 84, 58, 0, 25, 15, 177, 70, 0, 21, 31, 21, 52, 57, 172, 25, 153, 30, 29, 54, 45, 117, 19, 11, 34, 15, 4, 87, 155, 82, 255, 255, 0, 0, 0, 10, 114, 63, 54, 10, 29, 255, 125, 28, 183, 34, 0, 22, 212, 0, 74, 69, 0, 2, 59, 53, 0, 7, 0, 108, 79, 177, 42, 55, 3, 224, 0, 4, 217, 0, 29, 252, 34, 72, 38, 248, 200, 37, 140, 12, 27, 0, 199, 46, 0, 36, 0, 0, 0, 5, 0, 0, 51, 15, 44, 15, 116, 13, 0, 6, 0, 0, 58, 0, 87, 180, 80, 0, 0, 37, 126, 0, 185, 50, 146, 17, 0, 142, 46, 237, 126, 68, 0, 0, 94, 81, 113, 17, 0, 75, 31, 0, 69, 55, 173, 134, 0, 0, 27, 0, 77, 68, 171, 21, 172, 26, 28, 13, 56, 97, 56, 16, 0, 59, 4, 25, 55, 142, 255, 255, 16, 6, 0, 0, 158, 6, 93, 63, 27, 255, 92, 22, 178, 27, 0, 79, 205, 3, 87, 3, 25, 5, 115, 70, 20, 7, 0, 64, 100, 154, 0, 79, 3, 138, 0, 4, 255, 0, 7, 203, 0, 83, 93, 253, 193, 59, 140, 12, 56, 0, 157, 41, 0, 19, 0, 0, 0, 13, 24, 23, 7, 20, 79, 5, 131, 13, 0, 0, 0, 0, 43, 0, 72, 187, 95, 0, 0, 97, 47, 32, 199, 8, 77, 34, 11, 118, 74, 237, 132, 89, 0, 0, 91, 118, 68, 10, 0, 91, 29, 0, 27, 5, 178, 135, 36, 0, 65, 0, 69, 14, 182, 63, 207, 34, 72, 56, 71, 86, 18, 17, 0, 7, 4, 19, 61, 131, 255, 201, 0, 80, 0, 83, 156, 0, 83, 0, 11, 234, 109, 11, 176, 27, 0, 0, 255, 0, 128, 0, 0, 21, 134, 82, 0, 7, 0, 49, 100, 167, 0, 79, 3, 158, 36, 4, 71, 53, 9, 199, 0, 39, 0, 255, 145, 57, 140, 12, 104, 0, 222, 39, 101, 63, 0, 0, 0, 1, 0, 30, 35, 3, 42, 2, 151, 13, 0, 36, 0, 19, 53, 0, 40, 166, 130, 0, 0, 34, 58, 0, 193, 60, 165, 1, 12, 152, 65, 220, 110, 174, 0, 2, 77, 80, 121, 29, 0, 100, 45, 0, 0, 12, 158, 70, 0, 48, 75, 17, 36, 43, 162, 70, 207, 91, 43, 77, 16, 127, 0, 11, 0, 46, 30, 51, 82, 174, 226, 190, 0, 68, 0, 0, 152, 23, 85, 0, 18, 255, 67, 75, 153, 27, 0, 0, 255, 0, 91, 0, 0, 33, 119, 45, 4, 7, 0, 69, 110, 169, 0, 79, 22, 164, 43, 4, 147, 85, 11, 208, 0, 40, 0, 252, 255, 28, 84, 12, 5, 0, 93, 50, 0, 41, 0, 0, 0, 13, 18, 50, 77, 1, 109, 10, 146, 13, 0, 0, 111, 40, 18, 0, 140, 167, 123, 0, 0, 66, 101, 29, 178, 28, 136, 0, 108, 61, 25, 237, 91, 98, 0, 0, 78, 147, 123, 4, 0, 85, 5, 0, 69, 31, 179, 52, 8, 41, 48, 0, 35, 15, 147, 21, 184, 33, 79, 93, 23, 179, 53, 11, 0, 25, 4, 40, 129, 115, 242, 255, 35, 0, 0, 11, 143, 0, 93, 0, 40, 255, 121, 65, 101, 27, 0, 0, 255, 0, 119, 0, 91, 2, 181, 56, 0, 7, 0, 86, 103, 105, 0, 79, 5, 141, 1, 13, 99, 16, 20, 211, 0, 41, 0, 254, 207, 39, 88, 12, 6, 0, 133, 54, 0, 36, 0, 0, 0, 39, 0, 24, 4, 54, 63, 17, 143, 13, 0, 66, 0, 0, 107, 60, 107, 194, 102, 0, 0, 141, 59, 32, 191, 0, 123, 46, 0, 75, 0, 237, 117, 173, 0, 0, 94, 68, 100, 83, 0, 83, 5, 0, 65, 6, 137, 74, 30, 44, 55, 0, 59, 68, 177, 21, 202, 53, 53, 62, 64, 210, 0, 11, 0, 54, 4, 55, 45, 93, 246, 255, 15, 15, 0, 72, 154, 20, 62, 0, 22, 255, 129, 33, 116, 27, 0, 0, 237, 0, 108, 77, 0, 12, 75, 24, 0, 7, 0, 53, 107, 161, 0, 69, 3, 154, 0, 4, 217, 30, 28, 241, 32, 59, 0, 255, 251, 74, 138, 12, 17, 0, 91, 115, 0, 14, 0, 0, 0, 1, 39, 22, 41, 34, 92, 5, 197, 13, 0, 0, 0, 6, 95, 0, 101, 159, 70, 0, 0, 37, 86, 0, 172, 19, 115, 0, 55, 145, 25, 237, 114, 152, 0, 0, 94, 110, 103, 28, 0, 89, 22, 6, 60, 16, 157, 40, 0, 4, 20, 3, 59, 0, 157, 62, 207, 33, 9, 52, 71, 80, 0, 11, 0, 90, 4, 54, 51, 108, 248, 249, 1, 4, 11, 0, 118, 66, 93, 33, 24, 228, 135, 47, 133, 37, 54, 54, 255, 70, 138, 61, 0, 2, 100, 126, 42, 7, 26, 93, 60, 185, 0, 73, 3, 106, 18, 4, 197, 0, 11, 169, 0, 78, 79, 255, 186, 74, 121, 18, 1, 0, 42, 77, 0, 24, 0, 0, 0, 50, 115, 95, 26, 44, 104, 43, 182, 13, 0, 38, 8, 0, 86, 53, 47, 205, 69, 0, 0, 34, 92, 17, 196, 23, 70, 63, 0, 140, 37, 237, 119, 63, 37, 0, 89, 147, 83, 39, 0, 112, 27, 0, 69, 0, 122, 37, 0, 0, 60, 37, 28, 0, 103, 21, 207, 21, 42, 71, 78, 127, 72, 31, 0, 16, 4, 34, 76, 43, 233, 174, 0, 20, 65, 58, 122, 0, 41, 57, 21, 212, 108, 9, 162, 27, 0, 0, 255, 39, 197, 13, 0, 2, 142, 84, 0, 7, 0, 17, 71, 184, 84, 65, 3, 130, 37, 17, 16, 0, 14, 218, 83, 27, 0, 236, 130, 74, 85, 22, 1, 0, 41, 25, 0, 27, 0, 0, 0, 19, 3, 56, 7, 50, 55, 75, 229, 13, 0, 35, 0, 6, 121, 0, 46, 183, 246, 0, 0, 37, 67, 0, 166, 63, 111, 36, 0, 143, 26, 237, 105, 136, 0, 0, 89, 179, 100, 17, 0, 92, 40, 0, 63, 34, 96, 28, 0, 0, 20, 24, 16, 0, 173, 34, 173, 44, 81, 59, 78, 87, 0, 11, 0, 26, 4, 66, 125, 93, 227, 184, 24, 0, 0, 84, 160, 13, 38, 35, 19, 199, 145, 40, 173, 27, 0, 0, 255, 0, 155, 4, 0, 10, 76, 27, 34, 7, 0, 60, 58, 181, 0, 64, 3, 150, 0, 11, 71, 0, 7, 209, 35, 45, 0, 237, 150, 74, 80, 12, 1, 0, 130, 55, 76, 18, 0, 39, 0, 52, 0, 19, 16, 27, 49, 14, 189, 13, 0, 11, 0, 0, 111, 0, 78, 162, 255, 0, 0, 53, 103, 0, 161, 48, 159, 5, 0, 111, 8, 237, 132, 178, 0, 0, 58, 78, 104, 12, 0, 113, 39, 0, 69, 4, 121, 34, 0, 0, 27, 62, 20, 30, 160, 21, 201, 52, 43, 84, 45, 97, 0, 16, 0, 10, 4, 81, 117, 75, 253, 175, 12, 0, 0, 0, 169, 0, 93, 0, 7, 234, 145, 12, 171, 27, 10, 0, 248, 64, 52, 0, 38, 12, 165, 44, 0, 7, 0, 90, 86, 138, 0, 72, 3, 177, 53, 22, 154, 0, 18, 215, 0, 57, 60, 233, 246, 57, 92, 12, 1, 0, 160, 47, 15, 34, 0, 34, 0, 31, 17, 43, 15, 40, 48, 16, 182, 13, 0, 0, 168, 0, 91, 4, 92, 202, 1, 0, 0, 28, 97, 0, 199, 87, 178, 10, 11, 131, 36, 237, 133, 147, 0, 18, 49, 79, 98, 51, 0, 70, 15, 0, 69, 39, 169, 34, 0, 27, 33, 0, 49, 0, 184, 21, 196, 46, 33, 96, 8, 165, 83, 24, 0, 77, 4, 90, 60, 97, 255, 185, 39, 0, 0, 172, 135, 92, 93, 0, 12, 254, 145, 40, 170, 27, 9, 15, 197, 42, 6, 34, 0, 2, 81, 46, 7, 7, 0, 76, 107, 136, 0, 71, 3, 234, 48, 4, 147, 34, 24, 255, 0, 34, 0, 255, 206, 27, 108, 12, 1, 0, 131, 81, 0, 21, 0, 50, 0, 6, 0, 14, 50, 11, 46, 20, 173, 13, 0, 0, 0, 0, 64, 0, 71, 124, 40, 0, 0, 62, 73, 0, 199, 61, 117, 38, 25, 78, 50, 237, 229, 108, 0, 0, 85, 22, 98, 4, 0, 72, 15, 24, 55, 30, 176, 92, 0, 21, 7, 0, 104, 25, 184, 21, 172, 31, 0, 70, 63, 174, 29, 11, 0, 27, 4, 103, 28, 137, 255, 255, 31, 25, 0, 0, 169, 63, 93, 0, 7, 213, 71, 32, 183, 27, 90, 0, 183, 16, 81, 125, 0, 2, 71, 54, 70, 7, 28, 28, 100, 189, 0, 0, 3, 30, 16, 4, 178, 81, 36, 228, 0, 77, 0, 212, 150, 30, 130, 12, 128, 0, 92, 16, 0, 143, 0, 11, 0, 46, 140, 0, 49, 4, 37, 2, 137, 57, 0, 97, 0, 190, 38, 0, 51, 164, 7, 0, 0, 45, 0, 0, 124, 89, 104, 43, 0, 120, 88, 237, 252, 82, 0, 0, 73, 123, 133, 4, 0, 64, 11, 0, 29, 66, 153, 61, 0, 58, 31, 0, 59, 68, 149, 44, 198, 49, 27, 52, 26, 98, 70, 30, 25, 55, 4, 60, 65, 105, 255, 253, 48, 48, 43, 126, 169, 22, 93, 0, 45, 224, 145, 127, 146, 27, 0, 21, 255, 0, 120, 0, 79, 9, 97, 24, 0, 7, 0, 56, 51, 160, 0, 59, 3, 100, 12, 4, 28, 14, 35, 186, 17, 62, 45, 255, 187, 43, 106, 12, 104, 0, 79, 75, 0, 205, 0, 35, 0, 53, 0, 0, 52, 13, 91, 32, 151, 46, 0, 0, 0, 0, 54, 0, 78, 168, 26, 0, 0, 57, 85, 0, 199, 45, 94, 4, 32, 176, 47, 237, 102, 128, 0, 0, 74, 129, 102, 54, 0, 76, 9, 0, 58, 15, 156, 45, 0, 41, 25, 0, 75, 0, 168, 31, 201, 64, 45, 118, 28, 156, 30, 44, 68, 18, 4, 70, 82, 138, 255, 231, 53, 0, 0, 46, 169, 0, 57, 0, 32, 227, 145, 79, 158, 27, 81, 0, 212, 32, 91, 73, 0, 2, 162, 28, 0, 7, 0, 50, 115, 163, 0, 33, 3, 149, 1, 4, 30, 0, 20, 216, 16, 56, 32, 250, 203, 74, 137, 12, 50, 0, 124, 108, 52, 43, 0, 0, 0, 58, 0, 0, 67, 17, 98, 21, 177, 13, 0, 26, 0, 0, 71, 0, 33, 182, 86, 0, 0, 56, 84, 0, 199, 21, 93, 0, 11, 97, 47, 229, 104, 240, 0, 40, 85, 38, 100, 9, 0, 113, 27, 0, 69, 14, 174, 21, 48, 0, 36, 96, 13, 68, 187, 60, 207, 47, 31, 112, 54, 84, 0, 11, 0, 77, 4, 129, 44, 164, 255, 182, 31, 30, 0, 0, 169, 0, 91, 0, 5, 236, 131, 96, 183, 27, 97, 17, 237, 10, 6, 50, 0, 2, 80, 55, 0, 7, 0, 26, 86, 179, 0, 76, 3, 119, 1, 48, 188, 0, 33, 174, 0, 38, 0, 248, 204, 74, 95, 12, 62, 0, 105, 36, 0, 38, 0, 28, 0, 6, 39, 0, 24, 15, 55, 30, 197, 13, 0, 0, 0, 77, 94, 0, 64, 254, 1, 0, 0, 40, 70, 0, 199, 12, 189, 9, 0, 127, 15, 225, 99, 175, 0, 20, 85, 89, 100, 51, 0, 93, 5, 0, 69, 6, 186, 49, 22, 0, 46, 19, 31, 16, 167, 37, 207, 64, 50, 140, 78, 120, 22, 11, 0, 5, 4, 82, 107, 133, 249, 255, 3, 0, 0, 128, 169, 14, 89, 0, 5, 255, 122, 99, 136, 27, 106, 34, 236, 13, 27, 11, 0, 1, 109, 69, 22, 7, 0, 16, 99, 121, 20, 75, 3, 132, 5, 23, 64, 43, 21, 149, 0, 33, 0, 255, 193, 74, 115, 12, 9, 0, 138, 64, 0, 26, 0, 55, 0, 7, 10, 0, 10, 33, 67, 9, 210, 13, 0, 29, 0, 0, 120, 0, 120, 149, 28, 0, 0, 102, 68, 60, 199, 42, 148, 15, 0, 146, 0, 237, 176, 151, 0, 28, 94, 92, 79, 21, 0, 130, 5, 7, 16, 18, 181, 52, 0, 0, 38, 0, 29, 0, 141, 21, 171, 33, 49, 95, 47, 90, 0, 27, 0, 5, 4, 47, 59, 70, 255, 255, 5, 0, 0, 0, 144, 75, 90, 24, 5, 255, 98, 2, 127, 27, 43, 7, 255, 0, 85, 9, 8, 57, 101, 84, 0, 7, 0, 24, 90, 167, 0, 79, 3, 153, 20, 19, 50, 41, 17, 200, 9, 25, 0, 255, 227, 74, 140, 12, 28, 0, 165, 52, 0, 2, 0, 36, 0, 1, 0, 70, 17, 43, 52, 5, 192, 13, 0, 8, 0, 0, 72, 0, 55, 164, 7, 0, 0, 28, 60, 0, 199, 44, 191, 8, 3, 145, 22, 203, 123, 181, 0, 0, 94, 67, 97, 31, 0, 91, 36, 0, 54, 25, 161, 99, 0, 0, 74, 0, 52, 22, 146, 25, 170, 51, 41, 37, 67, 103, 0, 43, 9, 18, 4, 58, 56, 148, 235, 227, 31, 0, 0, 0, 145, 0, 79, 0, 37, 240, 94, 63, 176, 27, 26, 0, 254, 0, 111, 2, 0, 29, 124, 60, 0, 7, 0, 41, 82, 167, 0, 69, 3, 88, 37, 28, 191, 32, 14, 190, 0, 31, 0, 255, 234, 74, 98, 12, 1, 0, 76, 45, 0, 22, 0, 0, 0, 26, 56, 57, 34, 20, 70, 25, 160, 13, 0, 0, 0, 0, 53, 0, 74, 162, 44, 0, 0, 97, 73, 0, 191, 23, 145, 12, 20, 99, 48, 237, 146, 71, 0, 0, 43, 129, 91, 4, 0, 57, 21, 0, 69, 12, 168, 47, 0, 22, 37, 0, 20, 0, 183, 31, 196, 0, 75, 70, 8, 156, 92, 17, 42, 41, 4, 24, 119, 130, 255, 242, 13, 0, 0, 0, 130, 38, 77, 0, 29, 244, 37, 7, 163, 27, 0, 0, 255, 0, 78, 43, 25, 9, 162, 35, 74, 7, 0, 95, 106, 140, 0, 62, 3, 131, 0, 13, 246, 4, 17, 255, 0, 41, 0, 255, 215, 66, 123, 12, 1, 0, 92, 50, 84, 56, 3, 0, 0, 12, 0, 87, 27, 12, 82, 2, 202, 13, 0, 32, 0, 0, 89, 33, 113, 142, 37, 0, 0, 55, 88, 0, 192, 29, 120, 16, 0, 100, 18, 237, 110, 152, 0, 0, 94, 62, 151, 17, 0, 75, 19, 0, 63, 23, 157, 93, 0, 10, 131, 0, 69, 14, 169, 21, 207, 12, 57, 35, 41, 149, 45, 65, 0, 88, 16, 30, 58, 158, 238, 167, 11, 33, 0, 88, 129, 0, 80, 40, 16, 245, 35, 2, 183, 27, 13, 0, 240, 8, 136, 0, 0, 19, 90, 87, 81, 7, 0, 57, 93, 173, 58, 79, 27, 146, 27, 4, 151, 0, 7, 199, 0, 26, 0, 226, 143, 53, 59, 12, 41, 0, 67, 57, 0, 74, 0, 0, 0, 25, 86, 59, 42, 4, 54, 7, 183, 13, 0, 0, 23, 34, 53, 13, 39, 215, 81, 0, 0, 56, 118, 0, 182, 17, 78, 9, 76, 112, 41, 237, 56, 162, 0, 0, 44, 87, 102, 23, 0, 100, 8, 0, 48, 40, 173, 64, 0, 14, 69, 0, 25, 21, 181, 21, 205, 21, 19, 69, 10, 114, 32, 11, 104, 47, 4, 66, 118, 129, 255, 190, 46, 0, 0, 0, 142, 0, 59, 0, 10, 241, 117, 28, 139, 27, 0, 0, 246, 53, 6, 75, 0, 1, 159, 48, 0, 18, 0, 55, 69, 113, 91, 69, 3, 108, 5, 4, 199, 0, 17, 162, 0, 22, 0, 235, 136, 74, 140, 12, 16, 0, 164, 64, 7, 12, 0, 44, 0, 1, 28, 0, 25, 8, 48, 9, 180, 13, 0, 0, 0, 0, 71, 0, 65, 189, 69, 0, 0, 72, 53, 0, 193, 53, 150, 11, 0, 146, 33, 237, 159, 154, 0, 0, 20, 93, 83, 33, 0, 91, 5, 0, 55, 0, 161, 74, 0, 38, 30, 0, 64, 0, 136, 21, 195, 14, 6, 87, 43, 137, 4, 11, 16, 5, 4, 92, 97, 78, 234, 255, 22, 0, 0, 0, 36, 0, 73, 0, 82, 255, 130, 34, 149, 27, 17, 0, 238, 0, 182, 96, 0, 14, 129, 17, 31, 28, 0, 90, 111, 135, 97, 67, 3, 182, 18, 11, 169, 0, 7, 197, 0, 44, 5, 252, 212, 66, 123, 31, 22, 0, 105, 75, 0, 32, 123, 10, 0, 1, 43, 0, 40, 35, 51, 9, 202, 13, 0, 83, 0, 34, 90, 0, 95, 143, 106, 0, 0, 34, 124, 0, 191, 38, 106, 0, 0, 196, 17, 237, 122, 142, 0, 0, 78, 65, 100, 4, 0, 110, 15, 0, 40, 3, 171, 85, 0, 0, 44, 6, 92, 68, 145, 21, 205, 29, 40, 47, 75, 116, 23, 21, 0, 5, 7, 95, 28, 90, 255, 255, 11, 35, 0, 0, 169, 37, 88, 0, 8, 199, 48, 18, 72, 27, 74, 22, 215, 19, 31, 112, 7, 43, 118, 81, 0, 7, 0, 75, 103, 185, 0, 79, 3, 173, 1, 15, 16, 49, 25, 181, 0, 83, 0, 255, 189, 27, 135, 12, 1, 0, 103, 37, 0, 28, 0, 0, 0, 11, 0, 0, 41, 25, 37, 17, 194, 13, 0, 0, 0, 0, 54, 10, 76, 201, 51, 0, 0, 34, 61, 0, 141, 63, 125, 3, 76, 111, 31, 237, 85, 199, 0, 0, 83, 34, 96, 48, 0, 96, 14, 0, 48, 5, 170, 35, 0, 16, 43, 6, 42, 68, 157, 21, 202, 16, 49, 85, 43, 96, 15, 29, 0, 5, 11, 69, 57, 138, 240, 255, 4, 0, 17, 184, 156, 0, 93, 81, 34, 250, 119, 36, 152, 27, 112, 5, 171, 40, 15, 61, 0, 10, 88, 83, 0, 7, 1, 110, 103, 185, 22, 79, 18, 121, 1, 21, 47, 72, 53, 184, 8, 60, 0, 255, 147, 0, 122, 12, 1, 0, 61, 66, 0, 4, 4, 0, 0, 48, 64, 0, 107, 17, 185, 21, 128, 13, 0, 11, 0, 14, 75, 0, 110, 192, 1, 0, 0, 28, 72, 0, 98, 70, 79, 22, 0, 143, 73, 237, 87, 157, 0, 0, 58, 116, 103, 23, 0, 114, 7, 0, 0, 0, 174, 54, 0, 0, 61, 0, 41, 0, 181, 124, 177, 43, 55, 79, 46, 137, 6, 17, 11, 5, 4, 51, 82, 141, 238, 164, 0, 0, 59, 0, 124, 0, 93, 0, 23, 238, 90, 12, 155, 27, 14, 0, 255, 11, 134, 42, 0, 31, 178, 55, 0, 7, 0, 65, 88, 152, 15, 79, 23, 182, 0, 23, 115, 57, 25, 201, 0, 48, 0, 208, 160, 0, 116, 12, 1, 0, 137, 55, 0, 50, 0, 0, 0, 31, 22, 0, 23, 5, 86, 14, 159, 13, 0, 0, 0, 67, 48, 0, 135, 195, 41, 0, 0, 40, 129, 25, 163, 82, 80, 23, 93, 168, 9, 237, 124, 108, 0, 0, 92, 88, 80, 52, 0, 109, 47, 0, 0, 0, 179, 125, 0, 21, 18, 0, 78, 3, 171, 21, 207, 98, 60, 93, 46, 59, 45, 11, 0, 14, 4, 31, 88, 187, 255, 173, 0, 0, 0, 0, 152, 0, 93, 0, 13, 194, 101, 36, 183, 27, 0, 0, 255, 0, 116, 0, 22, 37, 124, 67, 0, 7, 0, 32, 97, 158, 95, 79, 8, 147, 10, 56, 164, 12, 15, 128, 0, 34, 0, 255, 194, 60, 95, 12, 10, 0, 111, 100, 0, 52, 0, 0, 0, 21, 76, 10, 58, 4, 36, 25, 154, 13, 0, 125, 0, 0, 28, 0, 96, 131, 64, 0, 0, 46, 125, 0, 199, 29, 123, 4, 0, 138, 55, 237, 64, 119, 0, 0, 91, 99, 105, 4, 0, 117, 14, 0, 12, 12, 117, 51, 29, 47, 58, 72, 27, 13, 159, 61, 188, 17, 72, 33, 31, 106, 38, 31, 0, 44, 4, 34, 108, 125, 166, 255, 13, 26, 0, 0, 150, 13, 93, 0, 26, 191, 105, 62, 103, 27, 0, 0, 255, 50, 6, 37, 8, 25, 170, 49, 0, 7, 0, 32, 82, 176, 0, 79, 3, 255, 34, 54, 194, 35, 23, 167, 24, 22, 0, 245, 220, 59, 125, 12, 1, 0, 150, 49, 2, 8, 0, 48, 0, 21, 6, 57, 34, 6, 96, 65, 164, 13, 0, 0, 0, 4, 55, 0, 75, 183, 65, 0, 0, 61, 195, 77, 187, 31, 180, 18, 0, 155, 25, 237, 84, 174, 0, 0, 72, 97, 102, 51, 0, 76, 16, 0, 0, 41, 152, 71, 0, 16, 55, 0, 41, 10, 134, 50, 203, 13, 71, 121, 21, 121, 5, 24, 0, 5, 4, 55, 46, 70, 240, 255, 54, 0, 0, 0, 154, 0, 72, 9, 27, 255, 132, 48, 183, 27, 0, 0, 255, 51, 82, 15, 0, 21, 94, 26, 0, 7, 0, 44, 101, 164, 0, 40, 3, 197, 31, 4, 166, 9, 38, 190, 0, 58, 0, 255, 204, 0, 106, 12, 1, 0, 124, 122, 0, 41, 0, 0, 0, 14, 44, 57, 60, 27, 63, 9, 152, 13, 0, 14, 43, 31, 27, 0, 127, 99, 112, 0, 0, 65, 93, 0, 199, 75, 139, 5, 41, 110, 34, 237, 99, 139, 0, 0, 27, 98, 94, 13, 0, 75, 37, 0, 0, 31, 138, 18, 0, 21, 29, 3, 25, 68, 109, 105, 198, 26, 59, 58, 0, 44, 40, 36, 0, 60, 4, 96, 57, 61, 232, 143, 0, 26, 0, 0, 141, 66, 68, 57, 44, 221, 108, 46, 183, 27, 0, 22, 255, 14, 38, 0, 0, 31, 143, 60, 4, 7, 28, 32, 67, 153, 0, 79, 3, 145, 36, 4, 157, 33, 14, 187, 69, 45, 0, 246, 180, 58, 115, 12, 1, 0, 145, 64, 0, 30, 0, 0, 0, 26, 20, 12, 35, 16, 47, 5, 168, 13, 0, 0, 0, 0, 53, 6, 98, 165, 117, 0, 0, 51, 74, 0, 195, 35, 129, 12, 41, 112, 33, 237, 137, 105, 0, 0, 89, 111, 110, 4, 0, 83, 46, 0, 69, 43, 171, 75, 0, 37, 76, 7, 53, 0, 158, 21, 147, 24, 43, 0, 52, 104, 56, 23, 41, 30, 26, 7, 66, 158, 255, 179, 36, 0, 0, 45, 147, 0, 60, 49, 49, 255, 86, 6, 164, 27, 0, 4, 255, 0, 150, 0, 3, 12, 115, 58, 21, 7, 0, 151, 108, 167, 0, 48, 3, 68, 0, 4, 50, 20, 7, 211, 0, 43, 0, 244, 227, 60, 105, 12, 1, 0, 150, 16, 0, 66, 0, 0, 0, 7, 0, 46, 8, 33, 99, 2, 167, 13, 0, 0, 29, 0, 74, 75, 80, 206, 110, 0, 0, 148, 105, 0, 195, 0, 120, 9, 32, 163, 77, 237, 120, 161, 0, 0, 60, 101, 129, 22, 0, 120, 50, 0, 53, 15, 177, 152, 0, 0, 44, 0, 50, 0, 187, 37, 190, 82, 41, 74, 78, 132, 18, 28, 0, 16, 46, 58, 53, 149, 255, 144, 6, 0, 0, 0, 139, 0, 78, 0, 5, 255, 125, 34, 150, 27, 0, 0, 255, 0, 135, 0, 6, 1, 125, 53, 0, 7, 0, 95, 88, 165, 0, 76, 3, 91, 11, 4, 201, 0, 11, 198, 0, 55, 40, 245, 243, 74, 122, 12, 1, 0, 164, 27, 91, 116, 0, 0, 0, 10, 13, 0, 40, 7, 89, 2, 187, 13, 0, 62, 0, 47, 91, 0, 71, 185, 140, 0, 0, 52, 49, 0, 199, 44, 172, 8, 24, 133, 85, 237, 107, 148, 0, 0, 94, 99, 86, 54, 0, 94, 18, 4, 25, 8, 129, 48, 48, 34, 11, 36, 32, 57, 173, 67, 207, 83, 10, 95, 50, 124, 77, 11, 93, 111, 17, 70, 48, 115, 220, 255, 0, 0, 0, 122, 101, 0, 93, 0, 61, 199, 95, 85, 142, 31, 6, 0, 254, 39, 71, 47, 0, 25, 73, 45, 0, 7, 0, 20, 100, 133, 41, 75, 3, 182, 7, 4, 86, 50, 12, 220, 0, 37, 0, 255, 186, 74, 130, 12, 7, 0, 90, 99, 0, 3, 15, 0, 0, 1, 0, 60, 88, 17, 80, 8, 203, 13, 0, 0, 0, 80, 92, 0, 110, 192, 132, 0, 0, 74, 147, 0, 179, 20, 113, 20, 0, 203, 34, 237, 104, 108, 0, 0, 94, 98, 102, 4, 0, 120, 13, 0, 69, 19, 166, 6, 0, 0, 29, 13, 17, 5, 124, 62, 202, 31, 3, 46, 78, 113, 0, 11, 38, 5, 28, 47, 64, 68, 226, 183, 37, 0, 30, 0, 135, 56, 93, 0, 30, 255, 111, 57, 142, 32, 0, 0, 232, 10, 86, 0, 1, 9, 141, 55, 0, 7, 0, 36, 108, 181, 57, 43, 3, 220, 23, 4, 176, 60, 12, 183, 0, 51, 0, 184, 160, 19, 132, 12, 10, 0, 140, 45, 0, 42, 0, 40, 0, 5, 42, 9, 42, 119, 51, 17, 162, 13, 0, 15, 0, 0, 33, 3, 61, 181, 194, 0, 0, 68, 62, 0, 168, 30, 115, 33, 0, 198, 0, 237, 142, 140, 0, 0, 94, 58, 92, 50, 0, 58, 18, 0, 69, 36, 172, 12, 0, 25, 39, 60, 29, 13, 160, 21, 194, 37, 3, 97, 32, 150, 34, 17, 21, 45, 24, 93, 109, 95, 255, 229, 39, 0, 0, 0, 167, 1, 78, 0, 47, 255, 119, 40, 154, 27, 42, 0, 239, 28, 52, 32, 0, 2, 61, 58, 61, 7, 0, 64, 98, 185, 0, 28, 3, 149, 10, 4, 187, 0, 14, 218, 0, 75, 0, 197, 196, 37, 137, 12, 1, 0, 82, 73, 154, 10, 0, 57, 0, 5, 30, 0, 57, 19, 109, 5, 199, 13, 0, 0, 0, 18, 44, 0, 73, 237, 52, 0, 0, 76, 45, 0, 187, 0, 92, 34, 82, 174, 0, 237, 96, 125, 0, 0, 48, 67, 120, 14, 0, 84, 22, 6, 58, 24, 171, 120, 0, 23, 95, 19, 69, 0, 147, 80, 190, 17, 45, 64, 49, 118, 0, 31, 0, 46, 13, 32, 87, 120, 227, 161, 15, 0, 0, 16, 157, 0, 84, 0, 13, 252, 121, 6, 173, 27, 0, 0, 255, 0, 133, 0, 39, 2, 160, 49, 27, 7, 0, 55, 101, 148, 0, 51, 3, 98, 46, 4, 168, 0, 14, 155, 0, 28, 0, 217, 151, 65, 132, 12, 64, 0, 75, 47, 5, 19, 0, 0, 0, 8, 40, 65, 15, 20, 69, 2, 193, 13, 0, 65, 0, 33, 44, 75, 52, 221, 1, 0, 0, 64, 29, 0, 193, 0, 113, 38, 0, 94, 48, 237, 90, 186, 0, 0, 79, 97, 115, 10, 0, 74, 21, 3, 69, 38, 149, 26, 0, 36, 40, 11, 22, 68, 187, 31, 207, 19, 36, 92, 78, 119, 54, 27, 0, 29, 4, 90, 97, 135, 254, 231, 0, 125, 42, 0, 146, 39, 73, 40, 14, 255, 110, 18, 177, 27, 52, 0, 255, 61, 108, 70, 0, 10, 102, 43, 22, 7, 0, 34, 108, 161, 0, 38, 3, 114, 17, 12, 119, 0, 17, 203, 0, 63, 11, 247, 212, 50, 140, 12, 10, 0, 94, 78, 0, 25, 0, 2, 0, 44, 30, 8, 25, 28, 136, 35, 182, 13, 0, 19, 0, 0, 85, 0, 71, 175, 84, 0, 0, 78, 70, 0, 199, 20, 117, 6, 0, 109, 8, 237, 119, 134, 0, 0, 79, 131, 112, 13, 0, 70, 7, 2, 62, 0, 109, 30, 3, 13, 39, 55, 34, 55, 184, 21, 203, 19, 35, 53, 78, 129, 12, 11, 0, 18, 4, 72, 72, 88, 212, 187, 0, 106, 8, 0, 148, 101, 88, 48, 37, 255, 111, 32, 143, 27, 0, 0, 255, 44, 133, 59, 0, 3, 129, 59, 5, 7, 0, 34, 109, 189, 35, 79, 3, 196, 0, 29, 150, 0, 7, 254, 36, 41, 16, 254, 151, 55, 124, 12, 10, 0, 48, 52, 0, 27, 0, 0, 0, 44, 0, 89, 32, 14, 71, 9, 138, 13, 0, 0, 68, 0, 58, 0, 28, 151, 208, 0, 0, 28, 59, 0, 167, 57, 82, 0, 70, 195, 24, 237, 94, 120, 0, 0, 81, 87, 158, 18, 0, 84, 10, 0, 69, 0, 126, 86, 0, 62, 51, 0, 58, 0, 174, 37, 199, 13, 97, 31, 78, 109, 46, 39, 0, 28, 4, 27, 73, 136, 226, 255, 3, 0, 43, 0, 105, 0, 91, 12, 72, 203, 106, 30, 133, 27, 39, 0, 247, 32, 82, 1, 28, 18, 129, 32, 0, 7, 0, 82, 99, 174, 0, 76, 3, 134, 0, 4, 159, 0, 19, 143, 26, 38, 0, 221, 168, 22, 111, 12, 11, 0, 41, 69, 8, 79, 78, 0, 0, 64, 9, 63, 8, 20, 35, 7, 146, 13, 0, 112, 0, 0, 120, 0, 119, 187, 35, 0, 0, 33, 167, 0, 68, 5, 52, 0, 0, 100, 53, 237, 61, 137, 0, 0, 84, 109, 117, 21, 0, 87, 37, 23, 69, 22, 167, 66, 0, 0, 122, 15, 37, 14, 185, 29, 207, 68, 29, 59, 63, 140, 0, 21, 0, 76, 4, 23, 49, 81, 254, 228, 14, 77, 0, 55, 161, 23, 89, 0, 24, 233, 113, 2, 171, 27, 0, 7, 233, 0, 55, 1, 31, 22, 137, 50, 0, 7, 0, 49, 101, 168, 0, 75, 3, 166, 2, 8, 135, 0, 18, 211, 19, 42, 0, 254, 171, 37, 75, 12, 49, 0, 136, 42, 0, 30, 0, 37, 0, 23, 51, 17, 22, 27, 24, 44, 156, 13, 0, 0, 47, 0, 84, 0, 53, 244, 126, 0, 0, 53, 131, 0, 199, 30, 80, 0, 49, 142, 75, 237, 150, 49, 4, 0, 78, 130, 119, 4, 0, 66, 35, 13, 69, 13, 186, 76, 67, 29, 17, 0, 28, 68, 171, 21, 207, 52, 0, 131, 78, 182, 69, 11, 0, 53, 16, 21, 28, 157, 255, 217, 6, 27, 0, 0, 169, 61, 93, 0, 21, 255, 73, 93, 135, 27, 0, 31, 212, 43, 66, 130, 0, 2, 121, 24, 26, 7, 0, 22, 115, 173, 13, 9, 3, 8, 89, 4, 100, 13, 12, 224, 20, 36, 0, 186, 196, 74, 129, 12, 68, 0, 139, 45, 0, 174, 0, 40, 0, 1, 0, 2, 28, 12, 36, 2, 187, 36, 0, 109, 0, 147, 41, 53, 90, 161, 87, 0, 0, 51, 0, 62, 174, 61, 78, 25, 36, 143, 36, 220, 220, 159, 0, 0, 94, 65, 104, 13, 0, 68, 42, 0, 39, 46, 142, 29, 0, 30, 52, 26, 39, 68, 150, 27, 191, 39, 55, 60, 63, 100, 21, 52, 0, 45, 4, 95, 78, 92, 255, 255, 0, 44, 0, 40, 138, 27, 93, 0, 18, 244, 97, 28, 113, 27, 0, 0, 255, 0, 13, 29, 0, 34, 85, 23, 3, 7, 0, 37, 55, 164, 0, 57, 3, 107, 55, 10, 74, 6, 69, 217, 12, 38, 0, 255, 223, 38, 118, 12, 106, 0, 200, 43, 0, 255, 0, 73, 4, 3, 0, 12, 26, 29, 85, 43, 156, 24, 0, 35, 0, 0, 48, 0, 35, 223, 110, 0, 0, 68, 114, 0, 199, 24, 170, 0, 42, 49, 8, 237, 66, 129, 0, 0, 94, 60, 116, 46, 0, 87, 5, 0, 69, 31, 116, 91, 0, 9, 56, 1, 68, 2, 125, 29, 188, 32, 38, 68, 78, 121, 5, 21, 0, 81, 4, 60, 31, 110, 209, 245, 19, 8, 9, 0, 152, 0, 85, 6, 66, 190, 120, 22, 120, 27, 106, 24, 177, 79, 46, 76, 0, 29, 111, 56, 0, 7, 0, 71, 108, 158, 0, 38, 3, 202, 1, 35, 214, 5, 9, 212, 0, 68, 0, 255, 179, 34, 140, 12, 69, 0, 90, 76, 0, 38, 0, 0, 0, 33, 59, 40, 41, 21, 94, 28, 180, 13, 0, 0, 0, 0, 87, 0, 56, 143, 34, 0, 0, 34, 77, 0, 199, 33, 66, 5, 0, 171, 39, 225, 122, 126, 0, 55, 91, 86, 164, 40, 0, 84, 32, 0, 69, 32, 166, 185, 0, 26, 84, 0, 74, 0, 184, 53, 185, 102, 5, 55, 78, 121, 77, 11, 0, 29, 6, 70, 36, 168, 255, 203, 13, 1, 0, 0, 167, 30, 90, 0, 5, 209, 107, 30, 183, 31, 92, 46, 252, 0, 165, 1, 0, 2, 138, 74, 0, 7, 0, 84, 102, 164, 0, 74, 3, 154, 11, 4, 128, 0, 21, 159, 0, 52, 61, 230, 182, 74, 86, 12, 57, 0, 104, 29, 0, 36, 0, 0, 0, 48, 25, 0, 19, 1, 88, 9, 147, 13, 0, 0, 0, 0, 44, 49, 69, 230, 27, 0, 0, 90, 128, 0, 199, 0, 77, 10, 11, 128, 83, 214, 62, 160, 0, 0, 78, 53, 93, 10, 0, 81, 26, 0, 69, 10, 186, 64, 0, 14, 41, 0, 40, 27, 176, 62, 195, 50, 1, 58, 78, 127, 47, 11, 43, 80, 8, 111, 94, 159, 255, 170, 10, 0, 60, 0, 167, 22, 84, 0, 5, 255, 104, 35, 172, 32, 95, 16, 216, 50, 53, 56, 0, 2, 93, 52, 26, 7, 0, 73, 86, 169, 0, 63, 3, 63, 10, 4, 175, 0, 22, 151, 0, 58, 80, 255, 170, 63, 125, 12, 12, 0, 89, 67, 0, 11, 0, 0, 0, 22, 59, 0, 23, 48, 91, 9, 180, 13, 0, 71, 0, 0, 80, 53, 143, 147, 29, 0, 0, 55, 66, 7, 199, 44, 78, 27, 0, 112, 22, 237, 137, 87, 15, 18, 87, 159, 110, 65, 0, 83, 41, 32, 62, 40, 186, 43, 0, 0, 33, 7, 37, 38, 178, 21, 190, 64, 55, 86, 4, 104, 40, 53, 0, 5, 4, 19, 50, 136, 251, 133, 0, 5, 61, 0, 169, 0, 75, 0, 5, 255, 116, 2, 119, 27, 0, 0, 255, 0, 161, 0, 49, 1, 193, 62, 21, 7, 0, 24, 54, 143, 53, 79, 3, 128, 45, 4, 55, 0, 18, 167, 41, 26, 0, 230, 153, 44, 69, 12, 10, 0, 76, 54, 0, 27, 0, 22, 0, 1, 0, 0, 19, 18, 82, 45, 173, 13, 0, 21, 0, 20, 101, 0, 46, 213, 49, 0, 0, 65, 40, 0, 199, 12, 51, 27, 4, 132, 26, 237, 69, 168, 5, 0, 94, 146, 102, 78, 0, 89, 20, 1, 51, 21, 165, 78, 0, 23, 50, 0, 26, 0, 175, 42, 184, 42, 82, 79, 18, 143, 81, 75, 0, 49, 4, 108, 115, 186, 198, 158, 6, 22, 16, 61, 163, 46, 58, 99, 29, 226, 130, 8, 183, 27, 19, 0, 220, 0, 89, 0, 0, 23, 138, 81, 63, 7, 0, 36, 91, 141, 89, 22, 3, 87, 43, 4, 32, 24, 21, 210, 0, 21, 0, 242, 230, 56, 90, 12, 1, 0, 132, 108, 0, 51, 0, 14, 0, 20, 0, 0, 8, 7, 99, 53, 197, 13, 0, 0, 0, 0, 123, 0, 109, 255, 106, 0, 0, 62, 84, 0, 184, 9, 118, 2, 28, 180, 14, 237, 117, 194, 0, 0, 94, 90, 102, 40, 0, 68, 63, 0, 61, 37, 128, 52, 0, 5, 26, 52, 35, 0, 180, 52, 185, 19, 69, 75, 53, 108, 0, 24, 50, 80, 4, 63, 42, 141, 232, 255, 45, 0, 0, 84, 105, 5, 93, 48, 5, 208, 142, 38, 81, 27, 90, 0, 215, 43, 33, 0, 14, 30, 106, 55, 31, 7, 0, 62, 65, 154, 74, 76, 3, 171, 0, 4, 145, 64, 7, 223, 38, 40, 11, 255, 204, 59, 113, 12, 1, 0, 33, 52, 0, 53, 0, 11, 0, 1, 0, 51, 22, 16, 117, 23, 179, 13, 0, 10, 0, 8, 118, 0, 86, 213, 8, 0, 0, 94, 116, 0, 187, 18, 168, 0, 0, 152, 14, 203, 108, 131, 0, 0, 84, 190, 69, 22, 0, 123, 49, 0, 46, 0, 156, 55, 0, 7, 25, 5, 48, 43, 144, 39, 207, 23, 22, 74, 39, 133, 0, 51, 0, 62, 4, 29, 67, 85, 236, 255, 0, 0, 0, 0, 154, 0, 83, 0, 23, 233, 82, 20, 183, 27, 0, 0, 255, 1, 161, 30, 0, 8, 171, 54, 0, 7, 0, 54, 94, 126, 59, 65, 3, 177, 21, 4, 203, 83, 7, 176, 0, 57, 0, 238, 177, 53, 140, 12, 24, 0, 158, 62, 0, 29, 0, 0, 0, 21, 0, 0, 112, 43, 97, 8, 142, 13, 0, 11, 0, 34, 61, 0, 55, 212, 255, 0, 0, 45, 95, 0, 199, 4, 120, 25, 0, 139, 33, 237, 58, 196, 11, 35, 69, 86, 135, 28, 0, 101, 92, 0, 69, 2, 182, 59, 1, 16, 37, 80, 42, 17, 171, 23, 185, 37, 46, 121, 59, 81, 4, 11, 0, 58, 28, 66, 69, 102, 255, 195, 51, 44, 0, 67, 140, 28, 93, 0, 12, 255, 108, 62, 110, 27, 50, 28, 254, 73, 38, 38, 0, 9, 90, 53, 0, 7, 0, 54, 62, 63, 0, 79, 15, 79, 2, 8, 29, 2, 27, 181, 67, 31, 0, 220, 143, 74, 108, 12, 1, 0, 150, 83, 0, 39, 0, 24, 0, 5, 0, 0, 54, 32, 86, 63, 179, 13, 0, 0, 0, 0, 101, 0, 27, 180, 21, 0, 0, 44, 6, 0, 199, 14, 138, 6, 16, 125, 40, 237, 71, 87, 0, 0, 74, 106, 90, 12, 0, 108, 13, 14, 69, 33, 167, 107, 57, 0, 100, 3, 77, 41, 184, 89, 159, 87, 69, 67, 16, 155, 86, 11, 0, 92, 4, 46, 97, 166, 255, 208, 47, 39, 73, 0, 164, 19, 85, 0, 20, 186, 130, 20, 133, 27, 5, 0, 253, 0, 158, 3, 13, 10, 151, 46, 0, 7, 0, 49, 100, 165, 0, 79, 3, 135, 18, 23, 49, 104, 7, 127, 0, 52, 0, 255, 116, 53, 67, 12, 59, 0, 95, 24, 0, 9, 0, 0, 0, 5, 0, 14, 56, 6, 90, 38, 137, 13, 0, 0, 0, 7, 107, 0, 59, 245, 77, 0, 0, 75, 111, 71, 199, 28, 74, 0, 0, 138, 16, 237, 129, 174, 0, 0, 78, 128, 135, 73, 0, 84, 9, 0, 69, 91, 152, 70, 0, 61, 37, 0, 18, 0, 152, 21, 99, 22, 16, 26, 22, 143, 86, 20, 77, 77, 4, 135, 55, 121, 203, 245, 30, 0, 19, 138, 123, 2, 75, 43, 40, 215, 97, 16, 145, 27, 0, 8, 255, 12, 108, 14, 0, 62, 116, 41, 24, 7, 0, 92, 101, 182, 0, 79, 3, 201, 1, 8, 45, 0, 9, 253, 0, 41, 0, 219, 216, 0, 109, 12, 16, 0, 195, 81, 25, 55, 33, 51, 0, 18, 0, 0, 60, 26, 177, 41, 188, 13, 0, 0, 59, 0, 84, 0, 95, 217, 54, 0, 0, 106, 127, 45, 199, 4, 103, 12, 47, 135, 33, 237, 97, 84, 8, 0, 84, 65, 146, 10, 0, 90, 16, 0, 25, 68, 176, 68, 0, 40, 65, 0, 53, 30, 153, 23, 207, 32, 59, 51, 25, 133, 56, 31, 0, 100, 4, 35, 62, 153, 255, 202, 16, 0, 0, 0, 133, 0, 93, 38, 41, 250, 107, 62, 170, 27, 29, 0, 255, 0, 109, 0, 14, 10, 167, 89, 38, 7, 0, 39, 109, 177, 0, 79, 3, 133, 3, 16, 25, 0, 11, 133, 0, 21, 1, 240, 185, 74, 131, 12, 1, 0, 69, 19, 35, 26, 0, 35, 0, 1, 0, 41, 4, 24, 65, 18, 147, 13, 0, 134, 0, 0, 72, 32, 84, 255, 79, 0, 0, 30, 86, 0, 166, 8, 91, 18, 0, 133, 47, 237, 102, 83, 0, 0, 84, 132, 132, 73, 0, 69, 5, 0, 65, 0, 141, 70, 12, 57, 32, 0, 52, 68, 169, 70, 207, 56, 66, 59, 78, 152, 32, 16, 22, 44, 4, 22, 28, 94, 220, 223, 19, 0, 0, 129, 125, 0, 89, 0, 11, 254, 111, 21, 152, 27, 73, 0, 255, 27, 69, 37, 0, 7, 95, 56, 0, 7, 0, 40, 117, 97, 0, 72, 3, 184, 27, 12, 83, 0, 29, 183, 55, 44, 55, 255, 228, 68, 130, 12, 36, 0, 89, 182, 0, 34, 0, 0, 0, 1, 0, 29, 35, 24, 55, 12, 193, 13, 0, 0, 0, 0, 89, 0, 86, 180, 28, 0, 0, 32, 133, 0, 145, 52, 99, 3, 24, 94, 42, 237, 108, 90, 0, 8, 86, 165, 108, 40, 0, 97, 11, 0, 37, 0, 175, 76, 0, 0, 41, 5, 45, 24, 173, 21, 207, 35, 90, 105, 66, 119, 40, 11, 0, 52, 4, 54, 45, 65, 255, 138, 25, 0, 29, 0, 165, 0, 78, 29, 5, 224, 141, 7, 161, 27, 0, 0, 255, 37, 85, 45, 5, 21, 184, 67, 0, 7, 0, 51, 99, 118, 0, 48, 11, 134, 28, 4, 197, 33, 58, 231, 51, 57, 0, 225, 177, 41, 138, 12, 59, 0, 132, 39, 0, 62, 0, 0, 0, 7, 0, 40, 23, 44, 33, 24, 165, 13, 0, 2, 0, 49, 49, 0, 85, 164, 118, 0, 0, 28, 17, 0, 195, 68, 139, 0, 33, 51, 85, 237, 93, 135, 0, 0, 86, 109, 97, 13, 0, 90, 9, 0, 69, 38, 111, 96, 0, 3, 51, 0, 47, 68, 187, 56, 177, 39, 58, 50, 16, 167, 7, 41, 0, 34, 4, 55, 41, 140, 226, 250, 14, 61, 0, 150, 169, 66, 80, 0, 5, 203, 110, 27, 135, 27, 0, 0, 229, 86, 75, 35, 0, 2, 91, 94, 3, 7, 0, 50, 110, 174, 0, 52, 3, 198, 0, 13, 199, 46, 25, 255, 0, 66, 0, 255, 213, 48, 97, 12, 1, 0, 93, 41, 0, 8, 0, 0, 0, 27, 9, 67, 74, 7, 96, 5, 196, 13, 0, 0, 0, 1, 70, 0, 85, 255, 145, 0, 0, 80, 154, 49, 182, 1, 110, 10, 0, 153, 59, 237, 91, 132, 0, 0, 70, 171, 122, 39, 0, 70, 11, 0, 69, 48, 162, 88, 17, 0, 54, 0, 49, 13, 187, 21, 194, 34, 48, 67, 6, 112, 0, 44, 0, 5, 4, 43, 53, 131, 250, 227, 16, 0, 0, 17, 169, 15, 67, 0, 36, 246, 96, 80, 183, 27, 0, 0, 255, 26, 161, 0, 0, 3, 128, 49, 0, 7, 0, 72, 79, 169, 147, 64, 3, 117, 0, 4, 41, 0, 31, 212, 0, 37, 0, 255, 191, 62, 103, 12, 1, 0, 137, 86, 0, 63, 0, 10, 0, 63, 0, 2, 77, 15, 72, 5, 179, 13, 0, 42, 0, 0, 110, 0, 32, 178, 255, 0, 0, 158, 71, 0, 199, 16, 126, 5, 0, 171, 28, 237, 97, 161, 0, 0, 80, 119, 92, 28, 0, 59, 5, 0, 69, 6, 174, 34, 0, 0, 38, 20, 29, 0, 164, 33, 190, 12, 60, 27, 43, 136, 7, 48, 0, 65, 4, 76, 54, 97, 255, 224, 43, 7, 0, 0, 153, 78, 93, 11, 42, 255, 96, 77, 183, 27, 0, 0, 236, 0, 135, 39, 0, 2, 90, 29, 0, 7, 0, 46, 104, 153, 0, 79, 3, 133, 0, 4, 114, 0, 17, 214, 3, 41, 48, 255, 181, 74, 125, 12, 1, 0, 94, 94, 45, 20, 0, 45, 0, 33, 0, 14, 23, 25, 26, 2, 196, 13, 0, 5, 0, 0, 93, 0, 56, 187, 176, 0, 0, 40, 84, 0, 196, 22, 101, 8, 46, 93, 28, 237, 85, 134, 0, 0, 89, 78, 104, 9, 0, 96, 16, 0, 69, 2, 152, 179, 0, 16, 89, 0, 93, 68, 148, 21, 171, 0, 37, 19, 64, 156, 48, 28, 9, 18, 7, 58, 77, 142, 255, 255, 26, 21, 0, 0, 144, 0, 70, 41, 37, 231, 84, 14, 141, 27, 51, 0, 230, 0, 144, 45, 0, 19, 173, 52, 0, 7, 29, 107, 105, 158, 0, 30, 3, 125, 20, 7, 118, 0, 9, 175, 0, 56, 0, 251, 162, 31, 87, 12, 14, 0, 109, 24, 0, 59, 0, 0, 0, 32, 44, 47, 8, 19, 59, 40, 155, 13, 0, 0, 59, 0, 54, 12, 93, 216, 57, 0, 0, 57, 125, 0, 193, 34, 101, 4, 75, 120, 58, 220, 89, 114, 0, 0, 94, 101, 149, 18, 0, 82, 54, 0, 69, 11, 186, 129, 17, 43, 39, 0, 54, 0, 171, 21, 199, 21, 34, 43, 78, 108, 17, 21, 13, 9, 19, 47, 94, 173, 248, 239, 8, 0, 0, 55, 67, 0, 93, 0, 37, 255, 96, 36, 99, 27, 0, 5, 243, 49, 70, 6, 0, 2, 101, 65, 22, 7, 0, 121, 71, 163, 0, 74, 3, 160, 7, 16, 128, 0, 11, 193, 0, 39, 84, 255, 188, 22, 140, 12, 6, 0, 168, 35, 0, 80, 141, 0, 0, 48, 0, 55, 48, 15, 97, 56, 150, 13, 0, 20, 0, 0, 51, 0, 106, 169, 43, 0, 0, 42, 81, 26, 158, 0, 91, 5, 0, 105, 138, 213, 62, 121, 0, 0, 94, 134, 138, 73, 0, 118, 20, 0, 69, 32, 156, 60, 67, 15, 60, 19, 47, 0, 185, 43, 199, 112, 3, 83, 23, 143, 61, 11, 57, 32, 42, 58, 102, 191, 251, 255, 39, 0, 0, 0, 165, 13, 93, 0, 5, 255, 119, 130, 171, 31, 0, 14, 255, 45, 45, 0, 0, 5, 80, 69, 0, 7, 0, 23, 63, 109, 0, 73, 3, 121, 10, 8, 109, 0, 7, 193, 0, 27, 98, 251, 173, 74, 91, 12, 36, 0, 227, 70, 0, 29, 0, 0, 0, 1, 0, 0, 63, 11, 167, 35, 158, 13, 0, 0, 0, 0, 70, 0, 87, 255, 90, 0, 0, 120, 116, 69, 96, 30, 134, 40, 22, 92, 36, 237, 80, 177, 0, 0, 94, 122, 100, 9, 0, 121, 53, 0, 47, 30, 186, 58, 0, 19, 96, 0, 63, 2, 156, 60, 207, 62, 13, 79, 69, 161, 0, 11, 37, 51, 46, 82, 109, 206, 214, 216, 13, 0, 21, 93, 166, 0, 83, 34, 23, 227, 116, 67, 139, 34, 22, 0, 255, 27, 82, 0, 0, 9, 152, 78, 0, 7, 0, 28, 104, 178, 0, 54, 3, 200, 22, 15, 68, 58, 7, 176, 0, 44, 0, 255, 218, 35, 99, 12, 48, 0, 122, 53, 0, 77, 0, 0, 0, 16, 0, 0, 39, 1, 85, 32, 113, 13, 0, 0, 0, 17, 29, 2, 99, 255, 146, 0, 0, 106, 158, 88, 159, 14, 87, 43, 0, 145, 28, 237, 57, 59, 0, 40, 83, 129, 132, 17, 0, 106, 13, 0, 58, 42, 181, 46, 29, 56, 115, 10, 39, 68, 174, 45, 207, 52, 62, 102, 47, 121, 69, 14, 0, 50, 30, 16, 81, 178, 202, 236, 7, 0, 17, 25, 164, 0, 93, 0, 5, 227, 84, 92, 176, 27, 22, 0, 255, 0, 102, 0, 64, 2, 203, 83, 25, 7, 0, 22, 97, 153, 0, 79, 3, 178, 16, 4, 69, 0, 12, 165, 0, 27, 13, 245, 156, 0, 116, 12, 1, 0, 102, 54, 0, 42, 0, 0, 0, 10, 0, 95, 52, 9, 136, 2, 98, 13, 0, 84, 66, 0, 65, 55, 66, 255, 82, 0, 0, 142, 176, 91, 146, 0, 53, 10, 17, 122, 67, 237, 92, 119, 0, 0, 85, 89, 145, 22, 0, 100, 39, 0, 67, 41, 165, 85, 0, 64, 16, 0, 54, 0, 184, 27, 181, 20, 45, 89, 29, 108, 60, 34, 0, 25, 4, 84, 31, 168, 213, 255, 91, 7, 7, 0, 129, 9, 93, 0, 12, 204, 131, 12, 125, 27, 0, 0, 255, 0, 96, 0, 0, 2, 124, 33, 74, 7, 0, 51, 94, 156, 0, 79, 3, 130, 0, 4, 36, 0, 14, 149, 2, 26, 0, 255, 193, 60, 113, 12, 27, 0, 195, 115, 0, 40, 0, 0, 0, 1, 0, 0, 88, 7, 123, 43, 186, 13, 0, 0, 10, 2, 90, 0, 99, 227, 30, 0, 0, 98, 90, 4, 140, 6, 136, 0, 15, 159, 35, 220, 121, 185, 0, 0, 81, 75, 66, 13, 0, 94, 54, 0, 53, 19, 148, 99, 0, 0, 28, 0, 68, 68, 174, 29, 207, 71, 51, 68, 20, 156, 41, 36, 0, 12, 4, 127, 93, 154, 255, 216, 31, 47, 0, 0, 118, 0, 93, 0, 37, 230, 109, 47, 107, 27, 73, 0, 214, 34, 36, 84, 0, 16, 95, 29, 62, 7, 0, 34, 86, 161, 0, 62, 3, 61, 7, 10, 62, 49, 11, 200, 0, 61, 0, 255, 155, 50, 122, 12, 47, 0, 87, 36, 44, 16, 0, 0, 0, 48, 0, 0, 65, 28, 42, 68, 207, 13, 0, 135, 0, 0, 78, 0, 104, 201, 1, 0, 0, 34, 43, 0, 172, 57, 174, 5, 0, 146, 26, 209, 117, 193, 0, 0, 77, 37, 90, 4, 0, 54, 20, 0, 69, 22, 139, 64, 0, 52, 60, 0, 31, 68, 121, 63, 201, 2, 60, 2, 22, 143, 0, 23, 0, 38, 4, 45, 73, 130, 250, 209, 0, 78, 24, 202, 80, 6, 88, 62, 85, 255, 111, 5, 170, 27, 105, 0, 186, 0, 53, 5, 19, 16, 150, 24, 24, 7, 0, 77, 95, 167, 0, 64, 3, 135, 16, 37, 82, 74, 16, 227, 3, 49, 0, 255, 192, 38, 109, 12, 14, 0, 28, 29, 0, 5, 0, 68, 0, 58, 0, 77, 14, 7, 36, 19, 194, 13, 0, 0, 65, 0, 81, 0, 124, 183, 1, 0, 0, 53, 145, 0, 128, 3, 154, 10, 42, 113, 4, 237, 90, 156, 0, 0, 82, 75, 112, 36, 0, 91, 15, 0, 69, 0, 155, 176, 0, 36, 105, 0, 78, 68, 144, 21, 207, 60, 40, 76, 78, 201, 0, 32, 0, 58, 4, 88, 119, 120, 245, 226, 0, 0, 2, 0, 169, 0, 90, 0, 23, 220, 56, 35, 107, 27, 112, 0, 251, 0, 162, 18, 0, 15, 105, 26, 36, 7, 0, 42, 111, 189, 0, 74, 3, 144, 13, 4, 157, 5, 11, 233, 27, 32, 0, 255, 171, 41, 105, 12, 29, 0, 190, 95, 0, 53, 0, 0, 0, 8, 0, 0, 45, 8, 26, 18, 165, 13, 0, 58, 0, 47, 91, 0, 65, 192, 7, 0, 0, 48, 85, 0, 130, 23, 106, 0, 0, 86, 29, 237, 116, 113, 0, 2, 94, 66, 142, 48, 0, 61, 5, 0, 69, 9, 154, 145, 34, 22, 73, 0, 113, 37, 184, 21, 207, 48, 7, 96, 78, 99, 2, 11, 3, 89, 4, 17, 77, 124, 255, 255, 43, 0, 0, 0, 136, 0, 88, 0, 16, 193, 72, 29, 169, 27, 10, 62, 231, 22, 144, 14, 12, 44, 106, 33, 5, 7, 0, 37, 89, 156, 31, 53, 3, 123, 8, 10, 102, 0, 15, 158, 0, 52, 0, 255, 149, 42, 100, 12, 11, 0, 86, 65, 0, 24, 0, 0, 0, 1, 64, 1, 37, 9, 74, 31, 160, 13, 0, 0, 0, 0, 128, 0, 20, 229, 67, 0, 0, 76, 102, 0, 199, 19, 93, 17, 63, 177, 70, 237, 189, 138, 0, 0, 78, 93, 89, 4, 0, 72, 53, 0, 69, 34, 175, 6, 67, 62, 14, 34, 6, 68, 164, 21, 190, 45, 0, 87, 78, 133, 0, 18, 0, 28, 4, 41, 28, 111, 218, 255, 1, 11, 0, 30, 169, 130, 93, 0, 23, 255, 112, 45, 128, 27, 2, 16, 211, 19, 62, 97, 40, 2, 71, 23, 6, 7, 0, 27, 113, 189, 0, 69, 3, 74, 56, 4, 60, 98, 23, 155, 31, 21, 0, 210, 199, 74, 134, 12, 120, 0, 58, 16, 0, 102, 0, 88, 0, 20, 42, 25, 4, 7, 72, 5, 92, 59, 0, 135, 0, 152, 44, 75, 23, 175, 82, 0, 0, 37, 18, 89, 83, 123, 112, 32, 0, 175, 24, 211, 248, 184, 0, 0, 94, 76, 111, 4, 0, 83, 19, 4, 46, 58, 146, 165, 0, 6, 19, 0, 62, 55, 168, 36, 207, 36, 52, 117, 0, 71, 0, 11, 0, 69, 4, 119, 100, 97, 255, 255, 17, 22, 38, 16, 134, 38, 93, 0, 11, 233, 111, 36, 52, 27, 52, 72, 255, 0, 116, 82, 0, 6, 64, 67, 0, 7, 0, 59, 67, 151, 0, 20, 3, 102, 4, 17, 156, 24, 32, 187, 0, 68, 82, 255, 191, 45, 140, 12, 48, 0, 63, 38, 0, 255, 5, 0, 0, 10, 44, 3, 23, 27, 38, 10, 148, 46, 0, 1, 0, 25, 84, 0, 25, 156, 52, 0, 0, 28, 78, 0, 199, 64, 99, 0, 12, 140, 39, 237, 140, 147, 0, 0, 94, 156, 75, 34, 0, 85, 24, 0, 69, 36, 146, 28, 11, 0, 46, 0, 22, 58, 141, 44, 207, 82, 29, 52, 70, 67, 0, 11, 0, 43, 4, 90, 50, 111, 248, 178, 13, 131, 1, 13, 154, 14, 93, 6, 5, 176, 129, 2, 183, 41, 0, 38, 255, 51, 132, 0, 0, 16, 118, 96, 0, 7, 0, 56, 54, 120, 59, 79, 3, 139, 39, 44, 35, 0, 32, 177, 0, 40, 105, 255, 176, 29, 98, 12, 47, 0, 102, 34, 0, 19, 0, 76, 0, 5, 0, 38, 30, 23, 73, 24, 153, 13, 0, 5, 0, 0, 76, 38, 45, 183, 149, 0, 0, 65, 97, 0, 199, 66, 152, 13, 0, 117, 20, 237, 128, 150, 0, 87, 94, 104, 90, 62, 0, 78, 24, 0, 69, 50, 154, 59, 0, 30, 30, 0, 41, 17, 169, 21, 167, 69, 25, 94, 57, 123, 8, 11, 0, 29, 7, 155, 99, 116, 224, 188, 11, 0, 0, 52, 157, 42, 31, 0, 5, 212, 88, 15, 167, 31, 16, 21, 238, 23, 156, 0, 0, 61, 104, 42, 0, 7, 0, 97, 88, 107, 0, 28, 3, 143, 46, 36, 76, 95, 47, 255, 31, 61, 0, 255, 254, 65, 108, 12, 64, 0, 113, 69, 0, 11, 0, 16, 0, 28, 0, 0, 46, 11, 48, 65, 160, 13, 0, 0, 67, 0, 84, 0, 103, 217, 61, 0, 0, 41, 105, 0, 199, 30, 166, 4, 48, 102, 41, 237, 138, 157, 0, 0, 88, 118, 132, 4, 0, 69, 51, 4, 53, 27, 141, 28, 0, 10, 75, 7, 26, 13, 104, 84, 157, 32, 6, 4, 78, 107, 57, 52, 0, 13, 35, 39, 118, 148, 230, 148, 36, 44, 0, 16, 131, 0, 86, 0, 44, 237, 97, 20, 106, 34, 0, 10, 255, 62, 51, 0, 30, 2, 84, 83, 0, 7, 0, 67, 51, 120, 0, 79, 9, 73, 20, 11, 48, 0, 7, 231, 0, 34, 0, 255, 193, 34, 91, 12, 1, 0, 203, 43, 0, 45, 0, 46, 0, 45, 0, 42, 39, 13, 100, 30, 160, 13, 0, 103, 0, 0, 60, 57, 66, 118, 135, 0, 0, 122, 96, 0, 156, 24, 174, 12, 0, 81, 31, 237, 161, 48, 0, 0, 88, 148, 100, 51, 0, 59, 67, 0, 38, 8, 166, 28, 16, 4, 45, 37, 31, 0, 155, 41, 207, 35, 62, 90, 64, 102, 56, 18, 0, 35, 4, 49, 40, 106, 251, 200, 30, 123, 0, 0, 153, 0, 82, 0, 36, 218, 138, 93, 63, 27, 34, 0, 255, 31, 37, 57, 10, 18, 125, 67, 49, 7, 0, 40, 61, 148, 0, 79, 3, 159, 72, 23, 87, 9, 28, 255, 0, 48, 36, 255, 174, 11, 140, 12, 65, 0, 69, 83, 0, 31, 0, 0, 0, 1, 0, 83, 51, 43, 7, 61, 153, 13, 0, 16, 2, 0, 31, 0, 60, 113, 94, 0, 0, 41, 69, 0, 199, 62, 87, 0, 23, 90, 52, 237, 164, 96, 0, 0, 94, 119, 83, 67, 0, 107, 14, 0, 69, 9, 137, 47, 45, 36, 32, 14, 37, 0, 130, 63, 203, 29, 67, 30, 65, 111, 92, 11, 0, 56, 4, 57, 45, 66, 222, 225, 3, 75, 0, 0, 114, 62, 93, 0, 54, 178, 127, 30, 183, 27, 0, 0, 254, 2, 35, 25, 8, 64, 124, 35, 16, 7, 0, 23, 71, 118, 0, 79, 3, 185, 63, 8, 20, 0, 36, 212, 53, 23, 0, 233, 156, 74, 117, 12, 1, 0, 130, 49, 0, 10, 0, 53, 0, 37, 0, 22, 9, 75, 51, 53, 193, 13, 0, 0, 3, 0, 39, 0, 78, 188, 122, 0, 0, 53, 105, 0, 199, 6, 124, 0, 37, 16, 24, 237, 105, 78, 0, 0, 94, 96, 133, 22, 0, 98, 16, 0, 62, 39, 160, 102, 0, 10, 80, 0, 65, 68, 172, 45, 160, 33, 40, 76, 49, 131, 72, 31, 0, 47, 4, 26, 55, 165, 245, 199, 10, 0, 3, 123, 169, 4, 93, 11, 5, 255, 81, 72, 183, 27, 56, 0, 244, 0, 58, 3, 0, 10, 145, 43, 0, 7, 0, 33, 104, 143, 0, 74, 3, 155, 13, 14, 68, 0, 26, 154, 0, 41, 0, 205, 171, 71, 120, 12, 80, 0, 72, 69, 0, 60, 0, 0, 30, 25, 0, 81, 43, 28, 147, 38, 119, 13, 0, 0, 0, 0, 43, 0, 106, 243, 46, 0, 0, 108, 107, 0, 199, 0, 61, 0, 30, 125, 53, 229, 125, 139, 0, 0, 77, 74, 101, 14, 0, 118, 22, 0, 28, 16, 128, 135, 0, 50, 33, 0, 62, 36, 158, 45, 189, 5, 44, 66, 78, 116, 38, 31, 0, 40, 4, 51, 29, 107, 254, 255, 4, 0, 0, 0, 122, 21, 72, 83, 37, 255, 138, 32, 183, 27, 53, 0, 233, 7, 105, 45, 0, 20, 89, 29, 0, 12, 0, 45, 83, 121, 0, 73, 3, 163, 55, 11, 130, 0, 17, 165, 1, 41, 0, 242, 196, 53, 140, 12, 9, 0, 56, 91, 0, 43, 0, 0, 0, 73, 44, 48, 44, 42, 109, 2, 136, 13, 0, 45, 0, 0, 38, 43, 100, 119, 99, 0, 0, 57, 70, 0, 160, 13, 133, 0, 14, 63, 60, 237, 107, 153, 0, 0, 82, 86, 95, 12, 0, 103, 34, 0, 36, 3, 138, 135, 0, 0, 56, 0, 68, 0, 144, 37, 164, 11, 75, 5, 68, 111, 28, 11, 0, 25, 4, 85, 50, 44, 244, 255, 16, 42, 65, 41, 145, 0, 93, 69, 28, 255, 77, 16, 148, 27, 89, 0, 234, 0, 147, 0, 0, 47, 174, 108, 0, 7, 0, 47, 99, 112, 0, 79, 3, 139, 42, 20, 38, 0, 16, 233, 0, 32, 0, 255, 183, 27, 105, 12, 12, 0, 82, 30, 0, 28, 0, 0, 0, 29, 31, 0, 16, 82, 77, 56, 196, 13, 0, 50, 0, 44, 78, 0, 72, 144, 1, 0, 0, 36, 50, 0, 152, 43, 126, 40, 0, 94, 51, 237, 84, 197, 0, 54, 94, 68, 107, 32, 0, 82, 9, 0, 69, 4, 145, 100, 67, 0, 126, 8, 92, 68, 181, 33, 205, 113, 47, 116, 44, 187, 0, 11, 0, 12, 4, 71, 72, 124, 254, 254, 68, 115, 27, 14, 163, 91, 79, 0, 5, 255, 120, 22, 117, 27, 48, 0, 198, 24, 35, 16, 0, 15, 65, 30, 0, 7, 0, 81, 73, 112, 0, 79, 3, 144, 24, 36, 209, 0, 21, 216, 36, 53, 0, 255, 244, 61, 49, 12, 77, 0, 91, 58, 18, 23, 0, 0, 0, 1, 31, 19, 35, 30, 48, 31, 205, 13, 0, 0, 153, 0, 74, 0, 59, 233, 37, 0, 0, 51, 125, 0, 199, 37, 121, 0, 52, 75, 29, 237, 123, 114, 0, 0, 94, 110, 45, 26, 0, 117, 23, 0, 69, 33, 176, 52, 0, 0, 62, 14, 23, 30, 168, 21, 205, 77, 4, 2, 61, 111, 79, 52, 0, 21, 4, 134, 167, 115, 240, 189, 55, 0, 39, 0, 163, 0, 72, 52, 60, 237, 100, 2, 183, 27, 0, 0, 255, 0, 125, 20, 8, 13, 112, 30, 36, 7, 0, 76, 67, 176, 4, 79, 3, 125, 20, 4, 101, 11, 11, 250, 0, 56, 0, 255, 210, 0, 140, 12, 29, 0, 136, 16, 24, 20, 0, 30, 0, 24, 10, 52, 15, 27, 65, 36, 199, 13, 0, 80, 0, 3, 60, 0, 59, 130, 108, 0, 0, 65, 57, 0, 199, 53, 117, 12, 0, 150, 17, 237, 111, 125, 0, 0, 94, 123, 102, 51, 0, 69, 16, 0, 69, 18, 171, 135, 0, 14, 54, 0, 75, 31, 179, 24, 184, 0, 73, 62, 73, 163, 44, 49, 0, 13, 12, 99, 47, 153, 255, 255, 10, 3, 53, 106, 144, 0, 51, 42, 21, 255, 53, 12, 166, 27, 41, 0, 230, 0, 179, 48, 0, 7, 93, 36, 56, 7, 0, 79, 99, 181, 0, 58, 3, 109, 0, 8, 16, 65, 12, 255, 0, 38, 0, 255, 216, 0, 124, 12, 1, 0, 84, 48, 19, 39, 0, 0, 0, 52, 0, 0, 30, 17, 65, 21, 187, 13, 0, 0, 0, 0, 97, 0, 86, 184, 98, 0, 0, 90, 70, 0, 185, 18, 135, 7, 43, 145, 19, 237, 82, 213, 0, 0, 84, 52, 106, 13, 0, 96, 15, 0, 58, 37, 132, 28, 0, 7, 60, 14, 28, 0, 155, 40, 147, 12, 0, 12, 16, 156, 0, 36, 0, 23, 20, 89, 44, 102, 214, 255, 17, 0, 0, 0, 96, 0, 93, 0, 82, 195, 59, 16, 72, 27, 34, 54, 220, 48, 53, 31, 0, 25, 102, 60, 46, 7, 0, 124, 92, 164, 0, 79, 3, 168, 3, 19, 82, 0, 7, 212, 0, 33, 0, 255, 210, 57, 122, 12, 1, 0, 161, 35, 0, 6, 0, 43, 0, 42, 18, 0, 41, 29, 62, 12, 151, 13, 0, 87, 0, 2, 109, 0, 112, 126, 37, 0, 0, 60, 120, 0, 151, 48, 161, 18, 0, 138, 6, 237, 102, 87, 0, 0, 81, 103, 137, 54, 0, 61, 5, 0, 60, 24, 124, 104, 3, 0, 29, 0, 77, 0, 154, 44, 118, 54, 81, 34, 56, 89, 31, 47, 46, 10, 4, 76, 55, 67, 220, 186, 63, 0, 0, 102, 24, 0, 85, 11, 175, 204, 121, 12, 158, 27, 0, 0, 237, 0, 104, 0, 0, 9, 114, 31, 24, 7, 0, 70, 108, 149, 0, 60, 3, 161, 25, 18, 77, 0, 29, 181, 48, 30, 159, 255, 192, 62, 125, 12, 18, 0, 111, 61, 0, 12, 0, 0, 0, 25, 0, 6, 21, 69, 48, 14, 193, 13, 0, 0, 54, 0, 128, 0, 46, 104, 101, 0, 0, 85, 73, 0, 196, 51, 121, 0, 53, 150, 28, 237, 103, 214, 0, 0, 89, 68, 87, 44, 0, 98, 5, 0, 50, 20, 164, 93, 0, 0, 28, 0, 21, 68, 187, 58, 207, 73, 60, 91, 63, 105, 0, 11, 0, 40, 4, 200, 149, 188, 255, 140, 19, 0, 0, 0, 169, 0, 79, 0, 5, 248, 87, 20, 105, 27, 0, 0, 241, 73, 150, 45, 0, 2, 115, 66, 50, 7, 0, 45, 86, 182, 27, 58, 3, 81, 13, 4, 111, 0, 14, 255, 0, 33, 87, 255, 232, 70, 120, 12, 24, 0, 140, 29, 0, 43, 0, 23, 0, 10, 83, 0, 47, 10, 63, 7, 181, 13, 0, 39, 0, 0, 65, 0, 54, 218, 133, 0, 0, 73, 45, 0, 181, 72, 197, 18, 0, 154, 40, 237, 73, 145, 0, 0, 72, 152, 95, 35, 0, 87, 5, 0, 63, 46, 183, 29, 0, 0, 18, 0, 27, 0, 161, 39, 203, 44, 32, 43, 71, 81, 17, 11, 0, 34, 4, 78, 54, 83, 255, 227, 9, 0, 0, 0, 133, 43, 78, 0, 15, 255, 103, 36, 169, 27, 0, 30, 255, 50, 100, 2, 0, 40, 78, 39, 0, 7, 0, 52, 78, 155, 0, 54, 3, 151, 0, 4, 158, 25, 21, 206, 0, 47, 0, 255, 191, 62, 104, 12, 10, 0, 90, 73, 0, 19, 5, 16, 0, 23, 0, 0, 48, 48, 67, 6, 194, 13, 0, 89, 0, 0, 92, 0, 97, 181, 181, 0, 0, 115, 18, 0, 191, 18, 135, 14, 0, 102, 7, 220, 105, 205, 0, 0, 89, 32, 110, 4, 0, 59, 37, 4, 58, 0, 160, 44, 0, 0, 24, 41, 37, 28, 180, 77, 201, 45, 0, 32, 78, 112, 36, 21, 43, 29, 4, 116, 28, 71, 255, 232, 23, 0, 0, 20, 169, 16, 93, 0, 18, 234, 93, 12, 143, 27, 100, 60, 200, 0, 18, 86, 0, 7, 107, 78, 39, 7, 0, 86, 111, 150, 31, 79, 3, 179, 0, 14, 182, 3, 7, 246, 0, 23, 0, 255, 186, 49, 125, 12, 27, 0, 142, 34, 0, 8, 0, 35, 0, 39, 55, 0, 34, 10, 41, 18, 174, 13, 0, 0, 33, 0, 65, 0, 42, 147, 15, 0, 0, 28, 81, 0, 192, 60, 143, 9, 16, 168, 11, 203, 128, 157, 0, 0, 83, 78, 114, 10, 0, 61, 38, 0, 65, 12, 162, 41, 47, 0, 75, 0, 47, 0, 164, 21, 207, 13, 38, 40, 78, 160, 0, 24, 0, 43, 4, 93, 94, 108, 240, 255, 60, 0, 0, 7, 159, 0, 93, 58, 5, 255, 130, 22, 131, 27, 75, 0, 198, 33, 6, 33, 0, 5, 96, 73, 21, 7, 0, 83, 76, 189, 0, 76, 3, 208, 31, 13, 255, 10, 11, 223, 0, 33, 18, 255, 227, 63, 84, 12, 54, 0, 134, 16, 0, 32, 7, 26, 0, 5, 6, 37, 4, 47, 40, 38, 116, 13, 0, 18, 0, 0, 69, 0, 39, 255, 55, 0, 0, 64, 102, 0, 199, 0, 120, 0, 41, 153, 28, 213, 142, 117, 0, 0, 94, 128, 65, 4, 0, 93, 5, 0, 60, 56, 144, 56, 37, 39, 50, 0, 43, 0, 182, 52, 207, 7, 14, 46, 43, 113, 31, 25, 0, 43, 18, 79, 95, 107, 251, 255, 17, 15, 0, 0, 158, 0, 93, 24, 5, 239, 115, 27, 183, 31, 0, 0, 255, 0, 136, 0, 0, 2, 97, 17, 0, 7, 0, 86, 97, 170, 0, 75, 3, 188, 0, 18, 62, 71, 12, 188, 56, 41, 0, 255, 234, 64, 122, 12, 10, 0, 174, 23, 0, 39, 0, 23, 0, 25, 0, 39, 22, 34, 80, 2, 201, 13, 0, 0, 0, 43, 72, 44, 47, 255, 187, 0, 0, 87, 116, 33, 92, 0, 173, 27, 0, 118, 5, 191, 42, 168, 0, 5, 77, 64, 113, 41, 0, 53, 8, 0, 69, 28, 154, 85, 0, 73, 27, 0, 50, 0, 174, 21, 171, 31, 49, 33, 58, 116, 0, 11, 0, 25, 32, 80, 68, 178, 175, 217, 54, 0, 0, 222, 138, 0, 41, 52, 8, 254, 116, 10, 171, 32, 83, 0, 160, 34, 47, 10, 0, 2, 155, 42, 4, 7, 0, 106, 89, 174, 0, 79, 3, 146, 0, 15, 122, 19, 36, 181, 0, 60, 0, 228, 230, 47, 58, 12, 1, 0, 43, 42, 0, 41, 0, 18, 13, 71, 10, 14, 30, 22, 137, 14, 140, 13, 0, 0, 0, 26, 64, 10, 118, 255, 15, 0, 0, 74, 182, 98, 87, 13, 125, 15, 9, 145, 26, 237, 96, 151, 0, 0, 94, 111, 114, 12, 0, 95, 19, 0, 69, 9, 155, 38, 56, 14, 61, 12, 17, 13, 144, 89, 174, 38, 111, 71, 10, 103, 0, 11, 0, 12, 4, 71, 145, 190, 220, 203, 38, 0, 57, 9, 116, 0, 78, 0, 17, 210, 129, 87, 100, 27, 0, 0, 242, 0, 127, 0, 12, 21, 130, 27, 12, 7, 0, 46, 90, 153, 0, 79, 3, 144, 18, 28, 153, 0, 19, 186, 0, 42, 51, 255, 211, 59, 94, 12, 1, 0, 45, 73, 0, 40, 0, 8, 0, 40, 32, 0, 34, 6, 27, 2, 137, 13, 0, 88, 0, 0, 69, 0, 148, 211, 148, 0, 0, 72, 169, 41, 160, 41, 96, 3, 0, 111, 4, 237, 68, 152, 0, 6, 84, 64, 97, 7, 0, 72, 5, 0, 69, 31, 171, 27, 0, 42, 38, 52, 27, 25, 175, 94, 207, 29, 29, 92, 38, 94, 55, 16, 0, 53, 4, 52, 82, 134, 219, 255, 39, 16, 0, 0, 146, 18, 84, 0, 13, 201, 88, 7, 168, 27, 112, 0, 243, 3, 37, 38, 0, 30, 114, 31, 22, 7, 0, 32, 74, 116, 0, 58, 3, 121, 18, 12, 161, 32, 11, 184, 0, 32, 23, 255, 179, 0, 119, 12, 1, 0, 83, 46, 0, 29, 0, 57, 0, 27, 42, 56, 28, 4, 63, 4, 161, 13, 0, 0, 59, 28, 76, 0, 101, 141, 1, 0, 0, 107, 87, 0, 199, 19, 143, 14, 24, 134, 30, 237, 84, 118, 0, 0, 94, 144, 106, 11, 0, 95, 9, 0, 67, 50, 175, 66, 0, 64, 17, 24, 33, 0, 166, 56, 195, 41, 84, 63, 47, 72, 54, 22, 32, 56, 4, 50, 84, 131, 218, 251, 24, 0, 20, 0, 132, 0, 76, 0, 25, 204, 86, 39, 131, 27, 19, 0, 255, 12, 62, 30, 0, 35, 136, 31, 38, 7, 0, 52, 70, 124, 77, 56, 3, 99, 14, 18, 139, 26, 17, 187, 0, 29, 0, 255, 197, 24, 112, 12, 1, 0, 132, 64, 0, 68, 52, 0, 0, 19, 0, 0, 68, 17, 61, 21, 179, 13, 0, 3, 0, 0, 107, 0, 129, 184, 150, 0, 0, 84, 83, 0, 199, 29, 161, 0, 0, 121, 33, 237, 111, 232, 0, 0, 85, 79, 116, 24, 0, 127, 59, 0, 63, 11, 175, 102, 18, 11, 61, 24, 53, 0, 115, 33, 199, 102, 27, 100, 52, 104, 0, 17, 0, 13, 4, 97, 87, 124, 245, 208, 24, 32, 0, 15, 134, 0, 93, 0, 21, 255, 142, 49, 136, 41, 38, 0, 255, 0, 53, 0, 0, 1, 52, 95, 0, 7, 0, 30, 83, 151, 0, 79, 3, 90, 42, 4, 110, 0, 14, 174, 0, 39, 0, 255, 218, 52, 132, 12, 77, 0, 135, 60, 0, 56, 0, 2, 0, 10, 37, 0, 77, 31, 47, 69, 173, 13, 0, 62, 0, 0, 38, 15, 51, 157, 76, 0, 0, 49, 14, 0, 182, 61, 117, 18, 30, 94, 22, 237, 104, 139, 0, 0, 94, 120, 70, 22, 0, 95, 22, 0, 58, 32, 157, 56, 0, 0, 53, 0, 37, 0, 104, 78, 174, 10, 1, 41, 78, 148, 0, 11, 51, 58, 4, 70, 46, 73, 249, 153, 63, 0, 0, 0, 125, 71, 85, 43, 37, 245, 126, 10, 176, 27, 0, 0, 253, 18, 82, 59, 0, 9, 130, 42, 42, 7, 0, 100, 90, 183, 0, 4, 3, 181, 30, 16, 147, 0, 22, 230, 11, 60, 40, 247, 184, 59, 136, 12, 9, 0, 135, 30, 0, 13, 0, 34, 0, 1, 32, 44, 39, 20, 63, 31, 178, 13, 0, 0, 29, 0, 57, 22, 62, 148, 142, 0, 0, 28, 81, 0, 184, 78, 89, 23, 52, 137, 25, 237, 101, 179, 0, 0, 94, 73, 125, 58, 0, 117, 21, 0, 65, 0, 171, 90, 0, 0, 13, 47, 81, 68, 156, 79, 207, 0, 13, 38, 78, 121, 0, 11, 20, 23, 4, 89, 101, 0, 250, 255, 10, 0, 0, 0, 149, 0, 62, 15, 40, 255, 121, 13, 175, 27, 78, 2, 199, 65, 70, 85, 0, 3, 141, 42, 39, 7, 0, 64, 109, 144, 80, 72, 3, 160, 19, 12, 253, 0, 16, 192, 14, 40, 35, 242, 208, 46, 93, 26, 25, 0, 142, 65, 0, 33, 126, 0, 0, 6, 71, 0, 30, 145, 58, 54, 168, 13, 0, 109, 0, 0, 80, 24, 84, 84, 16, 0, 0, 40, 9, 0, 199, 46, 138, 0, 0, 92, 11, 237, 132, 140, 0, 0, 26, 144, 63, 4, 0, 96, 5, 0, 60, 14, 186, 133, 0, 0, 0, 0, 65, 42, 160, 21, 203, 2, 14, 30, 0, 77, 15, 57, 13, 48, 4, 46, 48, 20, 255, 222, 25, 0, 0, 63, 122, 0, 93, 57, 16, 239, 61, 5, 169, 27, 0, 0, 255, 0, 152, 78, 8, 10, 99, 22, 0, 7, 0, 44, 113, 56, 50, 62, 3, 133, 3, 27, 125, 0, 17, 181, 55, 55, 11, 255, 214, 0, 115, 12, 28, 0, 255, 29, 0, 18, 0, 0, 0, 34, 0, 0, 23, 22, 18, 68, 181, 13, 0, 0, 0, 7, 80, 0, 74, 169, 99, 0, 0, 28, 62, 0, 199, 41, 148, 34, 61, 113, 37, 237, 183, 47, 0, 0, 78, 78, 170, 4, 0, 84, 106, 0, 49, 17, 167, 57, 0, 33, 29, 74, 39, 68, 169, 21, 201, 40, 0, 58, 50, 123, 82, 52, 15, 16, 4, 54, 50, 128, 221, 255, 0, 4, 37, 60, 169, 111, 93, 0, 5, 159, 36, 39, 159, 27, 86, 0, 177, 20, 113, 102, 0, 21, 106, 26, 13, 7, 0, 14, 111, 189, 41, 11, 3, 122, 14, 36, 82, 20, 51, 185, 10, 51, 0, 224, 178, 14, 108, 12, 139, 0, 24, 45, 0, 179, 0, 0, 0, 66, 71, 49, 110, 1, 29, 83, 64, 24, 0, 77, 0, 150, 43, 0, 37, 252, 18, 0, 0, 28, 70, 66, 59, 57, 87, 3, 0, 113, 114, 237, 190, 214, 0, 0, 62, 79, 115, 40, 0, 70, 26, 7, 57, 0, 172, 24, 0, 11, 37, 40, 19, 63, 128, 21, 202, 5, 31, 41, 32, 76, 55, 11, 13, 10, 4, 105, 137, 78, 255, 255, 0, 0, 2, 63, 158, 14, 93, 16, 39, 255, 95, 2, 76, 27, 0, 64, 239, 37, 99, 74, 0, 8, 73, 88, 40, 7, 0, 71, 71, 146, 0, 45, 3, 123, 7, 15, 88, 0, 22, 199, 15, 58, 0, 255, 194, 9, 140, 12, 54, 0, 73, 62, 5, 255, 0, 4, 0, 73, 0, 2, 25, 61, 50, 35, 155, 13, 0, 0, 0, 16, 62, 0, 75, 189, 105, 0, 0, 39, 61, 0, 199, 34, 107, 9, 0, 100, 20, 237, 88, 169, 0, 0, 94, 118, 112, 34, 0, 92, 19, 44, 69, 4, 158, 94, 0, 0, 35, 6, 47, 0, 175, 21, 146, 10, 93, 62, 46, 100, 0, 11, 0, 5, 10, 27, 68, 165, 255, 177, 4, 0, 37, 32, 139, 0, 93, 6, 5, 235, 113, 10, 52, 27, 0, 23, 247, 52, 84, 77, 7, 16, 148, 80, 0, 7, 0, 62, 87, 147, 0, 75, 5, 171, 6, 30, 132, 0, 7, 197, 0, 30, 0, 237, 189, 44, 133, 12, 21, 0, 155, 43, 0, 29, 0, 0, 0, 63, 0, 20, 49, 32, 104, 5, 148, 13, 0, 21, 0, 19, 56, 0, 91, 169, 171, 0, 0, 33, 83, 0, 137, 9, 90, 22, 40, 138, 75, 237, 108, 64, 0, 0, 82, 95, 137, 48, 0, 87, 12, 0, 69, 60, 127, 61, 0, 21, 79, 0, 67, 57, 169, 21, 201, 20, 85, 70, 43, 115, 86, 11, 22, 34, 20, 19, 86, 114, 243, 205, 12, 45, 47, 96, 154, 56, 93, 0, 11, 242, 107, 44, 171, 27, 57, 0, 224, 0, 105, 0, 5, 16, 126, 104, 26, 7, 0, 74, 114, 142, 84, 69, 6, 168, 12, 13, 135, 0, 7, 208, 41, 43, 56, 218, 187, 67, 113, 12, 1, 0, 86, 77, 0, 38, 0, 0, 0, 9, 16, 0, 19, 23, 89, 2, 148, 13, 0, 0, 0, 0, 83, 0, 75, 180, 114, 0, 0, 83, 152, 0, 179, 42, 69, 44, 0, 174, 66, 237, 137, 104, 0, 0, 77, 67, 76, 4, 0, 87, 10, 0, 55, 25, 139, 72, 0, 3, 80, 0, 86, 68, 132, 27, 207, 32, 17, 22, 39, 126, 45, 11, 0, 22, 12, 15, 69, 175, 233, 255, 18, 0, 9, 0, 139, 2, 93, 0, 17, 204, 57, 9, 118, 27, 0, 57, 255, 29, 158, 0, 38, 21, 146, 132, 30, 7, 0, 106, 82, 129, 0, 79, 3, 131, 1, 4, 67, 57, 7, 176, 0, 44, 0, 255, 126, 41, 137, 12, 10, 0, 56, 43, 0, 45, 0, 10, 0, 16, 23, 69, 8, 18, 86, 14, 121, 13, 0, 96, 0, 16, 76, 31, 44, 166, 92, 0, 0, 126, 98, 45, 182, 49, 91, 55, 0, 147, 49, 237, 138, 119, 0, 0, 94, 111, 128, 12, 0, 62, 44, 0, 32, 48, 147, 94, 17, 0, 90, 0, 39, 24, 125, 29, 207, 2, 67, 93, 40, 117, 32, 49, 0, 74, 28, 21, 51, 105, 215, 189, 30, 96, 0, 40, 116, 0, 89, 43, 28, 255, 90, 2, 169, 41, 0, 0, 255, 0, 68, 0, 47, 8, 118, 56, 0, 7, 0, 107, 78, 167, 0, 79, 3, 85, 35, 4, 62, 68, 7, 161, 0, 39, 0, 255, 190, 74, 92, 12, 80, 0, 190, 65, 0, 45, 0, 0, 0, 1, 0, 0, 27, 39, 62, 2, 161, 13, 0, 0, 135, 0, 83, 66, 0, 118, 118, 0, 0, 115, 44, 11, 154, 60, 126, 0, 14, 169, 28, 237, 138, 63, 0, 25, 94, 95, 98, 32, 0, 95, 17, 0, 37, 12, 153, 124, 50, 34, 69, 0, 76, 68, 159, 61, 159, 61, 28, 59, 78, 129, 61, 11, 0, 29, 4, 51, 37, 73, 242, 236, 21, 65, 0, 30, 169, 0, 84, 0, 32, 255, 128, 50, 123, 27, 0, 12, 223, 30, 14, 10, 0, 17, 135, 41, 64, 7, 0, 40, 89, 152, 31, 79, 3, 191, 91, 4, 144, 0, 9, 209, 9, 53, 49, 229, 154, 74, 98, 12, 1, 0, 122, 112, 24, 7, 0, 0, 0, 5, 6, 76, 34, 49, 67, 2, 116, 13, 0, 0, 0, 23, 47, 20, 45, 167, 92, 0, 0, 106, 107, 29, 152, 21, 104, 4, 36, 226, 53, 237, 123, 127, 0, 0, 84, 118, 146, 4, 0, 103, 5, 0, 60, 74, 157, 40, 14, 3, 59, 0, 63, 68, 139, 44, 172, 57, 40, 86, 47, 78, 2, 24, 0, 57, 4, 48, 44, 151, 248, 255, 44, 0, 0, 0, 169, 39, 93, 23, 29, 254, 124, 79, 152, 27, 0, 0, 255, 75, 152, 0, 0, 1, 81, 57, 0, 7, 0, 34, 52, 174, 0, 79, 3, 146, 40, 10, 47, 0, 21, 153, 11, 36, 132, 255, 190, 53, 88, 12, 113, 0, 95, 43, 0, 18, 0, 47, 0, 8, 0, 53, 20, 4, 104, 32, 120, 13, 0, 0, 51, 34, 55, 30, 44, 225, 103, 0, 0, 128, 90, 6, 161, 48, 138, 8, 52, 121, 0, 224, 91, 134, 0, 0, 55, 109, 98, 24, 0, 105, 21, 36, 69, 49, 87, 100, 0, 0, 85, 54, 89, 43, 169, 32, 176, 104, 48, 58, 53, 110, 0, 63, 0, 59, 4, 31, 105, 165, 219, 251, 26, 5, 0, 137, 138, 0, 62, 0, 69, 196, 125, 67, 123, 27, 10, 0, 247, 45, 131, 36, 0, 8, 151, 114, 0, 18, 0, 65, 108, 168, 23, 79, 3, 144, 0, 12, 93, 56, 31, 133, 0, 23, 0, 255, 161, 35, 118, 12, 12, 0, 90, 27, 0, 16, 0, 14, 0, 78, 2, 83, 8, 18, 99, 2, 142, 13, 0, 32, 0, 0, 77, 20, 74, 172, 123, 0, 0, 67, 107, 0, 121, 84, 143, 39, 0, 184, 25, 237, 112, 188, 0, 0, 92, 92, 107, 19, 0, 81, 9, 0, 38, 4, 158, 71, 0, 34, 0, 0, 37, 57, 169, 63, 152, 39, 39, 77, 27, 84, 20, 11, 0, 52, 4, 62, 137, 66, 255, 255, 31, 73, 12, 107, 149, 0, 93, 24, 23, 255, 87, 32, 183, 62, 16, 10, 226, 21, 31, 77, 0, 44, 128, 34, 16, 19, 0, 56, 96, 153, 0, 69, 3, 129, 3, 4, 94, 0, 14, 151, 0, 45, 0, 253, 169, 55, 140, 12, 1, 0, 83, 40, 75, 30, 0, 18, 0, 50, 53, 0, 44, 45, 120, 12, 148, 13, 0, 33, 0, 42, 97, 55, 102, 153, 78, 0, 0, 39, 49, 0, 138, 52, 153, 47, 67, 178, 25, 237, 136, 151, 0, 0, 91, 65, 115, 17, 0, 105, 5, 0, 41, 15, 176, 83, 0, 14, 46, 0, 48, 0, 167, 25, 205, 76, 37, 36, 56, 128, 68, 11, 0, 18, 4, 123, 113, 125, 246, 255, 52, 0, 33, 0, 114, 111, 93, 0, 15, 255, 55, 49, 155, 42, 3, 34, 252, 30, 94, 35, 6, 18, 103, 53, 0, 7, 0, 73, 85, 143, 0, 73, 3, 148, 7, 7, 95, 19, 7, 255, 0, 50, 30, 255, 238, 65, 135, 12, 5, 0, 119, 29, 30, 27, 0, 0, 0, 26, 16, 0, 58, 29, 85, 21, 136, 13, 0, 0, 7, 0, 89, 36, 119, 91, 89, 0, 0, 94, 124, 0, 179, 58, 129, 34, 0, 172, 20, 237, 127, 145, 0, 0, 83, 103, 172, 32, 0, 86, 46, 0, 69, 37, 181, 64, 0, 9, 112, 7, 34, 68, 146, 31, 202, 52, 65, 62, 37, 128, 0, 64, 0, 18, 4, 27, 91, 134, 202, 181, 62, 0, 0, 54, 169, 0, 93, 3, 21, 255, 81, 12, 166, 27, 0, 0, 227, 6, 120, 0, 21, 22, 201, 60, 8, 7, 0, 21, 95, 171, 0, 71, 3, 109, 31, 19, 148, 0, 7, 242, 0, 56, 20, 255, 255, 60, 92, 12, 6, 0, 105, 36, 0, 73, 0, 0, 0, 51, 18, 40, 52, 19, 62, 26, 182, 13, 0, 0, 0, 27, 112, 0, 22, 182, 79, 0, 0, 121, 69, 0, 183, 29, 70, 14, 30, 140, 31, 237, 114, 187, 0, 0, 94, 45, 66, 79, 0, 109, 22, 0, 28, 19, 178, 53, 0, 0, 50, 0, 42, 17, 141, 21, 181, 40, 28, 44, 43, 126, 0, 29, 52, 87, 18, 84, 88, 106, 255, 205, 35, 22, 0, 0, 158, 81, 78, 28, 14, 238, 118, 2, 178, 27, 112, 0, 255, 34, 69, 3, 0, 10, 120, 61, 28, 7, 0, 79, 90, 177, 0, 16, 3, 139, 16, 18, 30, 27, 7, 190, 0, 53, 0, 243, 200, 52, 69, 12, 73, 0, 116, 88, 64, 10, 0, 37, 18, 42, 69, 7, 35, 36, 81, 40, 202, 13, 0, 13, 0, 0, 73, 0, 81, 255, 39, 0, 0, 100, 81, 52, 199, 34, 136, 12, 8, 91, 44, 237, 112, 156, 0, 0, 82, 119, 136, 4, 0, 98, 56, 0, 69, 47, 181, 20, 37, 0, 104, 0, 18, 46, 166, 23, 167, 17, 40, 95, 46, 117, 0, 37, 0, 22, 32, 43, 45, 126, 220, 218, 26, 0, 0, 0, 150, 0, 93, 0, 38, 234, 118, 20, 113, 27, 23, 0, 255, 14, 100, 0, 36, 31, 154, 110, 0, 7, 0, 56, 92, 112, 0, 79, 3, 156, 0, 4, 70, 52, 29, 201, 19, 23, 0, 255, 219, 15, 134, 12, 1, 0, 170, 48, 0, 39, 0, 64, 26, 54, 0, 30, 12, 13, 31, 16, 153, 17, 0, 43, 0, 105, 101, 26, 52, 237, 66, 0, 0, 84, 96, 36, 199, 11, 108, 12, 0, 104, 3, 237, 76, 79, 0, 0, 46, 94, 89, 33, 0, 42, 13, 0, 60, 34, 162, 68, 0, 51, 35, 33, 35, 0, 181, 38, 146, 41, 52, 69, 0, 190, 64, 37, 28, 15, 8, 78, 58, 162, 208, 255, 26, 0, 9, 0, 125, 69, 72, 0, 57, 178, 126, 73, 9, 27, 0, 0, 226, 25, 49, 37, 0, 5, 126, 21, 0, 7, 0, 82, 95, 151, 0, 79, 3, 110, 1, 65, 101, 59, 52, 181, 0, 78, 0, 255, 171, 0, 140, 12, 18, 0, 118, 172, 52, 50, 13, 0, 0, 50, 0, 59, 44, 23, 88, 20, 181, 22, 0, 0, 0, 0, 139, 0, 88, 136, 167, 0, 0, 114, 121, 1, 199, 33, 89, 0, 22, 151, 9, 237, 94, 76, 0, 0, 88, 72, 149, 18, 0, 79, 22, 0, 50, 2, 161, 150, 39, 24, 33, 0, 76, 20, 173, 45, 175, 11, 49, 80, 78, 47, 25, 11, 47, 26, 27, 21, 84, 117, 242, 181, 22, 0, 58, 0, 167, 0, 93, 49, 5, 255, 116, 14, 62, 27, 106, 12, 255, 8, 89, 0, 0, 9, 88, 44, 22, 7, 0, 31, 57, 162, 0, 79, 3, 120, 3, 23, 148, 0, 7, 209, 0, 26, 0, 255, 210, 68, 118, 12, 24, 0, 37, 48, 0, 65, 0, 0, 0, 29, 31, 75, 31, 19, 82, 12, 173, 13, 0, 0, 0, 9, 85, 0, 92, 186, 8, 0, 0, 63, 76, 0, 185, 28, 66, 0, 22, 69, 49, 237, 122, 120, 0, 0, 73, 139, 96, 8, 0, 92, 19, 0, 50, 30, 171, 72, 0, 5, 60, 0, 69, 0, 141, 49, 194, 67, 33, 49, 62, 85, 46, 11, 15, 42, 37, 53, 103, 162, 255, 162, 28, 0, 0, 0, 163, 19, 83, 21, 25, 255, 142, 42, 167, 27, 41, 8, 255, 0, 69, 0, 52, 19, 119, 47, 16, 7, 0, 88, 68, 136, 0, 76, 3, 82, 6, 11, 89, 0, 15, 205, 0, 28, 0, 255, 218, 57, 84, 12, 44, 0, 175, 23, 0, 33, 0, 9, 0, 1, 0, 0, 17, 3, 113, 21, 187, 13, 0, 41, 0, 57, 73, 32, 67, 202, 15, 0, 0, 103, 58, 6, 194, 78, 110, 1, 43, 106, 18, 211, 96, 126, 0, 0, 85, 120, 136, 4, 0, 71, 30, 32, 45, 139, 160, 92, 0, 0, 42, 0, 69, 27, 146, 141, 115, 40, 30, 28, 0, 122, 59, 39, 34, 22, 13, 65, 63, 101, 187, 244, 36, 0, 26, 0, 133, 30, 71, 0, 117, 209, 116, 40, 138, 27, 21, 22, 255, 0, 106, 0, 0, 18, 172, 61, 35, 7, 0, 30, 97, 165, 45, 75, 3, 165, 29, 23, 127, 4, 7, 129, 0, 36, 0, 225, 179, 8, 44, 12, 37, 0, 190, 61, 0, 21, 27, 0, 0, 69, 0, 0, 49, 17, 134, 25, 128, 13, 0, 0, 39, 0, 104, 0, 75, 201, 57, 0, 0, 151, 126, 9, 187, 4, 68, 2, 0, 155, 50, 237, 112, 126, 0, 0, 84, 78, 87, 40, 0, 107, 41, 0, 69, 10, 186, 87, 37, 0, 90, 0, 60, 0, 168, 21, 207, 137, 44, 131, 78, 146, 14, 11, 0, 32, 4, 34, 28, 145, 255, 203, 4, 0, 30, 0, 169, 0, 93, 0, 5, 246, 136, 35, 123, 27, 53, 27, 250, 0, 105, 12, 0, 2, 98, 57, 42, 7, 0, 83, 81, 158, 56, 74, 5, 176, 39, 19, 135, 53, 7, 116, 0, 23, 0, 248, 172, 61, 86, 12, 33, 0, 116, 31, 0, 34, 0, 57, 0, 7, 38, 16, 19, 1, 45, 2, 111, 13, 0, 37, 0, 17, 53, 42, 88, 224, 13, 0, 0, 34, 63, 0, 187, 29, 122, 3, 24, 112, 37, 237, 107, 97, 0, 0, 85, 96, 77, 4, 0, 77, 5, 0, 69, 0, 167, 40, 0, 47, 17, 16, 48, 0, 187, 36, 207, 17, 50, 56, 54, 112, 92, 11, 41, 38, 19, 34, 65, 133, 255, 255, 6, 0, 0, 49, 169, 46, 89, 1, 5, 245, 78, 51, 183, 46, 97, 0, 251, 0, 6, 49, 0, 2, 141, 27, 0, 7, 0, 51, 104, 135, 67, 37, 3, 142, 0, 33, 111, 39, 7, 148, 6, 35, 0, 255, 185, 45, 131, 12, 1, 0, 143, 49, 0, 60, 0, 19, 0, 28, 0, 0, 41, 1, 69, 7, 153, 13, 0, 0, 0, 0, 85, 0, 121, 177, 66, 0, 0, 88, 131, 11, 150, 11, 121, 6, 38, 184, 10, 237, 69, 101, 0, 0, 92, 59, 131, 7, 0, 76, 7, 0, 69, 3, 180, 157, 0, 18, 58, 0, 105, 0, 184, 53, 199, 66, 39, 84, 70, 65, 49, 17, 82, 29, 32, 34, 74, 165, 255, 194, 16, 0, 0, 0, 133, 21, 85, 0, 5, 255, 133, 34, 122, 54, 77, 30, 208, 0, 188, 2, 8, 69, 146, 47, 0, 7, 0, 92, 86, 159, 137, 79, 3, 152, 0, 10, 144, 0, 20, 203, 0, 49, 81, 234, 212, 16, 96, 12, 27, 0, 194, 41, 0, 20, 0, 0, 0, 67, 61, 0, 15, 17, 35, 2, 115, 13, 0, 0, 0, 0, 25, 65, 102, 208, 37, 0, 0, 69, 98, 0, 195, 20, 92, 17, 0, 128, 42, 237, 92, 111, 0, 0, 91, 145, 148, 13, 0, 83, 44, 0, 65, 0, 157, 99, 0, 14, 24, 0, 36, 2, 172, 102, 196, 42, 51, 40, 78, 81, 31, 21, 0, 58, 4, 36, 64, 202, 236, 205, 8, 0, 0, 65, 135, 0, 84, 0, 7, 234, 95, 63, 121, 27, 0, 19, 243, 64, 97, 0, 0, 40, 93, 46, 0, 7, 0, 74, 68, 177, 0, 51, 3, 147, 0, 18, 125, 0, 40, 194, 0, 42, 23, 255, 210, 15, 87, 12, 41, 0, 246, 83, 0, 32, 0, 0, 0, 42, 0, 29, 12, 1, 11, 2, 142, 13, 0, 101, 0, 45, 53, 75, 131, 231, 113, 0, 0, 53, 98, 4, 130, 64, 158, 9, 0, 76, 56, 237, 49, 148, 0, 0, 89, 110, 110, 13, 0, 68, 5, 0, 60, 33, 174, 37, 0, 35, 66, 48, 53, 68, 174, 60, 165, 47, 61, 85, 78, 87, 29, 17, 0, 65, 10, 56, 50, 164, 255, 255, 15, 41, 0, 13, 123, 9, 76, 13, 38, 203, 112, 57, 179, 27, 16, 0, 255, 63, 97, 15, 0, 5, 114, 49, 61, 7, 0, 62, 93, 186, 0, 48, 3, 118, 22, 4, 149, 0, 29, 133, 0, 47, 0, 255, 130, 62, 117, 12, 23, 0, 64, 79, 0, 19, 0, 0, 0, 97, 33, 38, 10, 1, 75, 14, 153, 13, 0, 0, 41, 0, 118, 19, 75, 185, 157, 0, 0, 87, 93, 4, 184, 0, 88, 12, 26, 135, 65, 237, 134, 199, 0, 0, 94, 93, 79, 22, 0, 76, 5, 0, 69, 32, 181, 18, 0, 28, 52, 39, 16, 24, 137, 37, 195, 54, 18, 49, 48, 134, 0, 25, 18, 58, 16, 108, 153, 108, 255, 255, 30, 0, 0, 0, 123, 49, 93, 43, 25, 244, 107, 66, 171, 27, 76, 17, 255, 86, 34, 47, 0, 9, 83, 39, 25, 7, 0, 66, 86, 144, 0, 79, 3, 110, 16, 4, 29, 30, 24, 245, 0, 23, 0, 255, 209, 41, 140, 12, 24, 0, 110, 28, 0, 23, 1, 47, 0, 9, 0, 0, 19, 34, 67, 9, 189, 13, 0, 151, 0, 0, 108, 3, 39, 170, 20, 4, 0, 75, 51, 7, 199, 52, 162, 45, 0, 84, 44, 237, 116, 131, 0, 0, 94, 120, 124, 23, 0, 87, 35, 0, 69, 17, 178, 97, 0, 7, 44, 0, 75, 68, 142, 40, 201, 64, 25, 89, 12, 97, 4, 58, 4, 58, 4, 51, 152, 83, 221, 197, 3, 9, 10, 0, 138, 0, 93, 0, 41, 198, 124, 44, 171, 34, 0, 0, 243, 0, 93, 0, 0, 2, 169, 57, 36, 7, 0, 25, 77, 129, 0, 79, 3, 171, 34, 4, 96, 0, 27, 230, 30, 43, 16, 243, 211, 37, 137, 12, 58, 0, 81, 62, 0, 27, 0, 37, 0, 18, 0, 0, 75, 19, 61, 20, 203, 13, 0, 0, 26, 40, 84, 0, 66, 225, 99, 0, 0, 79, 93, 0, 192, 9, 70, 37, 0, 105, 4, 237, 80, 90, 0, 0, 89, 113, 103, 31, 0, 55, 5, 0, 69, 26, 178, 40, 0, 27, 73, 0, 46, 24, 180, 25, 171, 44, 100, 81, 78, 132, 72, 11, 10, 77, 4, 42, 66, 153, 255, 255, 45, 0, 56, 0, 146, 0, 93, 0, 19, 255, 141, 7, 183, 27, 0, 0, 237, 11, 78, 50, 0, 12, 148, 26, 28, 17, 0, 62, 101, 177, 42, 3, 3, 143, 29, 23, 211, 27, 23, 227, 0, 49, 0, 255, 210, 67, 82, 12, 1, 0, 133, 30, 0, 25, 0, 15, 0, 1, 0, 0, 39, 9, 106, 18, 165, 13, 0, 38, 12, 0, 87, 0, 40, 202, 94, 0, 0, 49, 120, 31, 199, 13, 55, 0, 51, 112, 40, 237, 160, 126, 0, 0, 92, 98, 99, 18, 0, 96, 17, 31, 47, 36, 175, 62, 0, 0, 35, 0, 44, 0, 172, 173, 203, 13, 18, 26, 15, 112, 18, 11, 0, 132, 4, 113, 70, 100, 255, 221, 0, 0, 0, 0, 138, 12, 93, 0, 14, 255, 112, 9, 176, 27, 4, 18, 255, 37, 143, 2, 0, 2, 100, 85, 0, 7, 0, 74, 90, 158, 3, 79, 3, 147, 0, 4, 143, 20, 24, 185, 0, 45, 0, 253, 177, 55, 98, 12, 1, 0, 121, 29, 0, 19, 0, 41, 0, 6, 8, 0, 8, 79, 52, 14, 164, 13, 0, 66, 0, 12, 76, 24, 56, 184, 8, 0, 0, 58, 65, 0, 199, 0, 126, 24, 0, 206, 0, 237, 157, 126, 0, 0, 91, 146, 117, 4, 0, 82, 5, 0, 69, 70, 178, 31, 0, 21, 13, 11, 52, 37, 169, 36, 198, 11, 104, 94, 0, 108, 0, 23, 0, 63, 4, 82, 96, 85, 255, 214, 31, 0, 0, 0, 129, 0, 93, 9, 47, 255, 109, 60, 167, 27, 0, 0, 255, 0, 71, 0, 0, 5, 152, 36, 42, 7, 0, 38, 117, 141, 49, 65, 3, 173, 0, 21, 255, 13, 40, 255, 8, 53, 0, 221, 223, 0, 85, 12, 12, 0, 175, 79, 0, 27, 0, 81, 0, 12, 0, 18, 31, 52, 45, 43, 159, 13, 0, 0, 0, 32, 58, 0, 110, 160, 165, 0, 0, 92, 85, 0, 199, 0, 125, 33, 9, 146, 3, 237, 225, 111, 0, 0, 78, 36, 95, 4, 0, 49, 21, 1, 3, 21, 186, 38, 0, 5, 22, 62, 35, 68, 111, 21, 207, 48, 0, 89, 16, 177, 43, 32, 0, 13, 4, 144, 59, 90, 250, 255, 0, 97, 0, 2, 152, 77, 93, 2, 37, 255, 76, 59, 114, 27, 96, 23, 192, 0, 28, 87, 0, 19, 79, 24, 40, 7, 0, 17, 117, 143, 8, 19, 3, 88, 60, 40, 109, 48, 17, 255, 69, 58, 0, 220, 223, 0, 105, 12, 203, 0, 78, 50, 0, 152, 0, 68, 0, 28, 43, 1, 67, 20, 65, 12, 93, 13, 0, 77, 0, 124, 0, 1, 57, 145, 21, 0, 0, 28, 8, 14, 120, 104, 183, 47, 69, 133, 24, 169, 214, 67, 0, 0, 63, 80, 120, 7, 0, 97, 75, 0, 69, 2, 168, 29, 0, 5, 78, 0, 67, 68, 89, 21, 191, 6, 14, 61, 35, 121, 24, 16, 64, 98, 4, 14, 68, 85, 255, 240, 10, 0, 37, 50, 151, 0, 71, 0, 35, 255, 129, 37, 41, 27, 0, 0, 255, 0, 136, 33, 43, 2, 116, 57, 34, 7, 0, 50, 95, 114, 0, 59, 3, 114, 26, 8, 175, 39, 7, 193, 0, 37, 9, 255, 244, 58, 131, 12, 86, 0, 47, 24, 0, 255, 54, 67, 0, 32, 7, 95, 39, 65, 39, 14, 165, 13, 0, 0, 0, 44, 80, 25, 48, 221, 107, 0, 0, 71, 82, 0, 199, 15, 81, 0, 0, 190, 30, 237, 116, 105, 0, 0, 94, 74, 62, 6, 0, 98, 22, 0, 69, 16, 173, 104, 0, 0, 41, 0, 87, 0, 139, 73, 189, 7, 45, 93, 78, 116, 48, 11, 0, 32, 20, 93, 29, 163, 255, 199, 9, 0, 68, 0, 169, 0, 51, 6, 5, 251, 104, 57, 104, 34, 8, 0, 243, 70, 60, 84, 0, 2, 141, 88, 61, 7, 0, 40, 85, 164, 0, 73, 7, 155, 77, 4, 255, 23, 7, 223, 0, 52, 0, 216, 203, 47, 131, 12, 118, 0, 96, 47, 26, 45, 0, 0, 0, 12, 46, 32, 85, 13, 111, 38, 127, 13, 0, 0, 7, 44, 0, 0, 79, 193, 80, 0, 0, 42, 93, 0, 188, 1, 139, 13, 61, 98, 52, 220, 120, 96, 0, 0, 60, 88, 91, 22, 0, 95, 17, 0, 69, 27, 156, 114, 0, 20, 71, 12, 57, 68, 159, 21, 207, 10, 8, 54, 63, 179, 73, 14, 57, 18, 32, 40, 35, 156, 255, 252, 0, 0, 45, 0, 169, 59, 83, 24, 13, 255, 60, 42, 144, 27, 0, 21, 252, 32, 69, 59, 0, 6, 75, 76, 24, 7, 0, 86, 107, 158, 0, 59, 8, 180, 3, 4, 105, 78, 7, 135, 0, 30, 0, 205, 149, 47, 130, 12, 19, 0, 93, 68, 0, 62, 0, 8, 0, 40, 34, 3, 50, 26, 107, 2, 161, 13, 0, 41, 0, 0, 69, 32, 141, 187, 107, 0, 0, 70, 109, 0, 168, 37, 131, 41, 0, 129, 43, 237, 135, 136, 0, 0, 59, 75, 162, 7, 0, 62, 5, 16, 69, 17, 122, 104, 0, 36, 52, 0, 42, 0, 148, 52, 165, 46, 151, 66, 56, 94, 74, 16, 58, 22, 18, 48, 57, 151, 219, 229, 80, 0, 0, 31, 41, 0, 71, 20, 70, 184, 95, 35, 170, 27, 34, 0, 237, 22, 68, 12, 3, 7, 175, 88, 6, 7, 0, 100, 79, 105, 23, 61, 3, 130, 4, 12, 95, 0, 20, 238, 0, 25, 0, 255, 193, 37, 140, 12, 34, 0, 122, 80, 0, 58, 0, 0, 0, 85, 0, 57, 8, 1, 68, 2, 196, 13, 0, 0, 14, 0, 113, 48, 79, 168, 74, 0, 0, 71, 106, 0, 182, 25, 173, 23, 0, 92, 65, 237, 59, 134, 0, 0, 94, 94, 84, 6, 0, 77, 5, 0, 69, 17, 162, 20, 0, 0, 30, 0, 42, 68, 152, 62, 207, 23, 44, 40, 78, 113, 5, 11, 0, 21, 4, 46, 96, 98, 242, 201, 20, 0, 29, 0, 136, 0, 86, 14, 11, 255, 68, 12, 69, 27, 0, 0, 219, 7, 81, 44, 0, 3, 136, 31, 35, 7, 0, 42, 96, 160, 135, 64, 3, 123, 3, 28, 243, 0, 7, 216, 0, 50, 67, 255, 177, 13, 120, 12, 20, 0, 94, 32, 0, 19, 0, 0, 0, 23, 20, 39, 20, 40, 43, 2, 169, 13, 0, 0, 0, 16, 91, 41, 62, 191, 83, 0, 0, 32, 84, 0, 171, 54, 116, 0, 47, 155, 26, 237, 129, 127, 0, 44, 94, 135, 106, 21, 0, 117, 22, 0, 29, 0, 175, 39, 34, 30, 34, 39, 34, 68, 178, 39, 200, 61, 61, 109, 78, 108, 72, 11, 0, 83, 4, 124, 54, 80, 253, 255, 2, 0, 0, 14, 169, 0, 82, 0, 5, 255, 119, 119, 40, 27, 0, 20, 255, 26, 44, 55, 35, 2, 101, 37, 0, 7, 54, 105, 83, 167, 0, 79, 3, 166, 47, 4, 16, 0, 40, 167, 92, 67, 0, 248, 173, 48, 127, 12, 1, 0, 138, 118, 0, 26, 0, 0, 0, 1, 0, 14, 80, 27, 32, 2, 151, 13, 0, 51, 49, 0, 41, 75, 118, 157, 113, 0, 0, 38, 46, 0, 181, 68, 164, 0, 0, 127, 9, 237, 115, 227, 0, 7, 77, 66, 99, 26, 0, 106, 37, 0, 69, 58, 160, 124, 54, 0, 50, 10, 36, 68, 163, 44, 162, 3, 40, 55, 39, 131, 0, 86, 0, 18, 4, 135, 68, 123, 226, 255, 0, 36, 0, 185, 169, 0, 89, 20, 22, 255, 119, 36, 154, 27, 59, 0, 223, 44, 103, 13, 2, 22, 151, 52, 0, 7, 0, 58, 79, 181, 0, 79, 3, 195, 11, 9, 44, 9, 24, 196, 0, 40, 0, 255, 191, 21, 94, 12, 57, 0, 70, 56, 0, 40, 0, 0, 0, 90, 71, 0, 45, 4, 92, 66, 109, 13, 0, 0, 0, 2, 68, 2, 35, 202, 51, 0, 0, 84, 94, 44, 199, 0, 172, 6, 54, 105, 22, 237, 76, 141, 0, 23, 62, 91, 129, 56, 0, 112, 5, 13, 69, 49, 168, 72, 44, 0, 87, 37, 37, 0, 177, 34, 173, 11, 85, 88, 28, 153, 0, 37, 0, 19, 4, 36, 33, 108, 235, 213, 21, 0, 0, 0, 165, 34, 85, 14, 41, 255, 129, 48, 183, 27, 0, 0, 252, 21, 61, 0, 37, 17, 152, 23, 0, 7, 0, 70, 100, 153, 0, 79, 3, 196, 74, 19, 207, 18, 18, 153, 29, 54, 0, 182, 201, 71, 137, 12, 1, 0, 92, 71, 0, 39, 0, 19, 30, 16, 14, 40, 40, 43, 67, 14, 133, 13, 0, 19, 0, 0, 32, 9, 79, 216, 59, 0, 0, 57, 123, 16, 187, 0, 106, 0, 0, 79, 27, 237, 150, 108, 0, 44, 72, 73, 65, 35, 0, 98, 11, 0, 24, 5, 173, 85, 1, 15, 16, 0, 45, 0, 145, 26, 182, 66, 61, 30, 14, 140, 14, 11, 0, 51, 4, 116, 58, 121, 241, 247, 31, 86, 36, 0, 167, 23, 93, 13, 24, 255, 79, 18, 103, 71, 43, 0, 255, 0, 102, 35, 0, 9, 129, 81, 0, 7, 0, 35, 108, 156, 124, 79, 3, 203, 0, 4, 224, 0, 16, 232, 37, 64, 65, 255, 239, 4, 140, 12, 1, 0, 94, 19, 0, 16, 0, 0, 0, 10, 10, 45, 13, 18, 13, 2, 82, 13, 0, 0, 21, 0, 0, 22, 113, 200, 81, 0, 0, 49, 157, 54, 183, 37, 135, 1, 49, 138, 46, 237, 127, 101, 0, 0, 70, 140, 148, 42, 0, 76, 5, 0, 37, 10, 172, 60, 0, 16, 33, 0, 30, 0, 178, 27, 207, 50, 34, 58, 28, 112, 86, 11, 0, 29, 4, 58, 109, 183, 231, 181, 59, 0, 0, 96, 117, 0, 72, 0, 11, 185, 91, 24, 102, 49, 0, 0, 242, 22, 116, 26, 0, 3, 96, 35, 5, 7, 0, 31, 98, 163, 0, 64, 9, 139, 0, 12, 78, 0, 12, 168, 0, 35, 0, 255, 183, 34, 125, 12, 7, 0, 235, 66, 81, 67, 0, 0, 0, 3, 0, 0, 75, 1, 97, 11, 123, 13, 0, 59, 0, 0, 27, 0, 126, 140, 187, 0, 0, 57, 71, 0, 165, 55, 131, 2, 0, 151, 23, 237, 113, 64, 0, 0, 84, 78, 70, 20, 0, 96, 5, 0, 69, 2, 171, 65, 0, 0, 90, 0, 67, 68, 187, 29, 207, 5, 42, 78, 49, 92, 84, 31, 0, 21, 4, 20, 74, 141, 255, 255, 19, 0, 38, 0, 169, 0, 93, 0, 11, 236, 70, 9, 137, 27, 9, 23, 250, 15, 110, 1, 0, 14, 145, 74, 67, 7, 21, 91, 63, 169, 0, 67, 3, 141, 0, 21, 196, 0, 12, 207, 0, 49, 0, 255, 173, 61, 120, 12, 10, 0, 50, 45, 0, 79, 0, 0, 0, 20, 36, 66, 14, 23, 7, 22, 162, 13, 0, 63, 0, 116, 109, 0, 71, 164, 78, 0, 0, 40, 81, 0, 170, 47, 66, 18, 41, 110, 43, 237, 136, 109, 0, 0, 66, 144, 101, 30, 0, 131, 5, 0, 26, 12, 169, 76, 0, 26, 23, 1, 28, 68, 169, 21, 207, 47, 28, 74, 24, 127, 67, 11, 16, 5, 4, 82, 76, 105, 255, 255, 0, 0, 54, 0, 169, 0, 89, 0, 5, 247, 98, 14, 76, 27, 0, 17, 255, 0, 134, 56, 5, 20, 135, 26, 2, 7, 0, 117, 68, 93, 34, 79, 3, 169, 25, 10, 17, 9, 24, 213, 46, 55, 0, 255, 173, 70, 105, 12, 44, 0, 115, 143, 0, 43, 0, 0, 0, 1, 0, 14, 27, 34, 42, 26, 157, 13, 0, 0, 0, 0, 128, 0, 128, 189, 116, 0, 0, 46, 129, 0, 199, 64, 145, 4, 0, 147, 30, 237, 119, 99, 0, 0, 63, 112, 123, 4, 0, 93, 50, 0, 69, 32, 167, 126, 0, 0, 65, 0, 36, 68, 164, 24, 199, 22, 48, 74, 39, 124, 0, 17, 0, 25, 4, 60, 70, 150, 226, 255, 21, 33, 0, 0, 120, 43, 84, 0, 12, 212, 108, 32, 130, 27, 0, 31, 255, 54, 166, 22, 0, 10, 119, 54, 37, 7, 0, 74, 105, 125, 23, 79, 3, 166, 0, 21, 133, 0, 11, 192, 0, 35, 35, 255, 155, 67, 115, 12, 35, 0, 55, 45, 0, 38, 0, 0, 0, 15, 3, 40, 66, 19, 131, 18, 135, 13, 0, 0, 0, 0, 106, 0, 66, 247, 182, 0, 0, 87, 103, 39, 199, 1, 96, 6, 0, 164, 15, 237, 98, 122, 0, 0, 58, 108, 126, 36, 0, 73, 71, 0, 69, 27, 176, 6, 0, 0, 113, 14, 39, 68, 156, 38, 200, 62, 51, 53, 33, 131, 0, 35, 0, 58, 48, 18, 54, 139, 230, 222, 50, 23, 0, 138, 169, 83, 83, 28, 17, 255, 134, 2, 115, 42, 0, 0, 224, 69, 29, 0, 30, 2, 120, 49, 26, 7, 0, 88, 66, 132, 126, 79, 8, 162, 7, 4, 227, 0, 14, 212, 0, 35, 2, 255, 212, 74, 104, 12, 12, 0, 146, 20, 0, 41, 0, 82, 0, 41, 0, 51, 4, 7, 81, 2, 135, 13, 0, 0, 0, 0, 49, 15, 30, 230, 170, 0, 0, 132, 37, 53, 177, 0, 127, 17, 0, 178, 0, 237, 123, 114, 0, 0, 72, 131, 78, 15, 0, 77, 20, 0, 60, 0, 175, 51, 0, 0, 16, 0, 52, 17, 146, 40, 161, 0, 41, 42, 54, 130, 0, 11, 13, 50, 18, 62, 84, 113, 255, 255, 28, 0, 0, 0, 166, 0, 93, 54, 11, 255, 88, 22, 119, 27, 61, 0, 247, 0, 104, 38, 13, 2, 123, 35, 61, 7, 0, 109, 99, 128, 109, 79, 3, 135, 10, 10, 178, 45, 7, 150, 0, 62, 12, 255, 189, 74, 128, 12, 1, 0, 78, 87, 0, 41, 28, 0, 3, 53, 0, 31, 4, 43, 45, 9, 173, 13, 0, 92, 0, 0, 100, 0, 112, 220, 49, 0, 0, 92, 104, 35, 169, 0, 138, 10, 0, 128, 0, 237, 95, 96, 0, 0, 82, 133, 164, 66, 0, 72, 37, 29, 47, 0, 171, 43, 0, 0, 39, 0, 50, 0, 123, 21, 196, 71, 46, 41, 78, 119, 5, 18, 1, 50, 4, 35, 115, 143, 227, 190, 104, 0, 29, 73, 133, 0, 72, 0, 39, 210, 134, 7, 142, 27, 25, 0, 255, 0, 94, 0, 3, 1, 139, 36, 22, 7, 0, 74, 95, 149, 71, 40, 3, 55, 14, 16, 51, 0, 7, 216, 0, 34, 98, 255, 188, 64, 78, 12, 78, 0, 26, 49, 0, 15, 0, 25, 0, 38, 0, 57, 4, 7, 31, 19, 182, 13, 0, 0, 70, 0, 122, 0, 64, 240, 62, 0, 0, 45, 83, 0, 199, 23, 128, 17, 13, 176, 36, 237, 104, 138, 0, 0, 94, 151, 70, 4, 0, 102, 14, 15, 63, 73, 179, 39, 0, 0, 45, 13, 45, 0, 179, 23, 191, 40, 46, 40, 78, 111, 0, 11, 0, 32, 4, 55, 144, 183, 255, 255, 9, 0, 0, 0, 144, 51, 58, 5, 16, 248, 113, 17, 130, 27, 0, 0, 255, 46, 174, 52, 0, 13, 108, 42, 12, 7, 0, 104, 78, 161, 0, 35, 3, 138, 21, 20, 174, 0, 9, 206, 0, 48, 0, 255, 186, 36, 136, 12, 103, 0, 86, 33, 3, 14, 55, 4, 0, 41, 23, 24, 17, 1, 93, 12, 161, 13, 0, 0, 0, 1, 104, 23, 133, 237, 146, 0, 0, 53, 131, 0, 199, 0, 121, 14, 16, 120, 32, 237, 96, 143, 19, 0, 94, 153, 145, 31, 0, 93, 31, 34, 47, 71, 186, 37, 0, 17, 59, 19, 16, 19, 187, 57, 173, 66, 59, 50, 78, 142, 87, 11, 0, 10, 4, 22, 28, 182, 255, 188, 8, 75, 0, 0, 161, 0, 66, 0, 30, 248, 84, 17, 168, 27, 0, 0, 255, 31, 69, 17, 0, 10, 140, 64, 0, 7, 0, 81, 91, 170, 0, 65, 7, 99, 0, 16, 16, 0, 23, 200, 0, 22, 30, 236, 161, 20, 117, 12, 18, 0, 206, 29, 0, 6, 0, 45, 0, 49, 0, 10, 40, 1, 100, 2, 134, 13, 0, 0, 0, 0, 48, 0, 89, 115, 197, 0, 0, 37, 67, 0, 185, 12, 113, 11, 41, 78, 20, 237, 144, 84, 0, 0, 94, 159, 125, 41, 0, 90, 19, 0, 55, 30, 174, 110, 0, 0, 47, 6, 51, 43, 170, 57, 207, 28, 37, 84, 0, 120, 92, 25, 0, 120, 4, 50, 34, 109, 234, 185, 4, 0, 0, 75, 169, 1, 38, 58, 5, 227, 125, 22, 183, 27, 0, 0, 255, 0, 180, 29, 0, 12, 102, 26, 0, 7, 0, 51, 108, 160, 8, 6, 3, 218, 7, 5, 34, 0, 21, 189, 0, 42, 8, 255, 181, 33, 135, 12, 24, 0, 83, 27, 0, 29, 0, 0, 0, 10, 0, 18, 61, 11, 40, 8, 124, 13, 0, 67, 0, 0, 21, 0, 54, 184, 238, 0, 0, 46, 113, 0, 199, 31, 65, 5, 0, 104, 30, 237, 97, 132, 0, 0, 91, 55, 99, 4, 0, 77, 24, 0, 29, 25, 130, 111, 0, 0, 75, 40, 42, 24, 109, 57, 207, 31, 38, 21, 58, 83, 0, 11, 0, 18, 4, 56, 45, 115, 205, 210, 46, 0, 0, 0, 77, 41, 80, 9, 45, 255, 125, 34, 57, 27, 52, 0, 233, 0, 58, 32, 0, 19, 114, 66, 32, 7, 0, 92, 50, 162, 0, 72, 3, 143, 28, 8, 150, 22, 20, 207, 0, 33, 0, 255, 194, 41, 134, 12, 98, 0, 145, 40, 0, 15, 0, 0, 0, 23, 13, 2, 11, 8, 52, 97, 160, 13, 0, 0, 16, 0, 22, 21, 89, 140, 47, 0, 0, 46, 88, 17, 193, 58, 158, 9, 30, 62, 59, 237, 92, 127, 0, 0, 91, 99, 82, 4, 0, 97, 15, 0, 34, 64, 182, 228, 0, 36, 66, 0, 90, 53, 158, 21, 174, 25, 16, 70, 73, 74, 36, 17, 74, 44, 19, 58, 41, 161, 255, 255, 3, 0, 0, 0, 117, 0, 85, 0, 5, 255, 86, 2, 142, 27, 0, 16, 244, 0, 179, 31, 0, 16, 117, 96, 0, 7, 0, 105, 82, 178, 0, 71, 6, 76, 14, 4, 249, 75, 37, 124, 0, 65, 0, 255, 184, 52, 131, 12, 40, 0, 146, 94, 0, 58, 0, 0, 0, 38, 10, 14, 10, 1, 36, 12, 152, 13, 0, 0, 0, 0, 71, 26, 68, 138, 81, 0, 0, 53, 84, 0, 150, 58, 129, 5, 11, 130, 82, 237, 94, 83, 0, 0, 89, 104, 130, 14, 0, 60, 16, 0, 69, 38, 181, 70, 8, 22, 60, 0, 46, 0, 187, 24, 174, 16, 50, 47, 78, 111, 44, 31, 96, 17, 43, 17, 79, 183, 255, 139, 3, 0, 75, 155, 162, 0, 84, 48, 11, 255, 111, 40, 183, 27, 61, 0, 224, 66, 52, 24, 0, 16, 125, 60, 0, 7, 0, 27, 68, 173, 0, 65, 7, 93, 0, 4, 175, 0, 14, 189, 0, 37, 0, 255, 185, 65, 127, 12, 10, 0, 70, 53, 76, 21, 0, 0, 0, 59, 0, 95, 43, 1, 102, 9, 144, 13, 0, 0, 0, 26, 59, 0, 71, 109, 66, 0, 0, 42, 0, 0, 199, 38, 47, 29, 0, 132, 44, 237, 49, 79, 0, 0, 73, 75, 168, 14, 0, 102, 40, 0, 69, 16, 179, 77, 0, 20, 111, 75, 72, 55, 181, 33, 207, 37, 43, 74, 50, 101, 51, 29, 0, 46, 25, 26, 68, 143, 207, 203, 40, 0, 73, 0, 157, 0, 76, 17, 8, 255, 104, 66, 183, 27, 112, 20, 238, 41, 26, 0, 22, 2, 155, 73, 35, 7, 0, 118, 70, 105, 0, 64, 3, 149, 12, 4, 159, 0, 12, 168, 0, 25, 0, 233, 154, 10, 102, 12, 1, 0, 25, 77, 0, 73, 0, 0, 0, 8, 27, 75, 67, 41, 206, 8, 135, 13, 0, 89, 0, 28, 101, 0, 44, 223, 10, 0, 0, 142, 74, 31, 199, 0, 50, 23, 0, 160, 117, 237, 92, 180, 0, 0, 69, 79, 100, 36, 0, 109, 74, 0, 69, 28, 178, 43, 0, 0, 60, 31, 20, 26, 174, 40, 207, 61, 69, 99, 31, 122, 27, 24, 0, 17, 4, 62, 129, 177, 253, 176, 49, 5, 0, 0, 167, 48, 89, 0, 24, 241, 112, 24, 177, 37, 0, 0, 255, 63, 114, 0, 0, 2, 134, 35, 11, 7, 0, 27, 83, 162, 0, 79, 3, 118, 13, 4, 40, 25, 16, 204, 0, 29, 0, 255, 173, 54, 109, 12, 1, 0, 100, 92, 0, 17, 0, 12, 0, 1, 0, 28, 136, 14, 120, 36, 157, 13, 0, 0, 0, 0, 85, 0, 44, 222, 118, 0, 0, 76, 92, 11, 199, 21, 122, 0, 15, 119, 0, 237, 94, 179, 0, 0, 89, 38, 147, 13, 0, 88, 5, 0, 69, 19, 156, 10, 0, 0, 48, 59, 57, 39, 187, 41, 207, 7, 40, 47, 78, 171, 44, 25, 0, 18, 4, 77, 193, 136, 255, 243, 3, 0, 19, 0, 154, 0, 63, 47, 40, 255, 126, 7, 173, 27, 0, 0, 203, 59, 62, 66, 0, 2, 51, 58, 26, 29, 0, 127, 111, 170, 0, 62, 3, 90, 0, 4, 181, 0, 43, 215, 0, 111, 0, 255, 194, 50, 134, 12, 1, 0, 83, 29, 0, 9, 2, 0, 0, 29, 81, 6, 7, 51, 66, 33, 229, 13, 0, 118, 0, 40, 109, 0, 85, 162, 72, 0, 0, 68, 79, 0, 178, 35, 115, 1, 53, 99, 8, 237, 160, 104, 0, 0, 91, 51, 90, 17, 0, 119, 26, 0, 69, 8, 129, 86, 25, 0, 78, 46, 65, 42, 134, 60, 203, 22, 71, 91, 50, 104, 39, 23, 0, 13, 4, 61, 94, 112, 245, 237, 0, 49, 0, 0, 139, 0, 93, 0, 13, 228, 113, 16, 177, 27, 40, 0, 255, 0, 84, 0, 73, 40, 179, 134, 0, 7, 0, 94, 82, 137, 0, 79, 3, 156, 50, 28, 199, 55, 31, 194, 0, 59, 0, 255, 168, 19, 105, 12, 17, 0, 80, 23, 0, 10, 0, 0, 0, 16, 81, 52, 8, 67, 54, 39, 196, 13, 0, 11, 22, 24, 75, 40, 46, 197, 29, 0, 0, 67, 114, 0, 199, 53, 105, 0, 8, 43, 25, 237, 108, 129, 0, 0, 74, 99, 108, 10, 0, 128, 14, 0, 46, 10, 172, 63, 0, 0, 23, 85, 40, 13, 184, 33, 188, 8, 52, 59, 78, 145, 21, 22, 0, 42, 4, 86, 67, 41, 255, 255, 0, 0, 0, 0, 157, 26, 93, 12, 7, 255, 113, 2, 177, 27, 28, 0, 255, 63, 35, 71, 0, 5, 37, 17, 0, 7, 0, 66, 104, 125, 0, 73, 3, 255, 9, 4, 68, 14, 42, 194, 64, 51, 0, 225, 169, 14, 140, 12, 1, 0, 75, 55, 0, 30, 0, 0, 0, 1, 0, 43, 39, 39, 55, 26, 165, 13, 0, 0, 1, 0, 46, 15, 112, 177, 1, 0, 0, 45, 76, 0, 199, 42, 92, 31, 31, 46, 9, 237, 235, 97, 0, 0, 94, 96, 122, 4, 0, 76, 44, 57, 20, 0, 183, 78, 47, 19, 23, 44, 43, 10, 147, 21, 196, 19, 0, 68, 78, 122, 47, 11, 0, 32, 4, 61, 28, 161, 251, 255, 1, 1, 0, 0, 169, 162, 93, 0, 5, 255, 93, 35, 127, 27, 0, 13, 233, 0, 144, 74, 30, 9, 110, 17, 3, 7, 0, 20, 100, 180, 0, 30, 3, 99, 47, 4, 174, 0, 16, 226, 10, 59, 0, 226, 238, 28, 122, 12, 60, 0, 75, 43, 0, 156, 0, 16, 0, 20, 46, 67, 79, 1, 82, 14, 59, 17, 0, 49, 0, 154, 10, 49, 94, 117, 35, 0, 0, 28, 66, 64, 119, 101, 101, 36, 64, 137, 27, 237, 177, 147, 0, 0, 83, 104, 78, 16, 0, 137, 49, 0, 69, 15, 185, 18, 22, 0, 50, 29, 21, 68, 181, 85, 193, 27, 41, 115, 41, 130, 20, 17, 0, 29, 4, 121, 64, 111, 255, 255, 15, 23, 22, 0, 169, 12, 93, 0, 5, 255, 99, 25, 164, 27, 9, 14, 255, 0, 41, 88, 14, 2, 72, 34, 44, 7, 0, 52, 85, 158, 0, 64, 3, 184, 26, 11, 68, 33, 21, 213, 0, 23, 0, 236, 253, 74, 140, 12, 24, 0, 125, 118, 0, 255, 0, 63, 0, 11, 0, 0, 94, 9, 84, 41, 143, 22, 0, 0, 0, 0, 63, 0, 128, 194, 118, 0, 0, 35, 66, 0, 193, 23, 169, 3, 102, 130, 21, 237, 40, 205, 0, 0, 85, 116, 112, 13, 0, 77, 13, 0, 69, 21, 160, 6, 46, 23, 47, 39, 18, 68, 187, 115, 192, 32, 49, 65, 78, 98, 0, 11, 0, 53, 12, 55, 47, 197, 214, 255, 28, 10, 0, 0, 164, 81, 93, 33, 23, 255, 135, 51, 183, 37, 52, 0, 237, 77, 22, 70, 0, 2, 105, 51, 81, 7, 0, 119, 67, 182, 0, 69, 3, 154, 0, 4, 172, 0, 7, 199, 0, 34, 58, 255, 248, 61, 140, 12, 1, 0, 174, 39, 0, 16, 0, 42, 0, 30, 0, 0, 40, 4, 116, 12, 125, 13, 0, 4, 0, 0, 36, 22, 79, 208, 106, 0, 0, 56, 100, 47, 199, 0, 130, 0, 0, 130, 28, 237, 124, 184, 0, 0, 89, 87, 119, 10, 0, 85, 25, 18, 56, 33, 139, 6, 0, 0, 82, 69, 14, 68, 175, 21, 196, 31, 15, 90, 30, 166, 25, 16, 0, 43, 4, 91, 59, 175, 247, 206, 22, 16, 0, 0, 163, 21, 76, 0, 5, 220, 80, 7, 172, 27, 10, 0, 255, 36, 64, 35, 0, 2, 148, 39, 6, 7, 0, 32, 49, 158, 125, 74, 3, 90, 3, 4, 75, 0, 7, 190, 0, 25, 42, 255, 177, 41, 130, 12, 27, 0, 123, 16, 0, 21, 0, 85, 0, 10, 0, 0, 41, 7, 60, 11, 181, 13, 0, 0, 0, 0, 117, 44, 79, 214, 100, 0, 0, 43, 55, 22, 199, 63, 158, 15, 10, 135, 8, 237, 86, 121, 0, 0, 94, 124, 94, 19, 0, 67, 14, 0, 69, 0, 155, 34, 19, 51, 42, 5, 31, 68, 175, 21, 207, 38, 67, 48, 78, 115, 49, 43, 37, 5, 4, 28, 46, 156, 237, 255, 4, 0, 0, 87, 157, 0, 66, 0, 21, 255, 92, 62, 159, 27, 14, 4, 255, 17, 71, 61, 25, 5, 52, 31, 0, 7, 0, 105, 81, 120, 16, 73, 6, 97, 7, 4, 102, 0, 29, 250, 0, 30, 23, 255, 166, 74, 128, 12, 38, 0, 154, 112, 0, 26, 0, 7, 0, 60, 0, 0, 23, 1, 74, 7, 177, 13, 0, 36, 0, 0, 93, 46, 58, 214, 157, 0, 0, 62, 96, 0, 199, 7, 74, 14, 0, 148, 12, 237, 94, 144, 0, 53, 94, 111, 132, 23, 0, 80, 20, 1, 69, 21, 180, 110, 23, 74, 4, 0, 60, 0, 187, 65, 205, 16, 36, 112, 43, 119, 42, 32, 42, 7, 4, 47, 39, 133, 210, 216, 12, 0, 0, 0, 106, 0, 88, 0, 5, 255, 120, 65, 50, 46, 31, 0, 254, 0, 90, 47, 0, 25, 151, 26, 4, 7, 0, 93, 88, 135, 0, 64, 7, 133, 2, 13, 192, 50, 7, 200, 23, 35, 0, 216, 202, 74, 140, 12, 1, 0, 193, 125, 0, 28, 0, 0, 0, 6, 0, 0, 46, 40, 113, 8, 165, 13, 0, 0, 0, 17, 99, 65, 63, 255, 57, 0, 0, 47, 95, 0, 199, 0, 93, 36, 0, 155, 53, 237, 118, 170, 0, 0, 94, 57, 100, 21, 0, 111, 23, 0, 63, 25, 153, 156, 0, 22, 133, 42, 72, 0, 156, 26, 193, 84, 16, 64, 66, 134, 0, 11, 0, 16, 4, 45, 109, 144, 255, 197, 22, 12, 0, 158, 85, 50, 89, 0, 24, 255, 136, 27, 116, 32, 22, 0, 255, 0, 65, 0, 26, 9, 81, 53, 0, 7, 0, 74, 72, 168, 0, 79, 3, 137, 23, 4, 137, 10, 7, 177, 0, 23, 0, 255, 223, 74, 107, 12, 33, 0, 140, 51, 0, 28, 0, 0, 0, 7, 38, 10, 44, 37, 93, 13, 144, 13, 0, 119, 0, 0, 40, 62, 67, 254, 42, 0, 0, 52, 113, 0, 171, 33, 166, 0, 0, 122, 24, 237, 57, 113, 0, 0, 89, 151, 103, 23, 0, 127, 5, 0, 69, 24, 114, 108, 27, 0, 147, 0, 39, 55, 170, 30, 182, 0, 12, 54, 78, 166, 71, 11, 0, 39, 4, 29, 92, 92, 248, 209, 28, 37, 0, 0, 112, 17, 84, 30, 43, 214, 125, 42, 183, 27, 0, 0, 255, 39, 161, 20, 0, 9, 123, 49, 37, 7, 0, 72, 105, 182, 0, 79, 6, 128, 0, 15, 30, 0, 11, 188, 0, 36, 8, 255, 147, 74, 111, 12, 12, 0, 178, 37, 59, 5, 0, 0, 0, 5, 11, 25, 41, 20, 111, 10, 212, 13, 0, 0, 107, 0, 113, 16, 110, 177, 70, 0, 0, 60, 47, 17, 176, 24, 127, 0, 63, 143, 34, 237, 163, 172, 0, 0, 94, 79, 115, 4, 0, 85, 9, 0, 69, 22, 145, 33, 0, 28, 39, 23, 33, 68, 187, 24, 198, 16, 39, 79, 66, 113, 0, 11, 81, 46, 4, 83, 105, 96, 255, 255, 27, 0, 0, 0, 163, 0, 93, 0, 5, 225, 101, 76, 167, 27, 23, 14, 255, 48, 114, 90, 34, 8, 129, 40, 0, 7, 0, 69, 84, 143, 0, 72, 7, 122, 16, 19, 86, 0, 16, 205, 11, 47, 0, 250, 172, 62, 136, 12, 1, 0, 59, 57, 0, 31, 0, 29, 0, 16, 0, 49, 47, 39, 74, 8, 190, 13, 0, 26, 0, 0, 103, 6, 82, 203, 112, 0, 0, 32, 88, 0, 185, 40, 138, 0, 0, 129, 49, 237, 78, 104, 0, 0, 94, 107, 105, 58, 0, 91, 29, 0, 55, 42, 183, 9, 0, 9, 66, 0, 23, 46, 187, 48, 181, 0, 22, 62, 78, 139, 46, 11, 0, 25, 4, 53, 61, 91, 243, 255, 2, 43, 0, 92, 139, 0, 93, 73, 5, 255, 126, 2, 178, 27, 0, 0, 239, 50, 56, 23, 0, 27, 83, 61, 55, 7, 0, 30, 91, 185, 0, 57, 3, 201, 32, 4, 101, 0, 22, 179, 13, 57, 0, 255, 155, 0, 104, 12, 18, 0, 139, 79, 0, 18, 0, 20, 0, 18, 0, 34, 48, 10, 122, 16, 98, 13, 0, 0, 47, 0, 28, 0, 41, 188, 84, 0, 0, 90, 96, 0, 199, 0, 95, 3, 49, 123, 23, 237, 101, 134, 0, 0, 94, 144, 73, 64, 0, 79, 5, 10, 51, 8, 175, 57, 50, 21, 27, 0, 43, 68, 164, 21, 199, 3, 30, 65, 38, 141, 52, 11, 36, 48, 4, 51, 94, 123, 255, 255, 0, 0, 0, 0, 137, 0, 78, 52, 21, 255, 95, 6, 146, 27, 0, 28, 255, 0, 139, 28, 0, 25, 85, 52, 26, 7, 0, 66, 110, 164, 51, 55, 3, 145, 9, 29, 177, 0, 15, 176, 11, 35, 28, 255, 160, 59, 129, 12, 1, 0, 109, 80, 0, 9, 69, 0, 0, 6, 0, 1, 23, 8, 67, 2, 159, 13, 0, 33, 0, 0, 78, 6, 90, 161, 118, 0, 0, 63, 109, 0, 177, 12, 78, 17, 0, 128, 13, 237, 111, 131, 0, 0, 94, 110, 121, 4, 0, 107, 5, 0, 27, 5, 181, 92, 0, 0, 33, 0, 58, 68, 128, 21, 207, 7, 81, 75, 23, 83, 30, 11, 0, 47, 4, 75, 45, 114, 255, 202, 22, 0, 1, 18, 154, 0, 72, 0, 5, 199, 89, 19, 98, 27, 0, 0, 255, 0, 186, 0, 0, 21, 142, 46, 56, 7, 0, 82, 89, 127, 0, 8, 3, 131, 9, 18, 27, 0, 15, 180, 13, 28, 27, 251, 180, 74, 125, 12, 1, 0, 62, 93, 0, 86, 0, 0, 0, 7, 0, 7, 58, 45, 11, 2, 212, 13, 0, 105, 0, 4, 119, 0, 82, 212, 108, 0, 0, 69, 57, 0, 169, 42, 137, 4, 8, 55, 26, 237, 168, 171, 0, 0, 80, 48, 138, 4, 0, 100, 25, 0, 64, 24, 172, 21, 47, 0, 66, 20, 36, 68, 159, 21, 207, 67, 31, 97, 65, 112, 0, 39, 0, 30, 4, 91, 44, 94, 255, 145, 21, 133, 22, 29, 152, 151, 76, 0, 5, 226, 103, 30, 53, 27, 63, 46, 229, 76, 19, 94, 0, 26, 88, 43, 0, 7, 0, 37, 80, 68, 0, 79, 27, 122, 7, 25, 124, 0, 37, 219, 9, 52, 0, 255, 180, 74, 119, 12, 71, 0, 78, 43, 0, 7, 0, 35, 0, 1, 59, 14, 39, 27, 40, 9, 222, 13, 0, 0, 77, 0, 66, 0, 97, 141, 47, 0, 0, 28, 24, 0, 199, 94, 120, 0, 88, 171, 16, 237, 129, 116, 0, 0, 74, 82, 82, 13, 0, 93, 5, 0, 61, 6, 178, 42, 35, 27, 42, 0, 32, 58, 158, 21, 195, 28, 22, 65, 66, 161, 45, 21, 0, 5, 4, 71, 88, 136, 251, 251, 4, 0, 0, 0, 118, 0, 77, 0, 17, 255, 76, 14, 116, 27, 0, 0, 251, 13, 126, 11, 9, 3, 102, 74, 1, 7, 0, 53, 89, 95, 0, 79, 3, 95, 10, 35, 125, 18, 14, 205, 0, 51, 0, 255, 182, 74, 123, 12, 83, 0, 106, 47, 0, 22, 81, 0, 0, 21, 0, 18, 15, 15, 47, 9, 196, 13, 0, 74, 0, 0, 97, 0, 64, 162, 117, 0, 0, 100, 74, 0, 199, 30, 114, 33, 0, 116, 34, 237, 93, 95, 0, 0, 94, 117, 109, 22, 0, 113, 70, 0, 54, 18, 178, 123, 0, 52, 79, 0, 63, 0, 187, 27, 173, 16, 44, 66, 71, 177, 92, 44, 0, 5, 64, 24, 77, 146, 248, 220, 38, 114, 0, 23, 139, 0, 72, 56, 5, 255, 68, 12, 183, 47, 0, 0, 255, 38, 96, 0, 0, 2, 148, 69, 0, 7, 0, 32, 110, 165, 0, 79, 11, 147, 0, 4, 109, 0, 17, 195, 0, 47, 0, 220, 184, 56, 130, 12, 1, 0, 185, 57, 0, 19, 0, 0, 0, 15, 4, 25, 8, 2, 117, 2, 138, 13, 0, 0, 56, 0, 55, 0, 110, 255, 109, 0, 0, 128, 102, 0, 179, 0, 83, 31, 55, 136, 58, 237, 61, 154, 0, 0, 56, 79, 86, 45, 0, 121, 73, 0, 69, 0, 186, 80, 0, 0, 70, 0, 27, 0, 184, 21, 171, 31, 95, 18, 71, 158, 0, 11, 55, 5, 4, 124, 100, 129, 255, 204, 42, 0, 0, 107, 164, 22, 93, 72, 14, 255, 85, 6, 159, 27, 0, 0, 241, 22, 113, 0, 0, 2, 99, 23, 5, 7, 0, 70, 83, 179, 0, 79, 3, 115, 0, 13, 46, 0, 7, 207, 0, 41, 0, 210, 199, 74, 94, 12, 1, 0, 175, 16, 0, 27, 0, 0, 0, 48, 0, 0, 4, 28, 43, 2, 177, 13, 0, 47, 0, 9, 131, 32, 113, 255, 100, 0, 0, 60, 33, 0, 183, 0, 118, 13, 0, 145, 69, 237, 78, 135, 0, 0, 37, 101, 123, 14, 0, 123, 50, 0, 69, 0, 184, 169, 0, 26, 87, 39, 62, 55, 170, 21, 193, 14, 142, 61, 78, 189, 0, 21, 84, 7, 4, 31, 166, 130, 246, 255, 6, 0, 0, 0, 120, 0, 93, 7, 35, 255, 89, 14, 80, 27, 0, 0, 255, 0, 210, 0, 82, 2, 205, 22, 0, 7, 0, 125, 104, 120, 16, 79, 3, 136, 6, 33, 42, 51, 24, 118, 11, 28, 0, 205, 187, 47, 81, 12, 46, 0, 153, 57, 0, 40, 0, 0, 0, 104, 0, 0, 10, 31, 55, 2, 176, 13, 0, 0, 47, 38, 144, 40, 121, 251, 124, 0, 0, 59, 141, 10, 115, 0, 128, 15, 36, 68, 20, 237, 120, 154, 0, 0, 94, 137, 96, 31, 0, 103, 45, 0, 49, 2, 159, 73, 0, 0, 79, 0, 50, 68, 175, 49, 207, 30, 61, 86, 62, 150, 26, 11, 0, 16, 4, 93, 106, 144, 234, 225, 27, 0, 2, 13, 144, 0, 36, 8, 23, 217, 128, 7, 148, 27, 7, 0, 243, 21, 90, 82, 0, 2, 105, 53, 28, 7, 0, 52, 91, 164, 78, 79, 3, 167, 17, 21, 170, 20, 11, 235, 0, 42, 15, 255, 179, 33, 132, 12, 98, 0, 149, 23, 0, 9, 0, 0, 0, 1, 0, 0, 46, 1, 82, 10, 172, 13, 0, 0, 0, 5, 61, 23, 132, 169, 88, 0, 0, 28, 80, 0, 167, 77, 94, 6, 0, 131, 13, 237, 170, 180, 0, 0, 94, 70, 60, 4, 0, 78, 56, 0, 69, 4, 153, 56, 0, 4, 26, 8, 33, 43, 171, 69, 189, 28, 69, 43, 62, 120, 86, 11, 30, 22, 4, 147, 118, 144, 255, 255, 21, 0, 14, 23, 153, 0, 51, 66, 57, 255, 84, 25, 183, 27, 96, 0, 255, 40, 97, 79, 0, 5, 72, 38, 67, 7, 0, 95, 81, 189, 0, 79, 3, 136, 7, 22, 46, 69, 20, 240, 0, 51, 0, 255, 168, 0, 140, 12, 63, 0, 79, 33, 4, 17, 57, 0, 0, 64, 0, 0, 45, 12, 57, 15, 120, 13, 0, 0, 0, 0, 55, 29, 99, 112, 49, 0, 0, 39, 83, 0, 199, 73, 123, 6, 17, 109, 24, 220, 136, 137, 0, 0, 94, 61, 75, 4, 0, 88, 39, 0, 65, 8, 174, 78, 0, 15, 20, 0, 49, 44, 152, 59, 190, 34, 35, 18, 22, 135, 29, 17, 0, 5, 4, 86, 69, 119, 255, 255, 0, 0, 0, 68, 169, 0, 93, 18, 20, 254, 74, 33, 148, 27, 58, 62, 217, 0, 32, 66, 14, 2, 106, 65, 2, 7, 0, 75, 103, 124, 0, 79, 7, 196, 1, 14, 41, 31, 7, 207, 11, 34, 0, 255, 164, 28, 128, 12, 45, 0, 162, 39, 0, 23, 0, 0, 0, 61, 0, 3, 51, 5, 45, 12, 91, 13, 0, 129, 0, 39, 45, 58, 135, 196, 19, 0, 0, 78, 103, 0, 183, 42, 185, 14, 0, 69, 22, 209, 117, 141, 0, 0, 43, 41, 125, 4, 0, 51, 15, 0, 50, 33, 139, 30, 0, 30, 34, 62, 36, 68, 157, 63, 207, 10, 35, 64, 8, 118, 57, 28, 26, 48, 4, 35, 116, 125, 184, 208, 14, 0, 37, 44, 164, 0, 93, 0, 89, 164, 70, 47, 48, 27, 112, 19, 204, 0, 54, 17, 0, 21, 159, 51, 55, 7, 0, 123, 105, 173, 21, 69, 8, 196, 9, 12, 200, 0, 7, 233, 0, 52, 0, 255, 217, 52, 124, 12, 11, 0, 46, 56, 0, 52, 0, 54, 0, 16, 10, 11, 7, 1, 31, 23, 155, 13, 0, 0, 32, 0, 102, 0, 88, 158, 28, 0, 0, 28, 128, 0, 155, 58, 92, 4, 27, 133, 26, 237, 155, 251, 0, 0, 57, 81, 113, 31, 0, 81, 10, 0, 30, 24, 157, 6, 0, 0, 55, 84, 1, 0, 172, 21, 207, 56, 58, 89, 64, 104, 0, 21, 22, 97, 4, 125, 70, 119, 225, 241, 49, 0, 0, 0, 156, 0, 93, 0, 5, 234, 120, 2, 164, 27, 67, 0, 246, 19, 47, 23, 0, 1, 99, 41, 37, 7, 0, 74, 47, 156, 4, 79, 4, 148, 23, 4, 56, 0, 23, 253, 50, 31, 25, 255, 197, 12, 140, 12, 12, 0, 179, 49, 0, 66, 0, 47, 0, 6, 0, 0, 12, 24, 73, 15, 170, 13, 0, 11, 0, 0, 78, 16, 29, 211, 47, 0, 0, 52, 70, 0, 171, 56, 184, 0, 6, 104, 28, 237, 91, 102, 0, 0, 94, 86, 85, 16, 0, 88, 13, 0, 67, 21, 166, 96, 58, 45, 50, 0, 64, 0, 187, 36, 187, 22, 35, 41, 71, 112, 0, 17, 6, 45, 9, 26, 89, 160, 255, 239, 34, 0, 0, 14, 155, 25, 93, 22, 5, 255, 73, 47, 176, 49, 17, 0, 240, 8, 114, 0, 0, 9, 100, 45, 71, 7, 0, 75, 53, 189, 0, 79, 5, 103, 0, 4, 124, 0, 12, 211, 0, 57, 14, 255, 207, 74, 125, 12, 1, 0, 82, 57, 51, 24, 0, 0, 0, 25, 3, 58, 39, 1, 81, 9, 126, 13, 0, 0, 0, 0, 91, 21, 94, 201, 70, 0, 0, 71, 106, 11, 191, 18, 112, 1, 59, 160, 8, 237, 69, 136, 0, 0, 73, 53, 133, 4, 0, 98, 21, 0, 64, 18, 157, 68, 0, 17, 72, 0, 30, 0, 174, 62, 161, 10, 79, 46, 29, 149, 0, 28, 41, 34, 6, 64, 115, 160, 202, 222, 23, 0, 0, 0, 146, 0, 93, 39, 19, 242, 45, 22, 172, 27, 27, 0, 233, 21, 51, 0, 36, 2, 122, 44, 52, 7, 0, 93, 106, 129, 9, 79, 7, 164, 11, 4, 109, 0, 12, 129, 0, 33, 65, 255, 188, 45, 108, 12, 53, 0, 102, 35, 21, 51, 0, 0, 0, 55, 46, 9, 18, 13, 57, 31, 132, 13, 0, 42, 0, 0, 74, 31, 89, 193, 11, 0, 0, 105, 106, 83, 199, 27, 130, 4, 0, 109, 16, 237, 109, 129, 0, 0, 94, 121, 138, 57, 0, 122, 31, 0, 69, 0, 138, 70, 0, 9, 36, 0, 29, 0, 161, 66, 207, 2, 57, 63, 65, 126, 0, 22, 0, 70, 8, 79, 100, 123, 243, 255, 1, 0, 0, 0, 91, 0, 86, 22, 8, 212, 102, 41, 147, 27, 15, 0, 255, 46, 70, 17, 0, 6, 102, 67, 36, 7, 0, 54, 76, 170, 38, 79, 8, 173, 21, 4, 167, 33, 14, 197, 0, 44, 0, 255, 193, 40, 123, 12, 76, 0, 98, 49, 81, 47, 36, 0, 0, 37, 0, 3, 45, 22, 27, 46, 170, 13, 0, 32, 0, 0, 95, 0, 44, 230, 109, 0, 0, 46, 120, 0, 199, 20, 107, 1, 0, 132, 21, 237, 128, 89, 0, 0, 34, 171, 101, 46, 0, 88, 5, 38, 69, 23, 111, 69, 0, 5, 97, 42, 40, 15, 133, 21, 196, 2, 76, 34, 29, 154, 67, 71, 5, 49, 6, 45, 118, 121, 222, 227, 29, 0, 5, 90, 153, 0, 60, 56, 67, 202, 102, 30, 147, 27, 0, 0, 255, 43, 175, 13, 0, 7, 158, 68, 4, 7, 0, 99, 98, 154, 0, 79, 3, 144, 0, 13, 32, 33, 7, 197, 28, 23, 0, 255, 160, 74, 104, 12, 6, 0, 119, 42, 0, 18, 0, 0, 0, 91, 0, 12, 27, 13, 90, 31, 222, 13, 0, 0, 5, 20, 132, 0, 71, 240, 255, 0, 0, 110, 153, 20, 193, 1, 114, 2, 62, 102, 18, 237, 95, 117, 0, 0, 83, 87, 104, 23, 0, 104, 19, 24, 69, 8, 132, 79, 0, 4, 46, 43, 54, 0, 179, 21, 154, 54, 55, 59, 55, 102, 16, 27, 0, 5, 8, 49, 63, 155, 255, 225, 3, 0, 5, 0, 94, 53, 51, 23, 43, 209, 114, 46, 127, 27, 63, 0, 243, 0, 46, 65, 0, 2, 81, 67, 13, 7, 0, 42, 102, 171, 31, 79, 3, 104, 0, 20, 140, 0, 7, 127, 0, 29, 119, 255, 214, 62, 111, 12, 1, 0, 78, 76, 0, 41, 0, 6, 0, 54, 0, 29, 64, 16, 158, 12, 201, 13, 0, 49, 0, 0, 107, 0, 79, 187, 41, 0, 0, 111, 83, 20, 199, 18, 105, 16, 0, 197, 34, 237, 99, 109, 0, 0, 94, 87, 88, 41, 0, 69, 5, 0, 29, 12, 157, 29, 0, 20, 33, 0, 39, 0, 187, 31, 198, 30, 21, 58, 78, 161, 49, 24, 0, 31, 4, 43, 74, 116, 255, 189, 0, 4, 0, 137, 145, 19, 89, 0, 10, 236, 81, 21, 42, 27, 57, 17, 196, 86, 14, 24, 0, 2, 159, 68, 0, 7, 0, 49, 107, 165, 2, 35, 3, 155, 5, 38, 109, 11, 20, 216, 0, 43, 0, 255, 186, 43, 126, 12, 1, 0, 107, 39, 0, 51, 0, 25, 0, 1, 16, 16, 27, 30, 56, 2, 177, 13, 0, 10, 0, 0, 92, 0, 131, 189, 34, 0, 0, 51, 108, 0, 175, 45, 131, 26, 4, 209, 62, 237, 171, 71, 0, 0, 70, 150, 159, 28, 0, 54, 66, 48, 69, 14, 180, 35, 0, 45, 10, 38, 45, 68, 152, 45, 207, 0, 0, 70, 78, 100, 60, 11, 0, 59, 4, 18, 34, 158, 244, 255, 0, 42, 17, 0, 169, 90, 85, 0, 8, 219, 97, 46, 154, 27, 31, 57, 191, 39, 11, 106, 6, 2, 69, 36, 0, 7, 0, 58, 111, 165, 0, 19, 3, 78, 33, 18, 196, 4, 59, 195, 0, 53, 0, 194, 192, 47, 134, 12, 80, 0, 54, 53, 0, 134, 0, 31, 0, 25, 0, 65, 10, 1, 11, 2, 60, 22, 0, 62, 0, 112, 22, 0, 85, 172, 35, 0, 0, 28, 67, 0, 77, 81, 104, 9, 18, 140, 81, 237, 170, 88, 0, 0, 84, 174, 100, 49, 0, 98, 42, 0, 67, 19, 170, 36, 0, 9, 38, 0, 24, 0, 110, 40, 199, 67, 78, 32, 32, 92, 73, 25, 0, 32, 4, 19, 47, 104, 255, 178, 2, 0, 52, 247, 107, 0, 93, 0, 5, 245, 85, 49, 82, 27, 0, 0, 253, 0, 36, 31, 18, 34, 144, 46, 30, 7, 0, 50, 62, 178, 0, 64, 7, 76, 49, 28, 134, 15, 7, 205, 0, 62, 0, 233, 217, 33, 122, 12, 49, 0, 138, 27, 0, 255, 35, 13, 0, 7, 0, 56, 20, 43, 69, 68, 189, 13, 0, 40, 0, 0, 63, 0, 116, 171, 146, 0, 0, 40, 92, 0, 140, 56, 113, 11, 6, 105, 48, 237, 73, 128, 0, 0, 89, 58, 97, 34, 0, 107, 28, 0, 53, 31, 157, 153, 0, 20, 59, 0, 55, 68, 187, 47, 190, 87, 107, 62, 78, 79, 69, 11, 0, 17, 14, 7, 64, 145, 255, 239, 7, 59, 38, 41, 129, 28, 62, 53, 35, 234, 141, 43, 169, 27, 60, 22, 255, 8, 98, 19, 0, 28, 127, 48, 76, 7, 0, 61, 73, 172, 32, 62, 4, 68, 26, 45, 255, 41, 7, 223, 0, 69, 0, 255, 169, 70, 122, 12, 33, 0, 110, 71, 0, 9, 0, 0, 0, 24, 118, 32, 12, 1, 91, 20, 192, 13, 0, 0, 18, 0, 78, 22, 131, 176, 40, 0, 0, 47, 31, 88, 199, 15, 70, 0, 0, 128, 86, 237, 108, 114, 0, 0, 84, 129, 101, 27, 0, 73, 16, 0, 41, 26, 159, 6, 0, 47, 30, 0, 7, 68, 184, 51, 185, 24, 69, 106, 20, 114, 60, 11, 0, 52, 4, 67, 132, 120, 255, 255, 5, 0, 18, 0, 169, 0, 93, 0, 5, 227, 123, 7, 127, 34, 0, 36, 255, 0, 60, 0, 0, 2, 134, 28, 34, 7, 26, 26, 87, 139, 0, 79, 5, 137, 10, 11, 42, 72, 27, 187, 0, 21, 0, 255, 139, 67, 140, 12, 1, 0, 78, 74, 0, 71, 0, 65, 0, 8, 0, 10, 32, 1, 43, 2, 172, 13, 0, 0, 0, 52, 128, 0, 124, 216, 119, 0, 0, 41, 76, 0, 199, 16, 95, 10, 31, 101, 21, 237, 61, 72, 0, 0, 83, 128, 119, 7, 0, 106, 13, 0, 69, 2, 180, 40, 0, 0, 47, 0, 31, 24, 171, 21, 199, 22, 26, 92, 75, 92, 25, 11, 0, 55, 4, 7, 93, 115, 255, 255, 4, 0, 0, 0, 157, 0, 93, 0, 12, 234, 93, 9, 104, 27, 0, 19, 255, 0, 78, 28, 0, 10, 105, 69, 76, 7, 0, 25, 101, 127, 0, 79, 3, 158, 0, 4, 141, 39, 12, 116, 13, 36, 0, 236, 127, 74, 135, 12, 1, 0, 87, 31, 69, 12, 0, 0, 0, 1, 18, 54, 8, 15, 65, 2, 173, 13, 0, 22, 0, 35, 89, 0, 89, 245, 167, 0, 0, 51, 118, 0, 199, 0, 86, 21, 0, 56, 13, 237, 87, 117, 0, 0, 94, 117, 95, 8, 0, 102, 5, 0, 56, 6, 177, 30, 16, 15, 54, 54, 20, 68, 166, 21, 205, 47, 79, 71, 78, 124, 92, 11, 0, 15, 4, 90, 52, 169, 242, 227, 0, 34, 38, 36, 164, 47, 85, 0, 5, 248, 121, 25, 83, 39, 0, 0, 255, 0, 76, 5, 20, 21, 150, 70, 9, 7, 0, 85, 90, 108, 0, 54, 3, 119, 41, 4, 70, 51, 7, 169, 8, 21, 0, 255, 166, 59, 132, 12, 80, 0, 203, 42, 18, 25, 0, 1, 0, 31, 0, 0, 4, 8, 71, 11, 130, 13, 0, 0, 18, 0, 63, 59, 48, 240, 78, 0, 0, 64, 112, 95, 184, 21, 163, 27, 65, 108, 25, 237, 99, 115, 0, 0, 94, 102, 107, 12, 0, 77, 22, 0, 62, 2, 163, 38, 42, 73, 45, 19, 37, 38, 173, 24, 160, 28, 44, 68, 78, 90, 0, 11, 60, 40, 4, 35, 40, 153, 229, 176, 14, 0, 16, 0, 144, 0, 86, 18, 5, 255, 128, 28, 51, 44, 34, 9, 228, 0, 136, 23, 0, 2, 93, 66, 49, 7, 0, 60, 92, 178, 0, 79, 3, 123, 4, 4, 170, 0, 7, 174, 0, 28, 30, 255, 168, 36, 135, 12, 1, 0, 64, 35, 78, 15, 0, 0, 0, 26, 49, 37, 7, 1, 53, 5, 121, 13, 0, 84, 0, 0, 37, 0, 123, 121, 167, 0, 0, 48, 104, 0, 107, 46, 54, 3, 0, 142, 47, 237, 96, 134, 0, 0, 83, 94, 113, 17, 0, 83, 5, 0, 49, 10, 154, 31, 3, 17, 57, 21, 32, 0, 166, 25, 185, 20, 21, 46, 78, 150, 54, 11, 0, 15, 4, 37, 69, 147, 234, 228, 14, 0, 14, 0, 150, 10, 72, 79, 7, 255, 111, 19, 177, 27, 56, 8, 251, 24, 93, 0, 0, 2, 126, 46, 0, 7, 20, 43, 104, 136, 143, 79, 3, 125, 0, 8, 173, 0, 7, 209, 0, 21, 114, 255, 168, 67, 131, 12, 1, 0, 120, 23, 0, 33, 0, 51, 0, 29, 0, 16, 36, 1, 71, 5, 141, 13, 0, 6, 5, 0, 93, 15, 135, 127, 19, 0, 0, 91, 92, 35, 158, 52, 149, 14, 0, 192, 7, 237, 112, 64, 4, 0, 94, 117, 102, 4, 0, 77, 15, 0, 50, 23, 147, 41, 4, 10, 49, 0, 61, 68, 158, 21, 182, 16, 47, 33, 64, 114, 92, 11, 64, 13, 4, 41, 63, 137, 239, 255, 58, 0, 26, 77, 157, 123, 93, 23, 39, 255, 94, 97, 107, 27, 0, 62, 245, 41, 96, 0, 47, 5, 73, 53, 0, 7, 0, 84, 91, 158, 0, 79, 3, 151, 0, 11, 25, 0, 15, 144, 0, 39, 50, 255, 169, 55, 107, 12, 5, 0, 84, 99, 0, 36, 0, 24, 0, 64, 0, 29, 29, 1, 77, 2, 144, 13, 0, 0, 2, 0, 103, 35, 82, 213, 106, 0, 0, 52, 125, 3, 199, 51, 75, 3, 71, 141, 23, 237, 101, 101, 0, 0, 94, 82, 96, 23, 0, 94, 11, 0, 65, 96, 161, 59, 0, 0, 70, 0, 43, 68, 172, 46, 190, 12, 65, 38, 31, 132, 41, 11, 0, 56, 4, 68, 78, 118, 242, 234, 1, 0, 0, 0, 169, 0, 89, 65, 18, 255, 101, 27, 170, 27, 0, 0, 253, 0, 111, 0, 0, 15, 184, 75, 68, 7, 0, 74, 100, 171, 0, 79, 3, 162, 5, 12, 164, 58, 7, 172, 6, 69, 0, 253, 151, 0, 102, 12, 29, 0, 95, 55, 0, 26, 0, 0, 0, 39, 20, 31, 30, 16, 92, 20, 109, 13, 0, 75, 0, 85, 37, 9, 31, 244, 86, 0, 0, 46, 91, 0, 199, 32, 94, 11, 0, 115, 49, 237, 100, 176, 0, 0, 80, 70, 91, 19, 0, 102, 13, 0, 60, 0, 133, 10, 39, 0, 41, 32, 43, 0, 166, 21, 155, 9, 78, 64, 70, 96, 0, 11, 0, 69, 4, 52, 33, 110, 241, 255, 6, 0, 0, 0, 87, 40, 85, 49, 53, 243, 92, 17, 155, 27, 27, 0, 219, 0, 67, 28, 20, 31, 151, 37, 0, 7, 0, 83, 60, 162, 0, 79, 3, 158, 1, 28, 255, 59, 21, 199, 0, 26, 0, 255, 232, 62, 134, 12, 27, 0, 62, 80, 0, 14, 0, 66, 0, 10, 44, 42, 38, 45, 62, 10, 173, 13, 0, 0, 0, 0, 116, 43, 126, 159, 45, 0, 0, 64, 65, 0, 199, 36, 200, 0, 0, 56, 0, 237, 109, 212, 0, 0, 69, 100, 116, 22, 0, 92, 18, 34, 61, 9, 153, 59, 0, 0, 36, 43, 20, 4, 150, 21, 205, 10, 105, 104, 0, 181, 55, 11, 23, 34, 4, 162, 104, 131, 242, 222, 9, 0, 0, 160, 169, 0, 0, 0, 5, 172, 124, 22, 99, 27, 0, 0, 196, 0, 103, 73, 0, 5, 130, 34, 33, 7, 0, 95, 92, 165, 0, 0, 3, 202, 8, 19, 17, 0, 70, 206, 36, 41, 0, 235, 197, 40, 134, 12, 36, 0, 117, 130, 0, 38, 0, 37, 0, 19, 0, 0, 91, 11, 40, 34, 181, 13, 0, 15, 33, 46, 98, 0, 97, 245, 89, 0, 0, 42, 106, 0, 175, 30, 148, 0, 38, 156, 0, 237, 139, 116, 0, 0, 63, 120, 101, 4, 0, 82, 45, 0, 19, 33, 169, 83, 39, 24, 27, 0, 64, 2, 172, 32, 198, 18, 32, 77, 0, 78, 50, 11, 0, 22, 4, 77, 41, 152, 237, 255, 21, 93, 39, 100, 156, 26, 93, 0, 5, 201, 79, 43, 127, 27, 0, 10, 224, 0, 179, 0, 28, 17, 119, 65, 3, 7, 0, 28, 84, 124, 0, 74, 3, 209, 2, 41, 105, 29, 7, 189, 0, 36, 1, 250, 190, 70, 117, 12, 1, 0, 111, 66, 40, 22, 0, 10, 0, 1, 0, 0, 86, 4, 37, 26, 162, 13, 0, 0, 0, 0, 56, 0, 102, 214, 90, 0, 0, 86, 118, 0, 195, 0, 83, 35, 0, 115, 12, 237, 103, 135, 0, 0, 67, 113, 121, 4, 0, 85, 5, 0, 62, 8, 167, 97, 0, 19, 61, 0, 72, 43, 179, 21, 198, 13, 16, 64, 64, 184, 37, 11, 0, 56, 4, 87, 87, 122, 252, 215, 16, 28, 0, 86, 149, 0, 85, 13, 9, 255, 121, 18, 111, 27, 48, 0, 255, 26, 64, 56, 0, 24, 85, 23, 38, 7, 0, 69, 103, 164, 0, 69, 3, 169, 16, 16, 95, 0, 7, 184, 0, 58, 0, 241, 173, 48, 140, 12, 36, 0, 162, 29, 0, 20, 0, 0, 15, 1, 0, 0, 19, 22, 70, 4, 173, 13, 0, 68, 0, 0, 43, 53, 93, 199, 59, 0, 0, 28, 39, 44, 195, 63, 114, 0, 65, 106, 51, 237, 56, 120, 0, 0, 62, 116, 102, 12, 0, 153, 40, 0, 51, 21, 147, 59, 0, 0, 78, 27, 37, 0, 152, 32, 187, 35, 66, 97, 70, 99, 12, 11, 0, 18, 4, 95, 135, 130, 206, 203, 12, 0, 0, 0, 140, 40, 93, 25, 9, 217, 134, 22, 149, 27, 0, 0, 255, 7, 65, 0, 0, 7, 173, 59, 48, 7, 0, 53, 62, 185, 48, 79, 3, 180, 64, 4, 116, 60, 20, 202, 0, 31, 0, 253, 200, 26, 85, 12, 84, 0, 174, 20, 16, 9, 0, 18, 0, 53, 0, 0, 8, 8, 64, 15, 148, 17, 0, 0, 0, 0, 57, 4, 69, 196, 148, 0, 0, 65, 91, 0, 156, 64, 187, 0, 0, 92, 3, 237, 81, 165, 0, 0, 42, 97, 103, 10, 0, 107, 47, 0, 69, 56, 183, 110, 0, 0, 45, 0, 34, 40, 178, 29, 180, 8, 122, 52, 70, 114, 0, 24, 65, 56, 4, 102, 126, 173, 255, 255, 20, 0, 0, 0, 152, 56, 84, 0, 25, 251, 127, 42, 145, 27, 0, 0, 235, 36, 141, 15, 0, 2, 84, 34, 60, 7, 0, 59, 89, 174, 0, 72, 3, 154, 10, 16, 105, 0, 28, 242, 0, 58, 58, 243, 210, 0, 102, 12, 57, 0, 56, 56, 44, 43, 0, 0, 0, 135, 17, 38, 4, 17, 87, 14, 166, 22, 0, 0, 0, 0, 88, 71, 83, 239, 173, 0, 0, 110, 107, 0, 164, 0, 158, 0, 7, 111, 25, 237, 151, 249, 0, 0, 94, 120, 85, 51, 0, 95, 42, 0, 7, 11, 180, 37, 0, 22, 34, 8, 25, 52, 172, 73, 207, 44, 40, 86, 56, 89, 0, 11, 97, 17, 4, 100, 96, 127, 246, 223, 5, 0, 0, 126, 132, 0, 59, 0, 5, 239, 119, 18, 140, 27, 0, 0, 248, 27, 64, 16, 0, 19, 87, 44, 13, 7, 0, 18, 88, 130, 0, 71, 3, 191, 0, 4, 110, 12, 37, 198, 0, 43, 0, 255, 235, 52, 140, 12, 11, 0, 129, 87, 0, 68, 0, 77, 0, 24, 6, 0, 25, 55, 31, 6, 197, 13, 0, 56, 0, 0, 72, 0, 93, 168, 102, 0, 0, 51, 78, 0, 191, 9, 171, 8, 18, 179, 21, 237, 100, 121, 0, 0, 94, 97, 154, 97, 0, 97, 9, 0, 48, 31, 143, 110, 0, 42, 33, 0, 85, 0, 167, 23, 182, 13, 28, 50, 70, 110, 68, 52, 82, 55, 4, 56, 104, 121, 169, 204, 15, 0, 20, 0, 151, 58, 72, 39, 5, 227, 80, 7, 158, 27, 65, 0, 255, 0, 98, 0, 0, 20, 140, 44, 51, 7, 0, 27, 93, 143, 0, 79, 3, 164, 3, 4, 196, 0, 7, 173, 0, 41, 0, 216, 228, 15, 79, 12, 46, 0, 181, 77, 0, 36, 0, 25, 0, 10, 26, 0, 38, 17, 63, 22, 161, 13, 0, 39, 0, 0, 65, 0, 83, 247, 46, 0, 0, 70, 124, 0, 199, 0, 102, 37, 0, 156, 18, 237, 91, 97, 0, 37, 94, 112, 117, 37, 0, 103, 32, 0, 69, 12, 137, 91, 0, 0, 59, 0, 41, 26, 187, 47, 207, 25, 33, 43, 78, 94, 84, 11, 0, 35, 4, 49, 31, 114, 231, 177, 22, 0, 57, 22, 154, 0, 93, 55, 23, 255, 115, 41, 152, 27, 48, 33, 255, 20, 29, 24, 0, 2, 109, 55, 51, 7, 0, 83, 79, 154, 144, 79, 3, 125, 10, 31, 117, 0, 7, 205, 0, 28, 0, 248, 190, 47, 83, 12, 67, 0, 128, 33, 0, 18, 0, 0, 0, 1, 0, 0, 15, 27, 79, 15, 135, 13, 0, 0, 0, 0, 89, 4, 94, 165, 90, 0, 0, 50, 55, 0, 199, 49, 75, 18, 0, 184, 2, 237, 61, 130, 0, 0, 79, 104, 98, 28, 0, 98, 53, 0, 69, 8, 166, 42, 0, 31, 51, 0, 42, 0, 178, 59, 203, 33, 40, 77, 78, 120, 61, 48, 41, 30, 4, 68, 100, 124, 255, 193, 15, 0, 0, 86, 169, 24, 93, 0, 39, 251, 129, 42, 124, 27, 5, 2, 255, 15, 101, 9, 0, 2, 122, 46, 61, 7, 0, 91, 78, 156, 19, 69, 10, 117, 3, 27, 82, 0, 7, 200, 0, 37, 0, 228, 169, 54, 89, 12, 14, 0, 127, 22, 0, 33, 0, 0, 0, 27, 0, 0, 37, 13, 64, 19, 176, 13, 0, 46, 0, 0, 72, 17, 93, 203, 100, 0, 0, 58, 49, 4, 175, 77, 104, 0, 18, 166, 30, 237, 100, 96, 0, 0, 69, 124, 113, 13, 0, 115, 68, 0, 69, 47, 173, 53, 0, 26, 100, 0, 44, 0, 181, 34, 201, 34, 24, 99, 30, 147, 56, 30, 104, 50, 4, 51, 65, 132, 255, 212, 19, 0, 19, 0, 163, 0, 78, 0, 12, 255, 99, 52, 124, 27, 0, 0, 255, 18, 128, 0, 0, 14, 114, 50, 0, 7, 0, 35, 82, 148, 5, 59, 11, 135, 0, 10, 183, 30, 20, 150, 0, 74, 12, 232, 196, 21, 96, 12, 20, 0, 54, 45, 0, 67, 0, 0, 0, 25, 47, 26, 61, 1, 91, 31, 190, 13, 0, 18, 0, 69, 90, 0, 92, 209, 58, 0, 0, 67, 80, 36, 188, 51, 95, 1, 19, 147, 29, 237, 71, 153, 0, 10, 32, 161, 130, 33, 0, 109, 17, 0, 69, 40, 130, 143, 0, 15, 105, 14, 55, 29, 184, 48, 207, 20, 59, 92, 11, 163, 0, 21, 96, 33, 4, 67, 61, 81, 203, 232, 32, 0, 0, 111, 148, 39, 62, 0, 5, 230, 110, 30, 183, 27, 47, 0, 245, 47, 119, 19, 1, 16, 143, 29, 0, 7, 0, 95, 98, 127, 0, 75, 5, 187, 16, 17, 65, 62, 29, 219, 0, 46, 0, 255, 151, 0, 108, 12, 1, 0, 86, 31, 0, 31, 0, 0, 0, 6, 0, 0, 44, 47, 133, 19, 179, 13, 0, 0, 0, 0, 91, 0, 58, 180, 48, 0, 0, 57, 92, 31, 170, 69, 82, 4, 0, 147, 40, 237, 123, 184, 0, 0, 89, 91, 73, 4, 0, 76, 21, 0, 58, 18, 168, 98, 0, 12, 45, 15, 54, 39, 135, 21, 207, 40, 48, 108, 64, 124, 0, 11, 0, 45, 12, 82, 108, 145, 255, 189, 0, 0, 0, 0, 139, 0, 93, 0, 16, 241, 85, 60, 183, 57, 74, 0, 255, 0, 107, 5, 0, 23, 120, 53, 20, 7, 0, 92, 105, 177, 0, 79, 5, 145, 13, 8, 23, 18, 17, 155, 0, 24, 0, 243, 193, 52, 132, 12, 33, 0, 154, 16, 13, 70, 0, 22, 0, 44, 0, 0, 23, 24, 49, 2, 151, 13, 0, 120, 33, 0, 11, 75, 109, 239, 37, 0, 0, 81, 103, 0, 193, 38, 161, 0, 0, 39, 25, 237, 117, 148, 0, 0, 94, 60, 117, 4, 0, 99, 11, 0, 44, 11, 150, 67, 0, 0, 41, 0, 41, 0, 82, 27, 169, 10, 82, 56, 34, 113, 0, 34, 0, 44, 7, 67, 125, 121, 222, 149, 27, 0, 0, 0, 101, 0, 93, 0, 64, 211, 106, 42, 178, 27, 21, 0, 231, 0, 117, 0, 0, 63, 186, 48, 0, 7, 0, 41, 102, 176, 20, 70, 10, 136, 26, 19, 105, 21, 41, 224, 24, 31, 0, 255, 157, 67, 140, 12, 66, 0, 119, 45, 0, 22, 0, 45, 0, 39, 10, 0, 38, 12, 22, 2, 172, 13, 0, 0, 0, 0, 52, 0, 87, 119, 12, 0, 0, 63, 53, 0, 199, 51, 119, 1, 36, 121, 21, 237, 142, 107, 0, 0, 85, 146, 99, 4, 0, 106, 5, 0, 36, 6, 170, 103, 0, 46, 14, 0, 66, 25, 154, 53, 196, 9, 80, 71, 53, 88, 12, 24, 1, 112, 11, 32, 49, 75, 255, 254, 18, 0, 1, 15, 163, 0, 93, 0, 5, 248, 138, 63, 157, 27, 12, 0, 249, 0, 47, 87, 0, 49, 92, 42, 0, 7, 0, 57, 103, 106, 49, 62, 11, 211, 10, 28, 222, 0, 24, 189, 0, 47, 13, 238, 202, 21, 140, 12, 31, 0, 102, 97, 0, 64, 0, 0, 0, 21, 0, 71, 78, 47, 77, 15, 157, 13, 0, 53, 0, 0, 46, 0, 95, 150, 31, 0, 0, 28, 153, 0, 161, 30, 101, 9, 0, 106, 21, 237, 110, 143, 0, 0, 94, 137, 96, 95, 0, 115, 5, 39, 60, 2, 154, 6, 17, 0, 47, 47, 42, 5, 167, 21, 196, 30, 29, 55, 47, 153, 0, 11, 0, 57, 7, 86, 79, 95, 246, 220, 20, 9, 0, 0, 123, 53, 41, 28, 11, 199, 103, 20, 160, 27, 53, 0, 255, 81, 53, 76, 0, 5, 101, 19, 0, 7, 16, 45, 95, 178, 96, 71, 3, 189, 7, 4, 25, 0, 7, 172, 0, 28, 99, 253, 173, 46, 125, 12, 10, 0, 70, 53, 0, 9, 0, 20, 0, 39, 0, 25, 51, 9, 123, 45, 188, 13, 0, 0, 0, 0, 90, 0, 141, 178, 85, 0, 0, 46, 106, 0, 140, 17, 121, 0, 2, 161, 0, 237, 99, 102, 0, 0, 94, 125, 145, 13, 0, 98, 24, 0, 50, 8, 153, 97, 0, 21, 87, 0, 86, 0, 141, 46, 102, 24, 41, 21, 38, 141, 40, 11, 0, 56, 11, 57, 76, 182, 236, 252, 31, 0, 27, 0, 56, 0, 93, 0, 71, 172, 109, 87, 153, 40, 0, 0, 255, 1, 103, 0, 0, 2, 128, 50, 5, 7, 0, 87, 109, 167, 0, 69, 3, 175, 10, 24, 101, 52, 7, 183, 0, 51, 0, 242, 192, 50, 106, 12, 1, 0, 74, 48, 0, 20, 0, 0, 0, 97, 5, 0, 30, 1, 55, 14, 170, 13, 0, 0, 0, 92, 86, 0, 94, 212, 70, 0, 0, 68, 173, 0, 199, 0, 103, 4, 0, 128, 32, 237, 156, 98, 0, 18, 94, 146, 87, 16, 0, 88, 15, 0, 0, 0, 147, 9, 0, 22, 6, 0, 23, 0, 171, 74, 203, 39, 71, 108, 46, 112, 62, 11, 70, 39, 4, 91, 47, 109, 255, 255, 29, 0, 32, 202, 169, 34, 93, 0, 5, 229, 114, 66, 92, 42, 0, 0, 250, 0, 119, 0, 0, 2, 101, 39, 0, 7, 0, 86, 97, 158, 0, 0, 3, 195, 0, 31, 17, 45, 29, 243, 0, 60, 0, 242, 178, 15, 116, 12, 1, 0, 89, 88, 0, 60, 0, 69, 0, 23, 0, 0, 46, 21, 29, 38, 177, 13, 0, 22, 27, 0, 52, 0, 123, 177, 121, 0, 0, 73, 90, 0, 163, 81, 156, 9, 31, 101, 19, 237, 205, 91, 0, 0, 78, 69, 101, 4, 0, 53, 29, 14, 69, 43, 169, 10, 0, 27, 12, 14, 32, 68, 133, 42, 157, 0, 0, 27, 62, 160, 0, 11, 44, 20, 4, 62, 37, 132, 233, 255, 0, 0, 0, 82, 169, 53, 93, 79, 45, 230, 79, 2, 183, 27, 72, 0, 169, 39, 14, 64, 30, 12, 85, 43, 0, 7, 0, 55, 102, 184, 0, 0, 3, 74, 7, 4, 145, 27, 22, 191, 50, 38, 0, 219, 169, 74, 132, 12, 80, 0, 14, 16, 0, 142, 0, 79, 0, 123, 29, 87, 4, 1, 34, 14, 113, 46, 0, 115, 0, 136, 65, 75, 60, 246, 10, 0, 0, 64, 77, 58, 107, 59, 113, 46, 7, 138, 10, 237, 176, 158, 0, 0, 85, 76, 116, 30, 0, 78, 8, 0, 7, 131, 171, 40, 0, 28, 82, 0, 57, 68, 139, 106, 181, 94, 42, 58, 39, 102, 0, 27, 0, 42, 4, 87, 28, 126, 255, 255, 15, 0, 0, 0, 169, 4, 93, 0, 8, 228, 131, 68, 100, 27, 0, 0, 221, 40, 66, 89, 0, 28, 106, 66, 56, 7, 0, 60, 112, 171, 0, 47, 3, 113, 38, 35, 255, 35, 44, 158, 0, 111, 0, 255, 229, 35, 140, 12, 108, 0, 64, 68, 51, 219, 0, 0, 0, 1, 61, 41, 23, 10, 62, 9, 125, 13, 0, 0, 0, 0, 36, 0, 123, 166, 31, 0, 0, 89, 126, 2, 199, 55, 112, 2, 31, 33, 35, 237, 156, 113, 0, 0, 94, 120, 121, 23, 0, 80, 48, 0, 63, 87, 186, 26, 45, 0, 26, 0, 60, 0, 149, 27, 181, 28, 89, 86, 15, 71, 0, 11, 0, 63, 21, 11, 31, 170, 255, 202, 23, 30, 0, 0, 169, 11, 93, 43, 11, 255, 135, 43, 116, 27, 0, 20, 255, 1, 64, 0, 19, 16, 159, 77, 0, 7, 0, 28, 73, 127, 0, 79, 5, 152, 11, 16, 255, 0, 7, 211, 0, 58, 0, 255, 226, 0, 131, 12, 18, 0, 125, 53, 0, 48, 0, 51, 0, 41, 32, 87, 24, 7, 79, 21, 146, 13, 0, 0, 0, 0, 59, 0, 121, 179, 113, 0, 0, 118, 79, 36, 199, 17, 142, 14, 31, 104, 33, 237, 88, 114, 0, 28, 89, 90, 122, 14, 0, 77, 29, 0, 55, 2, 168, 6, 0, 34, 18, 3, 51, 55, 186, 21, 179, 49, 10, 69, 61, 146, 77, 11, 0, 13, 4, 68, 79, 141, 255, 134, 15, 0, 0, 0, 159, 15, 93, 0, 5, 235, 139, 2, 183, 37, 83, 27, 209, 17, 45, 63, 0, 2, 119, 29, 32, 7, 0, 72, 107, 148, 0, 48, 5, 138, 23, 7, 57, 0, 12, 128, 0, 44, 38, 213, 147, 30, 140, 12, 1, 0, 128, 24, 156, 34, 0, 50, 0, 16, 8, 0, 59, 19, 146, 8, 186, 13, 0, 6, 0, 0, 62, 0, 116, 195, 19, 0, 0, 39, 77, 0, 199, 41, 95, 11, 3, 139, 8, 237, 122, 135, 0, 0, 84, 115, 76, 4, 0, 80, 53, 0, 69, 12, 157, 35, 0, 0, 45, 0, 32, 13, 175, 21, 186, 39, 30, 32, 71, 136, 51, 39, 44, 18, 6, 45, 105, 91, 255, 235, 3, 0, 0, 175, 158, 0, 88, 16, 65, 242, 87, 2, 179, 27, 35, 5, 238, 0, 74, 55, 0, 3, 112, 63, 37, 7, 0, 50, 107, 118, 0, 79, 3, 176, 29, 11, 81, 0, 15, 229, 18, 29, 47, 255, 152, 74, 129, 12, 1, 0, 113, 23, 0, 10, 0, 38, 0, 30, 11, 41, 16, 26, 92, 2, 170, 13, 0, 0, 0, 0, 82, 53, 79, 147, 85, 0, 0, 103, 76, 0, 199, 22, 120, 6, 0, 137, 17, 237, 90, 61, 0, 0, 87, 82, 115, 14, 0, 79, 50, 0, 55, 10, 160, 53, 0, 0, 54, 3, 56, 0, 187, 21, 202, 46, 40, 67, 78, 110, 92, 11, 0, 32, 8, 39, 67, 155, 236, 229, 0, 0, 0, 0, 164, 34, 93, 0, 5, 253, 114, 7, 64, 27, 0, 0, 238, 0, 55, 0, 0, 6, 110, 72, 41, 7, 0, 57, 80, 171, 22, 79, 3, 255, 0, 4, 226, 0, 7, 157, 4, 74, 0, 244, 205, 0, 106, 12, 33, 0, 98, 16, 0, 69, 0, 0, 0, 6, 0, 36, 17, 8, 12, 2, 117, 13, 0, 29, 0, 33, 14, 0, 56, 238, 55, 0, 0, 44, 110, 59, 189, 54, 107, 0, 0, 141, 30, 237, 104, 147, 0, 0, 84, 58, 76, 4, 0, 74, 5, 0, 69, 23, 120, 27, 29, 5, 39, 0, 56, 0, 145, 63, 176, 11, 67, 56, 59, 81, 47, 11, 36, 49, 4, 63, 126, 131, 207, 210, 0, 0, 29, 0, 127, 26, 93, 50, 22, 211, 132, 12, 103, 31, 58, 0, 255, 0, 75, 12, 0, 10, 93, 43, 44, 7, 0, 32, 92, 165, 0, 79, 3, 126, 58, 4, 179, 1, 7, 173, 0, 28, 0, 255, 199, 45, 95, 12, 64, 0, 81, 39, 0, 36, 0, 33, 0, 29, 81, 0, 24, 11, 80, 39, 164, 13, 0, 41, 0, 12, 93, 0, 46, 194, 29, 0, 0, 53, 92, 10, 82, 24, 123, 21, 40, 86, 14, 237, 107, 187, 0, 0, 84, 65, 113, 4, 0, 65, 5, 15, 69, 22, 142, 15, 0, 36, 43, 53, 31, 21, 177, 55, 160, 47, 68, 43, 71, 119, 13, 18, 0, 52, 4, 78, 78, 129, 205, 180, 22, 0, 3, 0, 68, 41, 75, 35, 77, 211, 82, 44, 176, 32, 112, 0, 223, 0, 41, 17, 24, 9, 122, 49, 31, 7, 0, 46, 107, 126, 0, 72, 3, 122, 109, 4, 16, 49, 12, 220, 0, 21, 0, 253, 173, 34, 113, 12, 1, 0, 82, 30, 0, 51, 0, 97, 0, 70, 0, 0, 11, 1, 69, 56, 162, 13, 0, 0, 19, 0, 68, 0, 114, 176, 1, 0, 0, 47, 144, 0, 199, 42, 173, 33, 0, 43, 0, 237, 54, 131, 0, 0, 94, 58, 108, 12, 0, 106, 62, 0, 69, 0, 148, 62, 0, 0, 110, 0, 46, 58, 119, 21, 171, 59, 79, 33, 63, 121, 29, 17, 0, 30, 4, 30, 89, 149, 215, 242, 8, 0, 0, 141, 100, 0, 49, 0, 67, 213, 90, 46, 157, 41, 66, 2, 239, 49, 30, 0, 23, 18, 180, 86, 0, 7, 18, 80, 109, 162, 0, 71, 3, 121, 2, 4, 69, 41, 7, 152, 0, 33, 0, 255, 127, 37, 109, 12, 34, 0, 113, 26, 0, 32, 0, 30, 0, 94, 8, 20, 27, 4, 51, 15, 147, 13, 0, 31, 0, 39, 104, 0, 8, 221, 1, 0, 0, 28, 119, 0, 178, 42, 116, 12, 0, 148, 31, 237, 77, 106, 0, 0, 84, 91, 129, 30, 0, 113, 13, 0, 69, 13, 182, 121, 0, 39, 65, 0, 41, 43, 181, 37, 207, 42, 49, 95, 34, 147, 92, 21, 0, 5, 4, 50, 70, 129, 249, 255, 0, 0, 9, 0, 148, 0, 93, 0, 11, 249, 120, 81, 120, 27, 29, 0, 243, 0, 80, 19, 0, 25, 109, 31, 48, 7, 0, 79, 106, 147, 0, 72, 3, 142, 0, 4, 52, 0, 7, 154, 14, 38, 0, 255, 185, 31, 103, 12, 10, 0, 128, 122, 40, 64, 0, 0, 0, 19, 0, 0, 10, 20, 66, 57, 180, 13, 0, 19, 0, 13, 78, 0, 69, 208, 20, 0, 0, 42, 72, 0, 199, 1, 105, 15, 11, 108, 68, 237, 138, 161, 0, 0, 41, 186, 117, 35, 0, 93, 21, 4, 69, 25, 137, 72, 0, 17, 26, 17, 39, 0, 180, 70, 205, 22, 67, 46, 4, 110, 63, 11, 72, 5, 4, 92, 86, 79, 205, 255, 27, 0, 11, 10, 133, 0, 72, 44, 66, 242, 112, 21, 143, 27, 0, 0, 255, 0, 129, 50, 4, 9, 162, 51, 5, 7, 0, 50, 90, 119, 62, 61, 3, 192, 1, 4, 98, 9, 34, 232, 3, 60, 0, 255, 231, 0, 140, 12, 18, 0, 87, 92, 0, 24, 0, 0, 0, 51, 0, 0, 40, 26, 45, 15, 203, 13, 0, 0, 0, 0, 119, 0, 87, 124, 163, 0, 0, 37, 114, 0, 177, 82, 102, 39, 0, 179, 65, 237, 133, 203, 0, 0, 70, 52, 83, 4, 0, 68, 24, 0, 58, 11, 139, 27, 0, 0, 4, 0, 30, 0, 176, 29, 199, 13, 38, 46, 34, 172, 8, 11, 72, 5, 4, 89, 117, 110, 248, 254, 51, 0, 31, 39, 161, 77, 70, 0, 16, 200, 102, 42, 66, 27, 30, 0, 209, 2, 67, 59, 0, 2, 96, 55, 0, 7, 0, 53, 93, 154, 16, 38, 3, 196, 3, 8, 148, 0, 22, 186, 16, 63, 0, 241, 195, 41, 126, 12, 24, 0, 51, 39, 0, 13, 45, 54, 0, 1, 53, 0, 66, 5, 70, 15, 124, 13, 0, 17, 0, 0, 75, 0, 105, 247, 1, 0, 0, 36, 176, 0, 135, 11, 114, 13, 0, 150, 0, 237, 92, 64, 0, 0, 66, 73, 147, 12, 0, 86, 5, 0, 57, 54, 155, 94, 0, 4, 31, 0, 83, 43, 169, 40, 148, 18, 26, 13, 0, 99, 78, 11, 0, 18, 4, 13, 47, 139, 255, 151, 18, 0, 20, 0, 111, 0, 72, 0, 16, 207, 87, 37, 133, 27, 0, 15, 221, 14, 26, 52, 0, 43, 116, 73, 32, 7, 0, 91, 96, 159, 0, 71, 3, 171, 9, 12, 255, 0, 7, 215, 0, 43, 0, 215, 154, 68, 140, 12, 27, 0, 192, 41, 94, 35, 0, 14, 0, 60, 69, 73, 23, 22, 117, 2, 143, 13, 0, 27, 0, 0, 44, 0, 79, 169, 76, 0, 0, 52, 52, 0, 161, 6, 90, 15, 0, 122, 29, 237, 111, 102, 0, 0, 77, 48, 77, 9, 0, 93, 5, 0, 69, 16, 178, 118, 0, 14, 73, 0, 96, 4, 172, 56, 177, 12, 45, 87, 78, 140, 11, 11, 39, 5, 4, 49, 82, 111, 255, 240, 22, 0, 10, 0, 131, 0, 93, 0, 8, 235, 122, 2, 143, 27, 0, 0, 197, 0, 145, 17, 0, 17, 143, 34, 10, 7, 0, 97, 100, 168, 0, 64, 3, 179, 27, 8, 190, 3, 7, 188, 0, 82, 27, 244, 215, 50, 122, 12, 47, 0, 101, 31, 67, 32, 0, 0, 0, 1, 25, 57, 30, 37, 69, 19, 167, 13, 0, 0, 81, 23, 92, 22, 101, 214, 39, 0, 0, 40, 59, 11, 186, 14, 90, 0, 80, 99, 60, 237, 70, 114, 0, 0, 82, 139, 129, 19, 0, 119, 31, 0, 61, 25, 178, 117, 0, 5, 66, 0, 53, 2, 171, 84, 201, 48, 5, 65, 78, 134, 0, 11, 0, 18, 4, 77, 54, 106, 255, 182, 10, 0, 42, 0, 149, 79, 93, 6, 16, 255, 127, 40, 150, 27, 25, 0, 224, 0, 81, 0, 0, 9, 154, 51, 29, 7, 0, 23, 103, 172, 0, 79, 3, 239, 45, 4, 186, 14, 12, 195, 50, 52, 0, 200, 228, 0, 77, 12, 5, 0, 63, 68, 100, 37, 0, 0, 0, 1, 0, 42, 23, 18, 75, 29, 99, 13, 0, 0, 0, 0, 0, 0, 116, 186, 94, 0, 0, 65, 143, 0, 199, 0, 44, 0, 0, 139, 68, 237, 67, 120, 0, 26, 94, 129, 130, 17, 0, 95, 11, 13, 54, 61, 176, 26, 0, 0, 83, 31, 24, 68, 187, 43, 154, 32, 52, 29, 78, 195, 0, 31, 0, 68, 4, 62, 58, 135, 255, 141, 23, 3, 37, 0, 157, 0, 93, 17, 29, 255, 101, 19, 147, 27, 0, 37, 213, 80, 154, 28, 0, 7, 169, 59, 27, 7, 0, 62, 110, 184, 0, 79, 3, 157, 7, 13, 133, 63, 16, 255, 0, 31, 0, 225, 199, 0, 104, 12, 19, 0, 65, 36, 0, 29, 0, 0, 34, 64, 0, 64, 30, 35, 136, 38, 126, 13, 0, 0, 0, 37, 67, 0, 41, 161, 219, 0, 0, 106, 39, 18, 199, 0, 74, 4, 0, 170, 20, 237, 134, 84, 0, 0, 89, 122, 95, 13, 0, 108, 24, 0, 4, 32, 162, 86, 0, 8, 39, 0, 71, 68, 174, 78, 207, 22, 58, 86, 55, 89, 30, 11, 64, 12, 4, 33, 68, 103, 255, 219, 15, 0, 0, 0, 167, 0, 67, 4, 21, 233, 123, 25, 138, 27, 0, 0, 249, 0, 134, 36, 46, 22, 112, 31, 18, 7, 0, 42, 103, 102, 3, 79, 3, 179, 0, 4, 66, 0, 23, 87, 56, 35, 10, 228, 162, 68, 116, 12, 20, 0, 58, 135, 0, 22, 0, 38, 0, 9, 0, 71, 32, 45, 77, 9, 184, 13, 0, 108, 0, 0, 99, 19, 102, 155, 114, 0, 0, 58, 74, 0, 199, 28, 82, 14, 0, 138, 29, 237, 129, 138, 0, 0, 72, 105, 103, 4, 0, 93, 11, 0, 69, 40, 186, 30, 38, 70, 43, 23, 27, 68, 168, 24, 200, 9, 37, 48, 46, 100, 9, 25, 0, 16, 4, 52, 34, 104, 243, 255, 12, 0, 0, 0, 167, 67, 77, 47, 37, 255, 76, 18, 130, 27, 14, 0, 255, 58, 70, 56, 35, 3, 125, 17, 42, 7, 0, 35, 92, 135, 0, 79, 3, 113, 7, 4, 82, 0, 7, 169, 29, 29, 23, 255, 195, 66, 135, 12, 1, 0, 149, 58, 4, 15, 0, 38, 0, 36, 29, 16, 22, 16, 44, 9, 137, 13, 0, 0, 0, 0, 41, 0, 93, 182, 64, 0, 0, 77, 74, 0, 178, 0, 172, 0, 0, 62, 11, 237, 48, 170, 0, 0, 94, 116, 115, 7, 0, 101, 22, 0, 67, 5, 182, 24, 42, 1, 68, 0, 9, 17, 184, 38, 200, 53, 14, 107, 78, 130, 13, 11, 0, 64, 4, 59, 45, 156, 219, 251, 15, 22, 0, 49, 167, 10, 93, 17, 22, 255, 109, 35, 170, 41, 9, 10, 255, 72, 45, 29, 0, 2, 67, 62, 0, 7, 0, 57, 98, 151, 0, 79, 3, 171, 10, 4, 54, 0, 7, 175, 0, 21, 60, 255, 208, 74, 135, 12, 1, 0, 255, 54, 0, 9, 0, 37, 0, 19, 0, 0, 19, 14, 44, 2, 138, 13, 0, 2, 0, 0, 39, 28, 101, 217, 69, 0, 0, 88, 73, 57, 179, 6, 167, 7, 25, 83, 0, 237, 58, 94, 0, 0, 78, 123, 85, 14, 0, 126, 63, 0, 64, 0, 153, 176, 0, 0, 95, 0, 95, 46, 166, 21, 207, 132, 59, 51, 64, 113, 39, 11, 0, 67, 4, 15, 113, 161, 255, 203, 18, 0, 34, 85, 152, 0, 93, 0, 37, 210, 124, 20, 168, 41, 0, 0, 255, 0, 208, 0, 0, 2, 174, 98, 41, 7, 0, 60, 100, 174, 0, 62, 3, 102, 0, 16, 97, 66, 7, 99, 0, 43, 0, 255, 124, 45, 86, 12, 1, 0, 108, 31, 0, 42, 0, 0, 0, 29, 0, 24, 20, 21, 22, 2, 169, 13, 0, 80, 0, 54, 88, 43, 56, 204, 152, 0, 0, 28, 60, 0, 156, 89, 62, 4, 0, 136, 83, 237, 112, 163, 0, 0, 88, 118, 80, 11, 0, 90, 30, 0, 65, 12, 163, 84, 0, 41, 24, 63, 54, 55, 187, 34, 200, 14, 47, 103, 53, 139, 0, 11, 0, 34, 14, 25, 40, 139, 255, 255, 54, 0, 0, 0, 152, 30, 69, 6, 10, 244, 110, 10, 183, 27, 0, 0, 239, 16, 74, 81, 16, 1, 76, 33, 0, 7, 0, 74, 95, 167, 0, 74, 3, 183, 0, 12, 116, 0, 29, 150, 0, 57, 0, 253, 237, 7, 111, 12, 1, 0, 126, 68, 0, 1, 0, 0, 0, 46, 0, 0, 89, 19, 51, 12, 178, 13, 0, 0, 35, 0, 44, 37, 112, 170, 92, 0, 0, 28, 117, 0, 199, 39, 104, 0, 41, 115, 26, 237, 91, 107, 0, 0, 85, 87, 138, 8, 0, 69, 25, 0, 26, 69, 159, 45, 38, 33, 53, 0, 73, 0, 178, 90, 184, 26, 61, 18, 36, 124, 31, 31, 0, 19, 24, 13, 103, 148, 209, 243, 11, 0, 0, 0, 129, 33, 56, 11, 11, 255, 92, 2, 183, 41, 47, 0, 236, 8, 67, 0, 4, 13, 118, 34, 57, 7, 0, 57, 71, 177, 12, 73, 3, 154, 18, 41, 129, 0, 7, 207, 0, 25, 23, 255, 194, 58, 74, 12, 1, 0, 64, 23, 62, 42, 0, 0, 0, 1, 9, 91, 31, 4, 87, 16, 121, 13, 0, 21, 0, 0, 99, 13, 62, 159, 62, 0, 0, 66, 108, 28, 77, 49, 129, 14, 0, 143, 37, 237, 56, 107, 0, 0, 94, 81, 114, 20, 0, 100, 7, 0, 69, 14, 120, 52, 52, 74, 135, 0, 73, 17, 169, 21, 190, 22, 61, 43, 56, 177, 54, 21, 0, 9, 4, 52, 43, 99, 222, 255, 31, 0, 33, 58, 137, 48, 77, 18, 32, 236, 119, 25, 183, 27, 18, 0, 253, 0, 53, 0, 25, 15, 131, 22, 58, 7, 0, 84, 113, 156, 0, 79, 3, 125, 34, 20, 116, 0, 12, 192, 0, 40, 0, 255, 205, 21, 78, 12, 1, 0, 32, 29, 0, 36, 0, 6, 0, 43, 7, 16, 27, 23, 104, 27, 154, 13, 0, 59, 71, 0, 104, 7, 63, 243, 50, 0, 0, 102, 113, 23, 125, 36, 53, 1, 24, 121, 32, 237, 107, 132, 0, 0, 72, 93, 86, 19, 0, 73, 5, 0, 18, 12, 167, 116, 14, 47, 56, 0, 82, 68, 178, 50, 207, 35, 37, 80, 62, 158, 54, 17, 0, 18, 4, 73, 50, 104, 245, 255, 35, 81, 0, 31, 138, 0, 93, 17, 31, 255, 123, 106, 143, 27, 0, 5, 248, 27, 106, 27, 0, 2, 84, 59, 0, 7, 0, 60, 108, 169, 0, 65, 3, 227, 29, 8, 155, 1, 59, 248, 15, 77, 0, 255, 193, 44, 106, 12, 41, 0, 101, 117, 0, 14, 0, 0, 0, 20, 16, 15, 98, 17, 34, 11, 115, 13, 0, 0, 57, 0, 25, 0, 99, 132, 84, 0, 0, 60, 132, 24, 199, 36, 87, 13, 97, 114, 54, 229, 129, 174, 0, 0, 64, 86, 124, 9, 0, 109, 5, 0, 0, 51, 138, 52, 0, 38, 13, 0, 54, 53, 132, 36, 199, 19, 24, 60, 38, 108, 34, 32, 0, 13, 4, 79, 68, 97, 249, 186, 1, 0, 0, 0, 157, 27, 93, 52, 52, 242, 126, 52, 158, 31, 8, 0, 255, 0, 59, 12, 0, 15, 139, 42, 30, 7, 0, 47, 101, 151, 0, 52, 3, 176, 0, 11, 236, 0, 11, 204, 0, 45, 0, 255, 209, 57, 89, 12, 6, 0, 203, 33, 0, 18, 0, 44, 0, 26, 38, 0, 39, 22, 78, 28, 119, 13, 0, 28, 0, 0, 44, 0, 79, 115, 26, 0, 0, 75, 120, 0, 178, 37, 150, 44, 0, 60, 0, 237, 87, 134, 0, 0, 94, 68, 110, 26, 0, 68, 9, 0, 0, 14, 148, 89, 0, 21, 70, 0, 63, 0, 161, 55, 159, 12, 46, 0, 54, 90, 20, 16, 0, 33, 4, 48, 94, 114, 241, 237, 27, 0, 0, 150, 97, 46, 93, 41, 39, 244, 128, 6, 183, 32, 20, 31, 236, 67, 78, 0, 0, 11, 92, 50, 0, 7, 0, 78, 89, 177, 0, 67, 3, 124, 0, 4, 35, 26, 14, 143, 0, 38, 0, 255, 199, 26, 52, 12, 1, 0, 160, 16, 0, 11, 0, 1, 0, 17, 0, 40, 23, 32, 60, 11, 155, 13, 0, 52, 0, 0, 98, 0, 71, 127, 1, 0, 0, 103, 89, 15, 141, 68, 144, 5, 0, 81, 27, 237, 91, 141, 0, 0, 94, 121, 92, 35, 0, 96, 25, 0, 37, 3, 166, 160, 0, 70, 53, 0, 67, 0, 156, 21, 102, 18, 44, 20, 78, 146, 6, 11, 0, 5, 4, 30, 33, 121, 231, 255, 25, 52, 0, 0, 60, 0, 93, 0, 78, 230, 113, 11, 172, 40, 11, 35, 255, 4, 38, 0, 0, 7, 196, 54, 26, 7, 0, 132, 99, 166, 10, 62, 3, 225, 55, 4, 154, 0, 11, 180, 0, 53, 0, 217, 222, 0, 140, 12, 1, 0, 255, 46, 0, 41, 0, 0, 0, 24, 0, 0, 4, 11, 44, 5, 111, 13, 0, 0, 95, 25, 34, 48, 110, 214, 26, 0, 0, 55, 163, 26, 199, 2, 162, 6, 62, 123, 79, 237, 83, 125, 4, 0, 71, 124, 147, 36, 0, 114, 65, 0, 4, 10, 170, 9, 0, 21, 41, 0, 27, 0, 137, 21, 197, 42, 22, 78, 66, 138, 39, 11, 37, 66, 4, 68, 53, 119, 237, 190, 24, 0, 8, 0, 133, 0, 89, 0, 37, 238, 135, 7, 163, 49, 20, 0, 255, 0, 14, 0, 25, 2, 72, 42, 45, 7, 0, 75, 78, 112, 0, 73, 3, 141, 29, 13, 54, 0, 14, 177, 0, 27, 18, 255, 219, 35, 126, 12, 36, 0, 230, 42, 90, 35, 0, 75, 0, 1, 0, 0, 4, 45, 39, 35, 151, 13, 0, 57, 0, 0, 67, 0, 130, 200, 42, 0, 0, 54, 98, 0, 199, 74, 197, 0, 0, 127, 8, 237, 181, 80, 0, 0, 46, 65, 104, 6, 0, 78, 80, 0, 69, 50, 174, 23, 0, 17, 34, 0, 40, 68, 90, 21, 178, 7, 0, 4, 78, 177, 56, 39, 0, 13, 4, 95, 55, 91, 199, 255, 0, 0, 56, 0, 169, 108, 84, 58, 113, 234, 104, 26, 183, 27, 25, 0, 206, 0, 119, 72, 0, 21, 85, 52, 72, 7, 0, 22, 104, 189, 0, 23, 3, 124, 84, 4, 187, 83, 7, 200, 37, 38, 0, 190, 228, 37, 130, 12, 112, 0, 70, 16, 0, 137, 0, 32, 0, 56, 109, 0, 17, 1, 38, 44, 73, 22, 0, 89, 0, 199, 39, 65, 42, 193, 69, 0, 0, 28, 96, 30, 156, 38, 62, 41, 0, 142, 29, 237, 203, 174, 0, 0, 89, 117, 108, 29, 0, 140, 51, 0, 69, 0, 183, 66, 0, 11, 39, 38, 56, 43, 162, 46, 177, 76, 64, 118, 78, 132, 0, 30, 0, 86, 4, 110, 76, 68, 255, 255, 25, 0, 0, 0, 169, 32, 84, 0, 7, 255, 123, 82, 163, 34, 6, 0, 255, 3, 129, 94, 0, 8, 70, 36, 7, 7, 0, 16, 103, 142, 0, 36, 3, 104, 41, 31, 143, 0, 51, 198, 13, 83, 41, 255, 251, 61, 126, 12, 81, 0, 75, 92, 38, 255, 0, 0, 0, 23, 71, 0, 51, 39, 99, 25, 174, 13, 0, 9, 0, 2, 54, 0, 115, 180, 155, 0, 0, 51, 58, 0, 199, 23, 108, 9, 0, 173, 61, 237, 154, 180, 0, 33, 89, 84, 140, 10, 0, 62, 16, 0, 55, 65, 145, 60, 0, 15, 26, 1, 58, 55, 184, 60, 205, 40, 39, 26, 3, 95, 0, 25, 0, 82, 4, 26, 28, 137, 227, 255, 11, 36, 0, 0, 169, 34, 76, 0, 7, 255, 106, 84, 177, 36, 11, 11, 231, 51, 69, 12, 18, 9, 99, 78, 0, 7, 0, 62, 56, 156, 0, 27, 3, 240, 30, 4, 255, 0, 7, 211, 2, 70, 0, 255, 255, 0, 140, 12, 24, 0, 164, 84, 0, 12, 0, 0, 0, 23, 0, 33, 40, 4, 68, 24, 105, 13, 0, 0, 0, 0, 15, 0, 70, 150, 149, 0, 0, 108, 111, 34, 199, 44, 142, 34, 0, 141, 41, 237, 142, 117, 0, 0, 77, 91, 111, 13, 0, 92, 21, 6, 40, 18, 123, 138, 0, 0, 19, 14, 68, 36, 161, 58, 201, 16, 51, 65, 0, 109, 48, 11, 0, 51, 4, 31, 66, 109, 255, 218, 14, 0, 0, 45, 120, 1, 91, 0, 25, 212, 145, 63, 173, 27, 96, 0, 250, 38, 124, 32, 0, 10, 87, 35, 0, 7, 0, 48, 86, 169, 0, 75, 3, 118, 27, 10, 110, 0, 15, 170, 0, 59, 0, 255, 192, 60, 124, 12, 1, 0, 88, 22, 0, 67, 0, 0, 0, 17, 1, 72, 11, 39, 25, 18, 196, 13, 0, 50, 0, 0, 70, 0, 78, 109, 25, 0, 0, 66, 48, 0, 199, 56, 124, 14, 28, 87, 52, 237, 131, 126, 0, 0, 79, 67, 94, 58, 0, 83, 5, 0, 69, 7, 125, 121, 0, 0, 35, 0, 48, 0, 174, 21, 166, 5, 40, 58, 64, 92, 0, 39, 0, 26, 7, 70, 139, 72, 231, 249, 46, 0, 0, 0, 116, 34, 81, 31, 9, 234, 139, 27, 167, 27, 65, 0, 255, 0, 100, 6, 0, 2, 154, 29, 28, 7, 0, 46, 83, 156, 0, 79, 3, 189, 17, 12, 173, 0, 7, 227, 0, 36, 27, 253, 223, 59, 132, 26, 1, 0, 53, 43, 0, 17, 0, 0, 0, 8, 36, 42, 12, 49, 83, 2, 171, 13, 0, 22, 0, 0, 80, 14, 49, 179, 73, 0, 0, 72, 83, 0, 199, 6, 87, 27, 0, 161, 28, 237, 160, 165, 0, 0, 76, 81, 106, 16, 0, 86, 22, 0, 69, 22, 182, 12, 0, 33, 57, 72, 8, 0, 179, 21, 189, 6, 58, 77, 78, 156, 41, 11, 0, 51, 21, 106, 93, 141, 252, 164, 24, 20, 0, 85, 160, 0, 93, 0, 9, 255, 72, 26, 75, 27, 0, 10, 255, 11, 91, 2, 8, 2, 113, 33, 4, 7, 0, 84, 88, 177, 41, 79, 3, 173, 10, 4, 30, 0, 7, 196, 0, 49, 10, 193, 185, 19, 96, 12, 17, 0, 164, 22, 0, 47, 0, 50, 0, 1, 0, 0, 29, 34, 43, 11, 174, 13, 0, 0, 0, 0, 49, 35, 113, 231, 93, 0, 0, 59, 92, 0, 159, 41, 176, 1, 104, 120, 13, 237, 96, 92, 0, 0, 70, 96, 80, 13, 0, 83, 5, 0, 69, 27, 173, 37, 1, 0, 54, 39, 47, 0, 185, 21, 142, 38, 68, 89, 68, 121, 21, 11, 58, 64, 4, 59, 97, 158, 253, 250, 0, 2, 27, 0, 169, 2, 89, 0, 5, 255, 65, 54, 121, 31, 25, 0, 233, 18, 62, 7, 0, 14, 101, 73, 49, 7, 0, 36, 104, 140, 122, 79, 3, 132, 0, 4, 129, 0, 9, 195, 0, 58, 0, 253, 193, 67, 90, 12, 1, 0, 102, 65, 0, 26, 0, 41, 1, 49, 53, 23, 77, 22, 106, 14, 137, 13, 0, 0, 0, 0, 77, 0, 60, 250, 64, 0, 0, 97, 115, 82, 170, 0, 82, 1, 9, 175, 19, 237, 128, 162, 0, 0, 87, 107, 154, 16, 0, 83, 45, 41, 47, 22, 162, 35, 17, 0, 55, 0, 20, 30, 187, 46, 148, 89, 47, 49, 56, 94, 10, 21, 0, 30, 4, 82, 77, 163, 234, 131, 64, 0, 0, 0, 100, 0, 85, 0, 77, 215, 116, 76, 167, 34, 1, 14, 255, 78, 50, 0, 0, 2, 135, 84, 0, 7, 0, 40, 76, 139, 106, 79, 3, 80, 0, 4, 35, 6, 39, 227, 0, 21, 0, 223, 153, 56, 112, 12, 1, 0, 89, 37, 0, 42, 0, 75, 0, 87, 0, 70, 8, 4, 34, 27, 184, 13, 0, 94, 0, 5, 93, 0, 94, 195, 72, 0, 0, 63, 84, 0, 192, 9, 94, 5, 0, 192, 41, 237, 80, 130, 0, 0, 85, 121, 121, 14, 0, 103, 5, 0, 69, 49, 175, 105, 0, 19, 26, 0, 37, 43, 166, 21, 189, 33, 55, 103, 62, 121, 59, 11, 0, 21, 16, 29, 39, 165, 255, 255, 0, 62, 0, 0, 86, 0, 78, 28, 49, 255, 93, 96, 139, 27, 0, 0, 244, 0, 85, 83, 0, 0, 133, 46, 50, 7, 7, 100, 107, 141, 0, 79, 3, 96, 5, 7, 175, 0, 7, 236, 0, 44, 0, 255, 219, 19, 93, 12, 53, 0, 180, 85, 89, 67, 0, 0, 0, 56, 8, 28, 65, 21, 136, 19, 173, 13, 0, 20, 1, 0, 80, 0, 137, 174, 199, 0, 0, 30, 117, 0, 166, 8, 148, 1, 7, 116, 35, 237, 119, 122, 0, 0, 77, 81, 122, 31, 0, 65, 33, 3, 69, 22, 183, 53, 46, 19, 36, 20, 62, 0, 187, 76, 203, 55, 63, 40, 40, 117, 8, 80, 0, 10, 4, 21, 126, 117, 255, 198, 0, 52, 0, 201, 121, 46, 93, 0, 53, 255, 100, 94, 116, 27, 0, 0, 202, 31, 51, 0, 12, 32, 129, 29, 5, 7, 0, 72, 97, 116, 0, 79, 3, 85, 0, 11, 89, 0, 27, 186, 33, 37, 40, 255, 151, 67, 107, 12, 18, 0, 101, 85, 0, 27, 0, 44, 0, 40, 45, 49, 21, 21, 75, 45, 177, 13, 0, 0, 18, 0, 128, 0, 72, 173, 28, 0, 0, 46, 52, 0, 199, 15, 95, 2, 42, 127, 32, 237, 120, 153, 0, 0, 68, 103, 82, 58, 0, 86, 47, 0, 69, 33, 113, 88, 0, 0, 43, 0, 73, 0, 170, 110, 170, 64, 35, 94, 28, 142, 9, 11, 95, 43, 4, 53, 94, 111, 220, 242, 15, 0, 0, 0, 97, 72, 52, 0, 13, 219, 112, 46, 136, 27, 0, 0, 245, 0, 132, 0, 0, 81, 125, 49, 46, 7, 0, 22, 94, 139, 0, 62, 3, 175, 16, 19, 168, 14, 37, 183, 0, 45, 0, 255, 180, 5, 130, 12, 24, 0, 74, 69, 0, 15, 0, 18, 0, 1, 16, 5, 29, 24, 77, 32, 181, 13, 0, 0, 0, 0, 68, 0, 104, 189, 57, 0, 0, 68, 97, 0, 187, 65, 78, 33, 0, 102, 24, 237, 122, 162, 0, 0, 74, 109, 102, 13, 0, 87, 16, 0, 62, 3, 173, 62, 0, 0, 99, 0, 28, 0, 163, 34, 190, 35, 76, 45, 78, 123, 2, 11, 0, 38, 4, 98, 37, 192, 255, 175, 54, 0, 0, 0, 157, 0, 93, 0, 14, 251, 113, 36, 127, 31, 15, 26, 255, 54, 27, 28, 38, 2, 83, 107, 0, 7, 0, 49, 76, 146, 0, 79, 3, 73, 23, 32, 168, 3, 7, 234, 0, 41, 0, 255, 230, 74, 111, 12, 17, 0, 184, 17, 0, 32, 0, 16, 0, 10, 6, 21, 38, 7, 87, 20, 143, 13, 0, 68, 0, 25, 43, 0, 88, 177, 89, 0, 0, 44, 76, 0, 185, 19, 125, 3, 0, 174, 24, 237, 132, 135, 3, 0, 64, 61, 85, 25, 0, 79, 5, 0, 69, 46, 164, 76, 0, 8, 45, 0, 39, 0, 181, 46, 190, 28, 25, 96, 71, 158, 43, 11, 0, 5, 4, 105, 84, 113, 255, 188, 0, 0, 0, 0, 72, 0, 62, 0, 31, 235, 58, 18, 136, 32, 47, 0, 228, 73, 24, 88, 0, 36, 160, 44, 0, 7, 0, 53, 113, 133, 30, 64, 3, 119, 5, 11, 174, 37, 7, 222, 0, 56, 34, 243, 173, 62, 130, 12, 35, 0, 137, 56, 0, 32, 0, 0, 0, 9, 0, 62, 50, 61, 99, 2, 193, 13, 0, 56, 0, 0, 76, 0, 88, 165, 1, 0, 0, 37, 34, 0, 155, 23, 165, 0, 0, 143, 41, 233, 79, 160, 0, 0, 89, 72, 139, 21, 0, 90, 5, 0, 44, 0, 169, 40, 0, 0, 57, 58, 40, 0, 154, 40, 207, 0, 120, 116, 78, 124, 0, 11, 0, 45, 4, 57, 58, 59, 255, 131, 0, 40, 0, 110, 48, 8, 93, 0, 5, 255, 101, 37, 177, 27, 0, 0, 195, 39, 17, 55, 0, 53, 112, 28, 0, 7, 0, 42, 106, 114, 30, 0, 3, 143, 47, 15, 160, 0, 7, 255, 51, 64, 0, 212, 199, 43, 128, 12, 80, 0, 108, 39, 0, 13, 0, 0, 0, 10, 0, 63, 42, 63, 16, 52, 220, 13, 0, 0, 140, 0, 98, 0, 70, 157, 49, 0, 0, 28, 34, 0, 149, 43, 134, 7, 51, 128, 18, 231, 114, 131, 0, 0, 94, 95, 113, 14, 0, 98, 7, 0, 15, 2, 142, 45, 18, 32, 32, 55, 44, 0, 131, 83, 207, 30, 22, 81, 47, 101, 28, 11, 0, 57, 4, 25, 127, 153, 236, 197, 5, 0, 0, 0, 135, 0, 93, 0, 5, 255, 102, 57, 142, 27, 0, 21, 249, 0, 49, 0, 0, 2, 133, 34, 0, 7, 0, 80, 95, 106, 0, 79, 3, 150, 73, 4, 147, 0, 7, 196, 15, 38, 0, 255, 228, 74, 112, 12, 7, 0, 120, 94, 0, 27, 0, 20, 0, 70, 39, 74, 29, 16, 66, 30, 200, 13, 0, 39, 0, 94, 78, 0, 100, 138, 98, 0, 0, 52, 97, 0, 192, 25, 151, 31, 0, 156, 6, 237, 158, 122, 0, 30, 94, 91, 100, 4, 0, 88, 10, 0, 51, 9, 178, 26, 0, 0, 26, 89, 23, 24, 157, 52, 207, 91, 79, 110, 59, 100, 38, 14, 0, 73, 4, 29, 113, 160, 254, 188, 35, 28, 0, 0, 169, 40, 93, 0, 16, 251, 145, 45, 142, 27, 0, 2, 255, 15, 26, 23, 19, 9, 146, 17, 0, 7, 0, 23, 89, 140, 0, 65, 3, 152, 54, 16, 133, 0, 15, 213, 0, 36, 39, 255, 252, 74, 107, 12, 10, 0, 97, 83, 0, 17, 0, 0, 0, 1, 0, 95, 51, 1, 55, 8, 186, 13, 0, 0, 0, 0, 76, 0, 76, 203, 77, 0, 0, 28, 87, 3, 187, 42, 148, 0, 0, 119, 14, 237, 138, 146, 0, 0, 78, 67, 90, 4, 0, 88, 22, 0, 69, 16, 151, 6, 0, 8, 37, 52, 23, 53, 123, 64, 207, 2, 48, 73, 70, 92, 0, 44, 0, 25, 4, 54, 87, 116, 255, 192, 22, 4, 2, 125, 161, 47, 83, 0, 24, 255, 104, 33, 124, 27, 36, 0, 239, 26, 6, 10, 0, 9, 100, 26, 0, 7, 0, 51, 63, 143, 0, 58, 3, 92, 20, 4, 114, 0, 43, 226, 16, 27, 0, 255, 219, 74, 132, 12, 1, 0, 68, 39, 0, 66, 0, 70, 0, 15, 0, 70, 8, 9, 39, 2, 192, 13, 0, 0, 0, 0, 100, 45, 28, 192, 13, 0, 0, 112, 24, 0, 189, 58, 154, 0, 0, 100, 8, 237, 128, 37, 0, 2, 92, 146, 101, 52, 0, 83, 24, 0, 69, 10, 180, 37, 67, 0, 107, 0, 36, 30, 156, 21, 184, 39, 12, 89, 28, 164, 0, 11, 0, 76, 4, 11, 64, 115, 255, 255, 46, 25, 31, 0, 160, 0, 93, 22, 5, 255, 119, 7, 135, 27, 0, 0, 255, 16, 114, 4, 0, 2, 89, 41, 63, 7, 0, 46, 63, 135, 30, 79, 3, 87, 14, 4, 162, 31, 7, 174, 8, 21, 0, 255, 148, 74, 120, 17, 33, 0, 111, 29, 0, 8, 0, 31, 0, 18, 0, 76, 10, 12, 42, 29, 176, 13, 0, 0, 3, 35, 95, 0, 28, 255, 124, 0, 0, 91, 55, 14, 193, 0, 84, 5, 50, 131, 4, 237, 94, 125, 0, 0, 82, 100, 99, 22, 0, 82, 5, 0, 67, 19, 179, 51, 0, 51, 82, 0, 16, 42, 133, 21, 183, 25, 39, 48, 60, 161, 0, 11, 0, 17, 4, 54, 115, 157, 203, 255, 20, 22, 0, 29, 150, 63, 93, 27, 37, 255, 116, 37, 155, 27, 80, 0, 237, 51, 73, 42, 13, 2, 154, 55, 0, 7, 0, 67, 111, 143, 21, 76, 3, 116, 7, 5, 113, 9, 12, 226, 0, 24, 0, 255, 235, 64, 137, 12, 1, 0, 98, 32, 55, 38, 0, 0, 0, 111, 0, 27, 51, 16, 91, 14, 163, 13, 0, 16, 0, 0, 86, 0, 103, 167, 6, 0, 0, 86, 108, 17, 170, 0, 130, 24, 6, 169, 9, 237, 124, 130, 0, 0, 92, 103, 121, 10, 0, 80, 5, 26, 63, 0, 160, 82, 0, 36, 25, 0, 28, 68, 145, 21, 196, 66, 96, 80, 63, 147, 76, 11, 0, 121, 4, 95, 108, 192, 255, 225, 10, 0, 0, 28, 130, 0, 93, 0, 52, 229, 131, 38, 12, 27, 0, 0, 240, 28, 139, 68, 0, 2, 90, 26, 25, 7, 0, 44, 112, 175, 0, 75, 3, 141, 58, 17, 107, 0, 14, 227, 0, 59, 0, 255, 154, 48, 125, 12, 1, 0, 146, 89, 129, 52, 0, 0, 28, 48, 0, 3, 81, 11, 51, 70, 175, 13, 0, 57, 0, 16, 31, 0, 80, 189, 79, 0, 0, 28, 95, 2, 170, 67, 113, 0, 51, 109, 52, 237, 116, 113, 0, 0, 91, 107, 118, 18, 0, 86, 25, 0, 69, 40, 153, 74, 0, 0, 95, 34, 63, 42, 116, 21, 187, 53, 8, 33, 51, 125, 24, 24, 0, 91, 48, 51, 89, 150, 255, 182, 31, 103, 0, 255, 121, 0, 93, 19, 60, 234, 129, 23, 183, 27, 0, 12, 212, 19, 147, 25, 3, 2, 120, 81, 0, 7, 0, 69, 102, 162, 0, 79, 5, 81, 2, 23, 81, 0, 7, 231, 0, 58, 67, 255, 208, 13, 75, 12, 9, 0, 155, 23, 69, 15, 0, 0, 0, 67, 0, 19, 4, 13, 47, 13, 171, 13, 0, 0, 24, 0, 44, 67, 87, 191, 112, 0, 0, 59, 42, 21, 146, 43, 121, 0, 12, 144, 39, 237, 106, 121, 0, 0, 94, 127, 83, 14, 0, 96, 28, 0, 69, 22, 168, 18, 5, 8, 21, 44, 41, 0, 185, 41, 187, 35, 79, 101, 49, 86, 8, 11, 0, 25, 39, 30, 90, 156, 255, 255, 7, 0, 0, 0, 135, 0, 93, 6, 13, 255, 120, 7, 92, 27, 0, 0, 246, 54, 10, 26, 14, 14, 171, 36, 0, 7, 0, 95, 57, 146, 0, 79, 3, 148, 21, 12, 237, 13, 16, 255, 0, 35, 6, 252, 210, 15, 122, 12, 39, 0, 171, 44, 0, 75, 0, 32, 0, 1, 0, 69, 16, 26, 75, 24, 155, 13, 0, 61, 0, 50, 53, 0, 65, 175, 135, 0, 0, 116, 44, 0, 139, 48, 147, 0, 0, 68, 40, 237, 137, 148, 0, 0, 94, 104, 87, 28, 0, 106, 10, 0, 53, 0, 151, 74, 35, 8, 21, 0, 48, 46, 154, 53, 191, 15, 40, 79, 65, 175, 36, 11, 0, 45, 4, 142, 28, 108, 255, 255, 12, 33, 0, 20, 142, 78, 49, 12, 17, 255, 97, 50, 164, 27, 0, 0, 255, 44, 64, 65, 0, 24, 92, 53, 16, 7, 0, 40, 99, 177, 43, 79, 3, 200, 49, 22, 170, 19, 29, 255, 0, 33, 0, 255, 255, 64, 116, 12, 10, 0, 111, 63, 66, 37, 0, 0, 8, 19, 0, 7, 98, 5, 107, 18, 174, 13, 0, 12, 0, 0, 61, 0, 103, 165, 154, 0, 0, 45, 52, 0, 157, 41, 166, 9, 28, 152, 30, 237, 146, 117, 0, 0, 67, 102, 133, 56, 0, 92, 86, 11, 16, 65, 154, 153, 0, 15, 99, 0, 61, 0, 187, 57, 135, 35, 12, 72, 73, 144, 17, 76, 0, 18, 112, 64, 57, 87, 225, 216, 47, 79, 0, 112, 159, 1, 75, 24, 18, 254, 116, 17, 183, 27, 0, 11, 255, 0, 94, 0, 0, 7, 144, 44, 68, 7, 30, 61, 99, 167, 0, 58, 39, 229, 54, 4, 97, 0, 20, 216, 0, 31, 18, 177, 181, 51, 76, 12, 53, 0, 146, 27, 74, 38, 0, 0, 0, 54, 0, 6, 10, 30, 148, 2, 137, 13, 0, 1, 0, 0, 24, 0, 44, 240, 111, 0, 0, 64, 82, 0, 199, 17, 121, 0, 14, 175, 49, 224, 102, 166, 0, 0, 58, 119, 97, 47, 0, 102, 5, 0, 60, 19, 133, 32, 0, 3, 77, 4, 20, 0, 187, 21, 207, 12, 43, 55, 64, 160, 53, 39, 0, 12, 4, 117, 94, 59, 225, 255, 22, 30, 0, 141, 146, 0, 59, 33, 17, 255, 127, 71, 114, 31, 23, 0, 236, 14, 65, 64, 0, 3, 162, 38, 51, 7, 24, 31, 96, 181, 41, 56, 3, 184, 10, 4, 34, 0, 14, 255, 61, 45, 0, 251, 163, 30, 140, 12, 10, 0, 125, 83, 0, 38, 0, 0, 0, 34, 0, 0, 46, 28, 76, 26, 159, 13, 0, 0, 0, 53, 78, 0, 18, 176, 118, 0, 0, 28, 73, 0, 199, 42, 130, 0, 12, 164, 54, 237, 72, 165, 0, 0, 94, 81, 86, 4, 0, 106, 13, 0, 32, 5, 166, 87, 0, 11, 76, 2, 54, 0, 161, 34, 199, 32, 34, 68, 78, 113, 37, 17, 0, 56, 4, 86, 76, 120, 219, 255, 13, 0, 0, 0, 73, 41, 93, 0, 21, 248, 96, 52, 100, 34, 0, 12, 228, 73, 22, 22, 0, 4, 106, 41, 64, 7, 0, 32, 34, 189, 0, 79, 3, 161, 2, 4, 210, 99, 19, 179, 0, 69, 0, 255, 195, 0, 100, 12, 1, 0, 150, 22, 0, 25, 0, 16, 0, 3, 37, 0, 61, 33, 49, 45, 148, 13, 0, 0, 0, 0, 77, 0, 84, 198, 114, 0, 0, 45, 137, 0, 133, 61, 112, 1, 0, 109, 9, 237, 92, 107, 0, 0, 94, 64, 100, 4, 0, 89, 9, 0, 14, 12, 161, 155, 0, 21, 87, 0, 97, 43, 184, 63, 180, 24, 53, 17, 78, 83, 30, 25, 0, 12, 4, 70, 59, 110, 255, 255, 0, 0, 0, 117, 167, 24, 93, 15, 5, 255, 118, 31, 144, 27, 53, 22, 211, 0, 64, 0, 0, 9, 98, 49, 75, 7, 0, 133, 88, 184, 0, 51, 3, 155, 0, 4, 206, 54, 17, 204, 0, 96, 0, 255, 172, 0, 140, 12, 1, 0, 62, 35, 0, 47, 26, 0, 0, 50, 31, 66, 12, 12, 53, 12, 122, 13, 0, 100, 0, 0, 60, 26, 67, 136, 1, 0, 0, 53, 55, 13, 105, 77, 96, 0, 0, 104, 86, 237, 99, 137, 0, 0, 94, 110, 87, 55, 0, 100, 43, 0, 47, 11, 153, 80, 0, 51, 30, 0, 43, 0, 186, 33, 190, 29, 14, 120, 78, 101, 75, 11, 0, 54, 4, 38, 58, 102, 217, 255, 6, 0, 0, 0, 110, 0, 86, 0, 17, 255, 128, 38, 163, 27, 47, 0, 255, 86, 22, 0, 0, 32, 113, 29, 0, 7, 0, 38, 65, 181, 0, 39, 3, 212, 8, 4, 230, 13, 17, 255, 0, 88, 0, 255, 242, 0, 132, 12, 1, 0, 124, 72, 0, 21, 0, 0, 0, 23, 0, 27, 17, 16, 49, 7, 133, 13, 0, 0, 0, 21, 77, 0, 86, 139, 6, 0, 0, 40, 57, 0, 199, 84, 126, 0, 30, 127, 88, 237, 200, 115, 0, 0, 89, 90, 153, 8, 0, 33, 70, 26, 69, 19, 175, 65, 0, 46, 5, 24, 24, 68, 182, 21, 189, 34, 0, 79, 78, 175, 0, 11, 7, 64, 4, 59, 34, 120, 255, 255, 0, 0, 0, 56, 169, 124, 82, 0, 23, 191, 101, 42, 159, 27, 96, 0, 167, 46, 22, 114, 0, 2, 94, 17, 35, 7, 0, 17, 115, 189, 0, 0, 3, 134, 28, 4, 71, 48, 46, 248, 40, 32, 0, 198, 192, 74, 137, 12, 17, 0, 62, 23, 0, 138, 0, 25, 32, 28, 0, 46, 32, 5, 26, 26, 100, 31, 0, 122, 0, 119, 17, 8, 71, 222, 8, 0, 0, 28, 66, 39, 172, 29, 108, 0, 1, 99, 46, 237, 200, 163, 0, 12, 94, 134, 100, 41, 0, 101, 34, 0, 1, 54, 182, 39, 0, 31, 46, 28, 22, 6, 161, 75, 173, 26, 66, 83, 78, 79, 0, 18, 0, 29, 4, 95, 47, 111, 255, 255, 21, 57, 0, 0, 162, 124, 93, 41, 10, 255, 136, 62, 171, 27, 0, 0, 255, 41, 106, 1, 0, 11, 40, 62, 78, 7, 0, 87, 76, 156, 7, 8, 3, 129, 60, 37, 163, 0, 35, 178, 27, 101, 67, 243, 255, 56, 120, 12, 34, 0, 155, 89, 73, 217, 0, 70, 0, 41, 52, 46, 23, 7, 90, 19, 116, 40, 0, 0, 0, 0, 18, 0, 107, 211, 181, 0, 0, 99, 99, 0, 199, 15, 184, 28, 64, 120, 16, 209, 104, 100, 0, 1, 75, 116, 65, 4, 0, 108, 47, 0, 0, 31, 178, 80, 0, 0, 71, 17, 34, 46, 154, 39, 193, 27, 15, 33, 54, 114, 60, 11, 0, 108, 4, 24, 77, 110, 255, 156, 0, 0, 0, 0, 169, 0, 93, 0, 32, 255, 47, 2, 176, 27, 0, 20, 253, 31, 58, 0, 0, 10, 172, 78, 0, 7, 0, 70, 96, 146, 92, 56, 3, 164, 25, 33, 178, 76, 7, 190, 0, 43, 0, 211, 178, 20, 88, 12, 1, 0, 255, 16, 0, 25, 0, 0, 0, 18, 0, 45, 10, 33, 38, 9, 98, 13, 0, 118, 0, 21, 13, 58, 106, 160, 64, 0, 0, 76, 50, 0, 199, 49, 180, 56, 0, 58, 37, 237, 130, 102, 0, 0, 27, 79, 112, 15, 0, 52, 12, 0, 59, 18, 127, 83, 0, 24, 49, 0, 75, 0, 122, 36, 186, 5, 27, 69, 3, 140, 43, 23, 0, 48, 4, 20, 133, 86, 215, 225, 11, 0, 5, 0, 97, 0, 76, 15, 97, 177, 145, 58, 175, 27, 41, 0, 242, 0, 138, 0, 0, 7, 170, 45, 19, 7, 0, 94, 117, 162, 29, 25, 3, 171, 9, 4, 175, 0, 7, 197, 38, 37, 0, 221, 195, 60, 81, 12, 28, 0, 54, 49, 0, 21, 0, 0, 0, 23, 1, 87, 31, 15, 36, 56, 199, 13, 0, 34, 50, 50, 109, 0, 78, 216, 58, 0, 0, 87, 153, 0, 199, 0, 82, 39, 18, 156, 50, 237, 49, 158, 0, 0, 44, 104, 99, 32, 0, 77, 23, 0, 69, 14, 167, 96, 0, 0, 35, 72, 64, 0, 153, 21, 160, 44, 120, 82, 3, 66, 0, 32, 0, 61, 4, 74, 167, 106, 233, 229, 55, 0, 0, 0, 139, 0, 62, 0, 21, 204, 132, 29, 152, 41, 76, 0, 233, 0, 82, 2, 0, 15, 149, 40, 16, 7, 0, 61, 76, 131, 32, 26, 3, 91, 20, 8, 111, 19, 36, 179, 0, 21, 0, 255, 244, 74, 114, 12, 32, 0, 127, 59, 0, 3, 0, 0, 0, 1, 0, 0, 28, 11, 75, 34, 223, 13, 0, 0, 0, 0, 126, 0, 81, 228, 92, 0, 0, 47, 85, 0, 178, 35, 114, 16, 0, 135, 13, 229, 96, 36, 0, 0, 94, 121, 113, 10, 0, 108, 27, 26, 55, 8, 178, 78, 0, 34, 103, 0, 72, 0, 174, 21, 176, 12, 26, 59, 78, 135, 69, 11, 0, 35, 21, 18, 78, 146, 244, 255, 67, 1, 56, 0, 151, 36, 93, 0, 16, 217, 133, 21, 144, 27, 32, 0, 255, 0, 86, 0, 12, 11, 156, 38, 47, 7, 0, 84, 56, 153, 0, 79, 3, 212, 58, 5, 133, 13, 12, 146, 0, 31, 0, 227, 133, 0, 91, 12, 1, 0, 37, 71, 0, 57, 0, 0, 0, 10, 7, 90, 40, 8, 100, 21, 131, 13, 0, 0, 12, 0, 24, 30, 95, 225, 64, 0, 0, 51, 148, 55, 154, 9, 36, 27, 79, 127, 75, 237, 124, 150, 0, 0, 78, 117, 83, 14, 0, 108, 30, 0, 40, 0, 186, 35, 0, 12, 44, 72, 32, 68, 184, 84, 207, 30, 29, 102, 78, 157, 41, 11, 46, 33, 4, 106, 36, 151, 255, 255, 0, 0, 48, 0, 169, 0, 93, 10, 9, 255, 94, 68, 131, 27, 37, 29, 255, 24, 48, 53, 0, 2, 65, 54, 12, 7, 0, 106, 83, 145, 0, 79, 3, 175, 29, 8, 87, 18, 18, 212, 0, 48, 0, 255, 201, 20, 135, 12, 1, 0, 112, 55, 0, 8, 0, 15, 0, 33, 0, 0, 12, 5, 46, 28, 185, 13, 0, 3, 0, 0, 71, 33, 124, 172, 96, 0, 0, 34, 125, 85, 177, 22, 130, 18, 0, 82, 18, 237, 121, 132, 19, 0, 88, 138, 112, 10, 0, 83, 52, 0, 69, 0, 183, 46, 0, 23, 73, 35, 34, 55, 187, 21, 203, 64, 52, 86, 78, 105, 17, 11, 0, 17, 4, 21, 42, 194, 255, 173, 12, 0, 0, 16, 159, 0, 93, 0, 25, 204, 102, 53, 99, 27, 41, 12, 255, 14, 126, 0, 14, 10, 113, 62, 0, 7, 0, 52, 112, 136, 0, 79, 3, 116, 0, 4, 51, 13, 49, 210, 0, 31, 0, 255, 161, 65, 134, 12, 1, 0, 72, 42, 0, 37, 0, 18, 0, 53, 0, 47, 10, 10, 27, 5, 166, 13, 0, 51, 0, 0, 72, 16, 48, 248, 52, 0, 0, 33, 87, 0, 187, 0, 71, 30, 5, 89, 64, 237, 136, 81, 0, 0, 94, 122, 56, 13, 0, 80, 13, 0, 55, 0, 172, 69, 19, 35, 40, 0, 44, 68, 135, 61, 207, 49, 30, 51, 43, 203, 59, 11, 0, 41, 4, 28, 70, 158, 231, 255, 33, 67, 0, 0, 151, 23, 84, 0, 62, 243, 62, 63, 73, 27, 0, 4, 245, 0, 101, 32, 0, 32, 144, 55, 55, 7, 0, 53, 111, 89, 26, 79, 3, 138, 0, 11, 50, 46, 30, 210, 0, 36, 45, 255, 169, 54, 128, 12, 41, 0, 116, 48, 1, 15, 0, 19, 0, 17, 0, 8, 19, 9, 38, 26, 163, 13, 0, 75, 5, 0, 76, 0, 93, 191, 98, 0, 0, 41, 106, 65, 185, 48, 105, 0, 5, 105, 21, 237, 95, 161, 0, 31, 94, 61, 111, 6, 0, 93, 19, 0, 40, 0, 167, 76, 0, 42, 36, 6, 106, 68, 176, 81, 201, 38, 72, 112, 64, 90, 0, 11, 0, 42, 4, 43, 59, 99, 233, 255, 3, 0, 0, 0, 169, 0, 76, 0, 26, 254, 101, 48, 139, 27, 78, 27, 185, 8, 122, 99, 0, 53, 56, 40, 14, 7, 0, 55, 81, 135, 0, 79, 3, 171, 6, 20, 205, 36, 28, 147, 0, 83, 0, 255, 178, 41, 124, 12, 1, 0, 89, 62, 0, 9, 0, 0, 0, 28, 124, 14, 40, 28, 58, 23, 157, 13, 0, 0, 0, 6, 74, 0, 84, 182, 35, 0, 0, 48, 83, 26, 195, 41, 56, 9, 3, 55, 65, 237, 124, 131, 0, 0, 94, 120, 147, 4, 0, 99, 5, 0, 69, 19, 150, 10, 29, 34, 66, 5, 18, 0, 179, 34, 154, 33, 7, 45, 65, 105, 0, 11, 0, 32, 4, 44, 33, 160, 252, 255, 24, 0, 0, 0, 138, 78, 93, 0, 19, 246, 136, 43, 170, 27, 0, 37, 255, 0, 95, 0, 0, 3, 114, 58, 36, 7, 26, 89, 60, 147, 0, 74, 3, 133, 32, 4, 233, 0, 7, 180, 0, 52, 0, 255, 207, 64, 136, 12, 1, 0, 101, 69, 5, 34, 0, 31, 0, 5, 16, 83, 29, 8, 133, 10, 184, 13, 0, 0, 9, 0, 94, 17, 39, 255, 37, 0, 0, 110, 131, 49, 189, 17, 108, 48, 0, 165, 7, 237, 92, 75, 40, 0, 94, 143, 98, 65, 0, 97, 5, 0, 61, 43, 164, 76, 37, 27, 88, 0, 43, 0, 111, 21, 118, 13, 15, 60, 73, 112, 82, 11, 48, 85, 4, 20, 34, 154, 225, 227, 51, 0, 0, 0, 133, 0, 93, 14, 17, 228, 103, 33, 138, 31, 0, 47, 255, 16, 88, 0, 17, 21, 119, 118, 33, 7, 0, 55, 84, 114, 0, 79, 3, 110, 46, 4, 47, 0, 7, 144, 0, 21, 0, 255, 161, 74, 119, 23, 1, 0, 185, 57, 0, 44, 28, 8, 0, 1, 0, 34, 13, 5, 90, 43, 184, 13, 0, 8, 0, 26, 52, 0, 66, 165, 87, 0, 0, 109, 125, 26, 181, 25, 136, 45, 26, 50, 35, 237, 120, 84, 0, 0, 78, 59, 77, 4, 0, 112, 5, 0, 40, 26, 156, 131, 0, 16, 68, 0, 37, 68, 89, 44, 198, 43, 34, 51, 70, 98, 66, 17, 15, 40, 4, 70, 70, 111, 255, 215, 24, 1, 0, 125, 137, 5, 51, 36, 11, 231, 139, 23, 141, 34, 40, 42, 252, 28, 40, 74, 0, 0, 61, 93, 10, 7, 0, 75, 73, 163, 0, 30, 3, 108, 82, 4, 159, 0, 7, 224, 0, 84, 41, 255, 156, 55, 119, 12, 64, 0, 110, 36, 0, 9, 0, 0, 0, 20, 63, 20, 33, 1, 82, 60, 174, 13, 0, 64, 0, 0, 10, 0, 75, 144, 62, 0, 0, 51, 112, 0, 192, 46, 93, 31, 0, 110, 52, 231, 175, 160, 0, 0, 94, 91, 97, 16, 0, 65, 5, 0, 48, 6, 116, 40, 0, 0, 21, 25, 14, 0, 141, 45, 207, 3, 64, 84, 54, 99, 22, 21, 52, 20, 4, 83, 152, 88, 255, 174, 21, 0, 0, 0, 112, 0, 93, 0, 18, 241, 126, 86, 164, 27, 0, 29, 255, 28, 55, 50, 0, 12, 133, 53, 0, 7, 18, 75, 68, 148, 0, 79, 3, 173, 26, 14, 74, 46, 20, 255, 0, 22, 0, 242, 180, 38, 101, 12, 1, 0, 133, 28, 0, 23, 0, 44, 0, 54, 0, 38, 31, 15, 55, 9, 198, 13, 0, 0, 24, 8, 57, 16, 94, 177, 69, 0, 0, 46, 77, 0, 163, 58, 204, 48, 19, 156, 15, 191, 81, 182, 0, 31, 94, 77, 113, 4, 0, 71, 22, 0, 13, 2, 147, 55, 0, 56, 32, 62, 65, 43, 162, 21, 207, 33, 49, 103, 62, 128, 56, 11, 62, 51, 4, 35, 52, 123, 211, 255, 6, 0, 0, 89, 128, 29, 93, 0, 12, 228, 63, 84, 94, 31, 2, 12, 236, 18, 42, 33, 15, 6, 160, 46, 0, 7, 0, 66, 95, 88, 0, 79, 3, 176, 12, 8, 154, 0, 7, 217, 13, 31, 32, 255, 176, 45, 123, 12, 9, 0, 174, 78, 0, 60, 0, 7, 0, 36, 16, 6, 105, 11, 39, 16, 163, 13, 0, 0, 0, 30, 78, 0, 78, 150, 80, 0, 0, 46, 148, 0, 187, 25, 103, 7, 35, 151, 33, 237, 100, 117, 0, 0, 94, 125, 104, 4, 0, 102, 32, 0, 69, 0, 179, 69, 11, 6, 55, 39, 49, 43, 179, 89, 199, 48, 60, 40, 22, 128, 50, 34, 0, 32, 16, 67, 107, 176, 255, 146, 44, 43, 10, 0, 140, 41, 93, 0, 17, 254, 137, 41, 172, 32, 0, 0, 255, 21, 83, 26, 0, 6, 112, 48, 0, 7, 0, 32, 107, 154, 0, 79, 3, 63, 0, 13, 220, 0, 16, 182, 0, 36, 0, 252, 215, 74, 99, 12, 28, 0, 187, 36, 35, 45, 0, 24, 0, 19, 0, 0, 35, 21, 43, 38, 146, 13, 0, 25, 0, 0, 58, 0, 66, 214, 100, 0, 0, 35, 104, 0, 118, 45, 89, 0, 0, 138, 22, 237, 70, 146, 0, 0, 78, 89, 101, 17, 0, 89, 20, 0, 69, 4, 161, 40, 0, 23, 31, 8, 17, 4, 174, 118, 196, 7, 51, 0, 78, 135, 69, 52, 14, 36, 4, 78, 56, 83, 255, 225, 31, 6, 0, 40, 113, 28, 57, 43, 38, 246, 99, 2, 183, 27, 0, 0, 219, 39, 6, 25, 0, 11, 74, 41, 0, 7, 0, 89, 105, 175, 5, 35, 3, 48, 1, 4, 132, 0, 25, 236, 30, 73, 52, 254, 173, 61, 135, 12, 1, 0, 167, 45, 30, 15, 0, 0, 0, 69, 32, 1, 4, 23, 71, 21, 208, 13, 0, 0, 16, 0, 119, 42, 89, 212, 108, 0, 0, 63, 121, 0, 181, 4, 106, 25, 27, 113, 23, 237, 129, 51, 0, 0, 91, 76, 115, 46, 0, 113, 20, 0, 64, 23, 123, 73, 0, 47, 70, 0, 77, 51, 150, 21, 199, 61, 45, 114, 32, 116, 67, 16, 0, 61, 4, 14, 87, 58, 209, 236, 0, 0, 24, 32, 99, 0, 93, 15, 13, 251, 101, 53, 158, 27, 0, 0, 255, 0, 78, 0, 17, 26, 176, 49, 40, 7, 0, 39, 90, 88, 0, 58, 3, 174, 23, 8, 113, 9, 9, 201, 27, 38, 0, 255, 179, 52, 116, 22, 18, 0, 67, 41, 0, 48, 0, 26, 0, 31, 19, 85, 22, 35, 48, 26, 196, 13, 0, 0, 6, 84, 112, 44, 34, 179, 56, 0, 0, 109, 79, 16, 199, 52, 77, 52, 52, 99, 29, 237, 57, 128, 0, 0, 91, 98, 98, 46, 0, 98, 5, 0, 40, 45, 173, 46, 0, 57, 76, 0, 24, 0, 186, 21, 186, 71, 51, 24, 21, 99, 0, 11, 0, 114, 4, 9, 80, 168, 213, 198, 17, 40, 0, 0, 129, 60, 93, 34, 18, 228, 117, 23, 177, 34, 0, 0, 251, 16, 76, 0, 0, 35, 135, 34, 66, 7, 0, 86, 87, 177, 0, 38, 3, 147, 0, 8, 177, 65, 11, 186, 0, 33, 0, 249, 185, 58, 110, 12, 24, 0, 157, 33, 0, 46, 0, 7, 0, 43, 40, 2, 29, 1, 81, 2, 119, 13, 0, 24, 0, 0, 42, 0, 91, 198, 101, 0, 0, 129, 71, 0, 179, 23, 126, 0, 34, 98, 39, 237, 151, 116, 0, 0, 49, 105, 108, 55, 0, 96, 5, 0, 30, 34, 160, 172, 0, 31, 28, 0, 77, 0, 173, 21, 185, 34, 19, 52, 64, 108, 24, 17, 0, 13, 4, 33, 62, 134, 255, 255, 28, 39, 0, 0, 152, 2, 93, 0, 50, 215, 141, 5, 98, 27, 0, 38, 255, 27, 182, 49, 10, 11, 102, 45, 0, 7, 0, 135, 112, 157, 64, 58, 3, 166, 0, 24, 182, 0, 7, 198, 0, 53, 90, 255, 193, 54, 132, 12, 1, 0, 120, 42, 0, 41, 0, 0, 0, 86, 0, 52, 7, 9, 92, 2, 155, 13, 0, 0, 20, 34, 84, 0, 122, 191, 73, 0, 0, 64, 129, 0, 159, 17, 135, 22, 15, 102, 69, 237, 83, 140, 0, 0, 74, 126, 87, 70, 0, 96, 5, 0, 61, 30, 118, 149, 0, 47, 62, 0, 94, 0, 170, 32, 195, 51, 57, 41, 0, 97, 15, 55, 0, 49, 71, 63, 28, 153, 230, 180, 7, 68, 0, 0, 129, 0, 77, 0, 23, 214, 125, 40, 139, 27, 0, 0, 248, 45, 174, 0, 11, 2, 188, 37, 0, 7, 0, 70, 94, 167, 54, 79, 7, 102, 2, 4, 96, 0, 17, 220, 0, 37, 0, 239, 167, 47, 122, 12, 27, 0, 99, 97, 0, 55, 0, 0, 0, 71, 9, 60, 28, 17, 92, 7, 184, 13, 0, 59, 0, 0, 60, 0, 28, 245, 111, 0, 0, 121, 134, 48, 115, 4, 108, 4, 17, 124, 82, 229, 84, 138, 0, 0, 94, 68, 124, 54, 0, 100, 17, 0, 19, 2, 153, 111, 23, 0, 55, 0, 90, 0, 172, 37, 196, 39, 60, 75, 78, 108, 2, 11, 0, 32, 4, 67, 28, 108, 255, 255, 37, 53, 0, 16, 132, 0, 93, 57, 11, 255, 138, 3, 170, 27, 42, 7, 214, 48, 129, 27, 0, 1, 84, 62, 23, 7, 0, 62, 69, 175, 48, 38, 3, 144, 14, 4, 144, 17, 7, 145, 46, 47, 0, 255, 186, 74, 88, 12, 35, 0, 73, 94, 0, 28, 0, 0, 0, 14, 51, 59, 29, 10, 100, 14, 163, 13, 0, 0, 108, 0, 81, 0, 109, 195, 1, 0, 0, 84, 96, 48, 184, 7, 90, 0, 53, 170, 55, 225, 114, 89, 0, 0, 62, 57, 125, 13, 0, 105, 14, 0, 55, 58, 175, 74, 0, 16, 13, 0, 77, 13, 145, 34, 207, 31, 12, 87, 48, 106, 9, 11, 0, 36, 4, 70, 31, 142, 255, 242, 13, 0, 0, 0, 169, 0, 93, 0, 18, 255, 139, 152, 24, 27, 14, 17, 227, 16, 51, 7, 51, 9, 140, 92, 0, 7, 0, 103, 71, 125, 62, 33, 3, 199, 55, 4, 224, 27, 7, 139, 33, 72, 38, 239, 206, 49, 132, 12, 12, 0, 104, 78, 0, 33, 0, 6, 0, 23, 79, 35, 43, 1, 129, 10, 119, 13, 0, 0, 0, 52, 22, 0, 116, 238, 13, 0, 0, 114, 100, 88, 187, 41, 126, 49, 0, 175, 36, 237, 105, 150, 0, 0, 85, 114, 100, 35, 0, 71, 16, 0, 24, 45, 145, 110, 0, 47, 65, 14, 51, 0, 173, 36, 207, 3, 33, 88, 39, 137, 40, 25, 11, 9, 4, 32, 125, 139, 246, 202, 31, 0, 0, 0, 163, 0, 93, 0, 21, 231, 106, 19, 122, 27, 0, 0, 250, 11, 119, 10, 0, 5, 118, 31, 32, 7, 59, 45, 76, 129, 0, 79, 3, 191, 0, 4, 188, 37, 29, 216, 0, 58, 0, 247, 164, 31, 123, 12, 1, 0, 178, 61, 99, 43, 0, 0, 0, 19, 13, 0, 19, 12, 54, 15, 181, 13, 0, 52, 0, 30, 53, 28, 87, 202, 57, 0, 0, 60, 144, 0, 115, 51, 127, 0, 0, 111, 64, 237, 114, 182, 0, 0, 84, 91, 87, 30, 0, 78, 13, 0, 42, 43, 182, 101, 0, 21, 15, 0, 51, 4, 181, 39, 207, 52, 17, 101, 54, 90, 0, 11, 0, 122, 4, 73, 82, 159, 253, 225, 0, 34, 0, 136, 133, 23, 93, 27, 11, 249, 99, 75, 134, 27, 0, 0, 255, 0, 154, 17, 0, 6, 102, 27, 35, 7, 56, 53, 99, 161, 0, 79, 3, 143, 24, 4, 87, 8, 7, 170, 0, 51, 16, 226, 204, 30, 128, 12, 1, 0, 110, 56, 89, 59, 0, 0, 0, 38, 12, 0, 67, 23, 137, 2, 140, 13, 0, 0, 0, 0, 55, 0, 44, 221, 146, 0, 0, 60, 76, 32, 193, 0, 142, 9, 78, 176, 53, 237, 98, 114, 0, 0, 94, 102, 177, 4, 0, 91, 19, 0, 69, 15, 164, 73, 0, 35, 50, 0, 54, 0, 144, 25, 190, 43, 58, 48, 65, 87, 82, 17, 10, 75, 19, 66, 116, 144, 232, 192, 21, 0, 42, 0, 75, 0, 93, 15, 25, 254, 119, 71, 147, 27, 76, 13, 255, 26, 87, 32, 0, 7, 127, 31, 27, 7, 0, 62, 36, 189, 0, 69, 3, 142, 22, 4, 69, 0, 11, 195, 28, 46, 71, 241, 210, 58, 113, 12, 1, 0, 47, 42, 14, 14, 0, 0, 0, 26, 0, 6, 41, 35, 160, 9, 144, 13, 0, 0, 0, 0, 91, 12, 117, 207, 51, 0, 0, 79, 107, 53, 191, 0, 67, 9, 0, 162, 28, 237, 108, 159, 6, 0, 85, 111, 96, 9, 0, 107, 12, 0, 69, 8, 149, 51, 0, 6, 70, 17, 18, 58, 110, 31, 203, 45, 22, 69, 73, 105, 26, 22, 0, 54, 32, 87, 158, 87, 255, 163, 25, 0, 0, 0, 115, 0, 83, 47, 24, 245, 117, 44, 162, 27, 0, 0, 255, 23, 101, 19, 0, 14, 110, 42, 14, 7, 0, 24, 106, 178, 0, 64, 3, 99, 36, 4, 79, 22, 12, 172, 0, 51, 0, 252, 148, 54, 114, 12, 17, 0, 171, 16, 34, 25, 0, 11, 0, 16, 31, 0, 4, 36, 63, 41, 184, 13, 0, 84, 0, 8, 58, 0, 51, 168, 48, 0, 0, 50, 41, 0, 187, 59, 165, 8, 0, 83, 19, 237, 140, 100, 5, 0, 80, 119, 125, 10, 0, 116, 11, 0, 69, 4, 123, 71, 67, 0, 54, 0, 37, 24, 161, 57, 201, 27, 36, 94, 54, 94, 79, 11, 0, 65, 4, 70, 63, 92, 193, 255, 10, 65, 0, 0, 134, 50, 93, 0, 18, 210, 128, 22, 167, 27, 1, 51, 255, 28, 102, 0, 36, 22, 141, 86, 0, 7, 0, 59, 59, 172, 0, 79, 3, 216, 8, 4, 58, 0, 29, 175, 25, 21, 0, 255, 174, 74, 101, 12, 9, 0, 174, 16, 49, 7, 0, 58, 0, 16, 26, 0, 45, 44, 59, 32, 146, 13, 0, 0, 31, 0, 80, 0, 81, 152, 38, 0, 0, 104, 95, 0, 199, 44, 154, 4, 15, 136, 33, 237, 192, 85, 19, 0, 0, 94, 86, 4, 0, 58, 5, 1, 69, 28, 166, 76, 0, 12, 15, 5, 26, 55, 181, 63, 207, 21, 3, 98, 64, 156, 78, 11, 6, 71, 4, 60, 28, 128, 242, 255, 6, 15, 1, 0, 169, 92, 93, 0, 5, 195, 69, 14, 164, 27, 93, 30, 211, 44, 63, 117, 0, 2, 67, 17, 0, 7, 0, 45, 113, 164, 89, 0, 3, 111, 34, 4, 67, 63, 52, 202, 11, 45, 0, 208, 153, 74, 108, 12, 44, 0, 98, 46, 0, 134, 0, 9, 0, 40, 72, 38, 28, 13, 61, 2, 132, 13, 0, 85, 0, 171, 69, 32, 107, 198, 18, 0, 0, 28, 39, 58, 199, 74, 113, 27, 30, 172, 36, 220, 155, 100, 24, 0, 87, 156, 117, 36, 0, 92, 12, 0, 0, 49, 168, 28, 2, 0, 73, 0, 41, 46, 152, 33, 156, 81, 133, 43, 30, 55, 34, 21, 0, 10, 4, 19, 73, 50, 255, 166, 14, 59, 20, 82, 149, 0, 93, 0, 31, 245, 137, 2, 166, 27, 0, 0, 255, 0, 168, 17, 26, 24, 195, 83, 0, 7, 0, 54, 80, 152, 0, 73, 3, 130, 97, 4, 16, 60, 12, 202, 65, 24, 0, 248, 185, 74, 124, 12, 1, 0, 95, 34, 0, 238, 0, 40, 0, 22, 0, 12, 4, 51, 44, 4, 156, 13, 0, 0, 0, 0, 19, 58, 6, 165, 183, 0, 0, 78, 111, 0, 199, 57, 76, 18, 56, 93, 14, 237, 100, 102, 0, 0, 89, 115, 138, 4, 0, 76, 50, 0, 0, 0, 182, 19, 21, 13, 51, 0, 22, 68, 179, 31, 163, 45, 57, 38, 78, 70, 44, 11, 0, 21, 9, 52, 62, 78, 222, 186, 0, 21, 0, 68, 169, 57, 93, 0, 5, 255, 115, 2, 125, 27, 0, 14, 239, 38, 6, 39, 0, 3, 63, 76, 0, 12, 0, 76, 85, 140, 0, 79, 3, 224, 54, 23, 132, 0, 7, 181, 22, 27, 18, 236, 162, 10, 130, 12, 1, 0, 224, 30, 0, 10, 0, 22, 0, 16, 0, 13, 8, 43, 10, 5, 94, 13, 0, 0, 0, 6, 0, 0, 72, 167, 95, 0, 0, 45, 82, 0, 199, 60, 95, 6, 122, 150, 28, 237, 113, 116, 0, 0, 28, 45, 107, 20, 0, 80, 60, 0, 43, 19, 169, 60, 0, 0, 84, 0, 59, 17, 149, 33, 207, 5, 35, 49, 54, 118, 92, 11, 73, 9, 13, 13, 152, 87, 220, 208, 13, 0, 0, 13, 149, 0, 93, 48, 69, 255, 125, 2, 183, 27, 22, 0, 218, 0, 11, 53, 0, 2, 157, 79, 0, 18, 0, 97, 105, 144, 1, 79, 3, 193, 3, 4, 247, 0, 24, 193, 0, 31, 0, 216, 215, 59, 140, 12, 40, 0, 132, 30, 0, 10, 18, 0, 0, 50, 65, 41, 43, 24, 75, 67, 129, 13, 0, 0, 36, 58, 29, 0, 34, 209, 61, 0, 0, 61, 99, 0, 195, 0, 88, 17, 0, 151, 31, 237, 105, 137, 0, 0, 94, 97, 86, 34, 0, 80, 23, 0, 34, 47, 164, 44, 0, 30, 26, 0, 66, 0, 154, 21, 205, 87, 67, 85, 78, 119, 17, 17, 43, 49, 4, 117, 80, 164, 236, 255, 14, 0, 7, 131, 156, 0, 93, 0, 8, 255, 90, 128, 130, 27, 86, 0, 233, 47, 6, 0, 23, 12, 98, 37, 0, 7, 0, 69, 85, 97, 0, 79, 3, 70, 20, 13, 79, 0, 7, 229, 0, 28, 25, 255, 200, 67, 125, 12, 94, 0, 91, 134, 32, 43, 0, 68, 0, 20, 7, 0, 118, 1, 124, 25, 161, 13, 0, 39, 0, 0, 75, 0, 53, 227, 40, 0, 0, 73, 114, 39, 195, 34, 100, 0, 0, 169, 37, 212, 97, 79, 0, 0, 94, 111, 129, 30, 0, 106, 52, 0, 59, 34, 143, 22, 0, 0, 53, 27, 84, 10, 179, 32, 180, 47, 51, 62, 78, 106, 0, 48, 0, 51, 4, 47, 51, 138, 253, 212, 16, 0, 0, 0, 137, 0, 88, 5, 26, 223, 86, 24, 141, 27, 0, 0, 255, 32, 68, 0, 0, 91, 149, 38, 52, 7, 0, 66, 101, 189, 0, 73, 3, 129, 60, 14, 135, 68, 7, 191, 0, 50, 0, 247, 177, 74, 81, 12, 74, 0, 82, 42, 38, 88, 0, 66, 0, 14, 0, 79, 25, 16, 45, 37, 162, 13, 0, 0, 0, 0, 69, 0, 34, 255, 98, 0, 0, 60, 120, 0, 154, 0, 114, 0, 107, 67, 2, 219, 86, 167, 0, 0, 92, 142, 102, 27, 0, 113, 22, 0, 69, 36, 122, 98, 0, 19, 47, 26, 71, 27, 171, 21, 197, 17, 35, 64, 78, 137, 6, 17, 0, 63, 4, 90, 107, 134, 238, 228, 17, 15, 42, 0, 162, 30, 93, 91, 32, 234, 109, 33, 127, 27, 0, 52, 255, 3, 139, 38, 0, 63, 95, 76, 22, 7, 0, 70, 104, 164, 0, 61, 3, 218, 32, 12, 154, 0, 7, 191, 5, 65, 0, 218, 180, 71, 120, 12, 64, 0, 141, 71, 84, 8, 41, 0, 0, 81, 0, 0, 11, 10, 173, 46, 144, 13, 0, 56, 0, 0, 60, 0, 105, 216, 255, 0, 0, 61, 116, 0, 181, 38, 68, 15, 0, 117, 14, 212, 143, 144, 12, 0, 88, 112, 150, 7, 0, 74, 62, 0, 69, 22, 136, 138, 0, 23, 58, 0, 56, 0, 181, 21, 196, 49, 51, 35, 78, 133, 26, 22, 0, 10, 4, 82, 111, 153, 245, 161, 23, 53, 5, 0, 139, 0, 93, 1, 52, 181, 89, 2, 100, 27, 2, 23, 255, 0, 196, 0, 51, 14, 159, 36, 0, 7, 0, 58, 103, 135, 0, 79, 3, 142, 27, 4, 48, 0, 17, 218, 0, 29, 0, 255, 183, 70, 89, 12, 1, 0, 96, 62, 61, 61, 0, 0, 0, 62, 0, 8, 4, 28, 64, 5, 179, 13, 0, 0, 18, 0, 46, 4, 62, 178, 148, 0, 0, 72, 113, 0, 199, 66, 111, 4, 31, 86, 14, 237, 93, 106, 0, 0, 91, 78, 128, 29, 0, 110, 33, 0, 69, 32, 159, 91, 0, 0, 95, 0, 53, 27, 152, 21, 207, 45, 42, 64, 56, 160, 61, 11, 0, 48, 4, 27, 31, 99, 253, 185, 36, 104, 19, 181, 123, 0, 72, 23, 61, 248, 117, 47, 177, 27, 22, 0, 255, 0, 117, 0, 0, 44, 151, 78, 0, 7, 0, 77, 117, 116, 0, 79, 3, 134, 76, 19, 52, 0, 7, 171, 9, 36, 0, 255, 172, 74, 135, 21, 24, 0, 84, 94, 0, 26, 0, 0, 0, 14, 0, 0, 13, 46, 106, 32, 189, 13, 0, 73, 76, 56, 108, 0, 42, 189, 75, 0, 0, 88, 60, 12, 199, 14, 81, 0, 14, 103, 0, 237, 92, 49, 0, 0, 78, 151, 96, 42, 0, 112, 35, 0, 63, 124, 150, 92, 0, 0, 39, 5, 87, 0, 135, 21, 207, 63, 70, 74, 45, 115, 87, 18, 0, 28, 4, 26, 57, 98, 255, 199, 26, 96, 0, 0, 117, 103, 83, 18, 44, 213, 133, 95, 110, 27, 0, 0, 255, 0, 125, 0, 0, 52, 163, 35, 3, 7, 0, 29, 101, 112, 0, 74, 3, 114, 18, 21, 135, 0, 19, 160, 49, 28, 0, 255, 218, 54, 97, 34, 36, 0, 235, 97, 9, 29, 0, 0, 0, 10, 0, 0, 4, 14, 151, 2, 196, 13, 0, 0, 0, 0, 138, 58, 69, 226, 177, 0, 0, 103, 83, 0, 196, 37, 136, 29, 11, 129, 7, 237, 79, 116, 0, 0, 77, 60, 72, 9, 0, 109, 40, 0, 65, 53, 153, 101, 0, 0, 58, 42, 75, 0, 146, 21, 203, 28, 76, 81, 48, 112, 8, 11, 69, 10, 9, 73, 62, 121, 236, 221, 15, 0, 7, 0, 150, 0, 62, 0, 23, 229, 103, 7, 127, 27, 4, 0, 233, 0, 69, 18, 24, 7, 139, 54, 8, 7, 0, 75, 108, 167, 0, 71, 3, 144, 14, 12, 255, 37, 24, 158, 0, 80, 0, 255, 191, 59, 95, 12, 27, 0, 132, 48, 0, 17, 0, 0, 0, 7, 103, 20, 13, 32, 55, 20, 174, 13, 0, 4, 0, 38, 77, 0, 45, 186, 85, 0, 0, 70, 94, 0, 132, 39, 108, 17, 58, 126, 10, 237, 79, 128, 24, 0, 85, 66, 130, 44, 0, 90, 58, 0, 30, 47, 144, 31, 0, 0, 72, 48, 30, 25, 132, 21, 181, 14, 110, 52, 64, 93, 62, 11, 93, 32, 13, 42, 80, 125, 169, 202, 8, 0, 0, 0, 169, 38, 93, 71, 32, 208, 138, 12, 167, 42, 32, 26, 253, 43, 42, 0, 0, 12, 183, 96, 34, 7, 0, 38, 71, 163, 0, 79, 3, 174, 19, 18, 16, 0, 25, 154, 16, 21, 0, 255, 174, 65, 121, 31, 18, 0, 162, 16, 0, 13, 0, 55, 0, 1, 0, 0, 4, 37, 55, 20, 156, 13, 0, 0, 0, 0, 69, 64, 44, 194, 61, 0, 0, 68, 114, 62, 154, 32, 167, 10, 0, 143, 11, 237, 84, 106, 3, 0, 94, 141, 82, 10, 0, 76, 38, 0, 33, 28, 163, 182, 0, 19, 69, 6, 78, 54, 185, 27, 189, 33, 72, 50, 78, 100, 82, 11, 0, 64, 4, 42, 120, 183, 237, 230, 14, 0, 0, 40, 169, 0, 91, 0, 5, 236, 97, 30, 183, 46, 0, 3, 255, 0, 101, 0, 42, 22, 112, 55, 67, 7, 0, 78, 61, 157, 16, 79, 3, 123, 35, 4, 30, 42, 14, 162, 0, 21, 0, 255, 147, 74, 93, 12, 24, 0, 110, 47, 0, 48, 0, 0, 0, 12, 0, 49, 44, 7, 175, 15, 127, 13, 0, 85, 0, 0, 92, 70, 86, 245, 87, 0, 0, 78, 132, 7, 199, 17, 102, 5, 0, 120, 13, 237, 111, 128, 0, 0, 82, 90, 149, 8, 0, 86, 21, 0, 55, 28, 154, 110, 0, 0, 41, 0, 68, 0, 175, 21, 207, 22, 31, 90, 78, 103, 40, 11, 20, 43, 4, 88, 95, 95, 255, 255, 39, 0, 7, 0, 153, 15, 89, 2, 5, 255, 112, 14, 171, 27, 37, 13, 255, 62, 83, 48, 15, 25, 95, 57, 0, 7, 0, 103, 55, 143, 0, 58, 3, 195, 28, 20, 99, 0, 29, 218, 0, 25, 0, 242, 219, 66, 70, 12, 1, 0, 100, 42, 14, 27, 0, 0, 0, 31, 0, 46, 59, 27, 130, 23, 147, 13, 0, 0, 54, 0, 76, 0, 105, 202, 100, 0, 0, 84, 97, 0, 187, 32, 140, 14, 56, 129, 37, 185, 99, 131, 0, 15, 73, 130, 72, 7, 0, 109, 21, 0, 44, 0, 169, 70, 0, 3, 30, 58, 44, 0, 179, 26, 178, 58, 28, 35, 78, 111, 30, 11, 65, 54, 4, 32, 61, 105, 255, 238, 39, 0, 0, 0, 151, 0, 93, 0, 23, 226, 84, 20, 179, 31, 0, 40, 255, 82, 58, 19, 4, 6, 145, 74, 0, 7, 0, 91, 95, 133, 0, 79, 3, 138, 0, 12, 55, 1, 7, 228, 0, 36, 0, 255, 157, 47, 135, 12, 10, 0, 207, 16, 0, 24, 0, 8, 0, 12, 0, 32, 36, 27, 70, 2, 169, 13, 0, 8, 0, 0, 122, 0, 91, 211, 76, 0, 0, 65, 107, 13, 189, 18, 155, 18, 0, 136, 37, 237, 105, 51, 0, 0, 89, 151, 145, 16, 0, 62, 22, 0, 11, 0, 175, 124, 27, 30, 58, 0, 81, 43, 141, 29, 193, 57, 89, 52, 55, 112, 45, 11, 17, 47, 9, 10, 57, 159, 255, 203, 18, 0, 0, 0, 160, 0, 93, 0, 34, 220, 94, 51, 128, 34, 0, 15, 255, 0, 148, 0, 0, 26, 167, 65, 28, 7, 0, 64, 95, 177, 0, 79, 3, 116, 0, 21, 113, 63, 15, 187, 0, 42, 0, 255, 201, 70, 104, 12, 1, 0, 88, 45, 106, 64, 0, 0, 0, 43, 0, 63, 6, 1, 22, 2, 175, 13, 0, 0, 0, 0, 57, 0, 127, 227, 115, 0, 0, 53, 116, 17, 185, 16, 89, 10, 0, 99, 79, 237, 105, 131, 0, 0, 94, 97, 123, 45, 0, 111, 38, 0, 69, 14, 169, 55, 0, 0, 31, 32, 28, 57, 125, 21, 207, 22, 61, 74, 78, 131, 0, 16, 20, 90, 13, 58, 86, 101, 220, 255, 30, 0, 0, 175, 119, 0, 57, 0, 56, 247, 134, 9, 112, 27, 0, 0, 243, 74, 6, 14, 0, 12, 112, 57, 0, 7, 0, 59, 59, 139, 0, 71, 3, 207, 0, 4, 205, 0, 12, 195, 0, 71, 23, 253, 221, 67, 137, 12, 1, 0, 175, 16, 19, 63, 0, 0, 0, 22, 47, 20, 8, 60, 27, 2, 192, 13, 0, 175, 0, 0, 68, 52, 90, 221, 137, 0, 0, 70, 113, 50, 199, 33, 118, 9, 0, 118, 43, 237, 161, 81, 0, 0, 94, 106, 94, 13, 0, 90, 53, 0, 65, 14, 153, 111, 0, 0, 57, 22, 60, 68, 178, 21, 207, 42, 92, 73, 55, 152, 0, 17, 0, 34, 9, 34, 72, 31, 219, 248, 22, 56, 30, 0, 99, 0, 93, 0, 43, 255, 119, 62, 159, 31, 0, 0, 255, 0, 140, 0, 17, 39, 138, 41, 0, 7, 0, 37, 63, 124, 62, 79, 3, 177, 17, 39, 177, 0, 28, 220, 91, 31, 63, 250, 239, 57, 76, 12, 9, 0, 77, 89, 0, 47, 0, 0, 0, 1, 0, 48, 39, 64, 106, 23, 175, 13, 0, 0, 0, 1, 92, 13, 84, 146, 35, 0, 0, 76, 46, 0, 199, 37, 85, 10, 38, 186, 15, 237, 90, 113, 0, 0, 87, 155, 103, 69, 0, 116, 5, 0, 57, 2, 169, 106, 0, 27, 30, 28, 70, 0, 180, 21, 198, 65, 93, 107, 78, 129, 53, 11, 9, 45, 4, 43, 47, 82, 242, 255, 6, 16, 20, 233, 136, 39, 93, 0, 5, 219, 103, 20, 152, 42, 0, 0, 255, 0, 144, 8, 0, 38, 82, 73, 35, 7, 0, 14, 99, 185, 0, 48, 3, 169, 0, 7, 40, 62, 40, 198, 0, 87, 0, 255, 179, 0, 117, 12, 53, 0, 61, 57, 0, 43, 0, 0, 0, 6, 0, 11, 74, 13, 39, 79, 177, 13, 0, 0, 0, 0, 65, 0, 36, 170, 133, 0, 0, 77, 33, 0, 188, 64, 62, 0, 0, 131, 79, 237, 114, 116, 0, 0, 43, 81, 93, 70, 0, 89, 25, 1, 69, 9, 169, 34, 6, 0, 16, 42, 35, 18, 169, 21, 173, 33, 15, 42, 63, 91, 36, 17, 0, 86, 4, 43, 52, 148, 232, 255, 0, 77, 0, 0, 143, 24, 93, 0, 29, 200, 131, 16, 169, 27, 5, 72, 243, 60, 46, 38, 0, 7, 110, 101, 0, 7, 0, 48, 77, 155, 0, 79, 3, 207, 0, 8, 127, 63, 11, 173, 0, 36, 0, 247, 189, 38, 134, 12, 38, 0, 117, 58, 31, 31, 0, 68, 0, 11, 0, 45, 14, 8, 27, 8, 137, 13, 0, 5, 0, 0, 66, 0, 94, 255, 85, 0, 0, 28, 164, 65, 143, 0, 137, 11, 37, 171, 29, 237, 125, 191, 0, 0, 80, 44, 114, 50, 0, 85, 5, 0, 62, 64, 137, 44, 0, 55, 16, 49, 67, 24, 147, 31, 178, 7, 51, 58, 54, 68, 37, 35, 0, 63, 4, 90, 83, 75, 182, 255, 4, 0, 0, 0, 153, 0, 70, 99, 47, 240, 134, 32, 142, 27, 51, 0, 249, 24, 51, 24, 14, 19, 135, 17, 65, 7, 0, 51, 77, 131, 0, 79, 3, 200, 5, 8, 132, 25, 35, 158, 44, 44, 0, 255, 244, 47, 122, 12, 35, 0, 62, 78, 0, 27, 33, 26, 0, 22, 79, 47, 19, 96, 51, 8, 145, 13, 0, 70, 0, 0, 54, 0, 92, 133, 21, 0, 0, 65, 128, 0, 199, 0, 174, 0, 0, 4, 39, 225, 169, 93, 0, 0, 78, 110, 104, 4, 0, 107, 19, 0, 14, 10, 179, 37, 17, 0, 64, 11, 47, 0, 173, 97, 178, 25, 47, 67, 78, 124, 16, 11, 0, 38, 4, 89, 83, 74, 243, 237, 25, 118, 7, 0, 152, 73, 93, 28, 39, 255, 140, 46, 177, 41, 0, 0, 255, 0, 216, 0, 48, 5, 136, 71, 38, 7, 1, 51, 79, 155, 0, 79, 3, 190, 0, 13, 16, 0, 7, 146, 72, 26, 12, 200, 218, 74, 134, 12, 1, 0, 93, 22, 0, 28, 0, 53, 0, 1, 0, 52, 4, 56, 8, 2, 170, 13, 0, 0, 14, 0, 74, 75, 100, 249, 148, 0, 0, 39, 92, 57, 199, 0, 141, 0, 0, 125, 0, 214, 119, 100, 0, 0, 63, 116, 85, 36, 0, 117, 5, 0, 69, 5, 180, 120, 28, 32, 13, 0, 61, 24, 167, 29, 194, 19, 72, 93, 78, 153, 19, 42, 0, 46, 4, 49, 54, 119, 255, 255, 14, 0, 20, 41, 146, 0, 80, 0, 22, 255, 106, 67, 133, 27, 0, 0, 255, 0, 194, 55, 20, 48, 131, 21, 39, 7, 0, 35, 96, 131, 0, 73, 3, 183, 11, 15, 62, 0, 16, 163, 25, 44, 0, 205, 158, 57, 134, 12, 1, 0, 83, 77, 0, 40, 0, 0, 0, 30, 17, 31, 4, 16, 21, 15, 211, 13, 0, 22, 0, 0, 60, 20, 108, 255, 208, 0, 0, 28, 140, 0, 185, 2, 85, 14, 10, 60, 36, 237, 98, 156, 0, 0, 77, 39, 82, 56, 0, 104, 33, 0, 69, 11, 167, 77, 0, 38, 34, 0, 48, 25, 186, 23, 194, 5, 37, 49, 65, 112, 22, 16, 0, 55, 14, 91, 40, 106, 253, 255, 0, 30, 0, 0, 147, 57, 66, 0, 33, 255, 89, 10, 130, 27, 97, 0, 255, 14, 104, 55, 0, 37, 82, 52, 0, 7, 0, 87, 100, 162, 113, 71, 3, 160, 0, 28, 184, 0, 7, 212, 0, 36, 27, 255, 234, 47, 131, 12, 1, 0, 95, 73, 0, 17, 0, 2, 0, 14, 52, 0, 20, 9, 42, 15, 181, 13, 0, 40, 0, 0, 56, 0, 64, 171, 4, 0, 0, 53, 105, 22, 187, 25, 88, 13, 0, 122, 48, 237, 149, 66, 0, 0, 71, 113, 124, 10, 0, 125, 56, 0, 40, 19, 168, 119, 0, 0, 16, 0, 72, 0, 110, 45, 207, 81, 38, 62, 42, 49, 82, 11, 0, 80, 4, 69, 41, 106, 255, 236, 2, 0, 0, 0, 117, 20, 89, 13, 34, 229, 76, 17, 135, 27, 4, 34, 255, 0, 157, 15, 0, 31, 129, 55, 28, 7, 38, 59, 56, 148, 0, 79, 3, 111, 37, 11, 72, 0, 12, 199, 0, 23, 0, 255, 173, 38, 125, 12, 64, 0, 180, 16, 47, 48, 0, 0, 0, 25, 31, 0, 32, 5, 49, 38, 203, 13, 0, 0, 18, 0, 64, 23, 38, 188, 86, 0, 0, 67, 62, 0, 199, 43, 133, 27, 40, 48, 52, 237, 88, 154, 0, 0, 94, 87, 68, 8, 0, 88, 18, 0, 69, 20, 131, 45, 0, 5, 21, 0, 48, 22, 132, 52, 207, 15, 77, 44, 63, 74, 34, 21, 0, 18, 4, 88, 53, 110, 228, 255, 13, 0, 0, 0, 92, 49, 51, 8, 61, 205, 78, 10, 154, 27, 30, 0, 223, 0, 101, 52, 0, 24, 114, 29, 0, 7, 0, 106, 96, 143, 0, 32, 3, 201, 25, 4, 82, 24, 9, 239, 26, 32, 0, 255, 216, 61, 126, 12, 17, 0, 58, 33, 0, 37, 0, 0, 0, 41, 8, 18, 12, 1, 52, 2, 173, 13, 0, 0, 0, 0, 81, 1, 99, 210, 113, 0, 0, 58, 139, 0, 199, 14, 166, 14, 23, 144, 38, 237, 96, 86, 0, 0, 94, 75, 135, 10, 0, 65, 10, 0, 69, 2, 139, 113, 0, 9, 64, 0, 113, 43, 150, 67, 207, 24, 69, 53, 64, 107, 31, 24, 0, 34, 4, 53, 135, 126, 255, 252, 57, 0, 63, 103, 112, 0, 72, 0, 41, 236, 96, 11, 139, 27, 20, 23, 193, 4, 99, 68, 0, 7, 144, 31, 0, 7, 0, 51, 99, 135, 83, 54, 3, 75, 0, 13, 48, 0, 11, 204, 0, 71, 60, 255, 176, 44, 128, 12, 1, 0, 116, 90, 0, 25, 0, 0, 0, 18, 2, 34, 39, 9, 37, 2, 166, 13, 0, 0, 7, 0, 101, 0, 59, 133, 94, 0, 0, 34, 97, 0, 185, 36, 93, 8, 10, 212, 67, 237, 117, 130, 0, 0, 94, 79, 122, 4, 0, 86, 21, 0, 69, 9, 156, 87, 0, 5, 41, 0, 69, 28, 177, 56, 207, 9, 13, 28, 71, 81, 0, 11, 0, 54, 4, 41, 107, 113, 250, 226, 12, 83, 0, 108, 155, 0, 93, 0, 21, 255, 100, 16, 117, 27, 5, 48, 222, 47, 105, 0, 21, 9, 101, 81, 19, 7, 0, 25, 63, 172, 26, 79, 3, 153, 14, 7, 156, 0, 7, 174, 0, 53, 50, 255, 179, 53, 110, 12, 1, 0, 93, 35, 96, 40, 0, 15, 0, 1, 0, 41, 81, 20, 54, 2, 133, 13, 0, 50, 0, 0, 85, 35, 105, 127, 56, 0, 0, 28, 56, 0, 191, 61, 100, 27, 0, 176, 67, 237, 202, 94, 0, 0, 27, 112, 123, 13, 0, 57, 82, 0, 47, 38, 138, 61, 0, 78, 34, 0, 37, 63, 182, 95, 192, 104, 0, 74, 63, 156, 0, 31, 0, 47, 4, 19, 31, 127, 212, 255, 21, 54, 0, 42, 169, 108, 93, 0, 37, 211, 109, 10, 183, 27, 45, 21, 194, 24, 117, 56, 33, 36, 84, 17, 5, 7, 0, 53, 95, 151, 0, 69, 3, 83, 96, 11, 149, 0, 39, 204, 6, 35, 0, 213, 156, 74, 113, 12, 96, 0, 67, 53, 0, 176, 0, 35, 0, 25, 102, 79, 26, 4, 74, 2, 106, 13, 0, 63, 0, 155, 30, 75, 19, 161, 65, 0, 0, 28, 65, 71, 192, 121, 86, 38, 77, 104, 48, 203, 166, 203, 0, 0, 88, 55, 119, 21, 0, 69, 12, 20, 65, 9, 139, 62, 0, 0, 48, 54, 84, 68, 132, 21, 185, 67, 125, 71, 35, 44, 0, 11, 29, 56, 4, 59, 89, 40, 255, 255, 6, 0, 0, 0, 164, 40, 93, 0, 94, 222, 132, 22, 175, 27, 102, 0, 217, 7, 6, 32, 0, 18, 83, 49, 0, 7, 0, 62, 80, 119, 0, 58, 3, 71, 76, 33, 139, 0, 42, 215, 0, 89, 12, 255, 255, 0, 93, 12, 70, 0, 88, 107, 92, 229, 0, 0, 0, 67, 0, 9, 68, 27, 30, 17, 199, 13, 0, 0, 0, 0, 101, 0, 63, 133, 1, 0, 0, 46, 88, 0, 195, 37, 105, 0, 24, 180, 45, 237, 120, 155, 0, 0, 69, 102, 92, 4, 0, 91, 18, 0, 60, 18, 136, 56, 0, 0, 79, 0, 25, 68, 129, 21, 199, 15, 19, 122, 35, 115, 0, 25, 0, 13, 4, 15, 79, 94, 250, 255, 0, 0, 0, 0, 146, 23, 93, 0, 18, 232, 82, 23, 124, 27, 0, 0, 255, 38, 58, 0, 0, 27, 74, 80, 0, 7, 0, 30, 73, 164, 0, 79, 3, 173, 27, 27, 225, 11, 9, 208, 0, 37, 0, 255, 207, 14, 108, 12, 18, 0, 224, 42, 0, 39, 0, 6, 0, 34, 21, 46, 10, 1, 33, 23, 180, 13, 0, 58, 0, 0, 82, 0, 79, 210, 81, 0, 0, 74, 102, 35, 184, 44, 129, 5, 33, 67, 29, 237, 123, 101, 0, 0, 52, 119, 122, 10, 0, 68, 26, 9, 25, 2, 134, 77, 0, 25, 85, 0, 85, 24, 151, 21, 203, 28, 101, 90, 67, 188, 29, 11, 51, 15, 4, 41, 76, 137, 232, 255, 67, 0, 7, 99, 126, 0, 93, 0, 20, 201, 92, 12, 106, 27, 0, 0, 255, 0, 171, 0, 0, 34, 153, 76, 32, 7, 0, 17, 101, 146, 0, 26, 3, 147, 0, 5, 34, 8, 52, 205, 0, 56, 0, 255, 170, 47, 85, 12, 18, 0, 140, 71, 0, 39, 0, 48, 0, 5, 0, 30, 43, 31, 18, 29, 187, 13, 0, 0, 0, 0, 101, 0, 116, 217, 160, 0, 0, 57, 129, 0, 134, 42, 97, 7, 40, 124, 11, 237, 84, 108, 0, 0, 83, 130, 115, 26, 0, 87, 17, 0, 56, 0, 172, 45, 0, 2, 61, 2, 43, 14, 159, 21, 194, 10, 47, 75, 78, 164, 0, 11, 0, 47, 4, 40, 53, 176, 224, 255, 10, 23, 0, 0, 163, 17, 93, 0, 52, 247, 99, 9, 111, 27, 0, 0, 211, 43, 29, 78, 0, 1, 108, 78, 81, 7, 0, 77, 86, 170, 14, 48, 3, 195, 6, 15, 217, 0, 17, 201, 0, 47, 66, 233, 170, 54, 140, 12, 53, 0, 243, 35, 32, 50, 0, 34, 0, 62, 0, 0, 28, 6, 76, 11, 148, 13, 0, 0, 0, 0, 57, 0, 101, 255, 255, 0, 0, 64, 152, 0, 171, 0, 116, 12, 0, 123, 22, 219, 172, 188, 0, 0, 92, 113, 130, 36, 0, 93, 15, 0, 41, 22, 147, 18, 0, 0, 48, 19, 30, 0, 164, 33, 198, 27, 71, 42, 70, 79, 0, 51, 33, 63, 4, 60, 45, 127, 255, 207, 43, 0, 0, 0, 107, 0, 81, 0, 29, 185, 128, 12, 109, 27, 3, 0, 242, 66, 33, 86, 0, 12, 95, 59, 0, 7, 0, 65, 47, 181, 147, 53, 3, 112, 76, 15, 160, 0, 17, 186, 0, 65, 128, 240, 157, 66, 140, 12, 122, 0, 123, 66, 21, 30, 0, 36, 0, 23, 22, 0, 37, 29, 142, 64, 183, 13, 0, 49, 0, 0, 63, 0, 74, 203, 122, 0, 0, 28, 42, 0, 146, 4, 138, 5, 0, 194, 3, 203, 137, 161, 0, 0, 88, 122, 69, 36, 0, 105, 25, 0, 69, 38, 118, 149, 0, 0, 19, 0, 80, 46, 158, 21, 184, 56, 40, 59, 0, 77, 35, 21, 0, 55, 4, 88, 72, 118, 255, 253, 13, 0, 1, 0, 138, 0, 84, 0, 28, 223, 96, 12, 174, 27, 0, 0, 255, 0, 160, 14, 0, 20, 122, 82, 17, 7, 0, 79, 69, 137, 120, 59, 3, 156, 50, 14, 242, 0, 16, 239, 0, 78, 49, 255, 207, 53, 125, 12, 101, 0, 77, 54, 0, 25, 0, 0, 0, 48, 28, 47, 17, 25, 95, 47, 206, 13, 0, 72, 0, 63, 79, 0, 77, 205, 198, 0, 0, 78, 92, 0, 179, 59, 98, 41, 0, 171, 16, 198, 107, 181, 0, 0, 88, 118, 107, 22, 0, 85, 22, 0, 69, 0, 152, 126, 0, 22, 18, 0, 38, 0, 177, 21, 191, 30, 28, 97, 78, 134, 29, 24, 0, 29, 4, 87, 57, 69, 255, 255, 34, 85, 0, 0, 109, 0, 93, 0, 67, 255, 36, 2, 112, 27, 55, 16, 255, 0, 140, 0, 40, 16, 129, 26, 0, 7, 0, 69, 117, 133, 17, 79, 3, 79, 0, 21, 104, 90, 21, 255, 13, 25, 0, 255, 232, 74, 100, 12, 1, 0, 151, 63, 0, 49, 0, 0, 0, 51, 0, 20, 8, 32, 7, 2, 220, 13, 0, 0, 86, 0, 132, 75, 70, 125, 8, 0, 0, 53, 58, 0, 199, 74, 134, 22, 0, 146, 32, 237, 129, 117, 0, 0, 49, 59, 103, 7, 0, 106, 8, 0, 69, 0, 170, 13, 0, 26, 18, 31, 40, 0, 131, 78, 207, 10, 18, 43, 40, 125, 52, 11, 0, 92, 4, 59, 48, 41, 231, 243, 34, 36, 0, 0, 122, 0, 52, 12, 36, 204, 122, 9, 164, 27, 26, 2, 229, 39, 65, 94, 0, 10, 114, 43, 44, 7, 0, 72, 111, 162, 34, 69, 3, 208, 9, 22, 90, 0, 7, 200, 26, 68, 0, 227, 192, 33, 132, 33, 10, 0, 194, 65, 0, 9, 0, 8, 0, 5, 0, 0, 24, 78, 19, 7, 191, 13, 0, 118, 0, 3, 64, 0, 96, 228, 1, 0, 0, 28, 73, 0, 199, 32, 185, 1, 4, 127, 30, 237, 145, 97, 0, 0, 78, 106, 100, 6, 0, 105, 11, 0, 62, 17, 175, 6, 0, 0, 53, 62, 38, 0, 137, 34, 192, 41, 125, 79, 64, 96, 14, 28, 0, 40, 4, 93, 69, 51, 255, 255, 39, 118, 9, 89, 67, 85, 75, 13, 72, 255, 131, 45, 173, 27, 0, 0, 242, 0, 82, 44, 0, 22, 97, 61, 81, 7, 0, 44, 50, 155, 23, 71, 3, 183, 74, 12, 54, 33, 29, 239, 36, 90, 42, 253, 177, 0, 135, 46, 94, 0, 78, 44, 56, 37, 0, 34, 0, 28, 23, 68, 62, 50, 39, 17, 151, 13, 0, 0, 13, 0, 76, 38, 69, 181, 164, 0, 0, 28, 88, 0, 170, 0, 123, 2, 45, 212, 0, 237, 162, 131, 0, 0, 94, 67, 115, 10, 0, 87, 16, 0, 69, 0, 186, 65, 0, 31, 19, 0, 44, 0, 184, 21, 190, 34, 34, 86, 54, 112, 16, 11, 45, 15, 4, 64, 61, 138, 255, 255, 1, 0, 12, 0, 88, 20, 58, 0, 18, 247, 118, 2, 139, 34, 57, 30, 255, 0, 76, 108, 0, 5, 83, 53, 64, 7, 0, 74, 98, 152, 48, 79, 3, 158, 0, 4, 74, 0, 35, 156, 0, 38, 63, 248, 166, 0, 135, 12, 35, 0, 77, 37, 0, 10, 0, 0, 0, 35, 4, 29, 20, 9, 62, 10, 148, 13, 0, 0, 0, 0, 63, 0, 119, 158, 64, 0, 0, 36, 110, 0, 166, 0, 71, 29, 0, 153, 21, 237, 123, 162, 0, 0, 94, 91, 112, 13, 0, 102, 18, 0, 55, 0, 183, 141, 0, 0, 47, 2, 57, 68, 173, 21, 176, 55, 21, 34, 41, 147, 5, 11, 105, 95, 12, 76, 111, 123, 255, 255, 3, 0, 0, 73, 164, 0, 93, 19, 5, 255, 115, 57, 147, 36, 13, 37, 255, 0, 126, 74, 1, 10, 96, 68, 4, 7, 0, 55, 109, 114, 0, 79, 3, 133, 3, 13, 78, 38, 12, 217, 5, 26, 0, 255, 188, 74, 126, 12, 10, 0, 167, 37, 42, 5, 0, 0, 0, 22, 5, 0, 12, 26, 62, 5, 140, 13, 0, 0, 0, 0, 78, 55, 82, 185, 77, 0, 0, 52, 85, 0, 183, 36, 164, 33, 0, 140, 50, 237, 128, 142, 2, 0, 94, 111, 81, 18, 0, 95, 13, 0, 38, 6, 164, 146, 0, 21, 44, 0, 81, 58, 167, 21, 198, 36, 54, 91, 39, 127, 18, 11, 0, 32, 20, 47, 82, 151, 255, 255, 3, 0, 0, 6, 153, 0, 90, 5, 9, 214, 88, 66, 129, 27, 0, 0, 255, 0, 157, 15, 0, 8, 102, 40, 39, 7, 0, 64, 117, 168, 0, 72, 3, 110, 43, 8, 16, 0, 7, 194, 0, 38, 15, 255, 208, 61, 130, 12, 1, 0, 172, 36, 0, 50, 0, 0, 0, 38, 0, 0, 8, 25, 54, 2, 176, 13, 0, 154, 0, 5, 59, 0, 110, 214, 92, 0, 0, 44, 82, 0, 176, 28, 207, 10, 0, 106, 10, 237, 59, 155, 0, 0, 62, 93, 134, 19, 0, 93, 28, 0, 63, 0, 132, 6, 0, 51, 29, 43, 47, 0, 154, 21, 199, 11, 40, 50, 78, 69, 0, 11, 0, 79, 4, 61, 134, 110, 208, 255, 29, 46, 0, 146, 143, 0, 83, 11, 16, 214, 108, 2, 141, 27, 16, 0, 218, 46, 26, 33, 2, 24, 118, 48, 57, 7, 0, 122, 79, 129, 0, 71, 3, 254, 60, 17, 83, 0, 16, 161, 0, 41, 67, 250, 187, 0, 112, 12, 1, 0, 95, 36, 0, 18, 0, 59, 0, 49, 49, 19, 36, 13, 56, 36, 139, 13, 0, 0, 0, 0, 44, 0, 91, 229, 123, 0, 0, 46, 211, 7, 192, 29, 164, 3, 0, 139, 6, 237, 87, 163, 0, 0, 80, 55, 110, 7, 0, 71, 44, 0, 65, 0, 144, 6, 0, 19, 44, 9, 51, 68, 147, 21, 196, 44, 31, 79, 71, 83, 0, 11, 0, 53, 4, 22, 76, 134, 255, 255, 12, 36, 25, 0, 156, 73, 93, 0, 11, 235, 130, 10, 136, 27, 61, 25, 254, 31, 69, 72, 4, 2, 117, 48, 0, 7, 0, 120, 73, 142, 1, 79, 3, 163, 25, 15, 192, 0, 21, 184, 0, 45, 38, 255, 155, 9, 126, 12, 43, 0, 48, 32, 0, 8, 0, 0, 0, 1, 84, 48, 34, 5, 14, 20, 137, 13, 0, 0, 87, 0, 58, 3, 83, 178, 48, 0, 0, 28, 117, 0, 181, 71, 81, 0, 67, 106, 46, 237, 102, 105, 0, 0, 94, 133, 90, 29, 0, 129, 14, 0, 0, 0, 178, 21, 23, 0, 52, 13, 44, 0, 176, 21, 186, 60, 37, 97, 67, 144, 23, 29, 90, 72, 4, 12, 63, 138, 254, 250, 0, 0, 0, 0, 169, 0, 89, 0, 8, 202, 124, 39, 120, 27, 0, 0, 255, 0, 175, 15, 0, 11, 81, 47, 36, 7, 0, 49, 108, 133, 37, 76, 3, 151, 16, 10, 69, 0, 16, 106, 0, 28, 0, 255, 145, 59, 113, 12, 19, 0, 180, 69, 0, 13, 0, 32, 0, 25, 0, 0, 7, 1, 35, 4, 184, 13, 0, 55, 0, 0, 89, 0, 73, 249, 169, 0, 0, 40, 98, 0, 199, 41, 148, 0, 0, 135, 49, 237, 80, 194, 0, 0, 92, 78, 95, 41, 0, 127, 9, 14, 48, 9, 170, 25, 0, 0, 62, 39, 41, 0, 172, 42, 198, 3, 11, 45, 51, 172, 0, 42, 0, 26, 4, 91, 55, 50, 217, 255, 12, 0, 0, 0, 139, 0, 84, 0, 33, 255, 139, 6, 183, 27, 44, 0, 255, 46, 82, 22, 0, 19, 131, 35, 3, 7, 0, 46, 93, 139, 0, 75, 3, 215, 29, 11, 200, 0, 11, 192, 0, 72, 0, 255, 177, 53, 122, 12, 21, 0, 83, 53, 0, 38, 0, 0, 0, 33, 94, 71, 8, 40, 42, 19, 186, 13, 0, 14, 0, 0, 104, 0, 80, 199, 72, 0, 0, 46, 128, 21, 199, 0, 109, 6, 40, 118, 69, 237, 72, 101, 9, 0, 91, 147, 138, 4, 0, 99, 17, 0, 63, 2, 186, 44, 0, 0, 30, 63, 56, 68, 173, 21, 160, 17, 126, 78, 71, 124, 47, 11, 0, 36, 12, 33, 77, 84, 255, 246, 21, 32, 22, 0, 114, 0, 93, 41, 90, 255, 138, 18, 183, 31, 0, 0, 248, 27, 130, 61, 13, 24, 165, 53, 18, 7, 0, 97, 86, 158, 0, 65, 3, 154, 8, 22, 16, 0, 7, 97, 64, 30, 0, 215, 167, 56, 140, 12, 6, 0, 95, 30, 67, 27, 0, 0, 0, 40, 0, 0, 4, 31, 101, 23, 157, 13, 0, 0, 44, 0, 105, 59, 155, 247, 136, 0, 0, 50, 120, 0, 199, 0, 132, 0, 126, 97, 38, 237, 90, 148, 0, 6, 85, 114, 132, 4, 0, 105, 37, 0, 69, 64, 181, 64, 0, 0, 90, 15, 47, 41, 181, 38, 199, 71, 39, 88, 78, 86, 0, 11, 0, 70, 4, 59, 70, 147, 255, 151, 11, 7, 0, 0, 169, 1, 88, 0, 8, 252, 134, 41, 72, 49, 0, 32, 243, 0, 144, 0, 0, 31, 93, 102, 53, 7, 0, 26, 84, 157, 2, 64, 3, 129, 0, 33, 132, 0, 7, 174, 0, 66, 64, 204, 175, 0, 92, 12, 14, 0, 148, 16, 0, 41, 0, 19, 0, 27, 41, 0, 29, 23, 70, 5, 146, 13, 0, 69, 0, 0, 42, 0, 71, 216, 114, 0, 0, 100, 45, 6, 193, 44, 147, 35, 0, 148, 42, 237, 118, 96, 0, 0, 59, 68, 85, 4, 0, 83, 34, 0, 69, 42, 168, 81, 0, 0, 62, 0, 100, 55, 182, 48, 196, 62, 16, 84, 37, 136, 0, 11, 4, 109, 4, 26, 66, 125, 234, 244, 11, 0, 18, 0, 165, 0, 89, 0, 11, 255, 112, 35, 164, 42, 23, 19, 251, 0, 90, 0, 0, 29, 115, 94, 27, 7, 0, 53, 73, 141, 0, 79, 3, 144, 2, 24, 255, 0, 14, 194, 0, 52, 0, 250, 174, 47, 85, 12, 54, 0, 92, 40, 0, 5, 0, 0, 0, 12, 34, 71, 10, 13, 79, 2, 155, 13, 0, 0, 0, 70, 61, 53, 84, 226, 80, 0, 0, 96, 81, 0, 108, 23, 83, 22, 35, 85, 37, 237, 135, 119, 0, 28, 72, 70, 127, 33, 0, 53, 14, 0, 69, 54, 164, 90, 0, 0, 73, 2, 59, 55, 177, 21, 200, 21, 29, 47, 53, 101, 34, 11, 37, 32, 4, 40, 51, 32, 229, 206, 10, 0, 0, 0, 131, 58, 84, 83, 18, 255, 99, 46, 162, 46, 91, 0, 255, 0, 113, 23, 0, 9, 81, 79, 38, 7, 0, 43, 87, 154, 36, 79, 3, 133, 4, 10, 85, 12, 54, 184, 50, 32, 0, 255, 176, 47, 116, 12, 9, 0, 18, 55, 0, 54, 0, 6, 0, 5, 5, 71, 11, 109, 53, 21, 181, 13, 0, 91, 0, 0, 108, 55, 0, 181, 1, 0, 0, 75, 50, 0, 199, 39, 76, 13, 0, 208, 79, 237, 88, 146, 0, 0, 94, 131, 106, 4, 0, 83, 16, 0, 69, 7, 170, 37, 1, 0, 59, 44, 39, 14, 167, 21, 173, 12, 50, 55, 78, 146, 0, 11, 0, 23, 4, 82, 116, 119, 247, 249, 26, 17, 35, 39, 123, 0, 93, 0, 39, 255, 129, 2, 160, 27, 0, 0, 255, 17, 122, 0, 0, 2, 111, 29, 43, 7, 0, 84, 86, 172, 0, 79, 3, 108, 0, 13, 16, 0, 7, 187, 0, 21, 0, 255, 198, 70, 124, 12, 12, 0, 85, 45, 0, 36, 0, 72, 0, 3, 0, 19, 47, 4, 46, 26, 173, 13, 0, 0, 85, 35, 126, 9, 66, 223, 109, 0, 0, 46, 139, 0, 166, 17, 107, 3, 46, 151, 29, 237, 87, 141, 0, 0, 84, 105, 100, 18, 0, 91, 5, 0, 49, 2, 177, 6, 0, 21, 27, 82, 21, 68, 166, 21, 194, 20, 13, 45, 65, 145, 57, 16, 0, 38, 24, 77, 82, 135, 255, 201, 35, 0, 0, 156, 155, 0, 93, 0, 19, 255, 145, 35, 147, 34, 0, 0, 232, 27, 11, 61, 0, 2, 79, 60, 61, 7, 0, 54, 100, 171, 0, 59, 6, 135, 0, 7, 51, 4, 22, 255, 0, 56, 2, 255, 204, 0, 119, 12, 1, 0, 120, 107, 0, 10, 0, 8, 0, 9, 13, 60, 33, 5, 106, 11, 173, 13, 0, 0, 0, 0, 53, 35, 105, 165, 131, 0, 0, 63, 96, 0, 150, 48, 141, 23, 59, 162, 19, 237, 68, 134, 0, 0, 60, 96, 96, 26, 0, 125, 45, 0, 69, 4, 172, 11, 0, 15, 62, 35, 20, 68, 178, 24, 180, 18, 98, 79, 73, 126, 58, 17, 0, 40, 59, 81, 37, 87, 241, 242, 16, 0, 0, 0, 134, 0, 90, 35, 56, 255, 145, 35, 154, 36, 20, 0, 251, 58, 53, 0, 17, 2, 180, 99, 0, 7, 0, 39, 100, 153, 0, 67, 7, 134, 17, 21, 183, 63, 7, 243, 0, 43, 0, 255, 255, 42, 140, 12, 1, 0, 137, 60, 0, 16, 0, 0, 0, 20, 0, 8, 31, 18, 90, 4, 123, 13, 0, 79, 0, 39, 61, 0, 53, 209, 92, 0, 0, 64, 105, 50, 182, 0, 145, 42, 0, 121, 16, 237, 109, 104, 0, 0, 74, 99, 96, 10, 0, 108, 45, 0, 58, 0, 167, 13, 0, 0, 41, 2, 27, 0, 168, 25, 207, 20, 92, 127, 38, 118, 72, 17, 23, 45, 4, 86, 97, 87, 255, 255, 0, 0, 33, 21, 154, 0, 81, 0, 31, 255, 128, 28, 148, 27, 35, 0, 255, 62, 16, 60, 0, 22, 119, 39, 0, 7, 0, 71, 86, 186, 96, 69, 3, 119, 2, 33, 191, 0, 15, 255, 29, 42, 0, 255, 217, 28, 134, 12, 27, 0, 100, 65, 0, 24, 0, 0, 0, 29, 0, 79, 50, 34, 7, 15, 202, 13, 0, 0, 18, 0, 121, 13, 63, 194, 35, 0, 0, 36, 57, 0, 199, 73, 84, 0, 0, 156, 42, 237, 133, 97, 0, 0, 94, 137, 107, 53, 0, 112, 63, 0, 34, 21, 170, 30, 0, 27, 42, 0, 36, 0, 172, 21, 196, 90, 62, 47, 14, 105, 69, 28, 0, 12, 4, 35, 44, 127, 247, 240, 15, 0, 36, 22, 163, 0, 69, 0, 8, 227, 70, 61, 172, 27, 0, 0, 255, 0, 131, 0, 3, 55, 131, 38, 0, 7, 0, 80, 94, 140, 0, 79, 3, 158, 1, 13, 42, 45, 11, 159, 0, 28, 0, 255, 164, 38, 114, 12, 44, 0, 143, 75, 0, 23, 0, 0, 0, 35, 0, 67, 15, 21, 72, 2, 169, 13, 0, 16, 0, 0, 89, 0, 94, 212, 93, 0, 0, 66, 74, 0, 199, 31, 85, 0, 0, 110, 35, 237, 141, 152, 0, 0, 94, 91, 111, 4, 0, 99, 13, 0, 40, 0, 167, 36, 0, 0, 45, 32, 41, 48, 185, 39, 166, 25, 71, 2, 54, 120, 0, 21, 0, 57, 4, 109, 59, 126, 255, 226, 51, 56, 0, 92, 164, 102, 85, 29, 40, 255, 116, 24, 146, 27, 24, 0, 246, 0, 116, 54, 0, 49, 67, 36, 81, 7, 28, 134, 103, 174, 0, 79, 3, 77, 1, 16, 90, 67, 16, 227, 11, 27, 0, 255, 248, 61, 122, 12, 55, 0, 159, 51, 0, 40, 0, 7, 0, 53, 0, 0, 7, 24, 71, 2, 158, 13, 0, 0, 7, 0, 86, 8, 61, 210, 91, 0, 0, 90, 93, 0, 188, 21, 164, 16, 36, 121, 41, 237, 105, 80, 0, 22, 94, 83, 86, 48, 0, 89, 21, 0, 69, 3, 150, 82, 0, 41, 27, 0, 88, 17, 159, 64, 196, 12, 14, 65, 70, 106, 60, 11, 0, 22, 4, 51, 111, 72, 246, 237, 28, 0, 78, 0, 147, 0, 46, 45, 8, 215, 79, 29, 124, 27, 12, 40, 211, 17, 84, 65, 0, 16, 108, 42, 68, 7, 0, 92, 82, 156, 0, 64, 3, 133, 74, 25, 154, 0, 7, 162, 0, 69, 0, 255, 142, 0, 131, 12, 1, 0, 29, 31, 0, 52, 0, 0, 0, 23, 68, 37, 35, 72, 17, 93, 159, 13, 0, 93, 0, 73, 38, 0, 111, 148, 63, 0, 0, 28, 76, 0, 194, 68, 107, 15, 25, 34, 44, 237, 209, 130, 0, 0, 94, 74, 94, 4, 0, 54, 34, 0, 28, 23, 152, 55, 0, 39, 0, 0, 49, 68, 182, 61, 183, 0, 0, 60, 78, 108, 0, 41, 0, 5, 4, 86, 32, 125, 255, 255, 1, 65, 78, 0, 150, 126, 93, 46, 11, 214, 74, 2, 183, 27, 0, 0, 172, 0, 151, 69, 0, 17, 98, 17, 71, 7, 0, 47, 83, 164, 1, 0, 3, 83, 7, 15, 147, 68, 72, 242, 8, 50, 0, 221, 235, 0, 140, 12, 123, 0, 19, 19, 28, 167, 0, 35, 0, 104, 98, 85, 26, 20, 11, 26, 104, 46, 0, 22, 0, 138, 42, 75, 101, 193, 77, 0, 0, 28, 36, 8, 187, 106, 121, 14, 50, 152, 8, 237, 215, 135, 0, 2, 94, 28, 96, 22, 0, 59, 32, 0, 2, 26, 148, 35, 37, 92, 60, 0, 47, 68, 182, 21, 172, 67, 8, 65, 54, 63, 55, 11, 0, 91, 4, 46, 55, 117, 229, 255, 34, 0, 19, 0, 160, 52, 84, 0, 5, 187, 142, 81, 124, 42, 106, 64, 189, 62, 6, 35, 40, 6, 59, 50, 66, 7, 0, 14, 60, 140, 0, 79, 3, 72, 11, 52, 94, 0, 7, 220, 55, 53, 170, 222, 221, 74, 110, 12, 31, 0, 28, 47, 22, 255, 11, 94, 0, 32, 99, 71, 15, 32, 54, 15, 115, 22, 0, 15, 0, 53, 75, 0, 114, 183, 1, 0, 0, 59, 98, 81, 196, 100, 113, 49, 0, 208, 76, 168, 123, 125, 30, 0, 70, 50, 100, 69, 0, 62, 5, 0, 0, 36, 91, 45, 0, 97, 99, 0, 7, 0, 118, 52, 124, 44, 20, 39, 62, 112, 8, 25, 0, 22, 4, 19, 64, 123, 173, 197, 104, 0, 0, 5, 125, 22, 76, 3, 57, 104, 141, 9, 165, 46, 27, 26, 204, 18, 88, 38, 0, 2, 127, 36, 28, 7, 0, 14, 72, 160, 0, 65, 3, 89, 0, 34, 23, 0, 15, 188, 30, 31, 100, 221, 184, 14, 135, 12, 24, 0, 37, 57, 0, 90, 18, 29, 0, 48, 0, 71, 4, 1, 7, 5, 169, 13, 0, 26, 0, 137, 118, 0, 104, 80, 47, 0, 0, 62, 127, 0, 138, 167, 152, 4, 10, 185, 16, 237, 131, 102, 0, 0, 71, 100, 101, 84, 0, 47, 5, 34, 0, 0, 181, 11, 55, 90, 85, 20, 34, 68, 140, 21, 202, 54, 46, 84, 42, 166, 25, 11, 0, 64, 4, 56, 32, 131, 211, 255, 37, 76, 27, 0, 126, 21, 75, 0, 10, 175, 139, 10, 173, 27, 0, 19, 199, 61, 50, 115, 0, 26, 48, 53, 30, 7, 0, 14, 86, 165, 0, 23, 3, 174, 17, 7, 115, 0, 41, 203, 32, 61, 165, 209, 203, 47, 140, 12, 24, 0, 86, 117, 12, 91, 0, 99, 0, 1, 3, 66, 54, 44, 7, 14, 143, 13, 0, 0, 98, 60, 13, 0, 136, 129, 132, 0, 0, 28, 136, 0, 199, 131, 99, 5, 43, 152, 3, 237, 106, 134, 0, 0, 94, 30, 68, 111, 0, 93, 13, 0, 0, 0, 178, 25, 0, 80, 52, 0, 52, 21, 101, 21, 189, 59, 22, 50, 78, 82, 0, 11, 0, 20, 4, 41, 52, 38, 204, 254, 0, 11, 0, 0, 169, 56, 55, 0, 69, 233, 127, 2, 96, 27, 73, 90, 131, 74, 44, 111, 0, 55, 131, 71, 0, 7, 0, 51, 82, 189, 0, 58, 3, 190, 12, 11, 162, 36, 17, 220, 107, 48, 26, 182, 181, 61, 140, 12, 33, 0, 18, 32, 0, 81, 0, 34, 0, 37, 97, 95, 4, 53, 19, 2, 123, 13, 0, 20, 0, 72, 25, 0, 90, 93, 1, 0, 0, 28, 70, 0, 193, 120, 141, 36, 0, 161, 26, 220, 181, 174, 0, 0, 91, 50, 114, 31, 0, 49, 44, 0, 0, 0, 169, 34, 0, 108, 39, 0, 49, 0, 127, 34, 148, 49, 24, 82, 78, 49, 0, 11, 17, 5, 4, 80, 40, 163, 196, 228, 24, 2, 0, 0, 141, 29, 78, 0, 62, 224, 145, 12, 120, 41, 39, 65, 153, 81, 46, 49, 0, 22, 83, 82, 0, 7, 0, 36, 39, 189, 0, 47, 3, 116, 4, 15, 90, 0, 7, 255, 68, 38, 77, 217, 244, 61, 110, 12, 1, 0, 112, 93, 0, 44, 0, 34, 0, 6, 43, 28, 7, 17, 7, 2, 159, 13, 0, 0, 50, 94, 9, 0, 171, 75, 37, 0, 0, 28, 128, 24, 159, 101, 180, 0, 15, 172, 53, 220, 66, 138, 0, 0, 91, 78, 97, 4, 0, 59, 31, 0, 0, 0, 186, 70, 0, 79, 116, 0, 77, 0, 173, 21, 158, 63, 24, 62, 78, 101, 12, 11, 0, 30, 4, 37, 56, 182, 223, 255, 42, 0, 2, 0, 120, 18, 30, 0, 11, 250, 145, 53, 169, 27, 105, 3, 142, 61, 6, 56, 0, 12, 111, 64, 38, 7, 0, 46, 78, 171, 0, 36, 3, 119, 0, 18, 189, 0, 7, 225, 0, 51, 104, 205, 232, 54, 103, 12, 1, 0, 67, 76, 0, 105, 0, 0, 0, 34, 50, 52, 20, 9, 25, 2, 90, 13, 0, 27, 0, 63, 21, 20, 158, 125, 28, 0, 0, 40, 93, 12, 191, 111, 111, 0, 0, 159, 88, 237, 107, 152, 0, 1, 94, 76, 104, 53, 0, 104, 11, 0, 0, 0, 186, 91, 33, 66, 54, 0, 42, 27, 160, 21, 184, 34, 0, 101, 78, 100, 0, 11, 0, 5, 4, 70, 82, 108, 255, 255, 12, 0, 0, 0, 139, 110, 62, 0, 11, 205, 145, 2, 129, 27, 44, 49, 196, 71, 105, 130, 0, 19, 90, 56, 0, 7, 13, 14, 73, 174, 0, 58, 3, 132, 0, 4, 204, 0, 7, 244, 59, 49, 163, 177, 228, 67, 134, 12, 33, 0, 161, 36, 68, 71, 0, 0, 0, 1, 34, 0, 10, 18, 7, 2, 137, 13, 0, 39, 0, 88, 21, 0, 152, 109, 10, 0, 0, 28, 38, 0, 199, 130, 180, 0, 0, 172, 60, 237, 108, 100, 0, 0, 94, 45, 91, 34, 0, 70, 7, 0, 0, 0, 159, 96, 0, 91, 32, 0, 95, 0, 164, 21, 170, 12, 0, 47, 66, 91, 0, 25, 0, 58, 4, 13, 66, 124, 229, 255, 25, 0, 59, 142, 140, 27, 78, 0, 33, 183, 145, 31, 126, 27, 52, 70, 184, 0, 119, 68, 0, 32, 97, 66, 35, 7, 0, 29, 86, 185, 0, 62, 3, 152, 0, 4, 32, 0, 7, 176, 77, 42, 105, 194, 154, 67, 134, 12, 12, 0, 62, 53, 0, 48, 0, 35, 0, 7, 47, 17, 10, 12, 35, 2, 108, 13, 0, 14, 0, 122, 73, 0, 158, 93, 1, 0, 0, 90, 102, 0, 193, 129, 125, 38, 2, 173, 59, 237, 99, 179, 0, 0, 94, 69, 100, 61, 0, 57, 10, 35, 0, 21, 182, 6, 48, 40, 88, 0, 46, 25, 172, 27, 161, 67, 28, 77, 70, 109, 0, 21, 0, 20, 4, 52, 34, 161, 243, 255, 58, 23, 0, 0, 169, 26, 70, 0, 17, 204, 145, 40, 73, 27, 4, 72, 147, 33, 54, 60, 0, 28, 101, 88, 70, 7, 61, 36, 49, 186, 0, 79, 3, 164, 0, 4, 196, 0, 7, 255, 32, 46, 81, 211, 191, 65, 135, 12, 112, 0, 66, 33, 0, 97, 0, 104, 0, 21, 108, 91, 11, 6, 37, 2, 99, 13, 0, 0, 53, 61, 9, 0, 124, 157, 82, 15, 0, 28, 144, 41, 137, 88, 128, 0, 27, 167, 33, 220, 167, 154, 0, 0, 94, 83, 67, 61, 0, 55, 37, 0, 0, 0, 141, 29, 0, 121, 81, 7, 40, 0, 97, 21, 139, 31, 0, 0, 78, 110, 15, 11, 54, 5, 4, 36, 82, 113, 184, 255, 13, 0, 0, 30, 169, 29, 62, 25, 18, 193, 139, 2, 170, 27, 64, 62, 223, 26, 54, 51, 0, 64, 92, 22, 29, 7, 31, 78, 115, 178, 0, 79, 3, 167, 40, 4, 25, 10, 7, 255, 98, 26, 52, 195, 188, 1, 140, 12, 17, 0, 134, 22, 0, 94, 0, 69, 0, 45, 6, 62, 10, 16, 47, 10, 143, 13, 0, 15, 1, 92, 0, 32, 173, 123, 51, 0, 0, 32, 160, 36, 157, 125, 192, 18, 0, 163, 18, 237, 152, 115, 0, 24, 94, 54, 117, 9, 0, 70, 16, 23, 0, 0, 151, 76, 0, 58, 92, 0, 97, 68, 110, 21, 175, 60, 0, 97, 71, 118, 0, 11, 70, 15, 18, 33, 28, 90, 190, 255, 59, 0, 43, 0, 151, 0, 58, 0, 30, 233, 138, 46, 137, 27, 48, 39, 180, 0, 83, 65, 33, 3, 126, 68, 43, 7, 0, 42, 101, 140, 0, 51, 3, 157, 30, 4, 255, 0, 7, 255, 89, 35, 233, 189, 255, 74, 107, 12, 38, 0, 83, 83, 0, 105, 0, 48, 0, 23, 118, 57, 11, 67, 28, 11, 116, 13, 0, 0, 15, 106, 20, 37, 128, 155, 38, 0, 0, 38, 102, 0, 172, 97, 114, 4, 0, 214, 12, 237, 116, 107, 0, 0, 94, 77, 100, 36, 0, 100, 31, 0, 0, 11, 180, 47, 67, 90, 87, 0, 26, 43, 118, 21, 180, 79, 5, 71, 64, 136, 62, 11, 0, 39, 32, 42, 33, 140, 215, 255, 42, 0, 0, 92, 158, 51, 49, 4, 8, 242, 145, 61, 103, 27, 0, 4, 222, 57, 21, 52, 0, 1, 92, 101, 2, 7, 0, 14, 100, 182, 0, 48, 3, 120, 17, 4, 42, 0, 7, 238, 52, 30, 97, 199, 232, 74, 140, 12, 36, 0, 130, 29, 0, 106, 0, 89, 0, 20, 8, 44, 4, 49, 37, 2, 107, 13, 0, 92, 0, 75, 9, 31, 153, 119, 55, 0, 0, 40, 73, 0, 176, 132, 114, 0, 0, 169, 25, 237, 108, 131, 0, 8, 94, 59, 87, 82, 0, 96, 5, 0, 0, 4, 150, 57, 9, 57, 58, 0, 41, 57, 154, 21, 189, 22, 0, 54, 78, 142, 68, 11, 0, 24, 4, 50, 43, 114, 194, 255, 16, 72, 0, 0, 138, 56, 75, 42, 24, 180, 145, 63, 183, 27, 17, 69, 209, 61, 6, 45, 0, 24, 110, 68, 81, 7, 0, 17, 103, 183, 0, 79, 3, 148, 19, 13, 118, 0, 7, 216, 75, 38, 125, 186, 200, 71, 116, 12, 1, 0, 117, 58, 0, 93, 0, 73, 0, 1, 88, 62, 10, 45, 29, 23, 135, 13, 0, 29, 0, 102, 55, 0, 147, 148, 1, 0, 0, 37, 128, 37, 199, 96, 136, 11, 0, 203, 23, 237, 118, 117, 20, 0, 94, 145, 101, 45, 0, 91, 18, 0, 0, 0, 111, 6, 0, 64, 38, 60, 8, 68, 116, 21, 107, 7, 9, 52, 75, 73, 84, 11, 0, 13, 4, 39, 28, 73, 189, 255, 50, 7, 0, 0, 144, 14, 80, 85, 51, 198, 145, 2, 154, 27, 2, 72, 238, 0, 77, 38, 0, 37, 81, 24, 81, 7, 0, 52, 82, 175, 0, 58, 3, 160, 38, 19, 16, 0, 7, 205, 116, 21, 104, 192, 237, 63, 134, 12, 42, 0, 158, 42, 0, 70, 0, 91, 0, 20, 0, 0, 4, 28, 22, 24, 119, 13, 0, 0, 150, 78, 34, 0, 138, 106, 23, 0, 0, 28, 114, 0, 188, 118, 201, 0, 37, 191, 0, 237, 114, 181, 1, 0, 94, 107, 100, 61, 0, 75, 68, 0, 0, 0, 186, 6, 0, 68, 56, 21, 22, 0, 178, 21, 153, 42, 9, 65, 71, 66, 52, 11, 25, 44, 4, 78, 53, 135, 245, 207, 13, 0, 0, 107, 160, 123, 61, 29, 5, 189, 142, 10, 118, 27, 74, 20, 195, 2, 51, 39, 0, 2, 83, 65, 5, 7, 13, 21, 108, 139, 0, 51, 6, 86, 0, 4, 40, 0, 11, 203, 71, 21, 157, 198, 204, 55, 135, 12, 38, 0, 105, 85, 0, 124, 0, 80, 0, 19, 10, 0, 4, 11, 46, 5, 114, 13, 0, 0, 0, 76, 52, 0, 109, 122, 24, 0, 0, 53, 29, 0, 199, 120, 193, 6, 0, 173, 21, 237, 134, 160, 0, 0, 91, 105, 100, 33, 0, 35, 11, 0, 4, 14, 169, 6, 3, 55, 59, 0, 26, 0, 187, 21, 177, 29, 18, 67, 68, 84, 0, 11, 5, 13, 4, 21, 83, 135, 217, 197, 58, 0, 10, 0, 123, 0, 62, 0, 5, 241, 141, 34, 119, 27, 18, 17, 196, 18, 93, 53, 0, 5, 90, 38, 64, 7, 0, 30, 67, 169, 0, 73, 7, 75, 3, 10, 210, 0, 12, 244, 44, 21, 173, 191, 205, 58, 134, 12, 47, 0, 37, 83, 19, 133, 55, 80, 0, 40, 71, 95, 4, 9, 45, 5, 100, 13, 0, 19, 0, 92, 47, 23, 125, 98, 65, 0, 0, 53, 38, 0, 131, 129, 138, 18, 0, 189, 34, 237, 130, 168, 57, 0, 91, 115, 83, 26, 0, 87, 5, 0, 0, 9, 148, 6, 0, 53, 118, 19, 15, 11, 166, 21, 193, 10, 49, 47, 56, 131, 72, 11, 0, 50, 4, 49, 50, 118, 215, 255, 70, 0, 20, 76, 127, 83, 93, 42, 13, 215, 139, 35, 142, 27, 0, 0, 220, 7, 73, 29, 0, 7, 51, 22, 41, 7, 0, 21, 63, 166, 0, 79, 3, 153, 14, 12, 16, 0, 7, 215, 90, 21, 138, 195, 190, 45, 140, 12, 10, 0, 134, 39, 0, 118, 0, 84, 0, 29, 0, 3, 4, 21, 59, 2, 112, 13, 0, 40, 0, 79, 36, 75, 160, 149, 105, 0, 0, 40, 77, 50, 191, 131, 235, 13, 94, 184, 0, 237, 80, 210, 0, 0, 60, 28, 102, 35, 0, 30, 9, 0, 29, 24, 149, 35, 0, 117, 84, 0, 65, 41, 174, 24, 164, 10, 19, 88, 78, 125, 0, 11, 0, 13, 4, 44, 64, 145, 215, 255, 27, 35, 0, 7, 159, 66, 54, 29, 5, 225, 124, 82, 101, 27, 112, 21, 117, 44, 6, 94, 0, 1, 107, 67, 23, 7, 0, 36, 99, 160, 0, 79, 3, 95, 0, 37, 74, 0, 7, 220, 54, 33, 103, 187, 225, 25, 140, 12, 1, 0, 14, 34, 0, 74, 0, 61, 0, 54, 77, 58, 4, 9, 79, 2, 112, 13, 0, 37, 7, 91, 16, 0, 109, 182, 1, 0, 0, 53, 126, 34, 105, 87, 117, 15, 0, 130, 3, 237, 107, 149, 0, 66, 82, 60, 94, 80, 0, 49, 40, 3, 41, 0, 182, 19, 30, 80, 64, 3, 71, 0, 181, 25, 163, 97, 8, 120, 65, 116, 0, 11, 0, 25, 4, 18, 34, 120, 217, 255, 16, 0, 0, 0, 151, 17, 34, 27, 5, 255, 120, 22, 113, 27, 0, 0, 166, 1, 72, 43, 0, 27, 94, 70, 5, 7, 0, 14, 92, 148, 0, 79, 3, 100, 0, 28, 244, 0, 7, 212, 86, 32, 222, 214, 233, 74, 131, 12, 36, 0, 87, 35, 21, 45, 0, 48, 0, 23, 54, 81, 4, 44, 7, 11, 120, 13, 0, 0, 14, 101, 24, 0, 97, 139, 100, 0, 0, 40, 18, 27, 199, 101, 117, 0, 0, 190, 25, 220, 119, 188, 0, 0, 82, 62, 98, 19, 0, 82, 44, 0, 0, 0, 180, 10, 31, 52, 57, 73, 6, 0, 164, 21, 207, 28, 16, 74, 49, 147, 4, 11, 25, 30, 4, 103, 57, 98, 206, 255, 21, 0, 0, 0, 150, 64, 78, 19, 5, 243, 89, 28, 116, 27, 31, 17, 168, 17, 26, 112, 0, 59, 145, 64, 15, 7, 0, 24, 75, 185, 0, 65, 3, 151, 0, 4, 102, 0, 26, 248, 34, 25, 105, 196, 226, 47, 130, 12, 48, 0, 112, 19, 0, 56, 45, 70, 0, 7, 20, 20, 4, 87, 7, 14, 141, 13, 0, 0, 0, 63, 28, 0, 141, 79, 1, 0, 0, 46, 40, 0, 167, 121, 206, 4, 0, 157, 4, 192, 135, 178, 0, 0, 94, 83, 98, 44, 0, 28, 5, 34, 15, 0, 162, 45, 0, 95, 39, 0, 55, 7, 146, 21, 166, 25, 17, 73, 78, 75, 33, 11, 0, 18, 4, 65, 53, 130, 189, 255, 31, 0, 25, 0, 129, 0, 1, 44, 13, 171, 127, 17, 164, 27, 17, 54, 159, 0, 127, 66, 0, 60, 78, 61, 35, 7, 0, 16, 61, 178, 0, 43, 3, 123, 0, 16, 136, 0, 21, 236, 50, 35, 139, 227, 217, 68, 123, 12, 1, 0, 29, 52, 0, 131, 28, 61, 0, 43, 24, 95, 4, 21, 7, 2, 160, 13, 0, 108, 0, 102, 83, 0, 97, 173, 123, 0, 0, 28, 97, 0, 199, 91, 113, 11, 0, 157, 47, 220, 121, 170, 0, 0, 82, 58, 100, 40, 0, 41, 5, 25, 0, 0, 143, 6, 0, 81, 58, 26, 6, 13, 142, 21, 186, 30, 0, 82, 71, 62, 86, 11, 0, 5, 4, 25, 46, 119, 200, 255, 14, 83, 0, 100, 152, 89, 62, 62, 5, 204, 145, 66, 115, 27, 54, 39, 189, 86, 6, 27, 0, 32, 97, 71, 14, 7, 0, 18, 60, 178, 0, 79, 3, 126, 0, 10, 83, 0, 7, 215, 88, 21, 103, 208, 168, 66, 117, 12, 1, 0, 108, 47, 0, 86, 0, 43, 0, 12, 71, 91, 4, 14, 7, 2, 152, 13, 0, 0, 75, 22, 47, 0, 86, 111, 35, 0, 0, 46, 59, 2, 186, 146, 193, 0, 61, 183, 46, 209, 58, 157, 22, 0, 82, 116, 100, 28, 0, 97, 5, 0, 0, 12, 172, 6, 54, 102, 67, 74, 6, 0, 143, 21, 170, 6, 43, 85, 64, 89, 84, 11, 33, 20, 4, 33, 67, 165, 206, 255, 77, 0, 0, 0, 107, 9, 93, 38, 11, 187, 145, 43, 95, 27, 0, 56, 223, 53, 62, 20, 0, 2, 93, 67, 81, 7, 0, 16, 56, 167, 0, 44, 3, 114, 0, 20, 35, 0, 7, 189, 15, 36, 154, 185, 175, 71, 140, 12, 1, 0, 203, 91, 23, 107, 0, 58, 0, 12, 8, 0, 20, 5, 26, 5, 107, 13, 0, 34, 0, 114, 61, 0, 153, 132, 121, 0, 0, 28, 103, 0, 146, 120, 205, 0, 73, 178, 23, 237, 156, 128, 0, 0, 94, 56, 100, 49, 0, 78, 17, 0, 0, 29, 161, 17, 0, 73, 68, 0, 30, 55, 157, 21, 186, 24, 30, 83, 31, 120, 87, 11, 0, 13, 4, 80, 91, 144, 178, 255, 61, 0, 36, 0, 154, 43, 69, 0, 18, 173, 145, 87, 55, 27, 59, 46, 177, 76, 6, 104, 28, 2, 80, 29, 0, 7, 0, 21, 100, 157, 0, 20, 3, 157, 5, 16, 109, 0, 7, 198, 48, 58, 178, 213, 245, 48, 140, 12, 24, 0, 91, 127, 41, 65, 0, 40, 0, 1, 60, 2, 25, 10, 63, 18, 96, 13, 0, 0, 17, 89, 65, 0, 160, 148, 21, 0, 0, 28, 176, 0, 127, 90, 139, 22, 0, 163, 18, 237, 95, 181, 0, 76, 94, 18, 96, 71, 0, 49, 22, 0, 0, 10, 150, 34, 0, 78, 66, 0, 43, 54, 184, 21, 167, 110, 0, 64, 78, 90, 0, 11, 0, 13, 4, 93, 28, 131, 212, 255, 42, 0, 37, 0, 164, 85, 41, 0, 5, 212, 120, 17, 145, 27, 112, 54, 139, 26, 46, 100, 0, 1, 101, 42, 54, 7, 0, 60, 67, 189, 0, 56, 3, 146, 1, 4, 195, 0, 7, 254, 71, 38, 167, 227, 243, 47, 128, 12, 46, 0, 24, 107, 0, 92, 0, 31, 0, 12, 123, 38, 8, 25, 7, 2, 142, 13, 0, 13, 0, 64, 25, 0, 104, 127, 1, 0, 0, 28, 111, 0, 161, 135, 135, 0, 14, 176, 40, 237, 141, 125, 0, 0, 94, 88, 89, 4, 0, 93, 33, 0, 0, 0, 175, 172, 1, 64, 51, 0, 84, 0, 170, 49, 177, 37, 12, 65, 78, 51, 13, 11, 0, 5, 4, 26, 64, 155, 255, 255, 14, 4, 0, 0, 138, 9, 93, 6, 5, 195, 78, 2, 113, 27, 39, 39, 217, 0, 178, 47, 0, 22, 110, 57, 54, 7, 0, 14, 71, 189, 0, 58, 3, 58, 3, 4, 93, 0, 15, 194, 54, 49, 197, 224, 157, 74, 124, 12, 90, 0, 99, 19, 12, 152, 0, 0, 0, 8, 77, 63, 10, 1, 7, 2, 128, 13, 0, 0, 0, 91, 31, 0, 75, 135, 24, 0, 0, 28, 19, 0, 199, 129, 94, 4, 47, 173, 74, 237, 136, 117, 34, 42, 94, 108, 112, 80, 0, 69, 21, 0, 0, 34, 142, 9, 0, 102, 40, 0, 8, 0, 115, 58, 159, 12, 12, 83, 78, 126, 46, 11, 16, 5, 4, 59, 85, 133, 175, 205, 29, 0, 0, 0, 128, 26, 39, 45, 16, 179, 135, 10, 155, 27, 0, 47, 241, 71, 21, 39, 0, 54, 78, 31, 62, 7, 0, 37, 97, 185, 0, 54, 3, 129, 0, 25, 35, 0, 7, 185, 25, 46, 112, 196, 202, 0, 137, 12, 1, 0, 160, 42, 0, 111, 0, 97, 0, 1, 2, 48, 33, 19, 22, 2, 111, 13, 0, 40, 0, 92, 0, 0, 150, 18, 35, 36, 0, 34, 96, 0, 199, 195, 220, 0, 56, 191, 2, 237, 206, 117, 5, 60, 94, 54, 91, 22, 0, 66, 29, 65, 0, 22, 165, 10, 56, 93, 32, 29, 15, 68, 121, 21, 182, 72, 0, 115, 78, 121, 0, 11, 34, 5, 4, 82, 28, 141, 182, 236, 20, 0, 0, 0, 155, 101, 85, 0, 11, 184, 128, 2, 163, 27, 0, 46, 202, 86, 54, 75, 0, 2, 82, 36, 57, 7, 0, 39, 88, 186, 0, 0, 3, 72, 40, 8, 63, 0, 15, 200, 83, 26, 69, 160, 217, 50, 140, 12, 131, 0, 121, 16, 41, 199, 0, 93, 0, 1, 25, 85, 15, 1, 11, 2, 85, 90, 0, 86, 8, 156, 0, 0, 117, 56, 28, 0, 0, 37, 27, 73, 126, 219, 171, 4, 13, 160, 3, 186};\n"
     ]
    }
   ],
   "source": [
    "#print of the output of layer err_lay\n",
    "err_lay = 8\n",
    "i = err_lay - 1\n",
    "y=out_list[i]\n",
    "index_arr = 0\n",
    "print('EXPECTED tensor_out layer{} RESULTS AFTER QUANTIZATION: \\n' .format(i+1))\n",
    "str_expected_out = 'static const GOLDEN_MODEL' + str(i+1) + '[] = {'\n",
    "for k in range(y.size(3)):\n",
    "    for j in range(y.size(2)):\n",
    "        for p in range(y.size(1)):\n",
    "            if (param_list[i]['act_o_bits'] == 8):\n",
    "                str_expected_out += str(int(y[0][p][k][j].item())) + ', '\n",
    "            index_arr += 1\n",
    "str_expected_out += '\\\\\\n'\n",
    "str_expected_out = str_expected_out[:-4] + '};'\n",
    "\n",
    "print(str_expected_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209.0 104 11 22\n",
      "torch.Size([1, 128, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "cont = 0\n",
    "for k in range(y.size(3)):\n",
    "    for j in range(y.size(2)):\n",
    "        for p in range(y.size(1)):\n",
    "            if cont == 42344:\n",
    "                print(y[0][p][k][j].item(), p, k, j)\n",
    "            cont += 1\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 3, 3])\n",
      "tensor([[[-224., -257., -106.],\n",
      "         [-275., -337., -145.],\n",
      "         [-122., -155.,  -82.]]], device='cuda:0')\n",
      "5\n",
      "0.5859534740447998\n"
     ]
    }
   ],
   "source": [
    "print(m2[2*(8-1)].weight.size())\n",
    "print(m2[2*(8-1)].weight.data[104] )\n",
    "print(int(-math.log2(m2[2*8-1].N_ZERO[104])))\n",
    "print((m2[2*8-1].M_ZERO[104].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([8301])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4864.])\n",
      "tensor([152.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([8301])\n",
    "a = a.mul(m2[2*8-1].M_ZERO[104])\n",
    "print(a)\n",
    "a = a.mul(m2[2*8-1].N_ZERO[104])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#define LAYER1() arm_convolve_HWC_u8_u4_u8_PACT_CH_icn(tensorIn, CONV1_IM_DIM, CONV1_IM_CH, conv1_wt, CONV1_OUT_CH, CONV1_KER_DIM, CONV1_L_PADDING, CONV1_R_PADDING, CONV1_T_PADDING, CONV1_B_PADDING, CONV1_STRIDE, conv1_bias, tensorOut, CONV1_OUT_DIM, CONV1_IN_Z, CONV1_W_Z, CONV1_OUT_Z, CONV1_M_ZERO, CONV1_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER2() arm_depthwise_separable_conv_HWC_u4_u4_u8_PACT_CH_icn(tensorIn, CONV2_IM_DIM, CONV2_IM_CH, conv2_wt, CONV2_OUT_CH, CONV2_KER_DIM, CONV2_L_PADDING, CONV2_R_PADDING, CONV2_T_PADDING, CONV2_B_PADDING, CONV2_STRIDE, conv2_bias, tensorOut, CONV2_OUT_DIM, CONV2_IN_Z, CONV2_W_Z, CONV2_OUT_Z, CONV2_M_ZERO, CONV2_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER3() arm_convolve_HWC_u4_u2_u8_PACT_CH_icn(tensorIn, CONV3_IM_DIM, CONV3_IM_CH, conv3_wt, CONV3_OUT_CH, CONV3_KER_DIM, CONV3_L_PADDING, CONV3_R_PADDING, CONV3_T_PADDING, CONV3_B_PADDING, CONV3_STRIDE, conv3_bias, tensorOut, CONV3_OUT_DIM, CONV3_IN_Z, CONV3_W_Z, CONV3_OUT_Z, CONV3_M_ZERO, CONV3_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER4() arm_depthwise_separable_conv_HWC_u2_u8_u8_PACT_CH_icn(tensorIn, CONV4_IM_DIM, CONV4_IM_CH, conv4_wt, CONV4_OUT_CH, CONV4_KER_DIM, CONV4_L_PADDING, CONV4_R_PADDING, CONV4_T_PADDING, CONV4_B_PADDING, CONV4_STRIDE, conv4_bias, tensorOut, CONV4_OUT_DIM, CONV4_IN_Z, CONV4_W_Z, CONV4_OUT_Z, CONV4_M_ZERO, CONV4_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER5() arm_convolve_HWC_u8_u4_u8_PACT_CH_icn(tensorIn, CONV5_IM_DIM, CONV5_IM_CH, conv5_wt, CONV5_OUT_CH, CONV5_KER_DIM, CONV5_L_PADDING, CONV5_R_PADDING, CONV5_T_PADDING, CONV5_B_PADDING, CONV5_STRIDE, conv5_bias, tensorOut, CONV5_OUT_DIM, CONV5_IN_Z, CONV5_W_Z, CONV5_OUT_Z, CONV5_M_ZERO, CONV5_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER6() arm_depthwise_separable_conv_HWC_u4_u4_u8_PACT_CH_icn(tensorIn, CONV6_IM_DIM, CONV6_IM_CH, conv6_wt, CONV6_OUT_CH, CONV6_KER_DIM, CONV6_L_PADDING, CONV6_R_PADDING, CONV6_T_PADDING, CONV6_B_PADDING, CONV6_STRIDE, conv6_bias, tensorOut, CONV6_OUT_DIM, CONV6_IN_Z, CONV6_W_Z, CONV6_OUT_Z, CONV6_M_ZERO, CONV6_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER7() arm_convolve_HWC_u4_u4_u8_PACT_CH_icn(tensorIn, CONV7_IM_DIM, CONV7_IM_CH, conv7_wt, CONV7_OUT_CH, CONV7_KER_DIM, CONV7_L_PADDING, CONV7_R_PADDING, CONV7_T_PADDING, CONV7_B_PADDING, CONV7_STRIDE, conv7_bias, tensorOut, CONV7_OUT_DIM, CONV7_IN_Z, CONV7_W_Z, CONV7_OUT_Z, CONV7_M_ZERO, CONV7_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER8() arm_depthwise_separable_conv_HWC_u4_u8_u8_PACT_CH_icn(tensorIn, CONV8_IM_DIM, CONV8_IM_CH, conv8_wt, CONV8_OUT_CH, CONV8_KER_DIM, CONV8_L_PADDING, CONV8_R_PADDING, CONV8_T_PADDING, CONV8_B_PADDING, CONV8_STRIDE, conv8_bias, tensorOut, CONV8_OUT_DIM, CONV8_IN_Z, CONV8_W_Z, CONV8_OUT_Z, CONV8_M_ZERO, CONV8_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER9() arm_convolve_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV9_IM_DIM, CONV9_IM_CH, conv9_wt, CONV9_OUT_CH, CONV9_KER_DIM, CONV9_L_PADDING, CONV9_R_PADDING, CONV9_T_PADDING, CONV9_B_PADDING, CONV9_STRIDE, conv9_bias, tensorOut, CONV9_OUT_DIM, CONV9_IN_Z, CONV9_W_Z, CONV9_OUT_Z, CONV9_M_ZERO, CONV9_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER10() arm_depthwise_separable_conv_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV10_IM_DIM, CONV10_IM_CH, conv10_wt, CONV10_OUT_CH, CONV10_KER_DIM, CONV10_L_PADDING, CONV10_R_PADDING, CONV10_T_PADDING, CONV10_B_PADDING, CONV10_STRIDE, conv10_bias, tensorOut, CONV10_OUT_DIM, CONV10_IN_Z, CONV10_W_Z, CONV10_OUT_Z, CONV10_M_ZERO, CONV10_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER11() arm_convolve_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV11_IM_DIM, CONV11_IM_CH, conv11_wt, CONV11_OUT_CH, CONV11_KER_DIM, CONV11_L_PADDING, CONV11_R_PADDING, CONV11_T_PADDING, CONV11_B_PADDING, CONV11_STRIDE, conv11_bias, tensorOut, CONV11_OUT_DIM, CONV11_IN_Z, CONV11_W_Z, CONV11_OUT_Z, CONV11_M_ZERO, CONV11_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER12() arm_depthwise_separable_conv_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV12_IM_DIM, CONV12_IM_CH, conv12_wt, CONV12_OUT_CH, CONV12_KER_DIM, CONV12_L_PADDING, CONV12_R_PADDING, CONV12_T_PADDING, CONV12_B_PADDING, CONV12_STRIDE, conv12_bias, tensorOut, CONV12_OUT_DIM, CONV12_IN_Z, CONV12_W_Z, CONV12_OUT_Z, CONV12_M_ZERO, CONV12_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER13() arm_convolve_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV13_IM_DIM, CONV13_IM_CH, conv13_wt, CONV13_OUT_CH, CONV13_KER_DIM, CONV13_L_PADDING, CONV13_R_PADDING, CONV13_T_PADDING, CONV13_B_PADDING, CONV13_STRIDE, conv13_bias, tensorOut, CONV13_OUT_DIM, CONV13_IN_Z, CONV13_W_Z, CONV13_OUT_Z, CONV13_M_ZERO, CONV13_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER14() arm_depthwise_separable_conv_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV14_IM_DIM, CONV14_IM_CH, conv14_wt, CONV14_OUT_CH, CONV14_KER_DIM, CONV14_L_PADDING, CONV14_R_PADDING, CONV14_T_PADDING, CONV14_B_PADDING, CONV14_STRIDE, conv14_bias, tensorOut, CONV14_OUT_DIM, CONV14_IN_Z, CONV14_W_Z, CONV14_OUT_Z, CONV14_M_ZERO, CONV14_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER15() arm_convolve_HWC_u8_u8_u4_PACT_CH_icn(tensorIn, CONV15_IM_DIM, CONV15_IM_CH, conv15_wt, CONV15_OUT_CH, CONV15_KER_DIM, CONV15_L_PADDING, CONV15_R_PADDING, CONV15_T_PADDING, CONV15_B_PADDING, CONV15_STRIDE, conv15_bias, tensorOut, CONV15_OUT_DIM, CONV15_IN_Z, CONV15_W_Z, CONV15_OUT_Z, CONV15_M_ZERO, CONV15_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER16() arm_depthwise_separable_conv_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV16_IM_DIM, CONV16_IM_CH, conv16_wt, CONV16_OUT_CH, CONV16_KER_DIM, CONV16_L_PADDING, CONV16_R_PADDING, CONV16_T_PADDING, CONV16_B_PADDING, CONV16_STRIDE, conv16_bias, tensorOut, CONV16_OUT_DIM, CONV16_IN_Z, CONV16_W_Z, CONV16_OUT_Z, CONV16_M_ZERO, CONV16_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER17() arm_convolve_HWC_u8_u8_u4_PACT_CH_icn(tensorIn, CONV17_IM_DIM, CONV17_IM_CH, conv17_wt, CONV17_OUT_CH, CONV17_KER_DIM, CONV17_L_PADDING, CONV17_R_PADDING, CONV17_T_PADDING, CONV17_B_PADDING, CONV17_STRIDE, conv17_bias, tensorOut, CONV17_OUT_DIM, CONV17_IN_Z, CONV17_W_Z, CONV17_OUT_Z, CONV17_M_ZERO, CONV17_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER18() arm_depthwise_separable_conv_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV18_IM_DIM, CONV18_IM_CH, conv18_wt, CONV18_OUT_CH, CONV18_KER_DIM, CONV18_L_PADDING, CONV18_R_PADDING, CONV18_T_PADDING, CONV18_B_PADDING, CONV18_STRIDE, conv18_bias, tensorOut, CONV18_OUT_DIM, CONV18_IN_Z, CONV18_W_Z, CONV18_OUT_Z, CONV18_M_ZERO, CONV18_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER19() arm_convolve_HWC_u8_u8_u4_PACT_CH_icn(tensorIn, CONV19_IM_DIM, CONV19_IM_CH, conv19_wt, CONV19_OUT_CH, CONV19_KER_DIM, CONV19_L_PADDING, CONV19_R_PADDING, CONV19_T_PADDING, CONV19_B_PADDING, CONV19_STRIDE, conv19_bias, tensorOut, CONV19_OUT_DIM, CONV19_IN_Z, CONV19_W_Z, CONV19_OUT_Z, CONV19_M_ZERO, CONV19_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER20() arm_depthwise_separable_conv_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV20_IM_DIM, CONV20_IM_CH, conv20_wt, CONV20_OUT_CH, CONV20_KER_DIM, CONV20_L_PADDING, CONV20_R_PADDING, CONV20_T_PADDING, CONV20_B_PADDING, CONV20_STRIDE, conv20_bias, tensorOut, CONV20_OUT_DIM, CONV20_IN_Z, CONV20_W_Z, CONV20_OUT_Z, CONV20_M_ZERO, CONV20_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER21() arm_convolve_HWC_u8_u8_u4_PACT_CH_icn(tensorIn, CONV21_IM_DIM, CONV21_IM_CH, conv21_wt, CONV21_OUT_CH, CONV21_KER_DIM, CONV21_L_PADDING, CONV21_R_PADDING, CONV21_T_PADDING, CONV21_B_PADDING, CONV21_STRIDE, conv21_bias, tensorOut, CONV21_OUT_DIM, CONV21_IN_Z, CONV21_W_Z, CONV21_OUT_Z, CONV21_M_ZERO, CONV21_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER22() arm_depthwise_separable_conv_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV22_IM_DIM, CONV22_IM_CH, conv22_wt, CONV22_OUT_CH, CONV22_KER_DIM, CONV22_L_PADDING, CONV22_R_PADDING, CONV22_T_PADDING, CONV22_B_PADDING, CONV22_STRIDE, conv22_bias, tensorOut, CONV22_OUT_DIM, CONV22_IN_Z, CONV22_W_Z, CONV22_OUT_Z, CONV22_M_ZERO, CONV22_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER23() arm_convolve_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV23_IM_DIM, CONV23_IM_CH, conv23_wt, CONV23_OUT_CH, CONV23_KER_DIM, CONV23_L_PADDING, CONV23_R_PADDING, CONV23_T_PADDING, CONV23_B_PADDING, CONV23_STRIDE, conv23_bias, tensorOut, CONV23_OUT_DIM, CONV23_IN_Z, CONV23_W_Z, CONV23_OUT_Z, CONV23_M_ZERO, CONV23_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER24() arm_depthwise_separable_conv_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV24_IM_DIM, CONV24_IM_CH, conv24_wt, CONV24_OUT_CH, CONV24_KER_DIM, CONV24_L_PADDING, CONV24_R_PADDING, CONV24_T_PADDING, CONV24_B_PADDING, CONV24_STRIDE, conv24_bias, tensorOut, CONV24_OUT_DIM, CONV24_IN_Z, CONV24_W_Z, CONV24_OUT_Z, CONV24_M_ZERO, CONV24_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER25() arm_convolve_HWC_u8_u8_u4_PACT_CH_icn(tensorIn, CONV25_IM_DIM, CONV25_IM_CH, conv25_wt, CONV25_OUT_CH, CONV25_KER_DIM, CONV25_L_PADDING, CONV25_R_PADDING, CONV25_T_PADDING, CONV25_B_PADDING, CONV25_STRIDE, conv25_bias, tensorOut, CONV25_OUT_DIM, CONV25_IN_Z, CONV25_W_Z, CONV25_OUT_Z, CONV25_M_ZERO, CONV25_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER26() arm_depthwise_separable_conv_HWC_u8_u8_u8_PACT_CH_icn(tensorIn, CONV26_IM_DIM, CONV26_IM_CH, conv26_wt, CONV26_OUT_CH, CONV26_KER_DIM, CONV26_L_PADDING, CONV26_R_PADDING, CONV26_T_PADDING, CONV26_B_PADDING, CONV26_STRIDE, conv26_bias, tensorOut, CONV26_OUT_DIM, CONV26_IN_Z, CONV26_W_Z, CONV26_OUT_Z, CONV26_M_ZERO, CONV26_N_ZERO, l1_tensor_scratch, NULL);\n",
      "#define LAYER27() arm_convolve_HWC_u8_u8_u2_PACT_CH_icn(tensorIn, CONV27_IM_DIM, CONV27_IM_CH, conv27_wt, CONV27_OUT_CH, CONV27_KER_DIM, CONV27_L_PADDING, CONV27_R_PADDING, CONV27_T_PADDING, CONV27_B_PADDING, CONV27_STRIDE, conv27_bias, tensorOut, CONV27_OUT_DIM, CONV27_IN_Z, CONV27_W_Z, CONV27_OUT_Z, CONV27_M_ZERO, CONV27_N_ZERO, l1_tensor_scratch, NULL);\n"
     ]
    }
   ],
   "source": [
    "#code generator\n",
    "inp = 8\n",
    "out = 8\n",
    "wt = 8\n",
    "index_layer = 1\n",
    "for item in param_list[:nlayer]:\n",
    "    out = item['act_o_bits']\n",
    "    wt = item['w_bits']\n",
    "    print(\"#define LAYER{}()\" .format(index_layer), end = ' ')\n",
    "    if (index_layer % 2):\n",
    "        if item['quant_type'] == 'PerLayerAsymPACT':\n",
    "            if item['fold_type'] == 'ICN':\n",
    "                print('arm_convolve_HWC_u{0}_u{1}_u{2}_icn(tensorIn, CONV{3}_IM_DIM, CONV{3}_IM_CH, conv{3}_wt, CONV{3}_OUT_CH, CONV{3}_KER_DIM, CONV{3}_L_PADDING, CONV{3}_R_PADDING, CONV{3}_T_PADDING, CONV{3}_B_PADDING, CONV{3}_STRIDE, conv{3}_bias, tensorOut, CONV{3}_OUT_DIM, CONV{3}_IN_Z, CONV{3}_W_Z, CONV{3}_OUT_Z, CONV{3}_M_ZERO, CONV{3}_N_ZERO, l1_tensor_scratch, NULL);' .format(inp,out,wt,index_layer))\n",
    "            elif item['fold_type'] == 'folding_weights':\n",
    "                print('arm_convolve_HWC_u{0}_u{1}_u{2}(tensorIn, CONV{3}_IM_DIM, CONV{3}_IM_CH, conv{3}_wt, CONV{3}_OUT_CH, CONV{3}_KER_DIM, CONV{3}_L_PADDING, CONV{3}_R_PADDING, CONV{3}_T_PADDING, CONV{3}_B_PADDING, CONV{3}_STRIDE, conv{3}_bias, tensorOut, CONV{3}_OUT_DIM, CONV{3}_IN_Z, CONV{3}_W_Z, CONV{3}_OUT_Z, CONV{3}_M_ZERO, CONV{3}_N_ZERO, l1_tensor_scratch, NULL);' .format(inp,out,wt,index_layer))\n",
    "            elif item['fold_type'] == 'thr':\n",
    "                print('arm_convolve_HWC_u{0}_u{1}_u{2}_thr(tensorIn, CONV{3}_IM_DIM, CONV{3}_IM_CH, conv{3}_wt, CONV{3}_OUT_CH, CONV{3}_KER_DIM, CONV{3}_L_PADDING, CONV{3}_R_PADDING, CONV{3}_T_PADDING, CONV{3}_B_PADDING, CONV{3}_STRIDE, conv{3}_bias, tensorOut, CONV{3}_OUT_DIM, CONV{3}_IN_Z, CONV{3}_W_Z, CONV{3}_OUT_Z, CONV{3}_M_ZERO, CONV{3}_N_ZERO, l1_tensor_scratch, NULL);' .format(inp,out,wt,index_layer))\n",
    "        elif item['quant_type'] == 'PerChannelsAsymMinMax':\n",
    "            if item['fold_type'] == 'ICN':\n",
    "                print('arm_convolve_HWC_u{0}_u{1}_u{2}_PACT_CH_icn(tensorIn, CONV{3}_IM_DIM, CONV{3}_IM_CH, conv{3}_wt, CONV{3}_OUT_CH, CONV{3}_KER_DIM, CONV{3}_L_PADDING, CONV{3}_R_PADDING, CONV{3}_T_PADDING, CONV{3}_B_PADDING, CONV{3}_STRIDE, conv{3}_bias, tensorOut, CONV{3}_OUT_DIM, CONV{3}_IN_Z, CONV{3}_W_Z, CONV{3}_OUT_Z, CONV{3}_M_ZERO, CONV{3}_N_ZERO, l1_tensor_scratch, NULL);' .format(inp,out,wt,index_layer))\n",
    "            elif item['fold_type'] == 'thr':\n",
    "                print('arm_convolve_HWC_u{0}_u{1}_u{2}_PACT_CH_thr(tensorIn, CONV{3}_IM_DIM, CONV{3}_IM_CH, conv{3}_wt, CONV{3}_OUT_CH, CONV{3}_KER_DIM, CONV{3}_L_PADDING, CONV{3}_R_PADDING, CONV{3}_T_PADDING, CONV{3}_B_PADDING, CONV{3}_STRIDE, conv{3}_bias, tensorOut, CONV{3}_OUT_DIM, CONV{3}_IN_Z, CONV{3}_W_Z, CONV{3}_OUT_Z, CONV{3}_M_ZERO, CONV{3}_N_ZERO, l1_tensor_scratch, NULL);' .format(inp,out,wt,index_layer))\n",
    "    else:\n",
    "        if item['quant_type'] == 'PerLayerAsymPACT':\n",
    "            if item['fold_type'] == 'ICN':\n",
    "                print('arm_depthwise_separable_conv_HWC_u{0}_u{1}_u{2}_icn(tensorIn, CONV{3}_IM_DIM, CONV{3}_IM_CH, conv{3}_wt, CONV{3}_OUT_CH, CONV{3}_KER_DIM, CONV{3}_L_PADDING, CONV{3}_R_PADDING, CONV{3}_T_PADDING, CONV{3}_B_PADDING, CONV{3}_STRIDE, conv{3}_bias, tensorOut, CONV{3}_OUT_DIM, CONV{3}_IN_Z, CONV{3}_W_Z, CONV{3}_OUT_Z, CONV{3}_M_ZERO, CONV{3}_N_ZERO, l1_tensor_scratch, NULL);' .format(inp,out,wt,index_layer))\n",
    "            elif item['fold_type'] == 'folding_weights':\n",
    "                print('arm_depthwise_separable_conv_HWC_u{0}_u{1}_u{2}(tensorIn, CONV{3}_IM_DIM, CONV{3}_IM_CH, conv{3}_wt, CONV{3}_OUT_CH, CONV{3}_KER_DIM, CONV{3}_L_PADDING, CONV{3}_R_PADDING, CONV{3}_T_PADDING, CONV{3}_B_PADDING, CONV{3}_STRIDE, conv{3}_bias, tensorOut, CONV{3}_OUT_DIM, CONV{3}_IN_Z, CONV{3}_W_Z, CONV{3}_OUT_Z, CONV{3}_M_ZERO, CONV{3}_N_ZERO, l1_tensor_scratch, NULL);' .format(inp,out,wt,index_layer))\n",
    "            elif item['fold_type'] == 'thr':\n",
    "                print('arm_depthwise_separable_conv_HWC_u{0}_u{1}_u{2}_thr(tensorIn, CONV{3}_IM_DIM, CONV{3}_IM_CH, conv{3}_wt, CONV{3}_OUT_CH, CONV{3}_KER_DIM, CONV{3}_L_PADDING, CONV{3}_R_PADDING, CONV{3}_T_PADDING, CONV{3}_B_PADDING, CONV{3}_STRIDE, conv{3}_bias, tensorOut, CONV{3}_OUT_DIM, CONV{3}_IN_Z, CONV{3}_W_Z, CONV{3}_OUT_Z, CONV{3}_M_ZERO, CONV{3}_N_ZERO, l1_tensor_scratch, NULL);' .format(inp,out,wt,index_layer))\n",
    "        elif item['quant_type'] == 'PerChannelsAsymMinMax':\n",
    "            if item['fold_type'] == 'ICN':\n",
    "                print('arm_depthwise_separable_conv_HWC_u{0}_u{1}_u{2}_PACT_CH_icn(tensorIn, CONV{3}_IM_DIM, CONV{3}_IM_CH, conv{3}_wt, CONV{3}_OUT_CH, CONV{3}_KER_DIM, CONV{3}_L_PADDING, CONV{3}_R_PADDING, CONV{3}_T_PADDING, CONV{3}_B_PADDING, CONV{3}_STRIDE, conv{3}_bias, tensorOut, CONV{3}_OUT_DIM, CONV{3}_IN_Z, CONV{3}_W_Z, CONV{3}_OUT_Z, CONV{3}_M_ZERO, CONV{3}_N_ZERO, l1_tensor_scratch, NULL);' .format(inp,out,wt,index_layer))\n",
    "            elif item['fold_type'] == 'thr':\n",
    "                print('arm_depthwise_separable_conv_HWC_u{0}_u{1}_u{2}_PACT_CH_thr(tensorIn, CONV{3}_IM_DIM, CONV{3}_IM_CH, conv{3}_wt, CONV{3}_OUT_CH, CONV{3}_KER_DIM, CONV{3}_L_PADDING, CONV{3}_R_PADDING, CONV{3}_T_PADDING, CONV{3}_B_PADDING, CONV{3}_STRIDE, conv{3}_bias, tensorOut, CONV{3}_OUT_DIM, CONV{3}_IN_Z, CONV{3}_W_Z, CONV{3}_OUT_Z, CONV{3}_M_ZERO, CONV{3}_N_ZERO, l1_tensor_scratch, NULL);' .format(inp,out,wt,index_layer))\n",
    "    \n",
    "    inp = out\n",
    "    index_layer += 1\n",
    "    #print('\\n', end = '')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#define LAYER1_WT_SHIFT (0)\n",
      "#define LAYER2_WT_SHIFT (0)\n",
      "#define LAYER3_WT_SHIFT (0)\n",
      "#define LAYER4_WT_SHIFT (0)\n",
      "#define LAYER5_WT_SHIFT (0)\n",
      "#define LAYER6_WT_SHIFT (0)\n",
      "#define LAYER7_WT_SHIFT (0)\n",
      "#define LAYER8_WT_SHIFT (0)\n",
      "#define LAYER9_WT_SHIFT (0)\n",
      "#define LAYER10_WT_SHIFT (0)\n",
      "#define LAYER11_WT_SHIFT (0)\n",
      "#define LAYER12_WT_SHIFT (0)\n",
      "#define LAYER13_WT_SHIFT (0)\n",
      "#define LAYER14_WT_SHIFT (0)\n",
      "#define LAYER15_WT_SHIFT (1)\n",
      "#define LAYER16_WT_SHIFT (0)\n",
      "#define LAYER17_WT_SHIFT (1)\n",
      "#define LAYER18_WT_SHIFT (0)\n",
      "#define LAYER19_WT_SHIFT (1)\n",
      "#define LAYER20_WT_SHIFT (0)\n",
      "#define LAYER21_WT_SHIFT (1)\n",
      "#define LAYER22_WT_SHIFT (0)\n",
      "#define LAYER23_WT_SHIFT (0)\n",
      "#define LAYER24_WT_SHIFT (0)\n",
      "#define LAYER25_WT_SHIFT (1)\n",
      "#define LAYER26_WT_SHIFT (0)\n",
      "#define LAYER27_WT_SHIFT (2)\n"
     ]
    }
   ],
   "source": [
    " #LAYER_OUT_SHIFT generator\n",
    "index_layer = 1\n",
    "for item in param_list[:nlayer]:\n",
    "    wt = item['w_bits']\n",
    "    print('#define LAYER{}_WT_SHIFT ({})' .format(index_layer, int(math.log(8/wt,2))))\n",
    "    index_layer += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#define LAYER1_OUT_SHIFT (1)\n",
      "#define LAYER2_OUT_SHIFT (1)\n",
      "#define LAYER3_OUT_SHIFT (2)\n",
      "#define LAYER4_OUT_SHIFT (0)\n",
      "#define LAYER5_OUT_SHIFT (1)\n",
      "#define LAYER6_OUT_SHIFT (1)\n",
      "#define LAYER7_OUT_SHIFT (1)\n",
      "#define LAYER8_OUT_SHIFT (0)\n",
      "#define LAYER9_OUT_SHIFT (0)\n",
      "#define LAYER10_OUT_SHIFT (0)\n",
      "#define LAYER11_OUT_SHIFT (0)\n",
      "#define LAYER12_OUT_SHIFT (0)\n",
      "#define LAYER13_OUT_SHIFT (0)\n",
      "#define LAYER14_OUT_SHIFT (0)\n",
      "#define LAYER15_OUT_SHIFT (0)\n",
      "#define LAYER16_OUT_SHIFT (0)\n",
      "#define LAYER17_OUT_SHIFT (0)\n",
      "#define LAYER18_OUT_SHIFT (0)\n",
      "#define LAYER19_OUT_SHIFT (0)\n",
      "#define LAYER20_OUT_SHIFT (0)\n",
      "#define LAYER21_OUT_SHIFT (0)\n",
      "#define LAYER22_OUT_SHIFT (0)\n",
      "#define LAYER23_OUT_SHIFT (0)\n",
      "#define LAYER24_OUT_SHIFT (0)\n",
      "#define LAYER25_OUT_SHIFT (0)\n",
      "#define LAYER26_OUT_SHIFT (0)\n",
      "#define LAYER27_OUT_SHIFT (0)\n"
     ]
    }
   ],
   "source": [
    " #LAYER_OUT_SHIFT generator\n",
    "inp = 8\n",
    "out = 8\n",
    "wt = 8\n",
    "index_layer = 1\n",
    "for item in param_list[:nlayer]:\n",
    "    out = item['act_o_bits']\n",
    "    print('#define LAYER{}_OUT_SHIFT ({})' .format(index_layer, int(math.log(8/out,2))))\n",
    "    index_layer += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 3, 3])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2[2].weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_Z(lay,param_list):\n",
    "    if param_list[lay-1]['quant_type'] == 'PerLayerAsymPACT':\n",
    "        str_z = '#define CONV'+ str(lay) + '_W_Z\t\t\t('+ str(int(- m2[2*(lay-1)].weight.data.min()))+ ')'\n",
    "        str_z += '\\n'\n",
    "        str_z += '#define CONV' + str(lay) + '_IN_Z\t\t\t(0)' \n",
    "        str_z += '\\n'\n",
    "        str_z += '#define CONV' + str(lay) + '_OUT_Z\t\t\t(0)'\n",
    "        str_z += '\\n'\n",
    "    elif param_list[lay-1]['quant_type'] == 'PerChannelsAsymMinMax':\n",
    "        str_z = 'static const int16_t CONV'+ str(lay) + '_W_Z[] ={'\n",
    "        for i in range(m2[2*(lay-1)].weight.size(0)):\n",
    "            str_z += str(int(- m2[2*(lay-1)].weight.data[i][:][:][:].min())) + ', ' \n",
    "        str_z += '\\\\n'\n",
    "        str_z = str_z[:-4]+'};'\n",
    "        str_z += '\\n'\n",
    "        str_z += '#define CONV' + str(lay) + '_IN_Z\t\t\t(0)' \n",
    "        str_z += '\\n'\n",
    "        str_z += '#define CONV' + str(lay) + '_OUT_Z\t\t\t(0)'\n",
    "        str_z += '\\n'\n",
    "    return str_z\n",
    "        \n",
    "def generate_M0_N0(lay,param_list):\n",
    "    if param_list[lay-1]['fold_type'] == 'ICN':\n",
    "        str_m0= 'static const int32_t CONV' + str(lay) + '_M_ZERO[] =  {'\n",
    "        for i in range(m2[2*lay-1].M_ZERO.size(0)):\n",
    "            str_m0 += str(int(2**31 * m2[2*lay-1].M_ZERO[i].item())) + ', '\n",
    "        str_m0 += '\\\\\\n'\n",
    "        str_m0 = str_m0[:-4]+'};'\n",
    "\n",
    "        str_n0 = 'static const int8_t CONV'+ str(lay) +'_N_ZERO[] = {'\n",
    "        for i in range(m2[2*lay-1].N_ZERO.size(0)):\n",
    "            str_n0 += str(int(-math.log2(m2[2*lay-1].N_ZERO[i])-1)) + ', '\n",
    "        str_n0 += '\\\\\\n'\n",
    "        str_n0 = str_n0[:-4] + '};'\n",
    "                          \n",
    "    elif param_list[lay-1]['fold_type'] == 'folding_weights':\n",
    "        str_m0 = '#define CONV'+ str(lay) +'_M_ZERO\t\t\t('+ str(int(2**31 *m2[2*lay-1].M_ZERO))+')'\n",
    "        str_n0 = '#define CONV'+ str(lay) +'_N_ZERO\t\t\t('+ str(int(-m2[2*lay-1].N_ZERO)-1)+')'\n",
    "    return str_m0 + '\\n' + str_n0\n",
    "            \n",
    "def generate_bias(lay):\n",
    "    str_bias = '#define CONV'+ str(lay) +'_BIAS {'\n",
    "    for i in range(m2[(lay-1)*2].bias.data.size(0)):\n",
    "        str_bias += str(int(m2[(lay-1)*2].bias.data[i])) + ', '\n",
    "    str_bias += '\\\\\\n'\n",
    "    str_bias = str_bias[:-4] + '}'\n",
    "    return str_bias\n",
    "\n",
    "def concat4bit(a,b):\n",
    "    return a*(2**4)+b\n",
    "\n",
    "def concat2bit(a,b,c,d):\n",
    "    return a*(2**6)+b*(2**4)+c*(2**2)+d\n",
    "\n",
    "#optimized to be tested\n",
    "def generate_weights_PL_wt8bit(lay, act_input_bits, param_list):\n",
    "    str_wt = '#define CONV'+ str(lay)+ '_WT {\\\\\\n' \n",
    "    wt_bits = param_list[lay-1]['w_bits']\n",
    "    Z_wt = - m2[2*(lay-1)].weight.data.min() \n",
    "    biased_wt = (m2[2*(lay-1)].weight.data + Z_wt)\n",
    "    if wt_bits == 8: #order of weights depends on the bitwise\n",
    "        if act_input_bits == 8:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #order is different if the layer function is depthwise (groups!=1) or convolve (odd layers)\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(1)):\n",
    "                                str_wt += str(int(biased_wt[i][p][j][k])) + ', '\n",
    "                            if lay == 1: #the first layer has to be zero padded\n",
    "                                str_wt += str(int(Z_wt)) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                \n",
    "            else:  #depthwise\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                                str_wt += str(int(biased_wt[p][i][j][k])) + ', '\n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                              \n",
    "        elif act_input_bits == 4:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: # convolve \n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1), 8):\n",
    "                                str_wt += str(int(biased_wt[i][p][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+1][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+4][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+5][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+2][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+3][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+6][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+7][j][k])) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                              \n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                                str_wt += str(int(biased_wt[p][i][j][k])) + ', '\n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                              \n",
    "        elif act_input_bits == 2:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve \n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1), 16):\n",
    "                                str_wt += str(int(biased_wt[i][p][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+1][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+8][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+9][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+2][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+3][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+10][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+11][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+4][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+5][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+12][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+13][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+6][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+7][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+14][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+15][j][k])) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                              \n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                                str_wt += str(int(biased_wt[p][i][j][k])) + ', '\n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                              \n",
    "def generate_weights_PL_wt4bit(lay,act_input_bits,param_list):\n",
    "    str_wt = '#define CONV'+ str(lay)+ '_WT {\\\\\\n' \n",
    "    wt_bits = param_list[lay-1]['w_bits']\n",
    "    Z_wt = - m2[2*(lay-1)].weight.data.min() \n",
    "    biased_wt = (m2[2*(lay-1)].weight.data + Z_wt)\n",
    "    if wt_bits == 4:     \n",
    "        if act_input_bits == 8:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-7, 8):\n",
    "                                w0w1 = concat4bit(int(biased_wt[i][p+1][j][k]),int(biased_wt[i][p][j][k]))\n",
    "                                w4w5 = concat4bit(int(biased_wt[i][p+5][j][k]),int(biased_wt[i][p+4][j][k]))\n",
    "                                w2w3 = concat4bit(int(biased_wt[i][p+3][j][k]),int(biased_wt[i][p+2][j][k]))\n",
    "                                w6w7 = concat4bit(int(biased_wt[i][p+7][j][k]),int(biased_wt[i][p+6][j][k]))\n",
    "                                str_wt += str(w0w1) + ', ' + str(w4w5)+ ', ' + str(w2w3) + ', ' + str(w6w7) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                \n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),2):\n",
    "                                str_wt += str(concat4bit(int(biased_wt[p+1][i][j][k]),int(biased_wt[p][i][j][k]))) + ', ' \n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "        \n",
    "        elif act_input_bits == 4:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-7, 8):\n",
    "                                w0w1 = concat4bit(int(biased_wt[i][p+1][j][k]),int(biased_wt[i][p][j][k]))\n",
    "                                w2w3 = concat4bit(int(biased_wt[i][p+3][j][k]),int(biased_wt[i][p+2][j][k]))\n",
    "                                w4w5 = concat4bit(int(biased_wt[i][p+5][j][k]),int(biased_wt[i][p+4][j][k]))\n",
    "                                w6w7 = concat4bit(int(biased_wt[i][p+7][j][k]),int(biased_wt[i][p+6][j][k]))\n",
    "                                str_wt += str(w0w1) + ', ' + str(w2w3)+ ', ' + str(w4w5) + ', ' + str(w6w7) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "\n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),2):\n",
    "                                str_wt += str(concat4bit(int(biased_wt[p+1][i][j][k]),int(biased_wt[p][i][j][k]))) + ', ' \n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                \n",
    "        elif act_input_bits == 2:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve \n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1), 8):\n",
    "                                w0w1w2w3 = concat2bit(int(biased_wt[i][p+3][j][k]),int(biased_wt[i][p+2][j][k]),int(biased_wt[i][p+1][j][k]),int(biased_wt[i][p][j][k]))\n",
    "                                w8w9w10w11 = concat2bit(int(biased_wt[i][p+11][j][k]),int(biased_wt[i][p+10][j][k]),int(biased_wt[i][p+9][j][k]),int(biased_wt[i][p+8][j][k]))\n",
    "                                w4w5w6w7 = concat2bit(int(biased_wt[i][p+7][j][k]),int(biased_wt[i][p+6][j][k]),int(biased_wt[i][p+5][j][k]),int(biased_wt[i][p+4][j][k]))\n",
    "                                w12w13w14w15 = concat2bit(int(biased_wt[i][p+15][j][k]),int(biased_wt[i][p+14][j][k]),int(biased_wt[i][p+13][j][k]),int(biased_wt[i][p+12][j][k]))\n",
    "                                str_wt += str(w0w1w2w3) + ', ' + str(w8w9w10w11)+ ', ' + str(w4w5w6w7) + ', ' + str(w12w13w14w15) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                               \n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),2):\n",
    "                                str_wt += str(concat4bit(int(biased_wt[p+1][i][j][k]),int(biased_wt[p][i][j][k]))) + ', ' \n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "\n",
    "def generate_weights_PL_wt2bit(lay,act_input_bits,param_list):\n",
    "    str_wt = '#define CONV'+ str(lay)+ '_WT {\\\\\\n' \n",
    "    wt_bits = param_list[lay-1]['w_bits']\n",
    "    Z_wt = - m2[2*(lay-1)].weight.data.min() \n",
    "    biased_wt = (m2[2*(lay-1)].weight.data + Z_wt)\n",
    "    if wt_bits == 2:\n",
    "        if act_input_bits == 8:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-15, 16):\n",
    "                                w0w1w4w5 = concat2bit(int(biased_wt[i][p+5][j][k]),int(biased_wt[i][p+4][j][k]),int(biased_wt[i][p+1][j][k]),int(biased_wt[i][p][j][k]))\n",
    "                                w8w9w12w13 = concat2bit(int(biased_wt[i][p+13][j][k]),int(biased_wt[i][p+12][j][k]),int(biased_wt[i][p+9][j][k]),int(biased_wt[i][p+8][j][k]))\n",
    "                                w2w3w6w7 = concat2bit(int(biased_wt[i][p+7][j][k]),int(biased_wt[i][p+6][j][k]),int(biased_wt[i][p+3][j][k]),int(biased_wt[i][p+2][j][k]))\n",
    "                                w10w11w14w15 = concat2bit(int(biased_wt[i][p+15][j][k]),int(biased_wt[i][p+14][j][k]),int(biased_wt[i][p+11][j][k]),int(biased_wt[i][p+10][j][k]))\n",
    "                                str_wt += str(w0w1w4w5) + ', ' + str(w8w9w12w13)+ ', ' + str(w2w3w6w7) + ', ' + str(w10w11w14w15) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                \n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),4):\n",
    "                                str_wt += str(concat2bit(int(biased_wt[p+3][i][j][k]),int(biased_wt[p+2][i][j][k]),int(biased_wt[p+1][i][j][k]),int(biased_wt[p][i][j][k]))) + ', ' \n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "            \n",
    "        elif act_input_bits == 4:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-15, 16):\n",
    "                                w0w1w2w3 = concat2bit(int(biased_wt[i][p+3][j][k]),int(biased_wt[i][p+2][j][k]),int(biased_wt[i][p+1][j][k]),int(biased_wt[i][p][j][k]))\n",
    "                                w8w9w10w11 = concat2bit(int(biased_wt[i][p+11][j][k]),int(biased_wt[i][p+10][j][k]),int(biased_wt[i][p+9][j][k]),int(biased_wt[i][p+8][j][k]))\n",
    "                                w4w5w6w7 = concat2bit(int(biased_wt[i][p+7][j][k]),int(biased_wt[i][p+6][j][k]),int(biased_wt[i][p+5][j][k] ),int(biased_wt[i][p+4][j][k]))\n",
    "                                w12w13w14w15 = concat2bit(int(biased_wt[i][p+15][j][k]),int(biased_wt[i][p+14][j][k]),int(biased_wt[i][p+13][j][k]),int(biased_wt[i][p+12][j][k]))\n",
    "                                str_wt += str(w0w1w2w3) + ', ' + str(w8w9w10w11)+ ', ' + str(w4w5w6w7) + ', ' + str(w12w13w14w15) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "            \n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),4):\n",
    "                                str_wt += str(concat2bit(int(biased_wt[p+3][i][j][k]),int(biased_wt[p+2][i][j][k]),int(biased_wt[p+1][i][j][k]),int(biased_wt[p][i][j][k]))) + ', ' \n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weights_PC_wt8bit(lay,act_input_bits,param_list):\n",
    "    str_wt = '#define CONV'+ str(lay)+ '_WT {\\\\\\n' \n",
    "    wt_bits = param_list[lay-1]['w_bits']\n",
    "    Z_wt = torch.Tensor(m2[2*(lay-1)].weight.data.size(0))\n",
    "    biased_wt = torch.Tensor(m2[2*(lay-1)].weight.data.size())\n",
    "    for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "        Z_wt[i] = - m2[2*(lay-1)].weight.data[i].min() \n",
    "        biased_wt[i] = m2[2*(lay-1)].weight.data[i] + Z_wt[i]\n",
    "    if wt_bits == 8: #order of weights depends on the bitwise\n",
    "        if act_input_bits == 8:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #order is different if the layer function is depthwise (groups!=1) or convolve (odd layers)\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(1)):\n",
    "                                str_wt += str(int(biased_wt[i][p][j][k])) + ', '\n",
    "                            if lay == 1: #the first layer has to be zero padded\n",
    "                                str_wt += str(int(Z_wt[i]) if Z_wt[i]<255 else 255) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                \n",
    "            else:  #depthwise\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                                str_wt += str(int(biased_wt[p][i][j][k])) + ', '\n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "\n",
    "        elif act_input_bits == 4:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: # convolve \n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1), 8):\n",
    "                                str_wt += str(int(biased_wt[i][p][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+1][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+4][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+5][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+2][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+3][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+6][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+7][j][k])) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                              \n",
    "            else: #depthwise\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                                str_wt += str(int(biased_wt[p][i][j][k])) + ', '\n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                              \n",
    "        elif act_input_bits == 2:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve \n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1), 16):\n",
    "                                str_wt += str(int(biased_wt[i][p][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+1][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+8][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+9][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+2][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+3][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+10][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+11][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+4][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+5][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+12][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+13][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+6][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+7][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+14][j][k])) + ', '\n",
    "                                str_wt += str(int(biased_wt[i][p+15][j][k])) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                              \n",
    "            else: #depthwise\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                                str_wt += str(int(biased_wt[p][i][j][k])) + ', '\n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "            \n",
    "\n",
    "def generate_weights_PC_wt4bit(lay,act_input_bits,param_list):\n",
    "    str_wt = '#define CONV'+ str(lay)+ '_WT {\\\\\\n' \n",
    "    wt_bits = param_list[lay-1]['w_bits']\n",
    "    Z_wt = torch.Tensor(m2[2*(lay-1)].weight.data.size(0))\n",
    "    biased_wt = torch.Tensor(m2[2*(lay-1)].weight.data.size())\n",
    "    for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "        Z_wt[i] = - m2[2*(lay-1)].weight.data[i].min() \n",
    "        biased_wt[i] = m2[2*(lay-1)].weight.data[i] + Z_wt[i]\n",
    "    if wt_bits == 4:     \n",
    "        if act_input_bits == 8:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-7, 8):\n",
    "                                w0w1 = concat4bit(int(biased_wt[i][p+1][j][k]),int(biased_wt[i][p][j][k]))\n",
    "                                w4w5 = concat4bit(int(biased_wt[i][p+5][j][k]),int(biased_wt[i][p+4][j][k]))\n",
    "                                w2w3 = concat4bit(int(biased_wt[i][p+3][j][k]),int(biased_wt[i][p+2][j][k]))\n",
    "                                w6w7 = concat4bit(int(biased_wt[i][p+7][j][k]),int(biased_wt[i][p+6][j][k]))\n",
    "                                str_wt += str(w0w1) + ', ' + str(w4w5)+ ', ' + str(w2w3) + ', ' + str(w6w7) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                \n",
    "            else: #depthwise\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),2):\n",
    "                                str_wt += str(concat4bit(int(biased_wt[p+1][i][j][k]),int(biased_wt[p][i][j][k]))) + ', ' \n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "        \n",
    "        elif act_input_bits == 4:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-7, 8):\n",
    "                                w0w1 = concat4bit(int(biased_wt[i][p+1][j][k]),int(biased_wt[i][p][j][k]))\n",
    "                                w2w3 = concat4bit(int(biased_wt[i][p+3][j][k]),int(biased_wt[i][p+2][j][k]))\n",
    "                                w4w5 = concat4bit(int(biased_wt[i][p+5][j][k]),int(biased_wt[i][p+4][j][k]))\n",
    "                                w6w7 = concat4bit(int(biased_wt[i][p+7][j][k]),int(biased_wt[i][p+6][j][k]))\n",
    "                                str_wt += str(w0w1) + ', ' + str(w2w3)+ ', ' + str(w4w5) + ', ' + str(w6w7) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "\n",
    "            else: #depthwise\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),2):\n",
    "                                str_wt += str(concat4bit(int(biased_wt[p+1][i][j][k]),int(biased_wt[p][i][j][k]))) + ', ' \n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                \n",
    "        elif act_input_bits == 2:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve \n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1), 8):\n",
    "                                w0w1w2w3 = concat2bit(int(biased_wt[i][p+3][j][k]),int(biased_wt[i][p+2][j][k]),int(biased_wt[i][p+1][j][k]),int(biased_wt[i][p][j][k]))\n",
    "                                w8w9w10w11 = concat2bit(int(biased_wt[i][p+11][j][k]),int(biased_wt[i][p+10][j][k]),int(biased_wt[i][p+9][j][k]),int(biased_wt[i][p+8][j][k]))\n",
    "                                w4w5w6w7 = concat2bit(int(biased_wt[i][p+7][j][k]),int(biased_wt[i][p+6][j][k]),int(biased_wt[i][p+5][j][k]),int(biased_wt[i][p+4][j][k]))\n",
    "                                w12w13w14w15 = concat2bit(int(biased_wt[i][p+15][j][k]),int(biased_wt[i][p+14][j][k]),int(biased_wt[i][p+13][j][k]),int(biased_wt[i][p+12][j][k]))\n",
    "                                str_wt += str(w0w1w2w3) + ', ' + str(w8w9w10w11)+ ', ' + str(w4w5w6w7) + ', ' + str(w12w13w14w15) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                               \n",
    "            else: #depthwise\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),2):\n",
    "                                str_wt += str(concat4bit(int(biased_wt[p+1][i][j][k]),int(biased_wt[p][i][j][k]))) + ', ' \n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "\n",
    "def generate_weights_PC_wt2bit(lay,act_input_bits,param_list):\n",
    "    str_wt = '#define CONV'+ str(lay)+ '_WT {\\\\\\n' \n",
    "    wt_bits = param_list[lay-1]['w_bits']\n",
    "    Z_wt = torch.Tensor(m2[2*(lay-1)].weight.data.size(0))\n",
    "    biased_wt = torch.Tensor(m2[2*(lay-1)].weight.data.size())\n",
    "    for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "        Z_wt[i] = - m2[2*(lay-1)].weight.data[i].min() \n",
    "        biased_wt[i] = m2[2*(lay-1)].weight.data[i] + Z_wt[i]\n",
    "    if wt_bits == 2:\n",
    "        if act_input_bits == 8:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-15, 16):\n",
    "                                w0w1w4w5 = concat2bit(int(biased_wt[i][p+5][j][k]),int(biased_wt[i][p+4][j][k]),int(biased_wt[i][p+1][j][k]),int(biased_wt[i][p][j][k]))\n",
    "                                w8w9w12w13 = concat2bit(int(biased_wt[i][p+13][j][k]),int(biased_wt[i][p+12][j][k]),int(biased_wt[i][p+9][j][k]),int(biased_wt[i][p+8][j][k]))\n",
    "                                w2w3w6w7 = concat2bit(int(biased_wt[i][p+7][j][k]),int(biased_wt[i][p+6][j][k]),int(biased_wt[i][p+3][j][k]),int(biased_wt[i][p+2][j][k]))\n",
    "                                w10w11w14w15 = concat2bit(int(biased_wt[i][p+15][j][k]),int(biased_wt[i][p+14][j][k]),int(biased_wt[i][p+11][j][k]),int(biased_wt[i][p+10][j][k]))\n",
    "                                str_wt += str(w0w1w4w5) + ', ' + str(w8w9w12w13)+ ', ' + str(w2w3w6w7) + ', ' + str(w10w11w14w15) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "                                \n",
    "            else: #depthwise\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),4):\n",
    "                                str_wt += str(concat2bit(int(biased_wt[p+3][i][j][k]),int(biased_wt[p+2][i][j][k]),int(biased_wt[p+1][i][j][k]),int(biased_wt[p][i][j][k]))) + ', ' \n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "            \n",
    "        elif act_input_bits == 4:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-15, 16):\n",
    "                                w0w1w2w3 = concat2bit(int(biased_wt[i][p+3][j][k]),int(biased_wt[i][p+2][j][k]),int(biased_wt[i][p+1][j][k]),int(biased_wt[i][p][j][k]))\n",
    "                                w8w9w10w11 = concat2bit(int(biased_wt[i][p+11][j][k]),int(biased_wt[i][p+10][j][k]),int(biased_wt[i][p+9][j][k]),int(biased_wt[i][p+8][j][k]))\n",
    "                                w4w5w6w7 = concat2bit(int(biased_wt[i][p+7][j][k]),int(biased_wt[i][p+6][j][k]),int(biased_wt[i][p+5][j][k] ),int(biased_wt[i][p+4][j][k]))\n",
    "                                w12w13w14w15 = concat2bit(int(biased_wt[i][p+15][j][k]),int(biased_wt[i][p+14][j][k]),int(biased_wt[i][p+13][j][k]),int(biased_wt[i][p+12][j][k]))\n",
    "                                str_wt += str(w0w1w2w3) + ', ' + str(w4w5w6w7)+ ', ' + str(w8w9w10w11) + ', ' + str(w12w13w14w15) + ', '\n",
    "                    str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt\n",
    "            \n",
    "            else: #depthwise\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),4):\n",
    "                                str_wt += str(concat2bit(int(biased_wt[p+3][i][j][k]),int(biased_wt[p+2][i][j][k]),int(biased_wt[p+1][i][j][k]),int(biased_wt[p][i][j][k]))) + ', ' \n",
    "                            str_wt += '\\\\\\n'\n",
    "                str_wt = str_wt[:-4]+'}'\n",
    "                return str_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7733, 0.9876, 0.5983, 0.7105, 0.8488, 0.7820, 0.5255, 0.8953, 0.5265,\n",
       "        0.6629, 0.7659, 0.8644, 0.7197, 0.7027, 0.7039, 0.5955, 0.8282, 0.5596,\n",
       "        0.5022, 0.9477, 0.8367, 0.5117, 0.8185, 0.6013, 0.5532, 0.8462, 0.7449,\n",
       "        0.9446, 0.5171, 0.5420, 0.6003, 0.6302], device='cuda:0')"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2[1].M_ZERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_input_bits = 8\n",
    "out = ''\n",
    "for lay in range(1,nlayer+1,1):\n",
    "    #print(lay, param_list[lay-1]['w_bits'], param_list[lay-1]['act_o_bits'])\n",
    "    out += '/* Layer' + str(lay) + 'weights and bias */' + '\\n'\n",
    "    out += generate_bias(lay) +'\\n'\n",
    "    if param_list[lay-1]['w_bits']== 8:\n",
    "        if param_list[lay-1]['quant_type'] == 'PerLayerAsymPACT':\n",
    "            out += generate_weights_PL_wt8bit(lay,act_input_bits,param_list)+'\\n'\n",
    "        elif param_list[lay-1]['quant_type'] == 'PerChannelsAsymMinMax':\n",
    "            out += generate_weights_PC_wt8bit(lay,act_input_bits,param_list)+'\\n'\n",
    "    elif param_list[lay-1]['w_bits']== 4:\n",
    "        if param_list[lay-1]['quant_type'] == 'PerLayerAsymPACT':\n",
    "            out += generate_weights_PL_wt4bit(lay,act_input_bits,param_list)+'\\n'    \n",
    "        elif param_list[lay-1]['quant_type'] == 'PerChannelsAsymMinMax':\n",
    "            out += generate_weights_PC_wt4bit(lay,act_input_bits,param_list)+'\\n'\n",
    "    elif param_list[lay-1]['w_bits']== 2:\n",
    "        if param_list[lay-1]['quant_type'] == 'PerLayerAsymPACT':\n",
    "            out += generate_weights_PL_wt2bit(lay,act_input_bits,param_list)+'\\n'\n",
    "        elif param_list[lay-1]['quant_type'] == 'PerChannelsAsymMinMax':\n",
    "            out += generate_weights_PC_wt2bit(lay,act_input_bits,param_list)+'\\n'\n",
    "    out += generate_Z(lay,param_list)+'\\n'\n",
    "    out += generate_M0_N0(lay,param_list)+'\\n'\n",
    "    out += '\\n'\n",
    "    if lay < nlayer+1:\n",
    "        act_input_bits = param_list[lay-1]['act_o_bits']\n",
    "\n",
    "f = open(\"prova.h\", \"w\") \n",
    "f.write(out) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lay in range(1,nlayer+1,1):\n",
    "    Z_wt = torch.Tensor(m2[2*(lay-1)].weight.data.size(0))\n",
    "    biased_wt = torch.Tensor(m2[2*(lay-1)].weight.data.size())\n",
    "    for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "        Z_wt[i] = - m2[2*(lay-1)].weight.data[i].min() \n",
    "        biased_wt[i] = (m2[2*(lay-1)].weight.data[i] + Z_wt[i])\n",
    "    check = False\n",
    "    #for item in biased_wt:\n",
    "    #    check = check or (int(item.max())>255)or(int(item.min())<0)\n",
    "    max_b = int(biased_wt.max())\n",
    "    min_b = int(biased_wt.min())\n",
    "    #print(lay, ')' , not check)\n",
    "    print(lay,')', param_list[lay-1]['w_bits'], '-', max_b, min_b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-f9f44afcdbf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgenerate_weights_PL_wt2bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact_input_bits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgenerate_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgenerate_M0_N0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlay\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnlayer\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-227-333f3a5c6f63>\u001b[0m in \u001b[0;36mgenerate_M0_N0\u001b[0;34m(lay, param_list)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fold_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ICN'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mstr_m0\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'static const int32_t CONV'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_M_ZERO[] =  {'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM_ZERO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mstr_m0\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m31\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM_ZERO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mstr_m0\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\\\\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "act_input_bits = 8\n",
    "out = ''\n",
    "for lay in range(1,nlayer+1,1):\n",
    "    out += '/* Layer' + str(lay) + 'weights and bias */' + '\\n'\n",
    "    out += generate_bias(lay) +'\\n'\n",
    "    if param_list[lay-1]['w_bits']== 8:\n",
    "        out += generate_weights_PL_wt8bit(lay,act_input_bits,param_list)+'\\n'\n",
    "    elif param_list[lay-1]['w_bits']== 4:\n",
    "        out += generate_weights_PL_wt4bit(lay,act_input_bits,param_list)+'\\n'\n",
    "    elif param_list[lay-1]['w_bits']== 2:\n",
    "        out += generate_weights_PL_wt2bit(lay,act_input_bits,param_list)+'\\n'\n",
    "    out += generate_Z(lay,param_list)+'\\n'\n",
    "    out += generate_M0_N0(lay,param_list)+'\\n'\n",
    "    out += '\\n'\n",
    "    if lay < nlayer+1:\n",
    "        act_input_bits = param_list[lay-1]['act_o_bits']\n",
    "\n",
    "f = open(\"prova.h\", \"w\") \n",
    "f.write(out) \n",
    "f.close()\n",
    "#f = open(str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "tensor([[[210., 255., 182.],\n",
      "         [165., 165., 184.],\n",
      "         [ 48.,   0.,  52.]]], device='cuda:0')\n",
      "tensor([[[210., 255., 182.],\n",
      "         [165., 165., 184.],\n",
      "         [ 48.,   0.,  52.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "k=2\n",
    "lay = 2\n",
    "Z_wt = torch.Tensor(m2[k].weight.size(0))\n",
    "for i in range(m2[k].weight.size(0)):\n",
    "    Z_wt[i]=m2[k].weight.data[i].min()\n",
    "    biased_wt[i] = m2[k].weight.data[i] - Z_wt[i]\n",
    "    print(int(Z_wt[i]), end = ', ')\n",
    "print('')\n",
    "#convolve\n",
    "print(m2[k].weight.data[1])\n",
    "print(biased_wt[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  58.,   84.,   89.],\n",
      "         [  -2.,   25.,  -24.],\n",
      "         [   0.,  -46.,  -68.]],\n",
      "\n",
      "        [[ -42.,  -27.,  -31.],\n",
      "         [-129.,  -87., -140.],\n",
      "         [ -92., -128., -164.]],\n",
      "\n",
      "        [[-102.,  -88., -102.],\n",
      "         [-156., -136., -166.],\n",
      "         [-158., -166., -166.]]], device='cuda:0')\n",
      "tensor([[[224., 250., 255.],\n",
      "         [164., 191., 142.],\n",
      "         [166., 120.,  98.]],\n",
      "\n",
      "        [[124., 139., 135.],\n",
      "         [ 37.,  79.,  26.],\n",
      "         [ 74.,  38.,   2.]],\n",
      "\n",
      "        [[ 64.,  78.,  64.],\n",
      "         [ 10.,  30.,   0.],\n",
      "         [  8.,   0.,   0.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "m2[0].weight.data\n",
    "biased_wt = m2[0].weight.data[0] - m2[0].weight.data[0].min()\n",
    "print(m2[0].weight.data[0])\n",
    "print(biased_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weights_PCH(lay, act_input_bits, param_list):\n",
    "    wt_bits = param_list[lay-1]['w_bits']\n",
    "    if wt_bits == 8:\n",
    "        print('#define CONV{}_WT \\ \\n' .format(lay), end = '{')\n",
    "        #order of weights depends on the bitwise\n",
    "        if act_input_bits == 8:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #order is different if the layer function is depthwise (groups!=1) or convolve (odd layers)\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    Z_wt = - m2[2*(lay-1)].weight.data[i].min()\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(1)):\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p][j][k] + Z_wt), end = '')\n",
    "                                if (count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2))-1):\n",
    "                                    print(', ', end = '')\n",
    "                                elif lay == 1:\n",
    "                                    print(', ', end = '')\n",
    "                                count += 1\n",
    "                            if lay == 1: #the first layer has to be zero padded\n",
    "                                if (count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2))-1):\n",
    "                                    print(int(Z_wt), end = ', ')\n",
    "                                else:\n",
    "                                    print(int(Z_wt), end = '} \\n \\n')\n",
    "                    if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                        print('\\ ', end = '\\n') \n",
    "                    else:\n",
    "                        if lay != 1:\n",
    "                            print(' }', end = '\\n \\n')  \n",
    "            else:  #depthwise\n",
    "                count = 0\n",
    "                Z_wt = torch.Tensor(m2[2*(lay-1)].weight.size(0))\n",
    "                for i in range(m2[2*(lay-1)].weight.size(0)):\n",
    "                    Z_wt[i]=-m2[2*(lay-1)].weight.data[i].min()\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                                if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                    print(int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p]), end = ', ')\n",
    "                                else:\n",
    "                                    print(int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p]), end = '')\n",
    "                                count += 1\n",
    "                            if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                print('\\ ', end = ' \\n') \n",
    "                            else:\n",
    "                                print(' }', end = '\\n \\n')  \n",
    "        elif act_input_bits == 4:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: # convolve \n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    Z_wt = - m2[2*(lay-1)].weight.data[i].min()\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1), 8):\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+1][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+4][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+5][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+2][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+3][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+6][j][k] + Z_wt), end = ', ')\n",
    "                                count += 8\n",
    "                                if (count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2))):\n",
    "                                    if not(count%32):\n",
    "                                        print(int(m2[2*(lay-1)].weight.data[i][p+7][j][k] + Z_wt), end = ', \\ \\n')\n",
    "                                    else:\n",
    "                                        print(int(m2[2*(lay-1)].weight.data[i][p+7][j][k] + Z_wt), end = ', ')\n",
    "                                else:\n",
    "                                    print(int(m2[2*(lay-1)].weight.data[i][p+7][j][k] + Z_wt), end = '} \\n')\n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                Z_wt = torch.Tensor(m2[2*(lay-1)].weight.size(0))\n",
    "                for i in range(m2[2*(lay-1)].weight.size(0)):\n",
    "                    Z_wt[i]=-m2[2*(lay-1)].weight.data[i].min()\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                                if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                    print(int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p]), end = ', ')\n",
    "                                else:\n",
    "                                    print(int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p]), end = '')\n",
    "                                count += 1\n",
    "                            if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                print('\\ ', end = ' \\n') \n",
    "                            else:\n",
    "                                print(' }', end = '\\n \\n')    \n",
    "        elif act_input_bits == 2:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve \n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    Z_wt = - m2[2*(lay-1)].weight.data[i].min()\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1), 16):\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+1][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+8][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+9][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+2][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+3][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+10][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+11][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+4][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+5][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+12][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+13][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+6][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+7][j][k] + Z_wt), end = ', ')\n",
    "                                print(int(m2[2*(lay-1)].weight.data[i][p+14][j][k] + Z_wt), end = ', ')\n",
    "                                count += 16\n",
    "                                if (count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2))):\n",
    "                                    if not(count%32):\n",
    "                                        print(int(m2[2*(lay-1)].weight.data[i][p+15][j][k] + Z_wt), end = ', \\ \\n')\n",
    "                                    else:\n",
    "                                        print(int(m2[2*(lay-1)].weight.data[i][p+15][j][k] + Z_wt), end = ', ')\n",
    "                                else:\n",
    "                                    print(int(m2[2*(lay-1)].weight.data[i][p+15][j][k] + Z_wt), end = '} \\n')\n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                Z_wt = torch.Tensor(m2[2*(lay-1)].weight.size(0))\n",
    "                for i in range(m2[2*(lay-1)].weight.size(0)):\n",
    "                    Z_wt[i]=-m2[2*(lay-1)].weight.data[i].min()\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                                if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                    print(int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p]), end = ', ')\n",
    "                                else:\n",
    "                                    print(int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p]), end = '')\n",
    "                                count += 1\n",
    "                            if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                print('\\ ', end = ' \\n') \n",
    "                            else:\n",
    "                                print(' }', end = '\\n \\n')   \n",
    "                                \n",
    "    elif wt_bits == 4:\n",
    "        print('#define CONV{}_WT \\ \\n' .format(lay), end = '{')\n",
    "        if act_input_bits == 8:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    Z_wt = - m2[2*(lay-1)].weight.data[i].min()\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-7, 8):\n",
    "                                w0w1 = concat4bit(int(m2[2*(lay-1)].weight.data[i][p+1][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p][j][k] + Z_wt))\n",
    "                                w4w5 = concat4bit(int(m2[2*(lay-1)].weight.data[i][p+5][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+4][j][k] + Z_wt))\n",
    "                                w2w3 = concat4bit(int(m2[2*(lay-1)].weight.data[i][p+3][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+2][j][k] + Z_wt))\n",
    "                                w6w7 = concat4bit(int(m2[2*(lay-1)].weight.data[i][p+7][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+6][j][k] + Z_wt))\n",
    "                                print(w0w1,',', w4w5,',',w2w3,',',w6w7 , end = '')\n",
    "                                count += 8\n",
    "                                if (count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2))-1):\n",
    "                                    print(', ', end = '')\n",
    "                    if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                        print('\\ ', end = '\\n') \n",
    "                    else:\n",
    "                        if lay != 1:\n",
    "                             print(' }', end = '\\n \\n') \n",
    "\n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                Z_wt = torch.Tensor(m2[2*(lay-1)].weight.size(0))\n",
    "                for i in range(m2[2*(lay-1)].weight.size(0)):\n",
    "                    Z_wt[i]=-m2[2*(lay-1)].weight.data[i].min()\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),2):\n",
    "                                if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                    print(concat4bit(int(m2[2*(lay-1)].weight.data[p+1][i][j][k] + Z_wt[p+1]),int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p])), end = ', ')\n",
    "                                else:\n",
    "                                    print(concat4bit(int(m2[2*(lay-1)].weight.data[p+1][i][j][k] + Z_wt[p+1]),int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p])), end = '')\n",
    "                                count += 2\n",
    "                            if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                print('\\ ', end = ' \\n') \n",
    "                            else:\n",
    "                                print(' }', end = '\\n \\n')\n",
    "        \n",
    "        elif act_input_bits == 4:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    Z_wt = - m2[2*(lay-1)].weight.data[i].min()\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-7, 8):\n",
    "                                w0w1 = concat4bit(int(m2[2*(lay-1)].weight.data[i][p+1][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p][j][k] + Z_wt))\n",
    "                                w2w3 = concat4bit(int(m2[2*(lay-1)].weight.data[i][p+3][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+2][j][k] + Z_wt))\n",
    "                                w4w5 = concat4bit(int(m2[2*(lay-1)].weight.data[i][p+5][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+4][j][k] + Z_wt))\n",
    "                                w6w7 = concat4bit(int(m2[2*(lay-1)].weight.data[i][p+7][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+6][j][k] + Z_wt))\n",
    "                                print(w0w1,',', w2w3,',',w4w5,',',w6w7 , end = '')\n",
    "                                count += 8\n",
    "                                if (count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2))-1):\n",
    "                                    print(', ', end = '')\n",
    "                    if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                        print('\\ ', end = '\\n') \n",
    "                    else:\n",
    "                        if lay != 1:\n",
    "                             print(' }', end = '\\n \\n') \n",
    "\n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                Z_wt = torch.Tensor(m2[2*(lay-1)].weight.size(0))\n",
    "                for i in range(m2[2*(lay-1)].weight.size(0)):\n",
    "                    Z_wt[i]=-m2[2*(lay-1)].weight.data[i].min()\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),2):\n",
    "                                if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                    print(concat4bit(int(m2[2*(lay-1)].weight.data[p+1][i][j][k] + Z_wt[p+1]),int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p])), end = ', ')\n",
    "                                else:\n",
    "                                    print(concat4bit(int(m2[2*(lay-1)].weight.data[p+1][i][j][k] + Z_wt[p+1]),int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p])), end = '')\n",
    "                                count += 2\n",
    "                            if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                print('\\ ', end = ' \\n') \n",
    "                            else:\n",
    "                                print(' }', end = '\\n \\n')\n",
    "                                \n",
    "        elif act_input_bits == 2:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve \n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    Z_wt = - m2[2*(lay-1)].weight.data[i].min()\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1), 8):\n",
    "                                w0w1w2w3 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+3][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+2][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+1][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p][j][k] + Z_wt))\n",
    "                                w8w9w10w11 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+11][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+10][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+9][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+8][j][k] + Z_wt))\n",
    "                                w4w5w6w7 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+7][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+6][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+5][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+4][j][k] + Z_wt))\n",
    "                                w12w13w14w15 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+15][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+14][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+13][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+12][j][k] + Z_wt))\n",
    "                                print(w0w1w2w3,',', w8w9w10w11,',',w4w5w6w7,',',w12w13w14w15 , end = '')\n",
    "                                count += 16\n",
    "                                if (count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2))-1):\n",
    "                                    print(', ', end = '')\n",
    "                    if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                        print('\\ ', end = '\\n') \n",
    "                    else:\n",
    "                        if lay != 1:\n",
    "                             print(' }', end = '\\n \\n') \n",
    "            else: #depthwise\n",
    "                count = 0\n",
    "                Z_wt = torch.Tensor(m2[2*(lay-1)].weight.size(0))\n",
    "                for i in range(m2[2*(lay-1)].weight.size(0)):\n",
    "                    Z_wt[i]=-m2[2*(lay-1)].weight.data[i].min()\n",
    "                for i in range(m2[2*(lay-1)].weight.size(1)):\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0,m2[2*(lay-1)].weight.data.size(0),2):\n",
    "                                if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                    print(concat4bit(int(m2[2*(lay-1)].weight.data[p+1][i][j][k] + Z_wt[p+1]),int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p])), end = ', ')\n",
    "                                else:\n",
    "                                    print(concat4bit(int(m2[2*(lay-1)].weight.data[p+1][i][j][k] + Z_wt[p+1]),int(m2[2*(lay-1)].weight.data[p][i][j][k] + Z_wt[p])), end = '')\n",
    "                                count += 2\n",
    "                            if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                                print('\\ ', end = ' \\n') \n",
    "                            else:\n",
    "                                print(' }', end = '\\n \\n')\n",
    "            \n",
    "            \n",
    "\n",
    "    elif wt_bits == 2:\n",
    "        print('#define CONV{}_WT \\ \\n' .format(lay), end = '{')\n",
    "        if act_input_bits == 8:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    Z_wt = - m2[2*(lay-1)].weight.data[i].min()\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-15, 16):\n",
    "                                w0w1w4w5 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+5][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+4][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+1][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p][j][k] + Z_wt))\n",
    "                                w8w9w12w13 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+13][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+12][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+9][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+8][j][k] + Z_wt))\n",
    "                                w2w3w6w7 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+7][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+6][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+3][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+2][j][k] + Z_wt))\n",
    "                                w10w11w14w15 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+15][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+14][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+11][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+10][j][k] + Z_wt))\n",
    "                                print(w0w1w4w5,',', w8w9w12w13,',',w2w3w6w7,',',w10w11w14w15 , end = '')\n",
    "                                count += 16\n",
    "                                if (count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2))-1):\n",
    "                                    print(', ', end = '')\n",
    "                    if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                        print('\\ ', end = '\\n') \n",
    "                    else:\n",
    "                        if lay != 1:\n",
    "                             print(' }', end = '\\n \\n') \n",
    "            else: #depthwise\n",
    "                print('to be developed')\n",
    "                \n",
    "        elif act_input_bits == 4:\n",
    "            if param_list[lay-1]['quant_conv'].groups == 1: #convolve\n",
    "                count = 0\n",
    "                for i in range(m2[2*(lay-1)].weight.data.size(0)):\n",
    "                    Z_wt = - m2[2*(lay-1)].weight.data[i].min()\n",
    "                    for j in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                        for k in range(m2[2*(lay-1)].weight.data.size(2)):\n",
    "                            for p in range(0, m2[2*(lay-1)].weight.data.size(1)-15, 16):\n",
    "                                w0w1w2w3 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+3][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+2][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+1][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p][j][k] + Z_wt))\n",
    "                                w8w9w10w11 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+11][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+10][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+9][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+8][j][k] + Z_wt))\n",
    "                                w4w5w6w7 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+7][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+6][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+5][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+4][j][k] + Z_wt))\n",
    "                                w12w13w14w15 = concat2bit(int(m2[2*(lay-1)].weight.data[i][p+15][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+14][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+13][j][k] + Z_wt),int(m2[2*(lay-1)].weight.data[i][p+12][j][k] + Z_wt))\n",
    "                                print(w0w1w2w3,',', w8w9w10w11,',',w4w5w6w7,',',w12w13w14w15 , end = '')\n",
    "                                count += 16\n",
    "                                if (count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2))-1):\n",
    "                                    print(', ', end = '')\n",
    "                    if count < (m2[2*(lay-1)].weight.data.size(0)*m2[2*(lay-1)].weight.data.size(1)*m2[2*(lay-1)].weight.data.size(2)*m2[2*(lay-1)].weight.data.size(2)-1):\n",
    "                        print('\\ ', end = '\\n') \n",
    "                    else:\n",
    "                        if lay != 1:\n",
    "                             print(' }', end = '\\n \\n') \n",
    "            else: #depthwise\n",
    "                print('to be developed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224_0_1\n"
     ]
    }
   ],
   "source": [
    "depth_multiplier = 1\n",
    "print((str(input_size).upper()+\"_\"+str(depth_multiplier).upper()).replace('.', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "1 1\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "1 1\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "1 1\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "1 1\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "1 1\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 17 Topology Parameters /\n",
      "#define CONV17_IM_DIM    \t(14)\n",
      "#define CONV17_IM_CH     \t(512)\n",
      "#define CONV17_KER_DIM   \t(1)\n",
      "#define CONV17_L_PADDING \t(0)\n",
      "#define CONV17_R_PADDING \t(0)\n",
      "#define CONV17_T_PADDING \t(0)\n",
      "#define CONV17_B_PADDING \t(0)\n",
      "#define CONV17_STRIDE    \t(1)\n",
      "#define CONV17_OUT_CH    \t(512)\n",
      "#define CONV17_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "1 1\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 17 Topology Parameters /\n",
      "#define CONV17_IM_DIM    \t(14)\n",
      "#define CONV17_IM_CH     \t(512)\n",
      "#define CONV17_KER_DIM   \t(1)\n",
      "#define CONV17_L_PADDING \t(0)\n",
      "#define CONV17_R_PADDING \t(0)\n",
      "#define CONV17_T_PADDING \t(0)\n",
      "#define CONV17_B_PADDING \t(0)\n",
      "#define CONV17_STRIDE    \t(1)\n",
      "#define CONV17_OUT_CH    \t(512)\n",
      "#define CONV17_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 18 Topology Parameters /\n",
      "#define CONV18_IM_DIM    \t(14)\n",
      "#define CONV18_IM_CH     \t(512)\n",
      "#define CONV18_KER_DIM   \t(3)\n",
      "#define CONV18_L_PADDING \t(1)\n",
      "#define CONV18_R_PADDING \t(1)\n",
      "#define CONV18_T_PADDING \t(1)\n",
      "#define CONV18_B_PADDING \t(1)\n",
      "#define CONV18_STRIDE    \t(1)\n",
      "#define CONV18_OUT_CH    \t(512)\n",
      "#define CONV18_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 17 Topology Parameters /\n",
      "#define CONV17_IM_DIM    \t(14)\n",
      "#define CONV17_IM_CH     \t(512)\n",
      "#define CONV17_KER_DIM   \t(1)\n",
      "#define CONV17_L_PADDING \t(0)\n",
      "#define CONV17_R_PADDING \t(0)\n",
      "#define CONV17_T_PADDING \t(0)\n",
      "#define CONV17_B_PADDING \t(0)\n",
      "#define CONV17_STRIDE    \t(1)\n",
      "#define CONV17_OUT_CH    \t(512)\n",
      "#define CONV17_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 18 Topology Parameters /\n",
      "#define CONV18_IM_DIM    \t(14)\n",
      "#define CONV18_IM_CH     \t(512)\n",
      "#define CONV18_KER_DIM   \t(3)\n",
      "#define CONV18_L_PADDING \t(1)\n",
      "#define CONV18_R_PADDING \t(1)\n",
      "#define CONV18_T_PADDING \t(1)\n",
      "#define CONV18_B_PADDING \t(1)\n",
      "#define CONV18_STRIDE    \t(1)\n",
      "#define CONV18_OUT_CH    \t(512)\n",
      "#define CONV18_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 19 Topology Parameters /\n",
      "#define CONV19_IM_DIM    \t(14)\n",
      "#define CONV19_IM_CH     \t(512)\n",
      "#define CONV19_KER_DIM   \t(1)\n",
      "#define CONV19_L_PADDING \t(0)\n",
      "#define CONV19_R_PADDING \t(0)\n",
      "#define CONV19_T_PADDING \t(0)\n",
      "#define CONV19_B_PADDING \t(0)\n",
      "#define CONV19_STRIDE    \t(1)\n",
      "#define CONV19_OUT_CH    \t(512)\n",
      "#define CONV19_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "1 1\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 17 Topology Parameters /\n",
      "#define CONV17_IM_DIM    \t(14)\n",
      "#define CONV17_IM_CH     \t(512)\n",
      "#define CONV17_KER_DIM   \t(1)\n",
      "#define CONV17_L_PADDING \t(0)\n",
      "#define CONV17_R_PADDING \t(0)\n",
      "#define CONV17_T_PADDING \t(0)\n",
      "#define CONV17_B_PADDING \t(0)\n",
      "#define CONV17_STRIDE    \t(1)\n",
      "#define CONV17_OUT_CH    \t(512)\n",
      "#define CONV17_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 18 Topology Parameters /\n",
      "#define CONV18_IM_DIM    \t(14)\n",
      "#define CONV18_IM_CH     \t(512)\n",
      "#define CONV18_KER_DIM   \t(3)\n",
      "#define CONV18_L_PADDING \t(1)\n",
      "#define CONV18_R_PADDING \t(1)\n",
      "#define CONV18_T_PADDING \t(1)\n",
      "#define CONV18_B_PADDING \t(1)\n",
      "#define CONV18_STRIDE    \t(1)\n",
      "#define CONV18_OUT_CH    \t(512)\n",
      "#define CONV18_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 19 Topology Parameters /\n",
      "#define CONV19_IM_DIM    \t(14)\n",
      "#define CONV19_IM_CH     \t(512)\n",
      "#define CONV19_KER_DIM   \t(1)\n",
      "#define CONV19_L_PADDING \t(0)\n",
      "#define CONV19_R_PADDING \t(0)\n",
      "#define CONV19_T_PADDING \t(0)\n",
      "#define CONV19_B_PADDING \t(0)\n",
      "#define CONV19_STRIDE    \t(1)\n",
      "#define CONV19_OUT_CH    \t(512)\n",
      "#define CONV19_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 20 Topology Parameters /\n",
      "#define CONV20_IM_DIM    \t(14)\n",
      "#define CONV20_IM_CH     \t(512)\n",
      "#define CONV20_KER_DIM   \t(3)\n",
      "#define CONV20_L_PADDING \t(1)\n",
      "#define CONV20_R_PADDING \t(1)\n",
      "#define CONV20_T_PADDING \t(1)\n",
      "#define CONV20_B_PADDING \t(1)\n",
      "#define CONV20_STRIDE    \t(1)\n",
      "#define CONV20_OUT_CH    \t(512)\n",
      "#define CONV20_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 17 Topology Parameters /\n",
      "#define CONV17_IM_DIM    \t(14)\n",
      "#define CONV17_IM_CH     \t(512)\n",
      "#define CONV17_KER_DIM   \t(1)\n",
      "#define CONV17_L_PADDING \t(0)\n",
      "#define CONV17_R_PADDING \t(0)\n",
      "#define CONV17_T_PADDING \t(0)\n",
      "#define CONV17_B_PADDING \t(0)\n",
      "#define CONV17_STRIDE    \t(1)\n",
      "#define CONV17_OUT_CH    \t(512)\n",
      "#define CONV17_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 18 Topology Parameters /\n",
      "#define CONV18_IM_DIM    \t(14)\n",
      "#define CONV18_IM_CH     \t(512)\n",
      "#define CONV18_KER_DIM   \t(3)\n",
      "#define CONV18_L_PADDING \t(1)\n",
      "#define CONV18_R_PADDING \t(1)\n",
      "#define CONV18_T_PADDING \t(1)\n",
      "#define CONV18_B_PADDING \t(1)\n",
      "#define CONV18_STRIDE    \t(1)\n",
      "#define CONV18_OUT_CH    \t(512)\n",
      "#define CONV18_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 19 Topology Parameters /\n",
      "#define CONV19_IM_DIM    \t(14)\n",
      "#define CONV19_IM_CH     \t(512)\n",
      "#define CONV19_KER_DIM   \t(1)\n",
      "#define CONV19_L_PADDING \t(0)\n",
      "#define CONV19_R_PADDING \t(0)\n",
      "#define CONV19_T_PADDING \t(0)\n",
      "#define CONV19_B_PADDING \t(0)\n",
      "#define CONV19_STRIDE    \t(1)\n",
      "#define CONV19_OUT_CH    \t(512)\n",
      "#define CONV19_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 20 Topology Parameters /\n",
      "#define CONV20_IM_DIM    \t(14)\n",
      "#define CONV20_IM_CH     \t(512)\n",
      "#define CONV20_KER_DIM   \t(3)\n",
      "#define CONV20_L_PADDING \t(1)\n",
      "#define CONV20_R_PADDING \t(1)\n",
      "#define CONV20_T_PADDING \t(1)\n",
      "#define CONV20_B_PADDING \t(1)\n",
      "#define CONV20_STRIDE    \t(1)\n",
      "#define CONV20_OUT_CH    \t(512)\n",
      "#define CONV20_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 21 Topology Parameters /\n",
      "#define CONV21_IM_DIM    \t(14)\n",
      "#define CONV21_IM_CH     \t(512)\n",
      "#define CONV21_KER_DIM   \t(1)\n",
      "#define CONV21_L_PADDING \t(0)\n",
      "#define CONV21_R_PADDING \t(0)\n",
      "#define CONV21_T_PADDING \t(0)\n",
      "#define CONV21_B_PADDING \t(0)\n",
      "#define CONV21_STRIDE    \t(1)\n",
      "#define CONV21_OUT_CH    \t(512)\n",
      "#define CONV21_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "1 1\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 17 Topology Parameters /\n",
      "#define CONV17_IM_DIM    \t(14)\n",
      "#define CONV17_IM_CH     \t(512)\n",
      "#define CONV17_KER_DIM   \t(1)\n",
      "#define CONV17_L_PADDING \t(0)\n",
      "#define CONV17_R_PADDING \t(0)\n",
      "#define CONV17_T_PADDING \t(0)\n",
      "#define CONV17_B_PADDING \t(0)\n",
      "#define CONV17_STRIDE    \t(1)\n",
      "#define CONV17_OUT_CH    \t(512)\n",
      "#define CONV17_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 18 Topology Parameters /\n",
      "#define CONV18_IM_DIM    \t(14)\n",
      "#define CONV18_IM_CH     \t(512)\n",
      "#define CONV18_KER_DIM   \t(3)\n",
      "#define CONV18_L_PADDING \t(1)\n",
      "#define CONV18_R_PADDING \t(1)\n",
      "#define CONV18_T_PADDING \t(1)\n",
      "#define CONV18_B_PADDING \t(1)\n",
      "#define CONV18_STRIDE    \t(1)\n",
      "#define CONV18_OUT_CH    \t(512)\n",
      "#define CONV18_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 19 Topology Parameters /\n",
      "#define CONV19_IM_DIM    \t(14)\n",
      "#define CONV19_IM_CH     \t(512)\n",
      "#define CONV19_KER_DIM   \t(1)\n",
      "#define CONV19_L_PADDING \t(0)\n",
      "#define CONV19_R_PADDING \t(0)\n",
      "#define CONV19_T_PADDING \t(0)\n",
      "#define CONV19_B_PADDING \t(0)\n",
      "#define CONV19_STRIDE    \t(1)\n",
      "#define CONV19_OUT_CH    \t(512)\n",
      "#define CONV19_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 20 Topology Parameters /\n",
      "#define CONV20_IM_DIM    \t(14)\n",
      "#define CONV20_IM_CH     \t(512)\n",
      "#define CONV20_KER_DIM   \t(3)\n",
      "#define CONV20_L_PADDING \t(1)\n",
      "#define CONV20_R_PADDING \t(1)\n",
      "#define CONV20_T_PADDING \t(1)\n",
      "#define CONV20_B_PADDING \t(1)\n",
      "#define CONV20_STRIDE    \t(1)\n",
      "#define CONV20_OUT_CH    \t(512)\n",
      "#define CONV20_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 21 Topology Parameters /\n",
      "#define CONV21_IM_DIM    \t(14)\n",
      "#define CONV21_IM_CH     \t(512)\n",
      "#define CONV21_KER_DIM   \t(1)\n",
      "#define CONV21_L_PADDING \t(0)\n",
      "#define CONV21_R_PADDING \t(0)\n",
      "#define CONV21_T_PADDING \t(0)\n",
      "#define CONV21_B_PADDING \t(0)\n",
      "#define CONV21_STRIDE    \t(1)\n",
      "#define CONV21_OUT_CH    \t(512)\n",
      "#define CONV21_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 22 Topology Parameters /\n",
      "#define CONV22_IM_DIM    \t(14)\n",
      "#define CONV22_IM_CH     \t(512)\n",
      "#define CONV22_KER_DIM   \t(3)\n",
      "#define CONV22_L_PADDING \t(1)\n",
      "#define CONV22_R_PADDING \t(1)\n",
      "#define CONV22_T_PADDING \t(1)\n",
      "#define CONV22_B_PADDING \t(1)\n",
      "#define CONV22_STRIDE    \t(1)\n",
      "#define CONV22_OUT_CH    \t(512)\n",
      "#define CONV22_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 17 Topology Parameters /\n",
      "#define CONV17_IM_DIM    \t(14)\n",
      "#define CONV17_IM_CH     \t(512)\n",
      "#define CONV17_KER_DIM   \t(1)\n",
      "#define CONV17_L_PADDING \t(0)\n",
      "#define CONV17_R_PADDING \t(0)\n",
      "#define CONV17_T_PADDING \t(0)\n",
      "#define CONV17_B_PADDING \t(0)\n",
      "#define CONV17_STRIDE    \t(1)\n",
      "#define CONV17_OUT_CH    \t(512)\n",
      "#define CONV17_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 18 Topology Parameters /\n",
      "#define CONV18_IM_DIM    \t(14)\n",
      "#define CONV18_IM_CH     \t(512)\n",
      "#define CONV18_KER_DIM   \t(3)\n",
      "#define CONV18_L_PADDING \t(1)\n",
      "#define CONV18_R_PADDING \t(1)\n",
      "#define CONV18_T_PADDING \t(1)\n",
      "#define CONV18_B_PADDING \t(1)\n",
      "#define CONV18_STRIDE    \t(1)\n",
      "#define CONV18_OUT_CH    \t(512)\n",
      "#define CONV18_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 19 Topology Parameters /\n",
      "#define CONV19_IM_DIM    \t(14)\n",
      "#define CONV19_IM_CH     \t(512)\n",
      "#define CONV19_KER_DIM   \t(1)\n",
      "#define CONV19_L_PADDING \t(0)\n",
      "#define CONV19_R_PADDING \t(0)\n",
      "#define CONV19_T_PADDING \t(0)\n",
      "#define CONV19_B_PADDING \t(0)\n",
      "#define CONV19_STRIDE    \t(1)\n",
      "#define CONV19_OUT_CH    \t(512)\n",
      "#define CONV19_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 20 Topology Parameters /\n",
      "#define CONV20_IM_DIM    \t(14)\n",
      "#define CONV20_IM_CH     \t(512)\n",
      "#define CONV20_KER_DIM   \t(3)\n",
      "#define CONV20_L_PADDING \t(1)\n",
      "#define CONV20_R_PADDING \t(1)\n",
      "#define CONV20_T_PADDING \t(1)\n",
      "#define CONV20_B_PADDING \t(1)\n",
      "#define CONV20_STRIDE    \t(1)\n",
      "#define CONV20_OUT_CH    \t(512)\n",
      "#define CONV20_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 21 Topology Parameters /\n",
      "#define CONV21_IM_DIM    \t(14)\n",
      "#define CONV21_IM_CH     \t(512)\n",
      "#define CONV21_KER_DIM   \t(1)\n",
      "#define CONV21_L_PADDING \t(0)\n",
      "#define CONV21_R_PADDING \t(0)\n",
      "#define CONV21_T_PADDING \t(0)\n",
      "#define CONV21_B_PADDING \t(0)\n",
      "#define CONV21_STRIDE    \t(1)\n",
      "#define CONV21_OUT_CH    \t(512)\n",
      "#define CONV21_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 22 Topology Parameters /\n",
      "#define CONV22_IM_DIM    \t(14)\n",
      "#define CONV22_IM_CH     \t(512)\n",
      "#define CONV22_KER_DIM   \t(3)\n",
      "#define CONV22_L_PADDING \t(1)\n",
      "#define CONV22_R_PADDING \t(1)\n",
      "#define CONV22_T_PADDING \t(1)\n",
      "#define CONV22_B_PADDING \t(1)\n",
      "#define CONV22_STRIDE    \t(1)\n",
      "#define CONV22_OUT_CH    \t(512)\n",
      "#define CONV22_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 23 Topology Parameters /\n",
      "#define CONV23_IM_DIM    \t(14)\n",
      "#define CONV23_IM_CH     \t(512)\n",
      "#define CONV23_KER_DIM   \t(1)\n",
      "#define CONV23_L_PADDING \t(0)\n",
      "#define CONV23_R_PADDING \t(0)\n",
      "#define CONV23_T_PADDING \t(0)\n",
      "#define CONV23_B_PADDING \t(0)\n",
      "#define CONV23_STRIDE    \t(1)\n",
      "#define CONV23_OUT_CH    \t(512)\n",
      "#define CONV23_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 17 Topology Parameters /\n",
      "#define CONV17_IM_DIM    \t(14)\n",
      "#define CONV17_IM_CH     \t(512)\n",
      "#define CONV17_KER_DIM   \t(1)\n",
      "#define CONV17_L_PADDING \t(0)\n",
      "#define CONV17_R_PADDING \t(0)\n",
      "#define CONV17_T_PADDING \t(0)\n",
      "#define CONV17_B_PADDING \t(0)\n",
      "#define CONV17_STRIDE    \t(1)\n",
      "#define CONV17_OUT_CH    \t(512)\n",
      "#define CONV17_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 18 Topology Parameters /\n",
      "#define CONV18_IM_DIM    \t(14)\n",
      "#define CONV18_IM_CH     \t(512)\n",
      "#define CONV18_KER_DIM   \t(3)\n",
      "#define CONV18_L_PADDING \t(1)\n",
      "#define CONV18_R_PADDING \t(1)\n",
      "#define CONV18_T_PADDING \t(1)\n",
      "#define CONV18_B_PADDING \t(1)\n",
      "#define CONV18_STRIDE    \t(1)\n",
      "#define CONV18_OUT_CH    \t(512)\n",
      "#define CONV18_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 19 Topology Parameters /\n",
      "#define CONV19_IM_DIM    \t(14)\n",
      "#define CONV19_IM_CH     \t(512)\n",
      "#define CONV19_KER_DIM   \t(1)\n",
      "#define CONV19_L_PADDING \t(0)\n",
      "#define CONV19_R_PADDING \t(0)\n",
      "#define CONV19_T_PADDING \t(0)\n",
      "#define CONV19_B_PADDING \t(0)\n",
      "#define CONV19_STRIDE    \t(1)\n",
      "#define CONV19_OUT_CH    \t(512)\n",
      "#define CONV19_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 20 Topology Parameters /\n",
      "#define CONV20_IM_DIM    \t(14)\n",
      "#define CONV20_IM_CH     \t(512)\n",
      "#define CONV20_KER_DIM   \t(3)\n",
      "#define CONV20_L_PADDING \t(1)\n",
      "#define CONV20_R_PADDING \t(1)\n",
      "#define CONV20_T_PADDING \t(1)\n",
      "#define CONV20_B_PADDING \t(1)\n",
      "#define CONV20_STRIDE    \t(1)\n",
      "#define CONV20_OUT_CH    \t(512)\n",
      "#define CONV20_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 21 Topology Parameters /\n",
      "#define CONV21_IM_DIM    \t(14)\n",
      "#define CONV21_IM_CH     \t(512)\n",
      "#define CONV21_KER_DIM   \t(1)\n",
      "#define CONV21_L_PADDING \t(0)\n",
      "#define CONV21_R_PADDING \t(0)\n",
      "#define CONV21_T_PADDING \t(0)\n",
      "#define CONV21_B_PADDING \t(0)\n",
      "#define CONV21_STRIDE    \t(1)\n",
      "#define CONV21_OUT_CH    \t(512)\n",
      "#define CONV21_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 22 Topology Parameters /\n",
      "#define CONV22_IM_DIM    \t(14)\n",
      "#define CONV22_IM_CH     \t(512)\n",
      "#define CONV22_KER_DIM   \t(3)\n",
      "#define CONV22_L_PADDING \t(1)\n",
      "#define CONV22_R_PADDING \t(1)\n",
      "#define CONV22_T_PADDING \t(1)\n",
      "#define CONV22_B_PADDING \t(1)\n",
      "#define CONV22_STRIDE    \t(1)\n",
      "#define CONV22_OUT_CH    \t(512)\n",
      "#define CONV22_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 23 Topology Parameters /\n",
      "#define CONV23_IM_DIM    \t(14)\n",
      "#define CONV23_IM_CH     \t(512)\n",
      "#define CONV23_KER_DIM   \t(1)\n",
      "#define CONV23_L_PADDING \t(0)\n",
      "#define CONV23_R_PADDING \t(0)\n",
      "#define CONV23_T_PADDING \t(0)\n",
      "#define CONV23_B_PADDING \t(0)\n",
      "#define CONV23_STRIDE    \t(1)\n",
      "#define CONV23_OUT_CH    \t(512)\n",
      "#define CONV23_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 24 Topology Parameters /\n",
      "#define CONV24_IM_DIM    \t(14)\n",
      "#define CONV24_IM_CH     \t(512)\n",
      "#define CONV24_KER_DIM   \t(3)\n",
      "#define CONV24_L_PADDING \t(0)\n",
      "#define CONV24_R_PADDING \t(1)\n",
      "#define CONV24_T_PADDING \t(0)\n",
      "#define CONV24_B_PADDING \t(1)\n",
      "#define CONV24_STRIDE    \t(2)\n",
      "#define CONV24_OUT_CH    \t(512)\n",
      "#define CONV24_OUT_DIM    \t(7)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 17 Topology Parameters /\n",
      "#define CONV17_IM_DIM    \t(14)\n",
      "#define CONV17_IM_CH     \t(512)\n",
      "#define CONV17_KER_DIM   \t(1)\n",
      "#define CONV17_L_PADDING \t(0)\n",
      "#define CONV17_R_PADDING \t(0)\n",
      "#define CONV17_T_PADDING \t(0)\n",
      "#define CONV17_B_PADDING \t(0)\n",
      "#define CONV17_STRIDE    \t(1)\n",
      "#define CONV17_OUT_CH    \t(512)\n",
      "#define CONV17_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 18 Topology Parameters /\n",
      "#define CONV18_IM_DIM    \t(14)\n",
      "#define CONV18_IM_CH     \t(512)\n",
      "#define CONV18_KER_DIM   \t(3)\n",
      "#define CONV18_L_PADDING \t(1)\n",
      "#define CONV18_R_PADDING \t(1)\n",
      "#define CONV18_T_PADDING \t(1)\n",
      "#define CONV18_B_PADDING \t(1)\n",
      "#define CONV18_STRIDE    \t(1)\n",
      "#define CONV18_OUT_CH    \t(512)\n",
      "#define CONV18_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 19 Topology Parameters /\n",
      "#define CONV19_IM_DIM    \t(14)\n",
      "#define CONV19_IM_CH     \t(512)\n",
      "#define CONV19_KER_DIM   \t(1)\n",
      "#define CONV19_L_PADDING \t(0)\n",
      "#define CONV19_R_PADDING \t(0)\n",
      "#define CONV19_T_PADDING \t(0)\n",
      "#define CONV19_B_PADDING \t(0)\n",
      "#define CONV19_STRIDE    \t(1)\n",
      "#define CONV19_OUT_CH    \t(512)\n",
      "#define CONV19_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 20 Topology Parameters /\n",
      "#define CONV20_IM_DIM    \t(14)\n",
      "#define CONV20_IM_CH     \t(512)\n",
      "#define CONV20_KER_DIM   \t(3)\n",
      "#define CONV20_L_PADDING \t(1)\n",
      "#define CONV20_R_PADDING \t(1)\n",
      "#define CONV20_T_PADDING \t(1)\n",
      "#define CONV20_B_PADDING \t(1)\n",
      "#define CONV20_STRIDE    \t(1)\n",
      "#define CONV20_OUT_CH    \t(512)\n",
      "#define CONV20_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 21 Topology Parameters /\n",
      "#define CONV21_IM_DIM    \t(14)\n",
      "#define CONV21_IM_CH     \t(512)\n",
      "#define CONV21_KER_DIM   \t(1)\n",
      "#define CONV21_L_PADDING \t(0)\n",
      "#define CONV21_R_PADDING \t(0)\n",
      "#define CONV21_T_PADDING \t(0)\n",
      "#define CONV21_B_PADDING \t(0)\n",
      "#define CONV21_STRIDE    \t(1)\n",
      "#define CONV21_OUT_CH    \t(512)\n",
      "#define CONV21_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 22 Topology Parameters /\n",
      "#define CONV22_IM_DIM    \t(14)\n",
      "#define CONV22_IM_CH     \t(512)\n",
      "#define CONV22_KER_DIM   \t(3)\n",
      "#define CONV22_L_PADDING \t(1)\n",
      "#define CONV22_R_PADDING \t(1)\n",
      "#define CONV22_T_PADDING \t(1)\n",
      "#define CONV22_B_PADDING \t(1)\n",
      "#define CONV22_STRIDE    \t(1)\n",
      "#define CONV22_OUT_CH    \t(512)\n",
      "#define CONV22_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 23 Topology Parameters /\n",
      "#define CONV23_IM_DIM    \t(14)\n",
      "#define CONV23_IM_CH     \t(512)\n",
      "#define CONV23_KER_DIM   \t(1)\n",
      "#define CONV23_L_PADDING \t(0)\n",
      "#define CONV23_R_PADDING \t(0)\n",
      "#define CONV23_T_PADDING \t(0)\n",
      "#define CONV23_B_PADDING \t(0)\n",
      "#define CONV23_STRIDE    \t(1)\n",
      "#define CONV23_OUT_CH    \t(512)\n",
      "#define CONV23_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 24 Topology Parameters /\n",
      "#define CONV24_IM_DIM    \t(14)\n",
      "#define CONV24_IM_CH     \t(512)\n",
      "#define CONV24_KER_DIM   \t(3)\n",
      "#define CONV24_L_PADDING \t(0)\n",
      "#define CONV24_R_PADDING \t(1)\n",
      "#define CONV24_T_PADDING \t(0)\n",
      "#define CONV24_B_PADDING \t(1)\n",
      "#define CONV24_STRIDE    \t(2)\n",
      "#define CONV24_OUT_CH    \t(512)\n",
      "#define CONV24_OUT_DIM    \t(7)\n",
      "\n",
      "\n",
      "/ Layer 25 Topology Parameters /\n",
      "#define CONV25_IM_DIM    \t(7)\n",
      "#define CONV25_IM_CH     \t(512)\n",
      "#define CONV25_KER_DIM   \t(1)\n",
      "#define CONV25_L_PADDING \t(0)\n",
      "#define CONV25_R_PADDING \t(0)\n",
      "#define CONV25_T_PADDING \t(0)\n",
      "#define CONV25_B_PADDING \t(0)\n",
      "#define CONV25_STRIDE    \t(1)\n",
      "#define CONV25_OUT_CH    \t(1024)\n",
      "#define CONV25_OUT_DIM    \t(7)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "1 1\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 17 Topology Parameters /\n",
      "#define CONV17_IM_DIM    \t(14)\n",
      "#define CONV17_IM_CH     \t(512)\n",
      "#define CONV17_KER_DIM   \t(1)\n",
      "#define CONV17_L_PADDING \t(0)\n",
      "#define CONV17_R_PADDING \t(0)\n",
      "#define CONV17_T_PADDING \t(0)\n",
      "#define CONV17_B_PADDING \t(0)\n",
      "#define CONV17_STRIDE    \t(1)\n",
      "#define CONV17_OUT_CH    \t(512)\n",
      "#define CONV17_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 18 Topology Parameters /\n",
      "#define CONV18_IM_DIM    \t(14)\n",
      "#define CONV18_IM_CH     \t(512)\n",
      "#define CONV18_KER_DIM   \t(3)\n",
      "#define CONV18_L_PADDING \t(1)\n",
      "#define CONV18_R_PADDING \t(1)\n",
      "#define CONV18_T_PADDING \t(1)\n",
      "#define CONV18_B_PADDING \t(1)\n",
      "#define CONV18_STRIDE    \t(1)\n",
      "#define CONV18_OUT_CH    \t(512)\n",
      "#define CONV18_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 19 Topology Parameters /\n",
      "#define CONV19_IM_DIM    \t(14)\n",
      "#define CONV19_IM_CH     \t(512)\n",
      "#define CONV19_KER_DIM   \t(1)\n",
      "#define CONV19_L_PADDING \t(0)\n",
      "#define CONV19_R_PADDING \t(0)\n",
      "#define CONV19_T_PADDING \t(0)\n",
      "#define CONV19_B_PADDING \t(0)\n",
      "#define CONV19_STRIDE    \t(1)\n",
      "#define CONV19_OUT_CH    \t(512)\n",
      "#define CONV19_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 20 Topology Parameters /\n",
      "#define CONV20_IM_DIM    \t(14)\n",
      "#define CONV20_IM_CH     \t(512)\n",
      "#define CONV20_KER_DIM   \t(3)\n",
      "#define CONV20_L_PADDING \t(1)\n",
      "#define CONV20_R_PADDING \t(1)\n",
      "#define CONV20_T_PADDING \t(1)\n",
      "#define CONV20_B_PADDING \t(1)\n",
      "#define CONV20_STRIDE    \t(1)\n",
      "#define CONV20_OUT_CH    \t(512)\n",
      "#define CONV20_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 21 Topology Parameters /\n",
      "#define CONV21_IM_DIM    \t(14)\n",
      "#define CONV21_IM_CH     \t(512)\n",
      "#define CONV21_KER_DIM   \t(1)\n",
      "#define CONV21_L_PADDING \t(0)\n",
      "#define CONV21_R_PADDING \t(0)\n",
      "#define CONV21_T_PADDING \t(0)\n",
      "#define CONV21_B_PADDING \t(0)\n",
      "#define CONV21_STRIDE    \t(1)\n",
      "#define CONV21_OUT_CH    \t(512)\n",
      "#define CONV21_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 22 Topology Parameters /\n",
      "#define CONV22_IM_DIM    \t(14)\n",
      "#define CONV22_IM_CH     \t(512)\n",
      "#define CONV22_KER_DIM   \t(3)\n",
      "#define CONV22_L_PADDING \t(1)\n",
      "#define CONV22_R_PADDING \t(1)\n",
      "#define CONV22_T_PADDING \t(1)\n",
      "#define CONV22_B_PADDING \t(1)\n",
      "#define CONV22_STRIDE    \t(1)\n",
      "#define CONV22_OUT_CH    \t(512)\n",
      "#define CONV22_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 23 Topology Parameters /\n",
      "#define CONV23_IM_DIM    \t(14)\n",
      "#define CONV23_IM_CH     \t(512)\n",
      "#define CONV23_KER_DIM   \t(1)\n",
      "#define CONV23_L_PADDING \t(0)\n",
      "#define CONV23_R_PADDING \t(0)\n",
      "#define CONV23_T_PADDING \t(0)\n",
      "#define CONV23_B_PADDING \t(0)\n",
      "#define CONV23_STRIDE    \t(1)\n",
      "#define CONV23_OUT_CH    \t(512)\n",
      "#define CONV23_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 24 Topology Parameters /\n",
      "#define CONV24_IM_DIM    \t(14)\n",
      "#define CONV24_IM_CH     \t(512)\n",
      "#define CONV24_KER_DIM   \t(3)\n",
      "#define CONV24_L_PADDING \t(0)\n",
      "#define CONV24_R_PADDING \t(1)\n",
      "#define CONV24_T_PADDING \t(0)\n",
      "#define CONV24_B_PADDING \t(1)\n",
      "#define CONV24_STRIDE    \t(2)\n",
      "#define CONV24_OUT_CH    \t(512)\n",
      "#define CONV24_OUT_DIM    \t(7)\n",
      "\n",
      "\n",
      "/ Layer 25 Topology Parameters /\n",
      "#define CONV25_IM_DIM    \t(7)\n",
      "#define CONV25_IM_CH     \t(512)\n",
      "#define CONV25_KER_DIM   \t(1)\n",
      "#define CONV25_L_PADDING \t(0)\n",
      "#define CONV25_R_PADDING \t(0)\n",
      "#define CONV25_T_PADDING \t(0)\n",
      "#define CONV25_B_PADDING \t(0)\n",
      "#define CONV25_STRIDE    \t(1)\n",
      "#define CONV25_OUT_CH    \t(1024)\n",
      "#define CONV25_OUT_DIM    \t(7)\n",
      "\n",
      "\n",
      "/ Layer 26 Topology Parameters /\n",
      "#define CONV26_IM_DIM    \t(7)\n",
      "#define CONV26_IM_CH     \t(1024)\n",
      "#define CONV26_KER_DIM   \t(3)\n",
      "#define CONV26_L_PADDING \t(1)\n",
      "#define CONV26_R_PADDING \t(1)\n",
      "#define CONV26_T_PADDING \t(1)\n",
      "#define CONV26_B_PADDING \t(1)\n",
      "#define CONV26_STRIDE    \t(1)\n",
      "#define CONV26_OUT_CH    \t(1024)\n",
      "#define CONV26_OUT_DIM    \t(7)\n",
      "\n",
      "\n",
      "\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "0 0\n",
      "#ifndef __224.0_1_PARAMETERS_H__\n",
      "#define __224.0_1_PARAMETERS_H__\n",
      "\n",
      "\n",
      "/ Layer 1 Topology Parameters /\n",
      "#define CONV1_IM_DIM    \t(224)\n",
      "#define CONV1_IM_CH     \t(4)\n",
      "#define CONV1_KER_DIM   \t(3)\n",
      "#define CONV1_L_PADDING \t(0)\n",
      "#define CONV1_R_PADDING \t(1)\n",
      "#define CONV1_T_PADDING \t(0)\n",
      "#define CONV1_B_PADDING \t(1)\n",
      "#define CONV1_STRIDE    \t(2)\n",
      "#define CONV1_OUT_CH    \t(32)\n",
      "#define CONV1_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 2 Topology Parameters /\n",
      "#define CONV2_IM_DIM    \t(112)\n",
      "#define CONV2_IM_CH     \t(32)\n",
      "#define CONV2_KER_DIM   \t(3)\n",
      "#define CONV2_L_PADDING \t(1)\n",
      "#define CONV2_R_PADDING \t(1)\n",
      "#define CONV2_T_PADDING \t(1)\n",
      "#define CONV2_B_PADDING \t(1)\n",
      "#define CONV2_STRIDE    \t(1)\n",
      "#define CONV2_OUT_CH    \t(32)\n",
      "#define CONV2_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 3 Topology Parameters /\n",
      "#define CONV3_IM_DIM    \t(112)\n",
      "#define CONV3_IM_CH     \t(32)\n",
      "#define CONV3_KER_DIM   \t(1)\n",
      "#define CONV3_L_PADDING \t(0)\n",
      "#define CONV3_R_PADDING \t(0)\n",
      "#define CONV3_T_PADDING \t(0)\n",
      "#define CONV3_B_PADDING \t(0)\n",
      "#define CONV3_STRIDE    \t(1)\n",
      "#define CONV3_OUT_CH    \t(64)\n",
      "#define CONV3_OUT_DIM    \t(112)\n",
      "\n",
      "\n",
      "/ Layer 4 Topology Parameters /\n",
      "#define CONV4_IM_DIM    \t(112)\n",
      "#define CONV4_IM_CH     \t(64)\n",
      "#define CONV4_KER_DIM   \t(3)\n",
      "#define CONV4_L_PADDING \t(0)\n",
      "#define CONV4_R_PADDING \t(1)\n",
      "#define CONV4_T_PADDING \t(0)\n",
      "#define CONV4_B_PADDING \t(1)\n",
      "#define CONV4_STRIDE    \t(2)\n",
      "#define CONV4_OUT_CH    \t(64)\n",
      "#define CONV4_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 5 Topology Parameters /\n",
      "#define CONV5_IM_DIM    \t(56)\n",
      "#define CONV5_IM_CH     \t(64)\n",
      "#define CONV5_KER_DIM   \t(1)\n",
      "#define CONV5_L_PADDING \t(0)\n",
      "#define CONV5_R_PADDING \t(0)\n",
      "#define CONV5_T_PADDING \t(0)\n",
      "#define CONV5_B_PADDING \t(0)\n",
      "#define CONV5_STRIDE    \t(1)\n",
      "#define CONV5_OUT_CH    \t(128)\n",
      "#define CONV5_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 6 Topology Parameters /\n",
      "#define CONV6_IM_DIM    \t(56)\n",
      "#define CONV6_IM_CH     \t(128)\n",
      "#define CONV6_KER_DIM   \t(3)\n",
      "#define CONV6_L_PADDING \t(1)\n",
      "#define CONV6_R_PADDING \t(1)\n",
      "#define CONV6_T_PADDING \t(1)\n",
      "#define CONV6_B_PADDING \t(1)\n",
      "#define CONV6_STRIDE    \t(1)\n",
      "#define CONV6_OUT_CH    \t(128)\n",
      "#define CONV6_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 7 Topology Parameters /\n",
      "#define CONV7_IM_DIM    \t(56)\n",
      "#define CONV7_IM_CH     \t(128)\n",
      "#define CONV7_KER_DIM   \t(1)\n",
      "#define CONV7_L_PADDING \t(0)\n",
      "#define CONV7_R_PADDING \t(0)\n",
      "#define CONV7_T_PADDING \t(0)\n",
      "#define CONV7_B_PADDING \t(0)\n",
      "#define CONV7_STRIDE    \t(1)\n",
      "#define CONV7_OUT_CH    \t(128)\n",
      "#define CONV7_OUT_DIM    \t(56)\n",
      "\n",
      "\n",
      "/ Layer 8 Topology Parameters /\n",
      "#define CONV8_IM_DIM    \t(56)\n",
      "#define CONV8_IM_CH     \t(128)\n",
      "#define CONV8_KER_DIM   \t(3)\n",
      "#define CONV8_L_PADDING \t(0)\n",
      "#define CONV8_R_PADDING \t(1)\n",
      "#define CONV8_T_PADDING \t(0)\n",
      "#define CONV8_B_PADDING \t(1)\n",
      "#define CONV8_STRIDE    \t(2)\n",
      "#define CONV8_OUT_CH    \t(128)\n",
      "#define CONV8_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 9 Topology Parameters /\n",
      "#define CONV9_IM_DIM    \t(28)\n",
      "#define CONV9_IM_CH     \t(128)\n",
      "#define CONV9_KER_DIM   \t(1)\n",
      "#define CONV9_L_PADDING \t(0)\n",
      "#define CONV9_R_PADDING \t(0)\n",
      "#define CONV9_T_PADDING \t(0)\n",
      "#define CONV9_B_PADDING \t(0)\n",
      "#define CONV9_STRIDE    \t(1)\n",
      "#define CONV9_OUT_CH    \t(256)\n",
      "#define CONV9_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 10 Topology Parameters /\n",
      "#define CONV10_IM_DIM    \t(28)\n",
      "#define CONV10_IM_CH     \t(256)\n",
      "#define CONV10_KER_DIM   \t(3)\n",
      "#define CONV10_L_PADDING \t(1)\n",
      "#define CONV10_R_PADDING \t(1)\n",
      "#define CONV10_T_PADDING \t(1)\n",
      "#define CONV10_B_PADDING \t(1)\n",
      "#define CONV10_STRIDE    \t(1)\n",
      "#define CONV10_OUT_CH    \t(256)\n",
      "#define CONV10_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 11 Topology Parameters /\n",
      "#define CONV11_IM_DIM    \t(28)\n",
      "#define CONV11_IM_CH     \t(256)\n",
      "#define CONV11_KER_DIM   \t(1)\n",
      "#define CONV11_L_PADDING \t(0)\n",
      "#define CONV11_R_PADDING \t(0)\n",
      "#define CONV11_T_PADDING \t(0)\n",
      "#define CONV11_B_PADDING \t(0)\n",
      "#define CONV11_STRIDE    \t(1)\n",
      "#define CONV11_OUT_CH    \t(256)\n",
      "#define CONV11_OUT_DIM    \t(28)\n",
      "\n",
      "\n",
      "/ Layer 12 Topology Parameters /\n",
      "#define CONV12_IM_DIM    \t(28)\n",
      "#define CONV12_IM_CH     \t(256)\n",
      "#define CONV12_KER_DIM   \t(3)\n",
      "#define CONV12_L_PADDING \t(0)\n",
      "#define CONV12_R_PADDING \t(1)\n",
      "#define CONV12_T_PADDING \t(0)\n",
      "#define CONV12_B_PADDING \t(1)\n",
      "#define CONV12_STRIDE    \t(2)\n",
      "#define CONV12_OUT_CH    \t(256)\n",
      "#define CONV12_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 13 Topology Parameters /\n",
      "#define CONV13_IM_DIM    \t(14)\n",
      "#define CONV13_IM_CH     \t(256)\n",
      "#define CONV13_KER_DIM   \t(1)\n",
      "#define CONV13_L_PADDING \t(0)\n",
      "#define CONV13_R_PADDING \t(0)\n",
      "#define CONV13_T_PADDING \t(0)\n",
      "#define CONV13_B_PADDING \t(0)\n",
      "#define CONV13_STRIDE    \t(1)\n",
      "#define CONV13_OUT_CH    \t(512)\n",
      "#define CONV13_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 14 Topology Parameters /\n",
      "#define CONV14_IM_DIM    \t(14)\n",
      "#define CONV14_IM_CH     \t(512)\n",
      "#define CONV14_KER_DIM   \t(3)\n",
      "#define CONV14_L_PADDING \t(1)\n",
      "#define CONV14_R_PADDING \t(1)\n",
      "#define CONV14_T_PADDING \t(1)\n",
      "#define CONV14_B_PADDING \t(1)\n",
      "#define CONV14_STRIDE    \t(1)\n",
      "#define CONV14_OUT_CH    \t(512)\n",
      "#define CONV14_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 15 Topology Parameters /\n",
      "#define CONV15_IM_DIM    \t(14)\n",
      "#define CONV15_IM_CH     \t(512)\n",
      "#define CONV15_KER_DIM   \t(1)\n",
      "#define CONV15_L_PADDING \t(0)\n",
      "#define CONV15_R_PADDING \t(0)\n",
      "#define CONV15_T_PADDING \t(0)\n",
      "#define CONV15_B_PADDING \t(0)\n",
      "#define CONV15_STRIDE    \t(1)\n",
      "#define CONV15_OUT_CH    \t(512)\n",
      "#define CONV15_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 16 Topology Parameters /\n",
      "#define CONV16_IM_DIM    \t(14)\n",
      "#define CONV16_IM_CH     \t(512)\n",
      "#define CONV16_KER_DIM   \t(3)\n",
      "#define CONV16_L_PADDING \t(1)\n",
      "#define CONV16_R_PADDING \t(1)\n",
      "#define CONV16_T_PADDING \t(1)\n",
      "#define CONV16_B_PADDING \t(1)\n",
      "#define CONV16_STRIDE    \t(1)\n",
      "#define CONV16_OUT_CH    \t(512)\n",
      "#define CONV16_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 17 Topology Parameters /\n",
      "#define CONV17_IM_DIM    \t(14)\n",
      "#define CONV17_IM_CH     \t(512)\n",
      "#define CONV17_KER_DIM   \t(1)\n",
      "#define CONV17_L_PADDING \t(0)\n",
      "#define CONV17_R_PADDING \t(0)\n",
      "#define CONV17_T_PADDING \t(0)\n",
      "#define CONV17_B_PADDING \t(0)\n",
      "#define CONV17_STRIDE    \t(1)\n",
      "#define CONV17_OUT_CH    \t(512)\n",
      "#define CONV17_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 18 Topology Parameters /\n",
      "#define CONV18_IM_DIM    \t(14)\n",
      "#define CONV18_IM_CH     \t(512)\n",
      "#define CONV18_KER_DIM   \t(3)\n",
      "#define CONV18_L_PADDING \t(1)\n",
      "#define CONV18_R_PADDING \t(1)\n",
      "#define CONV18_T_PADDING \t(1)\n",
      "#define CONV18_B_PADDING \t(1)\n",
      "#define CONV18_STRIDE    \t(1)\n",
      "#define CONV18_OUT_CH    \t(512)\n",
      "#define CONV18_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 19 Topology Parameters /\n",
      "#define CONV19_IM_DIM    \t(14)\n",
      "#define CONV19_IM_CH     \t(512)\n",
      "#define CONV19_KER_DIM   \t(1)\n",
      "#define CONV19_L_PADDING \t(0)\n",
      "#define CONV19_R_PADDING \t(0)\n",
      "#define CONV19_T_PADDING \t(0)\n",
      "#define CONV19_B_PADDING \t(0)\n",
      "#define CONV19_STRIDE    \t(1)\n",
      "#define CONV19_OUT_CH    \t(512)\n",
      "#define CONV19_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 20 Topology Parameters /\n",
      "#define CONV20_IM_DIM    \t(14)\n",
      "#define CONV20_IM_CH     \t(512)\n",
      "#define CONV20_KER_DIM   \t(3)\n",
      "#define CONV20_L_PADDING \t(1)\n",
      "#define CONV20_R_PADDING \t(1)\n",
      "#define CONV20_T_PADDING \t(1)\n",
      "#define CONV20_B_PADDING \t(1)\n",
      "#define CONV20_STRIDE    \t(1)\n",
      "#define CONV20_OUT_CH    \t(512)\n",
      "#define CONV20_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 21 Topology Parameters /\n",
      "#define CONV21_IM_DIM    \t(14)\n",
      "#define CONV21_IM_CH     \t(512)\n",
      "#define CONV21_KER_DIM   \t(1)\n",
      "#define CONV21_L_PADDING \t(0)\n",
      "#define CONV21_R_PADDING \t(0)\n",
      "#define CONV21_T_PADDING \t(0)\n",
      "#define CONV21_B_PADDING \t(0)\n",
      "#define CONV21_STRIDE    \t(1)\n",
      "#define CONV21_OUT_CH    \t(512)\n",
      "#define CONV21_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 22 Topology Parameters /\n",
      "#define CONV22_IM_DIM    \t(14)\n",
      "#define CONV22_IM_CH     \t(512)\n",
      "#define CONV22_KER_DIM   \t(3)\n",
      "#define CONV22_L_PADDING \t(1)\n",
      "#define CONV22_R_PADDING \t(1)\n",
      "#define CONV22_T_PADDING \t(1)\n",
      "#define CONV22_B_PADDING \t(1)\n",
      "#define CONV22_STRIDE    \t(1)\n",
      "#define CONV22_OUT_CH    \t(512)\n",
      "#define CONV22_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 23 Topology Parameters /\n",
      "#define CONV23_IM_DIM    \t(14)\n",
      "#define CONV23_IM_CH     \t(512)\n",
      "#define CONV23_KER_DIM   \t(1)\n",
      "#define CONV23_L_PADDING \t(0)\n",
      "#define CONV23_R_PADDING \t(0)\n",
      "#define CONV23_T_PADDING \t(0)\n",
      "#define CONV23_B_PADDING \t(0)\n",
      "#define CONV23_STRIDE    \t(1)\n",
      "#define CONV23_OUT_CH    \t(512)\n",
      "#define CONV23_OUT_DIM    \t(14)\n",
      "\n",
      "\n",
      "/ Layer 24 Topology Parameters /\n",
      "#define CONV24_IM_DIM    \t(14)\n",
      "#define CONV24_IM_CH     \t(512)\n",
      "#define CONV24_KER_DIM   \t(3)\n",
      "#define CONV24_L_PADDING \t(0)\n",
      "#define CONV24_R_PADDING \t(1)\n",
      "#define CONV24_T_PADDING \t(0)\n",
      "#define CONV24_B_PADDING \t(1)\n",
      "#define CONV24_STRIDE    \t(2)\n",
      "#define CONV24_OUT_CH    \t(512)\n",
      "#define CONV24_OUT_DIM    \t(7)\n",
      "\n",
      "\n",
      "/ Layer 25 Topology Parameters /\n",
      "#define CONV25_IM_DIM    \t(7)\n",
      "#define CONV25_IM_CH     \t(512)\n",
      "#define CONV25_KER_DIM   \t(1)\n",
      "#define CONV25_L_PADDING \t(0)\n",
      "#define CONV25_R_PADDING \t(0)\n",
      "#define CONV25_T_PADDING \t(0)\n",
      "#define CONV25_B_PADDING \t(0)\n",
      "#define CONV25_STRIDE    \t(1)\n",
      "#define CONV25_OUT_CH    \t(1024)\n",
      "#define CONV25_OUT_DIM    \t(7)\n",
      "\n",
      "\n",
      "/ Layer 26 Topology Parameters /\n",
      "#define CONV26_IM_DIM    \t(7)\n",
      "#define CONV26_IM_CH     \t(1024)\n",
      "#define CONV26_KER_DIM   \t(3)\n",
      "#define CONV26_L_PADDING \t(1)\n",
      "#define CONV26_R_PADDING \t(1)\n",
      "#define CONV26_T_PADDING \t(1)\n",
      "#define CONV26_B_PADDING \t(1)\n",
      "#define CONV26_STRIDE    \t(1)\n",
      "#define CONV26_OUT_CH    \t(1024)\n",
      "#define CONV26_OUT_DIM    \t(7)\n",
      "\n",
      "\n",
      "/ Layer 27 Topology Parameters /\n",
      "#define CONV27_IM_DIM    \t(7)\n",
      "#define CONV27_IM_CH     \t(1024)\n",
      "#define CONV27_KER_DIM   \t(1)\n",
      "#define CONV27_L_PADDING \t(0)\n",
      "#define CONV27_R_PADDING \t(0)\n",
      "#define CONV27_T_PADDING \t(0)\n",
      "#define CONV27_B_PADDING \t(0)\n",
      "#define CONV27_STRIDE    \t(1)\n",
      "#define CONV27_OUT_CH    \t(1024)\n",
      "#define CONV27_OUT_DIM    \t(7)\n",
      "\n",
      "\n",
      "\n",
      "/ Layer 28 Topology Parameters /\n",
      "#define FC28_IM_CH     \t(1024)\n",
      "#define FC28_OUT_CH    \t(1000)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.linear_quantized_modules import ClippedLinearQuantization, LearnedClippedLinearQuantization, ScaledClippedLinearQuantization, Conv2d_SAME, ScaledClippedLinearQuantizationChannel\n",
    "\n",
    "depth_multiplier = 1\n",
    "x = torch.zeros(1,3,int(input_size),int(input_size)).fill_(1).cuda()\n",
    "si = 0\n",
    "so = 0\n",
    "model_quant = quantizer.deployment_model.cuda()\n",
    "\n",
    "def compute_padding_same( IM_DIM, KER_DIM, STRIDE, DILATION ):\n",
    "    input_rows = IM_DIM\n",
    "    filter_rows = KER_DIM\n",
    "    effective_filter_size_rows = (filter_rows - 1) * DILATION + 1\n",
    "    out_rows = (input_rows + STRIDE - 1) // STRIDE\n",
    "    padding_needed = max(0, (out_rows - 1) * STRIDE + effective_filter_size_rows -\n",
    "                  input_rows)\n",
    "    padding_rows = max(0, (out_rows - 1) * STRIDE +\n",
    "                        (filter_rows - 1) * DILATION + 1 - input_rows)\n",
    "    rows_odd = (padding_rows % 2 != 0)\n",
    "    padding_cols = max(0, (out_rows - 1) * STRIDE +\n",
    "                        (filter_rows - 1) * DILATION + 1 - input_rows)\n",
    "    cols_odd = (padding_rows % 2 != 0)\n",
    "\n",
    "    print(padding_cols//2, padding_rows//2)\n",
    "    if rows_odd or cols_odd:\n",
    "        return [int(padding_cols//2), int(padding_cols//2)+int(cols_odd), int(padding_rows//2), int(padding_rows//2)+int(rows_odd)]\n",
    "    else:\n",
    "        return [int(padding_cols//2), int(padding_cols//2), int(padding_rows//2), int(padding_rows//2) ]\n",
    "\n",
    "def print_bias(conv_layer, Zw):\n",
    "    out_ch = conv_layer.bias.data.size(0)\n",
    "    str_v = ' {'\n",
    "    for v in range(out_ch):\n",
    "        str_v += str(int(conv_layer.bias.data[v].item())) + ', '\n",
    "    str_v = str_v[:-2]+'}'\n",
    "    return str_v\n",
    "\n",
    "\n",
    "def print_weight(conv_layer, Zw):\n",
    "    out_ch, in_ch, k_w  = conv_layer.weight.data.size(0), conv_layer.weight.data.size(1), conv_layer.weight.data.size(2)\n",
    "    str_v = ''\n",
    "    for v in range(out_ch):\n",
    "        if v == 0:\n",
    "            str_v += ' {'\n",
    "        else:\n",
    "            str_v += ' '\n",
    "        for i in range(k_w):\n",
    "            for j in range(k_w):\n",
    "                for k in range(in_ch):\n",
    "                    str_v += str(int(conv_layer.weight.data[v][k][i][j].item() + Zw)) + ', '\n",
    "        str_v += '\\\\\\n'\n",
    "    str_v = str_v[:-4]+'}'\n",
    "    return str_v\n",
    "\n",
    "def print_weight_linear(conv_layer, Zw):\n",
    "    out_ch, in_ch  = conv_layer.weight.data.size(0), conv_layer.weight.data.size(1)\n",
    "    str_v = ''\n",
    "    for v in range(out_ch):\n",
    "        if v == 0:\n",
    "            str_v += ' {'\n",
    "        else:\n",
    "            str_v += ' '\n",
    "        for k in range(in_ch):\n",
    "            str_v += str(int(conv_layer.weight.data[v][k].item() + Zw)) + ', '\n",
    "        str_v += '\\\\\\n'\n",
    "    str_v = str_v[:-4]+'}'\n",
    "    return str_v\n",
    "\n",
    "\n",
    "def print_weight_depthwise(conv_layer, Zw):\n",
    "    out_ch, in_ch, k_w  = conv_layer.weight.data.size(0), conv_layer.weight.data.size(1), conv_layer.weight.data.size(2)\n",
    "    str_v = ''\n",
    "\n",
    "    str_v = ''\n",
    "    for i in range(k_w):\n",
    "        if i == 0:\n",
    "            str_v += ' {'\n",
    "        else:\n",
    "            str_v += ' '\n",
    "        for j in range(k_w):\n",
    "            for v in range(out_ch):\n",
    "                for k in range(1):\n",
    "                    str_v += str(int(conv_layer.weight.data[v][k][i][j].item()+ Zw )) + ', '\n",
    "        str_v += '\\\\\\n'\n",
    "    str_v = str_v[:-4]+'}'\n",
    "\n",
    "    return str_v\n",
    "\n",
    "def print_weight_RGB(conv_layer, Zw):\n",
    "    out_ch, in_ch, k_w  = conv_layer.weight.data.size(0), conv_layer.weight.data.size(1), conv_layer.weight.data.size(2)\n",
    "    in_ch = 4\n",
    "    str_v = ''\n",
    "    for v in range(out_ch):\n",
    "        if v == 0:\n",
    "            str_v += ' {'\n",
    "        else:\n",
    "            str_v += ' '\n",
    "        for i in range(k_w):\n",
    "            for j in range(k_w):\n",
    "                for k in range(in_ch):\n",
    "                    if k == 3:\n",
    "                        str_v += str(int(Zw)) + ', '\n",
    "                    else:\n",
    "                        str_v += str(int(conv_layer.weight.data[v][k][i][j].item() + Zw)) + ', '\n",
    "        str_v += '\\\\\\n'\n",
    "    str_v = str_v[:-4]+'}'\n",
    "    return str_v\n",
    "\n",
    "params_txt = '#ifndef __'+str(input_size).upper()+\"_\"+(str(depth_multiplier).upper()).replace('.', '_')+\"_PARAMETERS_H__\\n\"\n",
    "params_txt += '#define __'+str(input_size).upper()+\"_\"+(str(depth_multiplier).upper()).replace('.', '_')+\"_PARAMETERS_H__\\n\\n\\n\"\n",
    "\n",
    "weights_txt = '#ifndef __'+str(input_size).upper()+\"_\"+(str(depth_multiplier).upper()).replace('.', '_')+\"_WEIGHTS_BIAS_H__\\n\"\n",
    "weights_txt += '#define __'+str(input_size).upper()+\"_\"+(str(depth_multiplier).upper()).replace('.', '_')+\"_WEIGHTS_BIAS_H__\\n\\n\\n\"\n",
    "\n",
    "for i_g,layer in enumerate(quantizer.param_to_quantize):\n",
    "    i_l = i_g + 1\n",
    "    quant_type = layer['quant_type']\n",
    "    w_bits = layer['w_bits']\n",
    "    conv_layer = layer['conv']\n",
    "    bias_bits = layer['bias_bits']\n",
    "\n",
    "    quant_layer = layer['quant_conv']\n",
    "    layer_type = type(quant_layer)\n",
    "    \n",
    "    if layer_type in [Conv2d_SAME, nn.Conv2d]:\n",
    "        param_head = 'CONV' + str(i_l)+'_'\n",
    "        \n",
    "        print(layer_type)\n",
    "        def print_size(model, input, output): \n",
    "            global si, so\n",
    "            si = input[0].size()\n",
    "            so = output[0].size()\n",
    "            \n",
    "        hook = quant_layer.register_forward_hook(print_size)\n",
    "        model_quant(x)\n",
    "        hook.remove() \n",
    "        \n",
    "        IM_DIM = si[3]\n",
    "        IM_CH = si[1]\n",
    "        if IM_CH == 3:\n",
    "            IM_CH = 4\n",
    "            \n",
    "        KER_DIM = quant_layer.kernel_size[0]\n",
    "        \n",
    "        DILATION = quant_layer.dilation[0]\n",
    "        STRIDE =  quant_layer.stride[0]\n",
    "        OUT_CH = so[0]\n",
    "        OUT_DIM = so[2]\n",
    "        \n",
    "        if layer_type is Conv2d_SAME :\n",
    "            PADDING = compute_padding_same( IM_DIM, KER_DIM, STRIDE, DILATION )\n",
    "        else:\n",
    "            PADDING = [quant_layer.output_padding[0] for x in range(4) ]\n",
    "        \n",
    "        W_Z = quant_layer.weight.data.min().mul(-1).int().item()\n",
    "\n",
    "        if i_g == 0:\n",
    "            IN_Z = 127\n",
    "        else:\n",
    "            IN_Z = 0\n",
    "\n",
    "        if type(layer['quant_act']) is models.linear_quantized_modules.ScaledClippedLinearQuantization:\n",
    "            OUT_Z = 0\n",
    "            M_ZERO = int(layer['quant_act'].M_ZERO * (2**31) )\n",
    "            N_ZERO = (layer['quant_act'].N_ZERO * -1)-1\n",
    "            print('M_ZERO: ',M_ZERO)\n",
    "            print('M_ZERO: ',N_ZERO)\n",
    "\n",
    "        else:\n",
    "            OUT_Z = 0\n",
    "            M_ZERO = 55\n",
    "            N_ZERO = 55   \n",
    "            \n",
    "        params_txt += '/ Layer '+ str(i_l)+' Topology Parameters /\\n'\n",
    "        params_txt += '#define '+param_head+'IM_DIM    \\t('+str(IM_DIM)+')\\n'\n",
    "        params_txt += '#define '+param_head+'IM_CH     \\t('+str(IM_CH)+')\\n'\n",
    "        params_txt += '#define '+param_head+'KER_DIM   \\t('+str(KER_DIM)+')\\n'\n",
    "        params_txt += '#define '+param_head+'L_PADDING \\t('+str(PADDING[0])+')\\n'\n",
    "        params_txt += '#define '+param_head+'R_PADDING \\t('+str(PADDING[1])+')\\n'\n",
    "        params_txt += '#define '+param_head+'T_PADDING \\t('+str(PADDING[2])+')\\n'\n",
    "        params_txt += '#define '+param_head+'B_PADDING \\t('+str(PADDING[3])+')\\n'\n",
    "        params_txt += '#define '+param_head+'STRIDE    \\t('+str(STRIDE)+')\\n'\n",
    "        params_txt += '#define '+param_head+'OUT_CH    \\t('+str(OUT_CH)+')\\n'\n",
    "        params_txt += '#define '+param_head+'OUT_DIM    \\t('+str(OUT_DIM)+')\\n'\n",
    "        params_txt += '\\n'\n",
    "        params_txt += '\\n'\n",
    "        \n",
    "        weights_txt += '/ Layer '+ str(i_l)+' Learned Parameters /\\n'\n",
    "        if IM_CH == 4:\n",
    "            weights_txt += '#define '+param_head+'WT \\\\\\n'+print_weight_RGB(quant_layer, W_Z)+'\\n'\n",
    "        elif quant_layer.groups == IM_CH: # depthwise\n",
    "            weights_txt += '#define '+param_head+'WT \\\\\\n'+print_weight_depthwise(quant_layer, W_Z)+'\\n'\n",
    "        else:\n",
    "            weights_txt += '#define '+param_head+'WT \\\\\\n'+print_weight(quant_layer, W_Z)+'\\n'\n",
    "        weights_txt += '#define '+param_head+'BIAS \\\\\\n'+print_bias(quant_layer, W_Z)+'\\n'\n",
    "        weights_txt += '#define '+param_head+'W_Z       \\t('+str(W_Z)+')\\n'\n",
    "        weights_txt += '#define '+param_head+'IN_Z      \\t('+str(IN_Z)+')\\n'\n",
    "        weights_txt += '#define '+param_head+'OUT_Z     \\t('+str(OUT_Z)+')\\n'\n",
    "        weights_txt += '#define '+param_head+'M_ZERO    \\t('+str(M_ZERO)+')\\n'\n",
    "        weights_txt += '#define '+param_head+'N_ZERO    \\t('+str(N_ZERO)+')\\n'\n",
    "        weights_txt += '\\n'\n",
    "        weights_txt += '\\n'\n",
    "\n",
    "    elif layer_type is nn.AvgPool2d:\n",
    "        param_head = 'AVGPOOL' + str(i_l)+'_'\n",
    "        \n",
    "        def print_size(model, input, output): \n",
    "            global si, so\n",
    "            si = input[0].size()\n",
    "            so = output[0].size()\n",
    "                    \n",
    "        IM_DIM = si[3]\n",
    "        IM_CH = si[1]\n",
    "        if IM_CH == 3:\n",
    "            IM_CH = 4\n",
    "            \n",
    "        KER_DIM = quant_layer.kernel_size[0]\n",
    "        \n",
    "        DILATION = quant_layer.dilation[0]\n",
    "        STRIDE =  quant_layer.stride[0]\n",
    "        OUT_CH = so[0]\n",
    "        OUT_DIM = so[2]\n",
    "        \n",
    "        params_txt += '/* Layer '+ str(i_l)+' Topology Parameters */\\n'\n",
    "        params_txt += '#define '+param_head+'IM_DIM    \\t('+str(IM_DIM)+')\\n'\n",
    "        params_txt += '#define '+param_head+'IM_CH     \\t('+str(IM_CH)+')\\n'\n",
    "        params_txt += '#define '+param_head+'KER_DIM   \\t('+str(KER_DIM)+')\\n'\n",
    "        params_txt += '#define '+param_head+'STRIDE    \\t('+str(STRIDE)+')\\n'\n",
    "        params_txt += '#define '+param_head+'OUT_CH    \\t('+str(OUT_CH)+')\\n'\n",
    "        params_txt += '#define '+param_head+'OUT_DIM    \\t('+str(OUT_DIM)+')\\n'\n",
    "        params_txt += '\\n'\n",
    "        params_txt += '\\n'\n",
    "    \n",
    "    elif layer_type is nn.Linear:\n",
    "        param_head = 'FC' + str(i_l)+'_'\n",
    "        \n",
    "        def print_size(model, input, output): \n",
    "            global si, so\n",
    "            si = input[0].size()\n",
    "            so = output[0].size()\n",
    "            \n",
    "        hook = quant_layer.register_forward_hook(print_size)\n",
    "        model_quant(x)\n",
    "        hook.remove() \n",
    "        \n",
    "        IM_CH = quant_layer.in_features \n",
    "        OUT_CH = quant_layer.out_features \n",
    "\n",
    "        W_Z = quant_layer.weight.data.min().mul(-1).int().item()\n",
    "        IN_Z = 0\n",
    "        \n",
    "        M0_OUT = 1\n",
    "        N_exp_OUT = 0\n",
    "        M_ZERO = int(M0_OUT* (2**31) )\n",
    "        N_ZERO = (N_exp_OUT*-1)-1\n",
    "        \n",
    "        Z_OUT = 0\n",
    "        \n",
    "        params_txt = '/ Layer '+ str(i_l)+' Topology Parameters /\\n'\n",
    "        params_txt += '#define '+param_head+'IM_CH     \\t('+str(IM_CH)+')\\n'\n",
    "        params_txt += '#define '+param_head+'OUT_CH    \\t('+str(OUT_CH)+')\\n'\n",
    "        params_txt += '\\n'\n",
    "        params_txt += '\\n'\n",
    "#        weights_txt += '/ Layer '+ str(i_l)+' Learned Parameters /\\n'\n",
    "#       weights_txt += '#define '+param_head+'WT \\\\\\n'+print_weight_linear(quant_layer, W_Z)+'\\n'\n",
    "#      weights_txt += '#define '+param_head+'BIAS \\\\\\n'+print_bias(quant_layer, W_Z)+'\\n'\n",
    "#        weights_txt += '#define '+param_head+'W_Z       \\t('+str(W_Z)+')\\n'\n",
    "#        weights_txt += '#define '+param_head+'IN_Z      \\t('+str(IN_Z)+')\\n'\n",
    "#        weights_txt += '#define '+param_head+'OUT_Z     \\t('+str(Z_OUT)+')\\n'\n",
    "#        weights_txt += '#define '+param_head+'M_ZERO    \\t('+str(M_ZERO)+')\\n'\n",
    "#        weights_txt += '#define '+param_head+'N_ZERO    \\t('+str(N_ZERO)+')\\n'\n",
    "#        weights_txt += '\\n'\n",
    "#        weights_txt += '\\n'\n",
    "    \n",
    "    #layer['quant_conv'].bias.data    \n",
    "    print(params_txt)\n",
    "params_txt += '#endif /*__'+str(input_size).upper()+\"_\"+(str(depth_multiplier).upper()).replace('.', '_')+\"_PARAMETERS_H__*/\\n\"\n",
    "weights_txt += '#endif /*__'+str(input_size).upper()+\"_\"+(str(depth_multiplier).upper()).replace('.', '_')+\"_WEIGHTS_BIAS_H__*/\\n\"\n",
    "\n",
    "#print(params_txt)\n",
    "#print(weights_txt)\n",
    "\n",
    "f = open(str(input_size)+\"_\"+(str(depth_multiplier).upper()).replace('.', '_')+\"_parameters.h\", \"w\") \n",
    "f.write(params_txt) \n",
    "f.close()\n",
    "#f = open(str(input_size)+\"_\"+(str(depth_multiplier).upper()).replace('.', '_')+\"_weights_bias.h\", \"w\") \n",
    "#f.write(weights_txt) \n",
    "#f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'models.imagenet.mobilenet.mobilenet_quant_devel'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5902066230773926 | [N0]:  -8\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -112.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5704110860824585 | [N0]:  -1\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.6489582657814026 | [N0]:  -6\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -113.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.7270773649215698 | [N0]:  -1\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.9988483190536499 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.8034564852714539 | [N0]:  -4\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.8689922094345093 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.812638521194458 | [N0]:  -6\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  116.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.8525304198265076 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.912525475025177 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -122.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.798747181892395 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.577545166015625 | [N0]:  -6\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5441548824310303 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.8080103993415833 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.9887223839759827 | [N0]:  -8\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.7502198815345764 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.9428861141204834 | [N0]:  -8\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -127.0 | Max:  127.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.6555109024047852 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.61571204662323 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.6276821494102478 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.6247415542602539 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -122.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5368722081184387 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.8213418126106262 | [N0]:  -8\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -126.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.9964178204536438 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5672622919082642 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  34.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.6727413535118103 | [N0]:  -2\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -127.0 | Max:  127.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5086777210235596 | [N0]:  -7\n",
      "<class 'torch.nn.modules.pooling.AvgPool2d'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "[WEIGHT] Min:  -100.0 | Max:  127.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(quantizer.deployment_model.modules() ):\n",
    "    print(type(item))\n",
    "    if type(item) in [models.linear_quantized_modules.Conv2d_SAME,torch.nn.modules.linear.Linear]:\n",
    "        print('[WEIGHT] Min: ',item.weight.data.min().item(),'| Max: ',item.weight.data.max().item())\n",
    "        print('[BIAS] Min: ',item.bias.data.min().item(),'| Max: ',item.bias.data.max().item())\n",
    "    if type(item) in [models.linear_quantized_modules.ScaledClippedLinearQuantization]:\n",
    "        print('[M0] ',item.M_ZERO,'| [N0]: ',item.N_ZERO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.999999999328193"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(2**31-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test trained model\n",
    "forward(val_loader, quantizer_load.deployment_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([        nan,         nan,         nan,  2.3262e-43,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan, -2.3262e-43,         nan,         nan,  4.4491e+00,\n",
       "                nan,         nan, -2.3262e-43,  2.3262e-43, -2.3262e-43,\n",
       "         2.3262e-43,         nan,         nan, -2.3262e-43], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.model[1][1].running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.deployment_model.fc[0].bias.data.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.module.fc.bias.data/quantizer.deployment_model.fc[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:22: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:40: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][0/1563]\tTime 10.679 (10.679)\tData 0.815 (0.815)\tPrec@1 90.625 (90.625)\tPrec@5 93.750 (93.750)\n",
      "EVALUATING - Epoch: [0][100/1563]\tTime 0.099 (0.308)\tData 0.001 (0.105)\tPrec@1 9.375 (59.561)\tPrec@5 68.750 (85.210)\n",
      "EVALUATING - Epoch: [0][200/1563]\tTime 0.143 (0.295)\tData 0.001 (0.142)\tPrec@1 56.250 (59.142)\tPrec@5 87.500 (84.593)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-9:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-13:\n",
      "Process Process-15:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-11:\n",
      "Process Process-10:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Process Process-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 130, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 875, in convert\n",
      "    self.load()\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 103, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/ImageFile.py\", line 236, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py\", line 175, in __call__\n",
      "    return F.resize(img, self.size, self.interpolation)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n",
      "    img = Image.open(f)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/transforms/functional.py\", line 200, in resize\n",
      "    return img.resize((ow, oh), interpolation)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 1745, in resize\n",
      "    return self._new(self.im.resize(size, resample, box))\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "KeyboardInterrupt\n",
      "Process Process-14:\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 2539, in open\n",
      "    prefix = fp.read(16)\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 130, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 875, in convert\n",
      "    self.load()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/ImageFile.py\", line 214, in load\n",
      "    s = read(self.decodermaxblock)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "Process Process-16:\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n",
      "    img = Image.open(f)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 2539, in open\n",
      "    prefix = fp.read(16)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n",
      "    img = Image.open(f)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 2539, in open\n",
      "    prefix = fp.read(16)\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-4e564a0c285f>\", line 2, in <module>\n",
      "    forward(val_loader, model,quantizer=quantizer )\n",
      "  File \"<ipython-input-25-07ea60390c3a>\", line 18, in forward\n",
      "    for i, (inputs, target) in enumerate(data_loader):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 330, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 309, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/queue.py\", line 164, in get\n",
      "    self.not_empty.wait()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 293, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "RuntimeError: DataLoader worker (pid 11953) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 672, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 715, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 684, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 669, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/usr/lib/python3.5/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "SystemError: <built-in function _error_if_any_worker_fails> returned a result with an error set\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "forward(val_loader, model,quantizer=quantizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = model.module.model[0][0].weight.data.clone().clamp(quantizer.param_to_quantize[0]['w_min_thr'].data,quantizer.param_to_quantize[0]['w_max_thr'].data)\n",
    "w_min,w_max = weight_tensor.min().item(),weight_tensor.max().item()\n",
    "nlevels = ((2**8)-1)\n",
    "Sw= (w_max-w_min)/nlevels\n",
    "Sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.store_and_quantize(training=False )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer_load.deployment_model.model[0].weight.data.mul(Sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.model[0][0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.restore_real_value()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.param_to_quantize[27]['batch_norm'] is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = './results' #results dir\n",
    "save = 'imagenet/mobilenet_test_8bit' #saved folder\n",
    "dataset = 'imagenet' #dataset name or folder\n",
    "model_name = 'mobilenet' #model architecture\n",
    "\n",
    "input_size = 224 #image input size\n",
    "additional_config = str() #additional architecture configuration (need to be string str() )\n",
    "ttype = 'torch.cuda.FloatTensor' #type of tensor - e.g torch.cuda.HalfTensor\n",
    "gpus = '0' #gpus used for training - e.g 0,1,3\n",
    "workers = 8 #number of data loading workers\n",
    "epochs = 60 # number of total epochs to run\n",
    "start_epoch = 0 # manual epoch number (useful on restarts)\n",
    "batch_size = 1 # mini-batch size \n",
    "optimizer = 'SGD' # optimizer function used\n",
    "lr = 0.1 # initial learning rate\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4 #weight decay\n",
    "print_freq = 10 # print frequency\n",
    "resume = '' #path to latest checkpoint\n",
    "evaluate = False # evaluate model FILE on validation set\n",
    "save_check = False # saving the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup GPU devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cuda' in ttype:\n",
    "    gpus = [int(i) for i in gpus.split(',')]\n",
    "    print('Selected GPUs: ', gpus)\n",
    "    torch.cuda.set_device(gpus[0])\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    gpus = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained Real model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_quant = 'LinearAsymmetricLayer' #type of binarization process\n",
    "#type_quant = None\n",
    "weight_bits = 4\n",
    "activ_bits = 4\n",
    "activ_type = 'learned' \n",
    "i_dim = 224\n",
    "width_mult = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size=224,num_classes=1000,dataset='imagenet'\n",
      "4 4 LinearAsymmetricLayer\n",
      "This is a quantized Mobilenet with alpha=  1  input_size:  224  activation bits:  4  weight bits:  4  activation type:  learned\n",
      "mobilenet_quant_devel(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=2, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.__dict__[model_name]\n",
    "nClasses = get_num_classes(dataset)\n",
    "model_config = {'input_size': input_size, 'dataset': dataset, 'num_classes': nClasses, \\\n",
    "                'type_quant': type_quant, 'weight_bits': weight_bits, 'activ_bits': activ_bits,\\\n",
    "                'activ_type': activ_type, 'width_mult': width_mult, 'input_dim': i_dim }\n",
    "\n",
    "if additional_config is not '':\n",
    "    model_config = dict(model_config, **literal_eval(additional_config))\n",
    "\n",
    "model = model(**model_config)\n",
    "if model is None :\n",
    "    print('ERORRR')\n",
    "    \n",
    "#logging.info(\"created model with configuration: %s\", model_config)\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_quant_devel(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (22): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (24): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=2, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (25): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.type(ttype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.type(ttype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:563: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  \"please use transforms.RandomResizedCrop instead.\")\n",
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "default_transform = {\n",
    "    'train': get_transform(dataset,\n",
    "                           input_size=input_size, augment=True),\n",
    "    'eval': get_transform(dataset,\n",
    "                          input_size=input_size, augment=False)\n",
    "}\n",
    "transform = getattr(model, 'input_transform', default_transform)\n",
    "regime = getattr(model, 'regime', {0: {'optimizer': optimizer,\n",
    "                                       'lr': lr,\n",
    "                                       'momentum': momentum,\n",
    "                                       'weight_decay': weight_decay}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = get_dataset(dataset, 'val', transform['eval'])\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True)\n",
    "\n",
    "\n",
    "train_data = get_dataset(dataset, 'train', transform['train'])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = dict(model.named_parameters())\n",
    "params = []\n",
    "for key, value in params_dict.items():\n",
    "    if 'clip_val' in key:\n",
    "        params += [{'params':value,'weight_decay': 5e-4}]\n",
    "    else:\n",
    "        params += [{'params':value}]\n",
    "optimizer = torch.optim.SGD(params, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = adjust_optimizer(optimizer, 0, regime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 2\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 3\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 4\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 5\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 6\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 7\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 8\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 9\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 10\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 11\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 12\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 13\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 14\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 15\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 16\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 17\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 18\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 19\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 20\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 21\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 22\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 23\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 24\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 25\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 26\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 27\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 28\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 29\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 30\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 31\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 32\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 33\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 34\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 35\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 36\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 37\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 38\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 39\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 40\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 41\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 42\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 43\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 44\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 45\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 46\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 47\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 48\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 49\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 50\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 51\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 52\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 53\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 54\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 55\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 56\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 57\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 58\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 59\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 60\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 61\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 62\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 63\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 64\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 65\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 66\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 67\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 68\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 69\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 70\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 71\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 72\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 73\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 74\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 75\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 76\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 77\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 78\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 79\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 80\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 81\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 82\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 83\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 84\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 85\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 86\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 87\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 88\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 89\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 90\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 91\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 92\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 93\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 94\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 95\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 96\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 97\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 98\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 99\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 100\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 101\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 102\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 103\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 104\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 105\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 106\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 107\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 108\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 109\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_config = [{'layer':0,'a_bits':4,'w_bits':8},{'layer':5,'a_bits':5,'w_bits':8}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "test_str = '[{\"layer\":0,\"a_bits\":4,\"w_bits\":8},{\"layer\":5,\"a_bits\":5,\"w_bits\":8}]'\n",
    "print(type(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'layer': 0, 'a_bits': 4, 'w_bits': 8}, {'layer': 5, 'a_bits': 5, 'w_bits': 8}]\n"
     ]
    }
   ],
   "source": [
    "my_dict = json.loads(test_str)\n",
    "print (my_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Folding:  False Batch Folding Delay:  0 Type folding_thresh\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "mobilenet_quant_devel(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (3): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (4): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (5): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "    (7): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (8): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "    (11): ScaledThresholdsQuantization4d(M=1, n_thresholds=31, inplace)\n",
      "    (12): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (13): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (14): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "    (15): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (16): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (17): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "    (19): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (20): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (21): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "    (23): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (24): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (25): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (27): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (29): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (31): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (32): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (33): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (35): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (36): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (37): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (38): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (39): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (40): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (41): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (43): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (44): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (45): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "    (47): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (48): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (49): ScaledThresholdsQuantization4d(M=1, n_thresholds=3, inplace)\n",
      "    (50): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "    (51): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (52): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (53): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (54): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "quantizer = quantization.QuantOp(model, type_quant, weight_bits, batch_fold_delay=0, act_bits=activ_bits,add_config=add_config  )\n",
    "\n",
    "quantizer.deployment_model.type(ttype)\n",
    "quantizer.add_params_to_optimizer(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.init_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 module.model.0.0.weight\n",
      "1 module.model.0.1.weight\n",
      "2 module.model.0.1.bias\n",
      "3 module.model.0.1.running_mean\n",
      "4 module.model.0.1.running_var\n",
      "5 module.model.0.1.num_batches_tracked\n",
      "6 module.model.1.0.weight\n",
      "7 module.model.1.1.weight\n",
      "8 module.model.1.1.bias\n",
      "9 module.model.1.1.running_mean\n",
      "10 module.model.1.1.running_var\n",
      "11 module.model.1.1.num_batches_tracked\n",
      "12 module.model.2.0.weight\n",
      "13 module.model.2.1.weight\n",
      "14 module.model.2.1.bias\n",
      "15 module.model.2.1.running_mean\n",
      "16 module.model.2.1.running_var\n",
      "17 module.model.2.1.num_batches_tracked\n",
      "18 module.model.3.0.weight\n",
      "19 module.model.3.1.weight\n",
      "20 module.model.3.1.bias\n",
      "21 module.model.3.1.running_mean\n",
      "22 module.model.3.1.running_var\n",
      "23 module.model.3.1.num_batches_tracked\n",
      "24 module.model.4.0.weight\n",
      "25 module.model.4.1.weight\n",
      "26 module.model.4.1.bias\n",
      "27 module.model.4.1.running_mean\n",
      "28 module.model.4.1.running_var\n",
      "29 module.model.4.1.num_batches_tracked\n",
      "30 module.model.5.0.weight\n",
      "31 module.model.5.1.weight\n",
      "32 module.model.5.1.bias\n",
      "33 module.model.5.1.running_mean\n",
      "34 module.model.5.1.running_var\n",
      "35 module.model.5.1.num_batches_tracked\n",
      "36 module.model.6.0.weight\n",
      "37 module.model.6.1.weight\n",
      "38 module.model.6.1.bias\n",
      "39 module.model.6.1.running_mean\n",
      "40 module.model.6.1.running_var\n",
      "41 module.model.6.1.num_batches_tracked\n",
      "42 module.model.7.0.weight\n",
      "43 module.model.7.1.weight\n",
      "44 module.model.7.1.bias\n",
      "45 module.model.7.1.running_mean\n",
      "46 module.model.7.1.running_var\n",
      "47 module.model.7.1.num_batches_tracked\n",
      "48 module.model.8.0.weight\n",
      "49 module.model.8.1.weight\n",
      "50 module.model.8.1.bias\n",
      "51 module.model.8.1.running_mean\n",
      "52 module.model.8.1.running_var\n",
      "53 module.model.8.1.num_batches_tracked\n",
      "54 module.model.9.0.weight\n",
      "55 module.model.9.1.weight\n",
      "56 module.model.9.1.bias\n",
      "57 module.model.9.1.running_mean\n",
      "58 module.model.9.1.running_var\n",
      "59 module.model.9.1.num_batches_tracked\n",
      "60 module.model.10.0.weight\n",
      "61 module.model.10.1.weight\n",
      "62 module.model.10.1.bias\n",
      "63 module.model.10.1.running_mean\n",
      "64 module.model.10.1.running_var\n",
      "65 module.model.10.1.num_batches_tracked\n",
      "66 module.model.11.0.weight\n",
      "67 module.model.11.1.weight\n",
      "68 module.model.11.1.bias\n",
      "69 module.model.11.1.running_mean\n",
      "70 module.model.11.1.running_var\n",
      "71 module.model.11.1.num_batches_tracked\n",
      "72 module.model.12.0.weight\n",
      "73 module.model.12.1.weight\n",
      "74 module.model.12.1.bias\n",
      "75 module.model.12.1.running_mean\n",
      "76 module.model.12.1.running_var\n",
      "77 module.model.12.1.num_batches_tracked\n",
      "78 module.model.13.0.weight\n",
      "79 module.model.13.1.weight\n",
      "80 module.model.13.1.bias\n",
      "81 module.model.13.1.running_mean\n",
      "82 module.model.13.1.running_var\n",
      "83 module.model.13.1.num_batches_tracked\n",
      "84 module.model.14.0.weight\n",
      "85 module.model.14.1.weight\n",
      "86 module.model.14.1.bias\n",
      "87 module.model.14.1.running_mean\n",
      "88 module.model.14.1.running_var\n",
      "89 module.model.14.1.num_batches_tracked\n",
      "90 module.model.15.0.weight\n",
      "91 module.model.15.1.weight\n",
      "92 module.model.15.1.bias\n",
      "93 module.model.15.1.running_mean\n",
      "94 module.model.15.1.running_var\n",
      "95 module.model.15.1.num_batches_tracked\n",
      "96 module.model.16.0.weight\n",
      "97 module.model.16.1.weight\n",
      "98 module.model.16.1.bias\n",
      "99 module.model.16.1.running_mean\n",
      "100 module.model.16.1.running_var\n",
      "101 module.model.16.1.num_batches_tracked\n",
      "102 module.model.17.0.weight\n",
      "103 module.model.17.1.weight\n",
      "104 module.model.17.1.bias\n",
      "105 module.model.17.1.running_mean\n",
      "106 module.model.17.1.running_var\n",
      "107 module.model.17.1.num_batches_tracked\n",
      "108 module.model.18.0.weight\n",
      "109 module.model.18.1.weight\n",
      "110 module.model.18.1.bias\n",
      "111 module.model.18.1.running_mean\n",
      "112 module.model.18.1.running_var\n",
      "113 module.model.18.1.num_batches_tracked\n",
      "114 module.model.19.0.weight\n",
      "115 module.model.19.1.weight\n",
      "116 module.model.19.1.bias\n",
      "117 module.model.19.1.running_mean\n",
      "118 module.model.19.1.running_var\n",
      "119 module.model.19.1.num_batches_tracked\n",
      "120 module.model.20.0.weight\n",
      "121 module.model.20.1.weight\n",
      "122 module.model.20.1.bias\n",
      "123 module.model.20.1.running_mean\n",
      "124 module.model.20.1.running_var\n",
      "125 module.model.20.1.num_batches_tracked\n",
      "126 module.model.21.0.weight\n",
      "127 module.model.21.1.weight\n",
      "128 module.model.21.1.bias\n",
      "129 module.model.21.1.running_mean\n",
      "130 module.model.21.1.running_var\n",
      "131 module.model.21.1.num_batches_tracked\n",
      "132 module.model.22.0.weight\n",
      "133 module.model.22.1.weight\n",
      "134 module.model.22.1.bias\n",
      "135 module.model.22.1.running_mean\n",
      "136 module.model.22.1.running_var\n",
      "137 module.model.22.1.num_batches_tracked\n",
      "138 module.model.23.0.weight\n",
      "139 module.model.23.1.weight\n",
      "140 module.model.23.1.bias\n",
      "141 module.model.23.1.running_mean\n",
      "142 module.model.23.1.running_var\n",
      "143 module.model.23.1.num_batches_tracked\n",
      "144 module.model.24.0.weight\n",
      "145 module.model.24.1.weight\n",
      "146 module.model.24.1.bias\n",
      "147 module.model.24.1.running_mean\n",
      "148 module.model.24.1.running_var\n",
      "149 module.model.24.1.num_batches_tracked\n",
      "150 module.model.25.0.weight\n",
      "151 module.model.25.1.weight\n",
      "152 module.model.25.1.bias\n",
      "153 module.model.25.1.running_mean\n",
      "154 module.model.25.1.running_var\n",
      "155 module.model.25.1.num_batches_tracked\n",
      "156 module.model.26.0.weight\n",
      "157 module.model.26.1.weight\n",
      "158 module.model.26.1.bias\n",
      "159 module.model.26.1.running_mean\n",
      "160 module.model.26.1.running_var\n",
      "161 module.model.26.1.num_batches_tracked\n",
      "162 module.fc.weight\n",
      "163 module.fc.bias\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(checkpoint):\n",
    "    print(i,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----   End Initialization --------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = model.module.model[1][0].weight.data.clone()\n",
    "batch_layer = model.module.model[1][1]\n",
    "weight_tensor_size = model.module.model[1][0].weight.data.size()\n",
    "v0,v1,v2,v3 = weight_tensor_size\n",
    "eps = batch_layer.eps\n",
    "gamma_tensor = batch_layer.weight.data \n",
    "beta_tensor = batch_layer.bias.data \n",
    "mu_tensor = batch_layer.running_mean\n",
    "var_tensor = batch_layer.running_var\n",
    "\n",
    "#assuming convolution bias == False\n",
    "bias_tensor = mu_tensor.clone() \n",
    "bias_tensor = bias_tensor.mul(-1)\n",
    "\n",
    "#folded weight\n",
    "#var_tensor[var_tensor.le(0)] = 0       #correction to prevent case sigma >= 0\n",
    "if var_tensor.le(0).sum() >0:\n",
    "    print('erorr')\n",
    "sigma_tensor = var_tensor.add(eps).sqrt()\n",
    "gamma_over_sigma = gamma_tensor / sigma_tensor\n",
    "gamma_over_sigma_tensor = gamma_over_sigma.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand(v0,v1,v2,v3 )\n",
    "weight_tensor = weight_tensor * gamma_over_sigma_tensor\n",
    "bias_tensor = (bias_tensor*gamma_over_sigma)+beta_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.2468,  1.6286,  2.2804, -1.0486, 11.8503,  1.6791,  1.5009, -0.4797,\n",
       "         1.7677,  1.2378,  0.9371,  0.0826,  0.1209, -0.3763,  5.4261, 12.1442,\n",
       "        -0.0959,  1.0655,  1.7398,  8.4671,  2.4145,  1.1532,  9.0097,  4.7418,\n",
       "         0.9083,  0.6108,  1.9244, -2.9012,  1.7007,  0.9227,  0.9232,  0.7745],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_over_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.2468,  2.2468,  2.2468],\n",
       "          [ 2.2468,  2.2468,  2.2468],\n",
       "          [ 2.2468,  2.2468,  2.2468]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6286,  1.6286,  1.6286],\n",
       "          [ 1.6286,  1.6286,  1.6286],\n",
       "          [ 1.6286,  1.6286,  1.6286]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2804,  2.2804,  2.2804],\n",
       "          [ 2.2804,  2.2804,  2.2804],\n",
       "          [ 2.2804,  2.2804,  2.2804]]],\n",
       "\n",
       "\n",
       "        [[[-1.0486, -1.0486, -1.0486],\n",
       "          [-1.0486, -1.0486, -1.0486],\n",
       "          [-1.0486, -1.0486, -1.0486]]],\n",
       "\n",
       "\n",
       "        [[[11.8503, 11.8503, 11.8503],\n",
       "          [11.8503, 11.8503, 11.8503],\n",
       "          [11.8503, 11.8503, 11.8503]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6791,  1.6791,  1.6791],\n",
       "          [ 1.6791,  1.6791,  1.6791],\n",
       "          [ 1.6791,  1.6791,  1.6791]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5009,  1.5009,  1.5009],\n",
       "          [ 1.5009,  1.5009,  1.5009],\n",
       "          [ 1.5009,  1.5009,  1.5009]]],\n",
       "\n",
       "\n",
       "        [[[-0.4797, -0.4797, -0.4797],\n",
       "          [-0.4797, -0.4797, -0.4797],\n",
       "          [-0.4797, -0.4797, -0.4797]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7677,  1.7677,  1.7677],\n",
       "          [ 1.7677,  1.7677,  1.7677],\n",
       "          [ 1.7677,  1.7677,  1.7677]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2378,  1.2378,  1.2378],\n",
       "          [ 1.2378,  1.2378,  1.2378],\n",
       "          [ 1.2378,  1.2378,  1.2378]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9371,  0.9371,  0.9371],\n",
       "          [ 0.9371,  0.9371,  0.9371],\n",
       "          [ 0.9371,  0.9371,  0.9371]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0826,  0.0826,  0.0826],\n",
       "          [ 0.0826,  0.0826,  0.0826],\n",
       "          [ 0.0826,  0.0826,  0.0826]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1209,  0.1209,  0.1209],\n",
       "          [ 0.1209,  0.1209,  0.1209],\n",
       "          [ 0.1209,  0.1209,  0.1209]]],\n",
       "\n",
       "\n",
       "        [[[-0.3763, -0.3763, -0.3763],\n",
       "          [-0.3763, -0.3763, -0.3763],\n",
       "          [-0.3763, -0.3763, -0.3763]]],\n",
       "\n",
       "\n",
       "        [[[ 5.4261,  5.4261,  5.4261],\n",
       "          [ 5.4261,  5.4261,  5.4261],\n",
       "          [ 5.4261,  5.4261,  5.4261]]],\n",
       "\n",
       "\n",
       "        [[[12.1442, 12.1442, 12.1442],\n",
       "          [12.1442, 12.1442, 12.1442],\n",
       "          [12.1442, 12.1442, 12.1442]]],\n",
       "\n",
       "\n",
       "        [[[-0.0959, -0.0959, -0.0959],\n",
       "          [-0.0959, -0.0959, -0.0959],\n",
       "          [-0.0959, -0.0959, -0.0959]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0655,  1.0655,  1.0655],\n",
       "          [ 1.0655,  1.0655,  1.0655],\n",
       "          [ 1.0655,  1.0655,  1.0655]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7398,  1.7398,  1.7398],\n",
       "          [ 1.7398,  1.7398,  1.7398],\n",
       "          [ 1.7398,  1.7398,  1.7398]]],\n",
       "\n",
       "\n",
       "        [[[ 8.4671,  8.4671,  8.4671],\n",
       "          [ 8.4671,  8.4671,  8.4671],\n",
       "          [ 8.4671,  8.4671,  8.4671]]],\n",
       "\n",
       "\n",
       "        [[[ 2.4145,  2.4145,  2.4145],\n",
       "          [ 2.4145,  2.4145,  2.4145],\n",
       "          [ 2.4145,  2.4145,  2.4145]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1532,  1.1532,  1.1532],\n",
       "          [ 1.1532,  1.1532,  1.1532],\n",
       "          [ 1.1532,  1.1532,  1.1532]]],\n",
       "\n",
       "\n",
       "        [[[ 9.0097,  9.0097,  9.0097],\n",
       "          [ 9.0097,  9.0097,  9.0097],\n",
       "          [ 9.0097,  9.0097,  9.0097]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7418,  4.7418,  4.7418],\n",
       "          [ 4.7418,  4.7418,  4.7418],\n",
       "          [ 4.7418,  4.7418,  4.7418]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9083,  0.9083,  0.9083],\n",
       "          [ 0.9083,  0.9083,  0.9083],\n",
       "          [ 0.9083,  0.9083,  0.9083]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6108,  0.6108,  0.6108],\n",
       "          [ 0.6108,  0.6108,  0.6108],\n",
       "          [ 0.6108,  0.6108,  0.6108]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9244,  1.9244,  1.9244],\n",
       "          [ 1.9244,  1.9244,  1.9244],\n",
       "          [ 1.9244,  1.9244,  1.9244]]],\n",
       "\n",
       "\n",
       "        [[[-2.9012, -2.9012, -2.9012],\n",
       "          [-2.9012, -2.9012, -2.9012],\n",
       "          [-2.9012, -2.9012, -2.9012]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7007,  1.7007,  1.7007],\n",
       "          [ 1.7007,  1.7007,  1.7007],\n",
       "          [ 1.7007,  1.7007,  1.7007]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9227,  0.9227,  0.9227],\n",
       "          [ 0.9227,  0.9227,  0.9227],\n",
       "          [ 0.9227,  0.9227,  0.9227]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9232,  0.9232,  0.9232],\n",
       "          [ 0.9232,  0.9232,  0.9232],\n",
       "          [ 0.9232,  0.9232,  0.9232]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7745,  0.7745,  0.7745],\n",
       "          [ 0.7745,  0.7745,  0.7745],\n",
       "          [ 0.7745,  0.7745,  0.7745]]]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_over_sigma_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.BatchNorm2d(5,momentum=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(5, eps=1e-05, momentum=1, affine=True)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.0.post4'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(5, eps=1e-05, momentum=1, affine=True)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.9503\n",
       " 0.9339\n",
       " 0.7016\n",
       " 0.6154\n",
       " 0.1521\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma=net.weight.data\n",
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=net.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.3295\n",
       " 0.3252\n",
       " 0.2393\n",
       " 0.3410\n",
       " 0.5642\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean=net.running_mean\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=net.running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3724e03b482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'var' is not defined"
     ]
    }
   ],
   "source": [
    "sigma = var.add(net.eps).sqrt()\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 3.5944\n",
       " 2.8608\n",
       " 2.3078\n",
       " 2.3921\n",
       " 0.3541\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_over_sigma = gamma / sigma\n",
    "gamma_over_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.8459\n",
       "-0.4420\n",
       "-0.3899\n",
       " 0.5558\n",
       " 0.1298\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((x.data[0]-mean)*gamma_over_sigma)+beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0941  0.1706  0.0703  0.5733  0.9306\n",
       " 0.2950  0.1742  0.1702  0.1021  0.0853\n",
       " 0.2228  0.1414  0.0300  0.1352  0.9231\n",
       " 0.7059  0.8143  0.6866  0.5534  0.3178\n",
       "[torch.FloatTensor of size 4x5]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.rand(4,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.9767 -0.5104 -0.4502  0.6417  0.1498\n",
       "-0.1429 -0.4987 -0.1842 -0.6599 -0.1958\n",
       "-0.4428 -0.6068 -0.5577 -0.5684  0.1467\n",
       " 1.5624  1.6159  1.1921  0.5865 -0.1008\n",
       "[torch.FloatTensor of size 4x5]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = net(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen Batch Normalization Layers statistics!\n",
      "Layer:  0\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  1\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  2\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  3\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  4\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  5\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  6\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  7\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  8\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  9\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  10\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  11\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  12\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  13\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  14\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  15\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  16\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  17\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  18\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  19\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  20\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  21\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  22\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  23\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  24\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  25\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  26\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n"
     ]
    }
   ],
   "source": [
    "training = False\n",
    "model.eval()\n",
    "quantizer.apply_graph_transforms(0)\n",
    "quantizer.generate_deployment_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single input\n",
    "inputs = torch.randn(1,3,32,32)\n",
    "input_var = Variable(inputs.type(ttype))\n",
    "quantizer.store_and_quantize(training=training)\n",
    "output_1 = model(input_var)\n",
    "quantizer.restore_real_value()            \n",
    "output_2 = quantizer.deployment_model(input_var)\n",
    "values_1, indices_1 = output_1.max(1)\n",
    "values_2, indices_2 = output_2.max(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.4000, 0.0000, 0.0000,  ..., 0.0000, 0.4000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8000, 0.0000],\n",
       "          [0.8000, 0.0000, 0.0000,  ..., 0.8000, 0.8000, 0.0000],\n",
       "          ...,\n",
       "          [1.6000, 0.4000, 1.6000,  ..., 3.2000, 0.0000, 0.0000],\n",
       "          [1.6000, 0.0000, 0.8000,  ..., 3.6000, 0.8000, 0.0000],\n",
       "          [2.4000, 0.0000, 0.4000,  ..., 2.4000, 1.6000, 0.0000]],\n",
       "\n",
       "         [[2.0000, 0.0000, 0.4000,  ..., 0.0000, 0.8000, 0.0000],\n",
       "          [3.6000, 0.0000, 0.4000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [2.8000, 0.0000, 0.0000,  ..., 0.0000, 0.4000, 0.4000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4000,  ..., 0.0000, 0.4000, 0.0000],\n",
       "          [0.0000, 0.4000, 0.8000,  ..., 0.0000, 0.8000, 0.8000]]]],\n",
       "       device='cuda:0', grad_fn=<LearnedClippedLinearQuantizeSTEBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(model.module.model[0],model.module.model[1])(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
       "          [ 1.,  0.,  0.,  ...,  3.,  2.,  0.],\n",
       "          ...,\n",
       "          [ 5.,  0.,  4.,  ..., 10.,  0.,  0.],\n",
       "          [ 5.,  0.,  2.,  ..., 10.,  2.,  0.],\n",
       "          [ 6.,  0.,  0.,  ...,  6.,  3.,  0.]],\n",
       "\n",
       "         [[ 3.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 6.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 5.,  0.,  0.,  ...,  0.,  0.,  1.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  1.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  2.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = nn.Sequential(quantizer.deployment_model.model[0],quantizer.deployment_model.model[1],quantizer.deployment_model.model[2],quantizer.deployment_model.model[3])\n",
    "first(input_var)\n",
    "#quantizer.deployment_model.model[0](input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0269, -0.6061,  0.3412,  0.7959,  0.3926, -0.1135, -0.1445,  0.3798,\n",
       "        -0.5357, -0.5702], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.fc[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "0\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3\n",
      "Different!!!!  tensor([4.7786], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "4\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "5\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "6\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "7\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "8\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "9\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "10\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "11\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "12\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "13\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "14\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "15\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "16\n",
      "Different!!!!  tensor([4.2413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "17\n",
      "Different!!!!  tensor([9.5760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "18\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "19\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "20\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "21\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "22\n",
      "Different!!!!  tensor([6.7767], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "23\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "24\n",
      "Different!!!!  tensor([7.6344], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "25\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "26\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "27\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "28\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "29\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "30\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "31\n",
      "Different!!!!  tensor([6.3714], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "32\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "33\n",
      "Different!!!!  tensor([3.1857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "34\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "35\n",
      "Different!!!!  tensor([8.3696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "36\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "37\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "38\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "39\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "40\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "41\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "42\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "43\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "44\n",
      "Different!!!!  tensor([9.7551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "45\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "46\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "47\n",
      "Different!!!!  tensor([7.8512], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "48\n",
      "Different!!!!  tensor([3.6852], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "49\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "50\n",
      "Different!!!!  tensor([4.4298], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "51\n",
      "Different!!!!  tensor([6.7767], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "52\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "53\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "54\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "55\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "56\n",
      "Different!!!!  tensor([7.8606], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "57\n",
      "Different!!!!  tensor([7.3611], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "58\n",
      "Different!!!!  tensor([7.7946], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "59\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "60\n",
      "Different!!!!  tensor([7.7758], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "61\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "62\n",
      "Different!!!!  tensor([8.3790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "63\n",
      "Different!!!!  tensor([7.5496], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "64\n",
      "Different!!!!  tensor([8.3130], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "65\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "66\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "67\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "68\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "69\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "70\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "71\n",
      "Different!!!!  tensor([9.4912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "72\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "73\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "74\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "75\n",
      "Different!!!!  tensor([9.7739], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "76\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "77\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "78\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "79\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "80\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "81\n",
      "Different!!!!  tensor([4.1094], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "82\n",
      "Different!!!!  tensor([7.5590], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "83\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "84\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "85\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "86\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "87\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "88\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "89\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "90\n",
      "Different!!!!  tensor([4.4393], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "91\n",
      "Different!!!!  tensor([6.6070], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "92\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "93\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "94\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "95\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "96\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "97\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "98\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "99\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "100\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "101\n",
      "Different!!!!  tensor([7.8417], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "102\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "103\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "104\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "105\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "106\n",
      "Different!!!!  tensor([6.6070], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "107\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "108\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "109\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "110\n",
      "Different!!!!  tensor([7.2480], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "111\n",
      "Different!!!!  tensor([8.1434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "112\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "113\n",
      "Different!!!!  tensor([3.7889], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "114\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "115\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "116\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "117\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "118\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "119\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "120\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "121\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "122\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "123\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "124\n",
      "Different!!!!  tensor([7.4365], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "125\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "126\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "127\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "128\n",
      "Different!!!!  tensor([6.7767], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "129\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "130\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "131\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "132\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "133\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "134\n",
      "Different!!!!  tensor([7.7758], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "135\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "136\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "137\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "138\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "139\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "140\n",
      "Different!!!!  tensor([6.6825], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "141\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "142\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "143\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "144\n",
      "Different!!!!  tensor([3.8737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "145\n",
      "Different!!!!  tensor([7.6250], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "146\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "147\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "148\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "149\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "150\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "151\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "152\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "153\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "154\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "155\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "156\n",
      "Different!!!!  tensor([4.3544], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "157\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "158\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "159\n",
      "Different!!!!  tensor([6.3714], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "160\n",
      "Different!!!!  tensor([7.2197], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "161\n",
      "Different!!!!  tensor([8.2565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "162\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "163\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "164\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "165\n",
      "Different!!!!  tensor([6.9181], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "166\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "167\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "168\n",
      "Different!!!!  tensor([7.5873], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "169\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "170\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "171\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "172\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "173\n",
      "Different!!!!  tensor([7.5401], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "174\n",
      "Different!!!!  tensor([7.0406], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "175\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "176\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "177\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "178\n",
      "Different!!!!  tensor([6.4562], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "179\n",
      "Different!!!!  tensor([4.5147], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "180\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "181\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "182\n",
      "Different!!!!  tensor([8.1151], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "183\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "184\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "185\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "186\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "187\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "188\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "189\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "190\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "191\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "192\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "193\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "194\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "195\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "196\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "197\n",
      "Different!!!!  tensor([4.0151], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "198\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "199\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "200\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "201\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "202\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "203\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "204\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "205\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "206\n",
      "Different!!!!  tensor([4.1377], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "207\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "208\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "209\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "210\n",
      "Different!!!!  tensor([8.7277], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "211\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "212\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "213\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "214\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "215\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "216\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "217\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "218\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "219\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "220\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "221\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "222\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "223\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "224\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "225\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "226\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "227\n",
      "Different!!!!  tensor([7.1537], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "228\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "229\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "230\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "231\n",
      "Different!!!!  tensor([4.1094], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "232\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "233\n",
      "Different!!!!  tensor([6.4562], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "234\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "235\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "236\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "237\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "238\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "239\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "240\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "241\n",
      "Different!!!!  tensor([7.7852], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "242\n",
      "Different!!!!  tensor([4.3544], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "243\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "244\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "245\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "246\n",
      "Different!!!!  tensor([7.0406], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "247\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "248\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "249\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "250\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "251\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "252\n",
      "Different!!!!  tensor([8.3224], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "253\n",
      "Different!!!!  tensor([9.2367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "254\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "255\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "256\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "257\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "258\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "259\n",
      "Different!!!!  tensor([7.8889], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "260\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "261\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "262\n",
      "Different!!!!  tensor([3.8926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "263\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "264\n",
      "Different!!!!  tensor([4.2413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "265\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "266\n",
      "Different!!!!  tensor([8.0208], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "267\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "268\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "269\n",
      "Different!!!!  tensor([3.4402], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "270\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "271\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "272\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "273\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "274\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "275\n",
      "Different!!!!  tensor([3.7229], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "276\n",
      "Different!!!!  tensor([3.9586], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "277\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "278\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "279\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "280\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "281\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "282\n",
      "Different!!!!  tensor([7.7946], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "283\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "284\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "285\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "286\n",
      "Different!!!!  tensor([7.2480], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "287\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "288\n",
      "Different!!!!  tensor([7.0406], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "289\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "290\n",
      "Different!!!!  tensor([7.7286], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "291\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "292\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "293\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "294\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "295\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "296\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "297\n",
      "Different!!!!  tensor([8.7843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "298\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "299\n",
      "Different!!!!  tensor([4.1565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "300\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "301\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "302\n",
      "Different!!!!  tensor([7.0500], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "303\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "304\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "305\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "306\n",
      "Different!!!!  tensor([7.4176], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "307\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "308\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "309\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "310\n",
      "Different!!!!  tensor([8.3790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "311\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "312\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "313\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "314\n",
      "Different!!!!  tensor([4.2413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "315\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "316\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "317\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "318\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "319\n",
      "Different!!!!  tensor([7.7381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "320\n",
      "Different!!!!  tensor([7.1726], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "321\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "322\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "323\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "324\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "325\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "326\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "327\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "328\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "329\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "330\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "331\n",
      "Different!!!!  tensor([8.2753], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "332\n",
      "Different!!!!  tensor([7.1820], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "333\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "334\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "335\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "336\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "337\n",
      "Different!!!!  tensor([7.5213], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "338\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "339\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "340\n",
      "Different!!!!  tensor([7.6344], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "341\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "342\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "343\n",
      "Different!!!!  tensor([3.7606], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "344\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "345\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "346\n",
      "Different!!!!  tensor([7.3611], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "347\n",
      "Different!!!!  tensor([9.1896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "348\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "349\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "350\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "351\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "352\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "353\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "354\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "355\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "356\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "357\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "358\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "359\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "360\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "361\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "362\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "363\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "364\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "365\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "366\n",
      "Different!!!!  tensor([8.5109], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "367\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "368\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "369\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "370\n",
      "Different!!!!  tensor([4.0528], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "371\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "372\n",
      "Different!!!!  tensor([7.2762], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "373\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "374\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "375\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "376\n",
      "Different!!!!  tensor([7.1537], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "377\n",
      "Different!!!!  tensor([3.6475], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "378\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "379\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "380\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "381\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "382\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "383\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "384\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "385\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "386\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "387\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "388\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "389\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "390\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "391\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "392\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "393\n",
      "Different!!!!  tensor([7.7381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "394\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "395\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "396\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "397\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "398\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "399\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "400\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "401\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "402\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "403\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "404\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "405\n",
      "Different!!!!  tensor([8.3507], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "406\n",
      "Different!!!!  tensor([7.7098], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "407\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "408\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "409\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "410\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "411\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "412\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "413\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "414\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "415\n",
      "Different!!!!  tensor([7.4270], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "416\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "417\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "418\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "419\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "420\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "421\n",
      "Different!!!!  tensor([8.9728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "422\n",
      "Different!!!!  tensor([7.7286], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "423\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "424\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "425\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "426\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "427\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "428\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "429\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "430\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "431\n",
      "Different!!!!  tensor([7.2291], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "432\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "433\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "434\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "435\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "436\n",
      "Different!!!!  tensor([7.4742], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "437\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "438\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "439\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "440\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "441\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "442\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "443\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "444\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "445\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "446\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "447\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "448\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "449\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "450\n",
      "Different!!!!  tensor([3.8926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "451\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "452\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "453\n",
      "Different!!!!  tensor([7.8040], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "454\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "455\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "456\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "457\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "458\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "459\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "460\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "461\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "462\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "463\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "464\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "465\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "466\n",
      "Different!!!!  tensor([7.5590], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "467\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "468\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "469\n",
      "Different!!!!  tensor([8.3790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "470\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "471\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "472\n",
      "Different!!!!  tensor([3.4213], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "473\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "474\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "475\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "476\n",
      "Different!!!!  tensor([4.4487], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "477\n",
      "Different!!!!  tensor([4.3921], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "478\n",
      "Different!!!!  tensor([4.2602], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "479\n",
      "Different!!!!  tensor([9.1424], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "480\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "481\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "482\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "483\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "484\n",
      "Different!!!!  tensor([7.5496], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "485\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "486\n",
      "Different!!!!  tensor([8.3978], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "487\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "488\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "489\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "490\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "491\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "492\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "493\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "494\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "495\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "496\n",
      "Different!!!!  tensor([7.1820], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "497\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "498\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "499\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "500\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "501\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "502\n",
      "Different!!!!  tensor([7.2480], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "503\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "504\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "505\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "506\n",
      "Different!!!!  tensor([7.4176], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "507\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "508\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "509\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "510\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "511\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "512\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "513\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "514\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "515\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "516\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "517\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "518\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "519\n",
      "Different!!!!  tensor([7.7004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "520\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "521\n",
      "Different!!!!  tensor([4.2885], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "522\n",
      "Different!!!!  tensor([7.0595], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "523\n",
      "Different!!!!  tensor([4.1659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "524\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "525\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "526\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "527\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "528\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "529\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "530\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "531\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "532\n",
      "Different!!!!  tensor([4.4487], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "533\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "534\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "535\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "536\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "537\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "538\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "539\n",
      "Different!!!!  tensor([4.3733], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "540\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "541\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "542\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "543\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "544\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "545\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "546\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "547\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "548\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "549\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "550\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "551\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "552\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "553\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "554\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "555\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "556\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "557\n",
      "Different!!!!  tensor([6.8804], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "558\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "559\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "560\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "561\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "562\n",
      "Different!!!!  tensor([4.2790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "563\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "564\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "565\n",
      "Different!!!!  tensor([7.9643], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "566\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "567\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "568\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "569\n",
      "Different!!!!  tensor([7.2291], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "570\n",
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "571\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "572\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "573\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "574\n",
      "Different!!!!  tensor([7.4836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "575\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "576\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "577\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "578\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "579\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "580\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "581\n",
      "Different!!!!  tensor([4.0340], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "582\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "583\n",
      "Different!!!!  tensor([7.2103], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "584\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "585\n",
      "Different!!!!  tensor([7.5873], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "586\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "587\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "588\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "589\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "590\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "591\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "592\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "593\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "594\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "595\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "596\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "597\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "598\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "599\n",
      "Different!!!!  tensor([3.6193], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "600\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "601\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "602\n",
      "Different!!!!  tensor([3.3836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "603\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "604\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "605\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "606\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "607\n",
      "Different!!!!  tensor([8.6335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "608\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "609\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "610\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "611\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "612\n",
      "Different!!!!  tensor([4.1282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "613\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "614\n",
      "Different!!!!  tensor([3.4779], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "615\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "616\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "617\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "618\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "619\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "620\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "621\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "622\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "623\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "624\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "625\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "626\n",
      "Different!!!!  tensor([8.6994], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "627\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "628\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "629\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "630\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "631\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "632\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "633\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "634\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "635\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "636\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "637\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "638\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "639\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "640\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "641\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "642\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "643\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "644\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "645\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "646\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "647\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "648\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "649\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "650\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "651\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "652\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "653\n",
      "Different!!!!  tensor([6.6825], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "654\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "655\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "656\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "657\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "658\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "659\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "660\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "661\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "662\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "663\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "664\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "665\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "666\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "667\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "668\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "669\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "670\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "671\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "672\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "673\n",
      "Different!!!!  tensor([9.0576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "674\n",
      "Different!!!!  tensor([9.1801], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "675\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "676\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "677\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "678\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "679\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "680\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "681\n",
      "Different!!!!  tensor([4.3167], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "682\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "683\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "684\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "685\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "686\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "687\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "688\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "689\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "690\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "691\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "692\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "693\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "694\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "695\n",
      "Different!!!!  tensor([7.5401], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "696\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "697\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "698\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "699\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "700\n",
      "Different!!!!  tensor([4.2225], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "701\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "702\n",
      "Different!!!!  tensor([6.3337], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "703\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "704\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "705\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "706\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "707\n",
      "Different!!!!  tensor([7.4270], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "708\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "709\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "710\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "711\n",
      "Different!!!!  tensor([8.6523], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "712\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "713\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "714\n",
      "Different!!!!  tensor([7.2291], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "715\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "716\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "717\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "718\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "719\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "720\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "721\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "722\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "723\n",
      "Different!!!!  tensor([4.1659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "724\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "725\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "726\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "727\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "728\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "729\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "730\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "731\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "732\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "733\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "734\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "735\n",
      "Different!!!!  tensor([4.9482], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "736\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "737\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "738\n",
      "Different!!!!  tensor([8.1339], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "739\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "740\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "741\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "742\n",
      "Different!!!!  tensor([7.0218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "743\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "744\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "745\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "746\n",
      "Different!!!!  tensor([4.1471], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "747\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "748\n",
      "Different!!!!  tensor([4.1188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "749\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "750\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "751\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "752\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "753\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "754\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "755\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "756\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "757\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "758\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "759\n",
      "Different!!!!  tensor([5.2969], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "760\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "761\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "762\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "763\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "764\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "765\n",
      "Different!!!!  tensor([4.2319], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "766\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "767\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "768\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "769\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "770\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "771\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "772\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "773\n",
      "Different!!!!  tensor([9.2932], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "774\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "775\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "776\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "777\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "778\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "779\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "780\n",
      "Different!!!!  tensor([7.1537], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "781\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "782\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "783\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "784\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "785\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "786\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "787\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "788\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "789\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "790\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "791\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "792\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "793\n",
      "Different!!!!  tensor([7.6909], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "794\n",
      "Different!!!!  tensor([4.5241], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "795\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "796\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "797\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "798\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "799\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "800\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "801\n",
      "Different!!!!  tensor([7.7286], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "802\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "803\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "804\n",
      "Different!!!!  tensor([6.6825], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "805\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "806\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "807\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "808\n",
      "Different!!!!  tensor([7.7004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "809\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "810\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "811\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "812\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "813\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "814\n",
      "Different!!!!  tensor([7.6909], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "815\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "816\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "817\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "818\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "819\n",
      "Different!!!!  tensor([6.9181], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "820\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "821\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "822\n",
      "Different!!!!  tensor([3.8926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "823\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "824\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "825\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "826\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "827\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "828\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "829\n",
      "Different!!!!  tensor([6.2677], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "830\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "831\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "832\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "833\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "834\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "835\n",
      "Different!!!!  tensor([8.9916], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "836\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "837\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "838\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "839\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "840\n",
      "Different!!!!  tensor([7.8983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "841\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "842\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "843\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "844\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "845\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "846\n",
      "Different!!!!  tensor([7.8889], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "847\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "848\n",
      "Different!!!!  tensor([3.4025], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "849\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "850\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "851\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "852\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "853\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "854\n",
      "Different!!!!  tensor([4.1471], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "855\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "856\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "857\n",
      "Different!!!!  tensor([8.8125], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "858\n",
      "Different!!!!  tensor([9.5854], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "859\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "860\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "861\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "862\n",
      "Different!!!!  tensor([8.7183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "863\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "864\n",
      "Different!!!!  tensor([8.6335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "865\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "866\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "867\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "868\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "869\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "870\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "871\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "872\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "873\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "874\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "875\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "876\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "877\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "878\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "879\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "880\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "881\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "882\n",
      "Different!!!!  tensor([8.5863], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "883\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "884\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "885\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "886\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "887\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "888\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "889\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "890\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "891\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "892\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "893\n",
      "Different!!!!  tensor([3.4779], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "894\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "895\n",
      "Different!!!!  tensor([7.0500], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "896\n",
      "Different!!!!  tensor([3.2328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "897\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "898\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "899\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "900\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "901\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "902\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "903\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "904\n",
      "Different!!!!  tensor([7.7004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "905\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "906\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "907\n",
      "Different!!!!  tensor([7.9737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "908\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "909\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "910\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "911\n",
      "Different!!!!  tensor([7.6344], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "912\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "913\n",
      "Different!!!!  tensor([8.4450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "914\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "915\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "916\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "917\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "918\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "919\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "920\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "921\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "922\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "923\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "924\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "925\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "926\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "927\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "928\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "929\n",
      "Different!!!!  tensor([7.5496], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "930\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "931\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "932\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "933\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "934\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "935\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "936\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "937\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "938\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "939\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "940\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "941\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "942\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "943\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "944\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "945\n",
      "Different!!!!  tensor([7.6627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "946\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "947\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "948\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "949\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "950\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "951\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "952\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "953\n",
      "Different!!!!  tensor([3.4685], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "954\n",
      "Different!!!!  tensor([4.1282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "955\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "956\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "957\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "958\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "959\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "960\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "961\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "962\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "963\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "964\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "965\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "966\n",
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "967\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "968\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "969\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "970\n",
      "Different!!!!  tensor([7.4365], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "971\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "972\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "973\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "974\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "975\n",
      "Different!!!!  tensor([7.1160], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "976\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "977\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "978\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "979\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "980\n",
      "Different!!!!  tensor([3.7795], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "981\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "982\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "983\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "984\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "985\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "986\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "987\n",
      "Different!!!!  tensor([3.8455], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "988\n",
      "Different!!!!  tensor([4.6749], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "989\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "990\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "991\n",
      "Different!!!!  tensor([6.8804], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "992\n",
      "Different!!!!  tensor([8.5581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "993\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "994\n",
      "Different!!!!  tensor([7.1820], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "995\n",
      "Different!!!!  tensor([8.2093], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "996\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "997\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "998\n",
      "Different!!!!  tensor([7.6532], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "999\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1000\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1001\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1002\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1003\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1004\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1005\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1006\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1007\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1008\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1009\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1010\n",
      "Different!!!!  tensor([3.5533], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1011\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1012\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1013\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1014\n",
      "Different!!!!  tensor([3.3836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1015\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1016\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1017\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1018\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1019\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1020\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1021\n",
      "Different!!!!  tensor([4.0623], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1022\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1023\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1024\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1025\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1026\n",
      "Different!!!!  tensor([8.5675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1027\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1028\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1029\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1030\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1031\n",
      "Different!!!!  tensor([3.5816], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1032\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1033\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1034\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1035\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1036\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1037\n",
      "Different!!!!  tensor([7.6061], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1038\n",
      "Different!!!!  tensor([4.9482], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1039\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1040\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1041\n",
      "Different!!!!  tensor([7.5024], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1042\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1043\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1044\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1045\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1046\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1047\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1048\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1049\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1050\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1051\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1052\n",
      "Different!!!!  tensor([3.9963], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1053\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1054\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1055\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1056\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1057\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1058\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1059\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1060\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1061\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1062\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1063\n",
      "Different!!!!  tensor([3.9491], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1064\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1065\n",
      "Different!!!!  tensor([7.5024], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1066\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1067\n",
      "Different!!!!  tensor([3.5250], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1068\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1069\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1070\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1071\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1072\n",
      "Different!!!!  tensor([6.4091], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1073\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1074\n",
      "Different!!!!  tensor([8.1339], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1075\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1076\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1077\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1078\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1079\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1080\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1081\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1082\n",
      "Different!!!!  tensor([6.9746], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1083\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1084\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1085\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1086\n",
      "Different!!!!  tensor([6.7673], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1087\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1088\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1089\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1090\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1091\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1092\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1093\n",
      "Different!!!!  tensor([4.2602], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1094\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1095\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1096\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1097\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1098\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1099\n",
      "Different!!!!  tensor([3.8172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1100\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1101\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1102\n",
      "Different!!!!  tensor([8.2942], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1103\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1104\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1105\n",
      "Different!!!!  tensor([8.5109], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1106\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1107\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1108\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1109\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1110\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1111\n",
      "Different!!!!  tensor([4.0151], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1112\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1113\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1114\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1115\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1116\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1117\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1118\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1119\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1120\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1121\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1122\n",
      "Different!!!!  tensor([7.2762], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1123\n",
      "Different!!!!  tensor([7.0218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1124\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1125\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1126\n",
      "Different!!!!  tensor([3.6664], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1127\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1128\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1129\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1130\n",
      "Different!!!!  tensor([3.8549], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1131\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1132\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1133\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1134\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1135\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1136\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1137\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1138\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1139\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1140\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1141\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1142\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1143\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1144\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1145\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1146\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1147\n",
      "Different!!!!  tensor([7.4176], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1148\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1149\n",
      "Different!!!!  tensor([7.6815], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1150\n",
      "Different!!!!  tensor([6.6825], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1151\n",
      "Different!!!!  tensor([3.7041], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1152\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1153\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1154\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1155\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1156\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1157\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1158\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1159\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1160\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1161\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1162\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1163\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1164\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1165\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1166\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1167\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1168\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1169\n",
      "Different!!!!  tensor([3.9491], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1170\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1171\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1172\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1173\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1174\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1175\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1176\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1177\n",
      "Different!!!!  tensor([8.4921], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1178\n",
      "Different!!!!  tensor([6.3714], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1179\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1180\n",
      "Different!!!!  tensor([7.4082], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1181\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1182\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1183\n",
      "Different!!!!  tensor([7.7852], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1184\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1185\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1186\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1187\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1188\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1189\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1190\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1191\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1192\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1193\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1194\n",
      "Different!!!!  tensor([6.8992], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1195\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1196\n",
      "Different!!!!  tensor([3.8360], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1197\n",
      "Different!!!!  tensor([8.1622], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1198\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1199\n",
      "Different!!!!  tensor([6.7673], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1200\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1201\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1202\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1203\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1204\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1205\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1206\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1207\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1208\n",
      "Different!!!!  tensor([5.2593], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1209\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1210\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1211\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1212\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1213\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1214\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1215\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1216\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1217\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1218\n",
      "Different!!!!  tensor([8.2282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1219\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1220\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1221\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1222\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1223\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1224\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1225\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1226\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1227\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1228\n",
      "Different!!!!  tensor([3.8360], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1229\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1230\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1231\n",
      "Different!!!!  tensor([6.7673], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1232\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1233\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1234\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1235\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1236\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1237\n",
      "Different!!!!  tensor([6.2677], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1238\n",
      "Different!!!!  tensor([7.8983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1239\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1240\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1241\n",
      "Different!!!!  tensor([7.1160], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1242\n",
      "Different!!!!  tensor([4.1282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1243\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1244\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1245\n",
      "Different!!!!  tensor([8.1434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1246\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1247\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1248\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1249\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1250\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1251\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1252\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1253\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1254\n",
      "Different!!!!  tensor([4.2602], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1255\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1256\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1257\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1258\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1259\n",
      "Different!!!!  tensor([3.9397], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1260\n",
      "Different!!!!  tensor([4.1754], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1261\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1262\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1263\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1264\n",
      "Different!!!!  tensor([7.6627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1265\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1266\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1267\n",
      "Different!!!!  tensor([9.1518], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1268\n",
      "Different!!!!  tensor([8.8502], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1269\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1270\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1271\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1272\n",
      "Different!!!!  tensor([6.9181], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1273\n",
      "Different!!!!  tensor([8.0868], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1274\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1275\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1276\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1277\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1278\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1279\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1280\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1281\n",
      "Different!!!!  tensor([7.4270], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1282\n",
      "Different!!!!  tensor([4.1659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1283\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1284\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1285\n",
      "Different!!!!  tensor([7.5024], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1286\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1287\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1288\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1289\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1290\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1291\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1292\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1293\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1294\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1295\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1296\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1297\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1298\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1299\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1300\n",
      "Different!!!!  tensor([3.8737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1301\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1302\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1303\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1304\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1305\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1306\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1307\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1308\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1309\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1310\n",
      "Different!!!!  tensor([7.7381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1311\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1312\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1313\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1314\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1315\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1316\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1317\n",
      "Different!!!!  tensor([8.7277], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1318\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1319\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1320\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1321\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1322\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1323\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1324\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1325\n",
      "Different!!!!  tensor([8.0774], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1326\n",
      "Different!!!!  tensor([7.8135], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1327\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1328\n",
      "Different!!!!  tensor([8.1622], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1329\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1330\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1331\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1332\n",
      "Different!!!!  tensor([4.5241], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1333\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1334\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1335\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1336\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1337\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1338\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1339\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1340\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1341\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1342\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1343\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1344\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1345\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1346\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1347\n",
      "Different!!!!  tensor([6.4562], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1348\n",
      "Different!!!!  tensor([3.6852], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1349\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1350\n",
      "Different!!!!  tensor([8.6900], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1351\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1352\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1353\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1354\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1355\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1356\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1357\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1358\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1359\n",
      "Different!!!!  tensor([7.4365], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1360\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1361\n",
      "Different!!!!  tensor([7.8700], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1362\n",
      "Different!!!!  tensor([7.3611], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1363\n",
      "Different!!!!  tensor([8.3036], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1364\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1365\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1366\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1367\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1368\n",
      "Different!!!!  tensor([4.4298], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1369\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1370\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1371\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1372\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1373\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1374\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1375\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1376\n",
      "Different!!!!  tensor([4.1000], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1377\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1378\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1379\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1380\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1381\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1382\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1383\n",
      "Different!!!!  tensor([3.3554], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1384\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1385\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1386\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1387\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1388\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1389\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1390\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1391\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1392\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1393\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1394\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1395\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1396\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1397\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1398\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1399\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1400\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1401\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1402\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1403\n",
      "Different!!!!  tensor([7.2291], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1404\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1405\n",
      "Different!!!!  tensor([8.6712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1406\n",
      "Different!!!!  tensor([9.8964], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1407\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1408\n",
      "Different!!!!  tensor([8.1339], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1409\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1410\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1411\n",
      "Different!!!!  tensor([6.3714], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1412\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1413\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1414\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1415\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1416\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1417\n",
      "Different!!!!  tensor([6.4091], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1418\n",
      "Different!!!!  tensor([3.8832], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1419\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1420\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1421\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1422\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1423\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1424\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1425\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1426\n",
      "Different!!!!  tensor([8.0962], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1427\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1428\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1429\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1430\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1431\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1432\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1433\n",
      "Different!!!!  tensor([7.7192], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1434\n",
      "Different!!!!  tensor([7.9737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1435\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1436\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1437\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1438\n",
      "Different!!!!  tensor([7.3234], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1439\n",
      "Different!!!!  tensor([4.4298], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1440\n",
      "Different!!!!  tensor([4.1282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1441\n",
      "Different!!!!  tensor([8.3978], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1442\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1443\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1444\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1445\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1446\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1447\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1448\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1449\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1450\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1451\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1452\n",
      "Different!!!!  tensor([3.9209], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1453\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1454\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1455\n",
      "Different!!!!  tensor([3.5250], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1456\n",
      "Different!!!!  tensor([3.9963], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1457\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1458\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1459\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1460\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1461\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1462\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1463\n",
      "Different!!!!  tensor([4.3921], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1464\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1465\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1466\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1467\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1468\n",
      "Different!!!!  tensor([7.6627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1469\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1470\n",
      "Different!!!!  tensor([4.3733], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1471\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1472\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1473\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1474\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1475\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1476\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1477\n",
      "Different!!!!  tensor([3.6852], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1478\n",
      "Different!!!!  tensor([4.1471], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1479\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1480\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1481\n",
      "Different!!!!  tensor([3.9209], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1482\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1483\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1484\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1485\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1486\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1487\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1488\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1489\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1490\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1491\n",
      "Different!!!!  tensor([4.2225], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1492\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1493\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1494\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1495\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1496\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1497\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1498\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1499\n",
      "Different!!!!  tensor([8.4167], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1500\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1501\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1502\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1503\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1504\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1505\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1506\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1507\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1508\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1509\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1510\n",
      "Different!!!!  tensor([8.7748], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1511\n",
      "Different!!!!  tensor([7.3705], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1512\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1513\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1514\n",
      "Different!!!!  tensor([3.7795], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1515\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1516\n",
      "Different!!!!  tensor([4.2036], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1517\n",
      "Different!!!!  tensor([7.9266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1518\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1519\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1520\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1521\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1522\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1523\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1524\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1525\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1526\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1527\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1528\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1529\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1530\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1531\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1532\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1533\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1534\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1535\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1536\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1537\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1538\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1539\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1540\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1541\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1542\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1543\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1544\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1545\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1546\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1547\n",
      "Different!!!!  tensor([8.9445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1548\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1549\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1550\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1551\n",
      "Different!!!!  tensor([7.0877], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1552\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1553\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1554\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1555\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1556\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1557\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1558\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1559\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1560\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1561\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1562\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1563\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1564\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1565\n",
      "Different!!!!  tensor([8.1811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1566\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1567\n",
      "Different!!!!  tensor([4.2131], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1568\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1569\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1570\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1571\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1572\n",
      "Different!!!!  tensor([3.1857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1573\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1574\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1575\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1576\n",
      "Different!!!!  tensor([8.1245], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1577\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1578\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1579\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1580\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1581\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1582\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1583\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1584\n",
      "Different!!!!  tensor([7.4365], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1585\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1586\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1587\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1588\n",
      "Different!!!!  tensor([7.8417], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1589\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1590\n",
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1591\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1592\n",
      "Different!!!!  tensor([8.1057], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1593\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1594\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1595\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1596\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1597\n",
      "Different!!!!  tensor([4.3167], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1598\n",
      "Different!!!!  tensor([9.4158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1599\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1600\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1601\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1602\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1603\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1604\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1605\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1606\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1607\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1608\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1609\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1610\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1611\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1612\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1613\n",
      "Different!!!!  tensor([9.6985], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1614\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1615\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1616\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1617\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1618\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1619\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1620\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1621\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1622\n",
      "Different!!!!  tensor([6.3337], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1623\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1624\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1625\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1626\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1627\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1628\n",
      "Different!!!!  tensor([7.3799], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1629\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1630\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1631\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1632\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1633\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1634\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1635\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1636\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1637\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1638\n",
      "Different!!!!  tensor([4.1188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1639\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1640\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1641\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1642\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1643\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1644\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1645\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1646\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1647\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1648\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1649\n",
      "Different!!!!  tensor([7.5873], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1650\n",
      "Different!!!!  tensor([4.2319], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1651\n",
      "Different!!!!  tensor([7.8606], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1652\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1653\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1654\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1655\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1656\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1657\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1658\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1659\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1660\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1661\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1662\n",
      "Different!!!!  tensor([4.6749], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1663\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1664\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1665\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1666\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1667\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1668\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1669\n",
      "Different!!!!  tensor([6.9746], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1670\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1671\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1672\n",
      "Different!!!!  tensor([7.9077], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1673\n",
      "Different!!!!  tensor([7.8417], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1674\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1675\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1676\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1677\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1678\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1679\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1680\n",
      "Different!!!!  tensor([7.3988], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1681\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1682\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1683\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1684\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1685\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1686\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1687\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1688\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1689\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1690\n",
      "Different!!!!  tensor([8.0397], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1691\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1692\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1693\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1694\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1695\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1696\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1697\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1698\n",
      "Different!!!!  tensor([7.0595], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1699\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1700\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1701\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1702\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1703\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1704\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1705\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1706\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1707\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1708\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1709\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1710\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1711\n",
      "Different!!!!  tensor([4.0528], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1712\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1713\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1714\n",
      "Different!!!!  tensor([8.1811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1715\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1716\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1717\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1718\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1719\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1720\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1721\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1722\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1723\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1724\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1725\n",
      "Different!!!!  tensor([8.0114], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1726\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1727\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1728\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1729\n",
      "Different!!!!  tensor([6.4562], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1730\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1731\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1732\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1733\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1734\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1735\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1736\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1737\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1738\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1739\n",
      "Different!!!!  tensor([5.7117], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1740\n",
      "Different!!!!  tensor([3.9491], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1741\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1742\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1743\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1744\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1745\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1746\n",
      "Different!!!!  tensor([4.3921], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1747\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1748\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1749\n",
      "Different!!!!  tensor([4.2790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1750\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1751\n",
      "Different!!!!  tensor([8.5958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1752\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1753\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1754\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1755\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1756\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1757\n",
      "Different!!!!  tensor([6.6070], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1758\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1759\n",
      "Different!!!!  tensor([4.4393], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1760\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1761\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1762\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1763\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1764\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1765\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1766\n",
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1767\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1768\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1769\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1770\n",
      "Different!!!!  tensor([3.9586], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1771\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1772\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1773\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1774\n",
      "Different!!!!  tensor([4.1471], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1775\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1776\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1777\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1778\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1779\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1780\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1781\n",
      "Different!!!!  tensor([4.2790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1782\n",
      "Different!!!!  tensor([4.1565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1783\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1784\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1785\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1786\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1787\n",
      "Different!!!!  tensor([7.0877], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1788\n",
      "Different!!!!  tensor([8.6052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1789\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1790\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1791\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1792\n",
      "Different!!!!  tensor([7.8983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1793\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1794\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1795\n",
      "Different!!!!  tensor([6.9181], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1796\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1797\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1798\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1799\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1800\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1801\n",
      "Different!!!!  tensor([9.1330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1802\n",
      "Different!!!!  tensor([3.7606], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1803\n",
      "Different!!!!  tensor([7.6344], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1804\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1805\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1806\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1807\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1808\n",
      "Different!!!!  tensor([6.4562], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1809\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1810\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1811\n",
      "Different!!!!  tensor([7.2103], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1812\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1813\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1814\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1815\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1816\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1817\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1818\n",
      "Different!!!!  tensor([4.3450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1819\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1820\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1821\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1822\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1823\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1824\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1825\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1826\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1827\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1828\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1829\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1830\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1831\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1832\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1833\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1834\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1835\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1836\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1837\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1838\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1839\n",
      "Different!!!!  tensor([8.3130], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1840\n",
      "Different!!!!  tensor([7.8512], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1841\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1842\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1843\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1844\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1845\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1846\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1847\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1848\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1849\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1850\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1851\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1852\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1853\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1854\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1855\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1856\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1857\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1858\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1859\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1860\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1861\n",
      "Different!!!!  tensor([4.2413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1862\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1863\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1864\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1865\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1866\n",
      "Different!!!!  tensor([8.2282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1867\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1868\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1869\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1870\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1871\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1872\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1873\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1874\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1875\n",
      "Different!!!!  tensor([4.0623], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1876\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1877\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1878\n",
      "Different!!!!  tensor([6.7484], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1879\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1880\n",
      "Different!!!!  tensor([7.2103], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1881\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1882\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1883\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1884\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1885\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1886\n",
      "Different!!!!  tensor([4.1188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1887\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1888\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1889\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1890\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1891\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1892\n",
      "Different!!!!  tensor([7.2951], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1893\n",
      "Different!!!!  tensor([7.7381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1894\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1895\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1896\n",
      "Different!!!!  tensor([7.8040], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1897\n",
      "Different!!!!  tensor([8.4073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1898\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1899\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1900\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1901\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1902\n",
      "Different!!!!  tensor([7.5496], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1903\n",
      "Different!!!!  tensor([6.9746], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1904\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1905\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1906\n",
      "Different!!!!  tensor([4.2885], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1907\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1908\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1909\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1910\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1911\n",
      "Different!!!!  tensor([4.0717], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1912\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1913\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1914\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1915\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1916\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1917\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1918\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1919\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1920\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1921\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1922\n",
      "Different!!!!  tensor([7.6909], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1923\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1924\n",
      "Different!!!!  tensor([7.7475], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1925\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1926\n",
      "Different!!!!  tensor([6.3337], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1927\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1928\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1929\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1930\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1931\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1932\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1933\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1934\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1935\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1936\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1937\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1938\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1939\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1940\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1941\n",
      "Different!!!!  tensor([7.6627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1942\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1943\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1944\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1945\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1946\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1947\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1948\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1949\n",
      "Different!!!!  tensor([8.6712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1950\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1951\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1952\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1953\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1954\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1955\n",
      "Different!!!!  tensor([4.1188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1956\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1957\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1958\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1959\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1960\n",
      "Different!!!!  tensor([3.7229], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1961\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1962\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1963\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1964\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1965\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1966\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1967\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1968\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1969\n",
      "Different!!!!  tensor([7.9266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1970\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1971\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1972\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1973\n",
      "Different!!!!  tensor([4.5147], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1974\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1975\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1976\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1977\n",
      "Different!!!!  tensor([8.5769], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1978\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1979\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1980\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1981\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1982\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1983\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1984\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1985\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1986\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1987\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1988\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1989\n",
      "Different!!!!  tensor([7.8040], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1990\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1991\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1992\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1993\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1994\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1995\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1996\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1997\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1998\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1999\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2000\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2001\n",
      "Different!!!!  tensor([4.0340], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2002\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2003\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2004\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2005\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2006\n",
      "Different!!!!  tensor([7.4082], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2007\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2008\n",
      "Different!!!!  tensor([3.7229], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2009\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2010\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2011\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2012\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2013\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2014\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2015\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2016\n",
      "Different!!!!  tensor([4.0340], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2017\n",
      "Different!!!!  tensor([3.7324], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2018\n",
      "Different!!!!  tensor([3.7795], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2019\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2020\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2021\n",
      "Different!!!!  tensor([7.6250], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2022\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2023\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2024\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2025\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2026\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2027\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2028\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2029\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2030\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2031\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2032\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2033\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2034\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2035\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2036\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2037\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2038\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2039\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2040\n",
      "Different!!!!  tensor([8.4450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2041\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2042\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2043\n",
      "Different!!!!  tensor([7.2008], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2044\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2045\n",
      "Different!!!!  tensor([7.5119], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2046\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2047\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2048\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2049\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2050\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2051\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2052\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2053\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2054\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2055\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2056\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2057\n",
      "Different!!!!  tensor([9.5100], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2058\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2059\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2060\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2061\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2062\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2063\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2064\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2065\n",
      "Different!!!!  tensor([3.5062], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2066\n",
      "Different!!!!  tensor([6.7484], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2067\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2068\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2069\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2070\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2071\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2072\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2073\n",
      "Different!!!!  tensor([3.6664], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2074\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2075\n",
      "Different!!!!  tensor([8.4638], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2076\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2077\n",
      "Different!!!!  tensor([3.3931], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2078\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2079\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2080\n",
      "Different!!!!  tensor([3.8737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2081\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2082\n",
      "Different!!!!  tensor([6.2677], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2083\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2084\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2085\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2086\n",
      "Different!!!!  tensor([7.0218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2087\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2088\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2089\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2090\n",
      "Different!!!!  tensor([7.6532], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2091\n",
      "Different!!!!  tensor([7.2197], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2092\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2093\n",
      "Different!!!!  tensor([4.0811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2094\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2095\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2096\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2097\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2098\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2099\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2100\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2101\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2102\n",
      "Different!!!!  tensor([3.9963], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2103\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2104\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2105\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2106\n",
      "Different!!!!  tensor([9.0670], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2107\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2108\n",
      "Different!!!!  tensor([7.4647], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2109\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2110\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2111\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2112\n",
      "Different!!!!  tensor([8.2188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2113\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2114\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2115\n",
      "Different!!!!  tensor([8.0962], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2116\n",
      "Different!!!!  tensor([3.9586], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2117\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2118\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2119\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2120\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2121\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2122\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2123\n",
      "Different!!!!  tensor([4.0717], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2124\n",
      "Different!!!!  tensor([7.7004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2125\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2126\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2127\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2128\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2129\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2130\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2131\n",
      "Different!!!!  tensor([6.0698], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2132\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2133\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2134\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2135\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2136\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2137\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2138\n",
      "Different!!!!  tensor([3.5627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2139\n",
      "Different!!!!  tensor([7.8700], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2140\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2141\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2142\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2143\n",
      "Different!!!!  tensor([7.5496], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2144\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2145\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2146\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2147\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2148\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2149\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2150\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2151\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2152\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2153\n",
      "Different!!!!  tensor([3.8455], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2154\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2155\n",
      "Different!!!!  tensor([7.8040], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2156\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2157\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2158\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2159\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2160\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2161\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2162\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2163\n",
      "Different!!!!  tensor([7.6155], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2164\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2165\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2166\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2167\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2168\n",
      "Different!!!!  tensor([6.0698], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2169\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2170\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2171\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2172\n",
      "Different!!!!  tensor([8.0303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2173\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2174\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2175\n",
      "Different!!!!  tensor([4.1659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2176\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2177\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2178\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2179\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2180\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2181\n",
      "Different!!!!  tensor([3.7135], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2182\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2183\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2184\n",
      "Different!!!!  tensor([8.8220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2185\n",
      "Different!!!!  tensor([7.1820], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2186\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2187\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2188\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2189\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2190\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2191\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2192\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2193\n",
      "Different!!!!  tensor([7.9266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2194\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2195\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2196\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2197\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2198\n",
      "Different!!!!  tensor([4.1377], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2199\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2200\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2201\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2202\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2203\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2204\n",
      "Different!!!!  tensor([3.7983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2205\n",
      "Different!!!!  tensor([3.8266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2206\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2207\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2208\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2209\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2210\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2211\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2212\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2213\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2214\n",
      "Different!!!!  tensor([3.1574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2215\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2216\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2217\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2218\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2219\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2220\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2221\n",
      "Different!!!!  tensor([8.5863], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2222\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2223\n",
      "Different!!!!  tensor([3.5627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2224\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2225\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2226\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2227\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2228\n",
      "Different!!!!  tensor([8.5486], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2229\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2230\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2231\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2232\n",
      "Different!!!!  tensor([4.4298], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2233\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2234\n",
      "Different!!!!  tensor([6.7673], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2235\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2236\n",
      "Different!!!!  tensor([9.0199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2237\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2238\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2239\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2240\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2241\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2242\n",
      "Different!!!!  tensor([7.2103], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2243\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2244\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2245\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2246\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2247\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2248\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2249\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2250\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2251\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2252\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2253\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2254\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2255\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2256\n",
      "Different!!!!  tensor([3.8455], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2257\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2258\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2259\n",
      "Different!!!!  tensor([6.3337], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2260\n",
      "Different!!!!  tensor([4.2225], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2261\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2262\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2263\n",
      "Different!!!!  tensor([6.4091], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2264\n",
      "Different!!!!  tensor([8.0585], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2265\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2266\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2267\n",
      "Different!!!!  tensor([6.7484], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2268\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2269\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2270\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2271\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2272\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2273\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2274\n",
      "Different!!!!  tensor([9.1895], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2275\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2276\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2277\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2278\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2279\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2280\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2281\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2282\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2283\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2284\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2285\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2286\n",
      "Different!!!!  tensor([7.8983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2287\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2288\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2289\n",
      "Different!!!!  tensor([4.0811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2290\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2291\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2292\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2293\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2294\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2295\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2296\n",
      "Different!!!!  tensor([7.9643], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2297\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2298\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2299\n",
      "Different!!!!  tensor([6.8992], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2300\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2301\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2302\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2303\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2304\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2305\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2306\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2307\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2308\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2309\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2310\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2311\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2312\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2313\n",
      "Different!!!!  tensor([4.4487], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2314\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2315\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2316\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2317\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2318\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2319\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2320\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2321\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2322\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2323\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2324\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2325\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2326\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2327\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2328\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2329\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2330\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2331\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2332\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2333\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2334\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2335\n",
      "Different!!!!  tensor([7.3611], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2336\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2337\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2338\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2339\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2340\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2341\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2342\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2343\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2344\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2345\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2346\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2347\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2348\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2349\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2350\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2351\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2352\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2353\n",
      "Different!!!!  tensor([7.3234], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2354\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2355\n",
      "Different!!!!  tensor([4.2131], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2356\n",
      "Different!!!!  tensor([4.2885], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2357\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2358\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2359\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2360\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2361\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2362\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2363\n",
      "Different!!!!  tensor([4.3733], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2364\n",
      "Different!!!!  tensor([4.5147], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2365\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2366\n",
      "Different!!!!  tensor([7.2762], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2367\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2368\n",
      "Different!!!!  tensor([4.1942], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2369\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2370\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2371\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2372\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2373\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2374\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2375\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2376\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2377\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2378\n",
      "Different!!!!  tensor([7.5401], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2379\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2380\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2381\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2382\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2383\n",
      "Different!!!!  tensor([6.5694], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2384\n",
      "Different!!!!  tensor([8.4450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2385\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2386\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2387\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2388\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2389\n",
      "Different!!!!  tensor([7.4270], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2390\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2391\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2392\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2393\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2394\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2395\n",
      "Different!!!!  tensor([9.2178], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2396\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2397\n",
      "Different!!!!  tensor([8.7371], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2398\n",
      "Different!!!!  tensor([4.2225], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2399\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2400\n",
      "Different!!!!  tensor([7.6061], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2401\n",
      "Different!!!!  tensor([4.1754], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2402\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2403\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2404\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2405\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2406\n",
      "Different!!!!  tensor([7.0500], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2407\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2408\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2409\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2410\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2411\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2412\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2413\n",
      "Different!!!!  tensor([7.7569], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2414\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2415\n",
      "Different!!!!  tensor([7.5684], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2416\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2417\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2418\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2419\n",
      "Different!!!!  tensor([7.6532], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2420\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2421\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2422\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2423\n",
      "Different!!!!  tensor([7.7475], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2424\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2425\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2426\n",
      "Different!!!!  tensor([7.1160], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2427\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2428\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2429\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2430\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2431\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2432\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2433\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2434\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2435\n",
      "Different!!!!  tensor([7.5684], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2436\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2437\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2438\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2439\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2440\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2441\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2442\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2443\n",
      "Different!!!!  tensor([8.2093], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2444\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2445\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2446\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2447\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2448\n",
      "Different!!!!  tensor([6.4091], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2449\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2450\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2451\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2452\n",
      "Different!!!!  tensor([8.4355], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2453\n",
      "Different!!!!  tensor([4.1000], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2454\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2455\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2456\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2457\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2458\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2459\n",
      "Different!!!!  tensor([7.4836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2460\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2461\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2462\n",
      "Different!!!!  tensor([7.3234], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2463\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2464\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2465\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2466\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2467\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2468\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2469\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2470\n",
      "Different!!!!  tensor([4.1942], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2471\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2472\n",
      "Different!!!!  tensor([7.7946], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2473\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2474\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2475\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2476\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2477\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2478\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2479\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2480\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2481\n",
      "Different!!!!  tensor([8.3507], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2482\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2483\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2484\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2485\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2486\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2487\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2488\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2489\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2490\n",
      "Different!!!!  tensor([8.7089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2491\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2492\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2493\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2494\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2495\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2496\n",
      "Different!!!!  tensor([4.0434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2497\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2498\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2499\n",
      "Different!!!!  tensor([7.5590], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2500\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2501\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2502\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2503\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2504\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2505\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2506\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2507\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2508\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2509\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2510\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2511\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2512\n",
      "Different!!!!  tensor([7.7758], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2513\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2514\n",
      "Different!!!!  tensor([7.5119], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2515\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2516\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2517\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2518\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2519\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2520\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2521\n",
      "Different!!!!  tensor([6.0698], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2522\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2523\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2524\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2525\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2526\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2527\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2528\n",
      "Different!!!!  tensor([8.9916], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2529\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2530\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2531\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2532\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2533\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2534\n",
      "Different!!!!  tensor([7.9454], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2535\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2536\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2537\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2538\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2539\n",
      "Different!!!!  tensor([6.9746], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2540\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2541\n",
      "Different!!!!  tensor([7.6909], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2542\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2543\n",
      "Different!!!!  tensor([6.7767], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2544\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2545\n",
      "Different!!!!  tensor([8.7089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2546\n",
      "Different!!!!  tensor([4.9482], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2547\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2548\n",
      "Different!!!!  tensor([9.2084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2549\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2550\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2551\n",
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2552\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2553\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2554\n",
      "Different!!!!  tensor([2.9218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2555\n",
      "Different!!!!  tensor([7.0218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2556\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2557\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2558\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2559\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2560\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2561\n",
      "Different!!!!  tensor([4.2131], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2562\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2563\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2564\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2565\n",
      "Different!!!!  tensor([8.2093], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2566\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2567\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2568\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2569\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2570\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2571\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2572\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2573\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2574\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2575\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2576\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2577\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2578\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2579\n",
      "Different!!!!  tensor([3.5533], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2580\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2581\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2582\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2583\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2584\n",
      "Different!!!!  tensor([7.2008], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2585\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2586\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2587\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2588\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2589\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2590\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2591\n",
      "Different!!!!  tensor([3.9586], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2592\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2593\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2594\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2595\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2596\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2597\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2598\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2599\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2600\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2601\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2602\n",
      "Different!!!!  tensor([4.0151], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2603\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2604\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2605\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2606\n",
      "Different!!!!  tensor([7.4647], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2607\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2608\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2609\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2610\n",
      "Different!!!!  tensor([7.0595], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2611\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2612\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2613\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2614\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2615\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2616\n",
      "Different!!!!  tensor([10.0284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2617\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2618\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2619\n",
      "Different!!!!  tensor([3.9963], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2620\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2621\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2622\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2623\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2624\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2625\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2626\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2627\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2628\n",
      "Different!!!!  tensor([8.1057], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2629\n",
      "Different!!!!  tensor([7.5119], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2630\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2631\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2632\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2633\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2634\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2635\n",
      "Different!!!!  tensor([4.0151], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2636\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2637\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2638\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2639\n",
      "Different!!!!  tensor([4.1094], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2640\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2641\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2642\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2643\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2644\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2645\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2646\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2647\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2648\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2649\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2650\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2651\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2652\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2653\n",
      "Different!!!!  tensor([7.5024], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2654\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2655\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2656\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2657\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2658\n",
      "Different!!!!  tensor([7.7004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2659\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2660\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2661\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2662\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2663\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2664\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2665\n",
      "Different!!!!  tensor([3.7324], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2666\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2667\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2668\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2669\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2670\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2671\n",
      "Different!!!!  tensor([4.3450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2672\n",
      "Different!!!!  tensor([9.2555], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2673\n",
      "Different!!!!  tensor([8.2753], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2674\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2675\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2676\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2677\n",
      "Different!!!!  tensor([4.3544], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2678\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2679\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2680\n",
      "Different!!!!  tensor([7.3611], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2681\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2682\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2683\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2684\n",
      "Different!!!!  tensor([4.2413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2685\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2686\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2687\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2688\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2689\n",
      "Different!!!!  tensor([7.8229], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2690\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2691\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2692\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2693\n",
      "Different!!!!  tensor([3.8643], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2694\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2695\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2696\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2697\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2698\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2699\n",
      "Different!!!!  tensor([7.5684], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2700\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2701\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2702\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2703\n",
      "Different!!!!  tensor([8.3413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2704\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2705\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2706\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2707\n",
      "Different!!!!  tensor([3.8643], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2708\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2709\n",
      "Different!!!!  tensor([8.2565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2710\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2711\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2712\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2713\n",
      "Different!!!!  tensor([8.2093], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2714\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2715\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2716\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2717\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2718\n",
      "Different!!!!  tensor([6.7673], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2719\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2720\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2721\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2722\n",
      "Different!!!!  tensor([8.1622], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2723\n",
      "Different!!!!  tensor([6.0698], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2724\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2725\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2726\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2727\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2728\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2729\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2730\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2731\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2732\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2733\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2734\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2735\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2736\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2737\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2738\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2739\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2740\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2741\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2742\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2743\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2744\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2745\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2746\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2747\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2748\n",
      "Different!!!!  tensor([8.2188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2749\n",
      "Different!!!!  tensor([7.2291], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2750\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2751\n",
      "Different!!!!  tensor([7.8983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2752\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2753\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2754\n",
      "Different!!!!  tensor([7.6532], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2755\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2756\n",
      "Different!!!!  tensor([7.0218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2757\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2758\n",
      "Different!!!!  tensor([9.6608], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2759\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2760\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2761\n",
      "Different!!!!  tensor([7.7663], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2762\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2763\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2764\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2765\n",
      "Different!!!!  tensor([3.9869], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2766\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2767\n",
      "Different!!!!  tensor([7.6155], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2768\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2769\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2770\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2771\n",
      "Different!!!!  tensor([8.8314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2772\n",
      "Different!!!!  tensor([3.8643], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2773\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2774\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2775\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2776\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2777\n",
      "Different!!!!  tensor([6.9746], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2778\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2779\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2780\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2781\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2782\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2783\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2784\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2785\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2786\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2787\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2788\n",
      "Different!!!!  tensor([4.9482], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2789\n",
      "Different!!!!  tensor([7.4836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2790\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2791\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2792\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2793\n",
      "Different!!!!  tensor([7.7192], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2794\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2795\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2796\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2797\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2798\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2799\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2800\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2801\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2802\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2803\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2804\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2805\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2806\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2807\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2808\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2809\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2810\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2811\n",
      "Different!!!!  tensor([8.2470], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2812\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2813\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2814\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2815\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2816\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2817\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2818\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2819\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2820\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2821\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2822\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2823\n",
      "Different!!!!  tensor([7.2480], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2824\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2825\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2826\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2827\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2828\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2829\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2830\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2831\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2832\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2833\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2834\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2835\n",
      "Different!!!!  tensor([7.8323], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2836\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2837\n",
      "Different!!!!  tensor([7.1160], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2838\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2839\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2840\n",
      "Different!!!!  tensor([9.2744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2841\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2842\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2843\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2844\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2845\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2846\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2847\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2848\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2849\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2850\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2851\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2852\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2853\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2854\n",
      "Different!!!!  tensor([8.2753], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2855\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2856\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2857\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2858\n",
      "Different!!!!  tensor([8.0020], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2859\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2860\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2861\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2862\n",
      "Different!!!!  tensor([7.4836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2863\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2864\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2865\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2866\n",
      "Different!!!!  tensor([4.3733], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2867\n",
      "Different!!!!  tensor([5.7117], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2868\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2869\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2870\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2871\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2872\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2873\n",
      "Different!!!!  tensor([7.4647], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2874\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2875\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2876\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2877\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2878\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2879\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2880\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2881\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2882\n",
      "Different!!!!  tensor([7.3234], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2883\n",
      "Different!!!!  tensor([8.4732], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2884\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2885\n",
      "Different!!!!  tensor([7.8229], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2886\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2887\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2888\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2889\n",
      "Different!!!!  tensor([8.3036], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2890\n",
      "Different!!!!  tensor([8.2470], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2891\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2892\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2893\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2894\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2895\n",
      "Different!!!!  tensor([8.6712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2896\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2897\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2898\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2899\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2900\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2901\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2902\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2903\n",
      "Different!!!!  tensor([7.7098], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2904\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2905\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2906\n",
      "Different!!!!  tensor([4.3544], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2907\n",
      "Different!!!!  tensor([8.0020], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2908\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2909\n",
      "Different!!!!  tensor([7.8417], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2910\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2911\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2912\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2913\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2914\n",
      "Different!!!!  tensor([4.4487], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2915\n",
      "Different!!!!  tensor([8.0208], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2916\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2917\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2918\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2919\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2920\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2921\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2922\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2923\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2924\n",
      "Different!!!!  tensor([3.6381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2925\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2926\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2927\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2928\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2929\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2930\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2931\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2932\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2933\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2934\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2935\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2936\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2937\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2938\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2939\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2940\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2941\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2942\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2943\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2944\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2945\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2946\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2947\n",
      "Different!!!!  tensor([3.8926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2948\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2949\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2950\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2951\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2952\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2953\n",
      "Different!!!!  tensor([4.1000], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2954\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2955\n",
      "Different!!!!  tensor([7.2103], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2956\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2957\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2958\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2959\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2960\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2961\n",
      "Different!!!!  tensor([7.6721], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2962\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2963\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2964\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2965\n",
      "Different!!!!  tensor([7.6344], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2966\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2967\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2968\n",
      "Different!!!!  tensor([7.4742], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2969\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2970\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2971\n",
      "Different!!!!  tensor([7.6155], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2972\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2973\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2974\n",
      "Different!!!!  tensor([3.4025], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2975\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2976\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2977\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2978\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2979\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2980\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2981\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2982\n",
      "Different!!!!  tensor([4.0717], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2983\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2984\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2985\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2986\n",
      "Different!!!!  tensor([7.6061], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2987\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2988\n",
      "Different!!!!  tensor([3.9774], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2989\n",
      "Different!!!!  tensor([7.7475], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2990\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2991\n",
      "Different!!!!  tensor([7.7475], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2992\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2993\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2994\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2995\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2996\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2997\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2998\n",
      "Different!!!!  tensor([4.0528], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2999\n",
      "Different!!!!  tensor([6.8804], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3000\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3001\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3002\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3003\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3004\n",
      "Different!!!!  tensor([6.9181], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3005\n",
      "Different!!!!  tensor([3.8078], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3006\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3007\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3008\n",
      "Different!!!!  tensor([7.4270], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3009\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3010\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3011\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3012\n",
      "Different!!!!  tensor([6.2677], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3013\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3014\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3015\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3016\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3017\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3018\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3019\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3020\n",
      "Different!!!!  tensor([7.8700], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3021\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3022\n",
      "Different!!!!  tensor([8.2847], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3023\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3024\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3025\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3026\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3027\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3028\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3029\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3030\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3031\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3032\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3033\n",
      "Different!!!!  tensor([7.3988], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3034\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3035\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3036\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3037\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3038\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3039\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3040\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3041\n",
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3042\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3043\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3044\n",
      "Different!!!!  tensor([7.1726], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3045\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3046\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3047\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3048\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3049\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3050\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3051\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3052\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3053\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3054\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3055\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3056\n",
      "Different!!!!  tensor([7.5684], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3057\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3058\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3059\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3060\n",
      "Different!!!!  tensor([4.4298], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3061\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3062\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3063\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3064\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3065\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3066\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3067\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3068\n",
      "Different!!!!  tensor([7.7663], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3069\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3070\n",
      "Different!!!!  tensor([4.2131], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3071\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3072\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3073\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3074\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3075\n",
      "Different!!!!  tensor([4.4393], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3076\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3077\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3078\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3079\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3080\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3081\n",
      "Different!!!!  tensor([7.0500], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3082\n",
      "Different!!!!  tensor([8.2659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3083\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3084\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3085\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3086\n",
      "Different!!!!  tensor([8.4355], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3087\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3088\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3089\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3090\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3091\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3092\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3093\n",
      "Different!!!!  tensor([8.1622], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3094\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3095\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3096\n",
      "Different!!!!  tensor([7.3988], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3097\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3098\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3099\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3100\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3101\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3102\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3103\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3104\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3105\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3106\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3107\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3108\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3109\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3110\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3111\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3112\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3113\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3114\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3115\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3116\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3117\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3118\n",
      "Different!!!!  tensor([3.6570], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3119\n",
      "Different!!!!  tensor([8.2188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3120\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3121\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3122\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3123\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3124\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3125\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3126\n",
      "Different!!!!  tensor([8.5675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3127\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3128\n",
      "Different!!!!  tensor([8.2470], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3129\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3130\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3131\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3132\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3133\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3134\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3135\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3136\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3137\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3138\n",
      "Different!!!!  tensor([7.8700], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3139\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3140\n",
      "Different!!!!  tensor([7.7758], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3141\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3142\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3143\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3144\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3145\n",
      "Different!!!!  tensor([6.8050], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3146\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3147\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3148\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3149\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3150\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3151\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3152\n",
      "Different!!!!  tensor([7.5024], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3153\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3154\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3155\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3156\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3157\n",
      "Different!!!!  tensor([7.0500], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3158\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3159\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3160\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3161\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3162\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3163\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3164\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3165\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3166\n",
      "Different!!!!  tensor([9.0387], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3167\n",
      "Different!!!!  tensor([4.0246], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3168\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3169\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3170\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3171\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3172\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3173\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3174\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3175\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3176\n",
      "Different!!!!  tensor([7.4082], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3177\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3178\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3179\n",
      "Different!!!!  tensor([7.6815], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3180\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3181\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3182\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3183\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3184\n",
      "Different!!!!  tensor([3.6381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3185\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3186\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3187\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3188\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3189\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3190\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3191\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3192\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3193\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3194\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3195\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3196\n",
      "Different!!!!  tensor([8.0397], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3197\n",
      "Different!!!!  tensor([4.0811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3198\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3199\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3200\n",
      "Different!!!!  tensor([6.8050], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3201\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3202\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3203\n",
      "Different!!!!  tensor([7.2480], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3204\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3205\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3206\n",
      "Different!!!!  tensor([5.7117], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3207\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3208\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3209\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3210\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3211\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3212\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3213\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3214\n",
      "Different!!!!  tensor([3.6664], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3215\n",
      "Different!!!!  tensor([3.6287], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3216\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3217\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3218\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3219\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3220\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3221\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3222\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3223\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3224\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3225\n",
      "Different!!!!  tensor([3.6287], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3226\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3227\n",
      "Different!!!!  tensor([7.2008], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3228\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3229\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3230\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3231\n",
      "Different!!!!  tensor([4.3544], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3232\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3233\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3234\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3235\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3236\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3237\n",
      "Different!!!!  tensor([7.7663], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3238\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3239\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3240\n",
      "Different!!!!  tensor([4.5147], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3241\n",
      "Different!!!!  tensor([3.9869], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3242\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3243\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3244\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3245\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3246\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3247\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3248\n",
      "Different!!!!  tensor([4.0246], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3249\n",
      "Different!!!!  tensor([6.2677], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3250\n",
      "Different!!!!  tensor([8.3507], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3251\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3252\n",
      "Different!!!!  tensor([7.9266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3253\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3254\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3255\n",
      "Different!!!!  tensor([3.7135], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3256\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3257\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3258\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3259\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3260\n",
      "Different!!!!  tensor([3.7889], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3261\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3262\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3263\n",
      "Different!!!!  tensor([8.0020], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3264\n",
      "Different!!!!  tensor([8.3601], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3265\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3266\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3267\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3268\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3269\n",
      "Different!!!!  tensor([9.7174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3270\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3271\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3272\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3273\n",
      "Different!!!!  tensor([4.0434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3274\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3275\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3276\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3277\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3278\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3279\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3280\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3281\n",
      "Different!!!!  tensor([7.0877], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3282\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3283\n",
      "Different!!!!  tensor([4.1377], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3284\n",
      "Different!!!!  tensor([6.0698], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3285\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3286\n",
      "Different!!!!  tensor([7.8889], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3287\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3288\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3289\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3290\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3291\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3292\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3293\n",
      "Different!!!!  tensor([7.2762], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3294\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3295\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3296\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3297\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3298\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3299\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3300\n",
      "Different!!!!  tensor([4.3921], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3301\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3302\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3303\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3304\n",
      "Different!!!!  tensor([4.0811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3305\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3306\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3307\n",
      "Different!!!!  tensor([8.0020], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3308\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3309\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3310\n",
      "Different!!!!  tensor([4.5241], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3311\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3312\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3313\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3314\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3315\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3316\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3317\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3318\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3319\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3320\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3321\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3322\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3323\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3324\n",
      "Different!!!!  tensor([9.7928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3325\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3326\n",
      "Different!!!!  tensor([8.0397], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3327\n",
      "Different!!!!  tensor([8.2659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3328\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3329\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3330\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3331\n",
      "Different!!!!  tensor([8.3884], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3332\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3333\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3334\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3335\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3336\n",
      "Different!!!!  tensor([7.6438], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3337\n",
      "Different!!!!  tensor([4.1659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3338\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3339\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3340\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3341\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3342\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3343\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3344\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3345\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3346\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3347\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3348\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3349\n",
      "Different!!!!  tensor([7.4082], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3350\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3351\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3352\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3353\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3354\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3355\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3356\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3357\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3358\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3359\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3360\n",
      "Different!!!!  tensor([4.2885], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3361\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3362\n",
      "Different!!!!  tensor([7.6061], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3363\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3364\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3365\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3366\n",
      "Different!!!!  tensor([4.2131], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3367\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3368\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3369\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3370\n",
      "Different!!!!  tensor([8.1622], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3371\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3372\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3373\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3374\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3375\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3376\n",
      "Different!!!!  tensor([7.7663], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3377\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3378\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3379\n",
      "Different!!!!  tensor([4.1282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3380\n",
      "Different!!!!  tensor([7.9737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3381\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3382\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3383\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3384\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3385\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3386\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3387\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3388\n",
      "Different!!!!  tensor([4.0623], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3389\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3390\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3391\n",
      "Different!!!!  tensor([7.0406], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3392\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3393\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3394\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3395\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3396\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3397\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3398\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3399\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3400\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3401\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3402\n",
      "Different!!!!  tensor([7.4459], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3403\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3404\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3405\n",
      "Different!!!!  tensor([9.6043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3406\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3407\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3408\n",
      "Different!!!!  tensor([7.9266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3409\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3410\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3411\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3412\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3413\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3414\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3415\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3416\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3417\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3418\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3419\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3420\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3421\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3422\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3423\n",
      "Different!!!!  tensor([6.8804], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3424\n",
      "Different!!!!  tensor([4.2790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3425\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3426\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3427\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3428\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3429\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3430\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3431\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3432\n",
      "Different!!!!  tensor([7.1160], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3433\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3434\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3435\n",
      "Different!!!!  tensor([8.2565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3436\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3437\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3438\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3439\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3440\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3441\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3442\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3443\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3444\n",
      "Different!!!!  tensor([7.7758], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3445\n",
      "Different!!!!  tensor([6.3337], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3446\n",
      "Different!!!!  tensor([4.2319], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3447\n",
      "Different!!!!  tensor([7.9549], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3448\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3449\n",
      "Different!!!!  tensor([6.7767], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3450\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3451\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3452\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3453\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3454\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3455\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3456\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3457\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3458\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3459\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3460\n",
      "Different!!!!  tensor([3.9774], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3461\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3462\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3463\n",
      "Different!!!!  tensor([8.7183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3464\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3465\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3466\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3467\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3468\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3469\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3470\n",
      "Different!!!!  tensor([7.3705], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3471\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3472\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3473\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3474\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3475\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3476\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3477\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3478\n",
      "Different!!!!  tensor([8.2565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3479\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3480\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3481\n",
      "Different!!!!  tensor([8.3978], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3482\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3483\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3484\n",
      "Different!!!!  tensor([8.9539], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3485\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3486\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3487\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3488\n",
      "Different!!!!  tensor([7.6815], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3489\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3490\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3491\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3492\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3493\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3494\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3495\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3496\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3497\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3498\n",
      "Different!!!!  tensor([7.7286], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3499\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3500\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3501\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3502\n",
      "Different!!!!  tensor([4.0434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3503\n",
      "Different!!!!  tensor([6.6825], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3504\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3505\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3506\n",
      "Different!!!!  tensor([8.0020], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3507\n",
      "Different!!!!  tensor([8.7654], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3508\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3509\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3510\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3511\n",
      "Different!!!!  tensor([3.7512], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3512\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3513\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3514\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3515\n",
      "Different!!!!  tensor([7.3988], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3516\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3517\n",
      "Different!!!!  tensor([3.0820], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3518\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3519\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3520\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3521\n",
      "Different!!!!  tensor([7.7663], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3522\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3523\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3524\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3525\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3526\n",
      "Different!!!!  tensor([6.4091], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3527\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3528\n",
      "Different!!!!  tensor([7.9360], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3529\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3530\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3531\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3532\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3533\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3534\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3535\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3536\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3537\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3538\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3539\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3540\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3541\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3542\n",
      "Different!!!!  tensor([7.8323], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3543\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3544\n",
      "Different!!!!  tensor([4.1000], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3545\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3546\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3547\n",
      "Different!!!!  tensor([3.2517], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3548\n",
      "Different!!!!  tensor([4.0528], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3549\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3550\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3551\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3552\n",
      "Different!!!!  tensor([8.4167], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3553\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3554\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3555\n",
      "Different!!!!  tensor([3.8266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3556\n",
      "Different!!!!  tensor([7.7098], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3557\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3558\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3559\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3560\n",
      "Different!!!!  tensor([6.7484], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3561\n",
      "Different!!!!  tensor([7.5213], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3562\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3563\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3564\n",
      "Different!!!!  tensor([8.1339], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3565\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3566\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3567\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3568\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3569\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3570\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3571\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3572\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3573\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3574\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3575\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3576\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3577\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3578\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3579\n",
      "Different!!!!  tensor([4.4487], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3580\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3581\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3582\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3583\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3584\n",
      "Different!!!!  tensor([7.6155], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3585\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3586\n",
      "Different!!!!  tensor([4.7786], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3587\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3588\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3589\n",
      "Different!!!!  tensor([7.4459], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3590\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3591\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3592\n",
      "Different!!!!  tensor([6.6070], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3593\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3594\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3595\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3596\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3597\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3598\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3599\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3600\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3601\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3602\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3603\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3604\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3605\n",
      "Different!!!!  tensor([4.4016], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3606\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3607\n",
      "Different!!!!  tensor([8.5392], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3608\n",
      "Different!!!!  tensor([6.8050], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3609\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3610\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3611\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3612\n",
      "Different!!!!  tensor([7.3988], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3613\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3614\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3615\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3616\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3617\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3618\n",
      "Different!!!!  tensor([8.1811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3619\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3620\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3621\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3622\n",
      "Different!!!!  tensor([7.2951], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3623\n",
      "Different!!!!  tensor([7.5684], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3624\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3625\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3626\n",
      "Different!!!!  tensor([3.8737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3627\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3628\n",
      "Different!!!!  tensor([4.1942], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3629\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3630\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3631\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3632\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3633\n",
      "Different!!!!  tensor([8.4167], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3634\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3635\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3636\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3637\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3638\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3639\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3640\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3641\n",
      "Different!!!!  tensor([9.1047], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3642\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3643\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3644\n",
      "Different!!!!  tensor([4.3450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3645\n",
      "Different!!!!  tensor([6.3714], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3646\n",
      "Different!!!!  tensor([5.2593], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3647\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3648\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3649\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3650\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3651\n",
      "Different!!!!  tensor([7.9549], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3652\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3653\n",
      "Different!!!!  tensor([4.0434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3654\n",
      "Different!!!!  tensor([8.4450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3655\n",
      "Different!!!!  tensor([7.3705], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3656\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3657\n",
      "Different!!!!  tensor([3.8737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3658\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3659\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3660\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3661\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3662\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3663\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3664\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3665\n",
      "Different!!!!  tensor([3.7418], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3666\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3667\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3668\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3669\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3670\n",
      "Different!!!!  tensor([8.7371], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3671\n",
      "Different!!!!  tensor([7.4459], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3672\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3673\n",
      "Different!!!!  tensor([5.7117], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3674\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3675\n",
      "Different!!!!  tensor([7.2762], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3676\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3677\n",
      "Different!!!!  tensor([3.6004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3678\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3679\n",
      "Different!!!!  tensor([8.1057], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3680\n",
      "Different!!!!  tensor([7.0877], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3681\n",
      "Different!!!!  tensor([3.9491], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3682\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3683\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3684\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3685\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3686\n",
      "Different!!!!  tensor([3.9114], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3687\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3688\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3689\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3690\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3691\n",
      "Different!!!!  tensor([8.0868], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3692\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3693\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3694\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3695\n",
      "Different!!!!  tensor([4.2790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3696\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3697\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3698\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3699\n",
      "Different!!!!  tensor([6.5694], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3700\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3701\n",
      "Different!!!!  tensor([7.9831], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3702\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3703\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3704\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3705\n",
      "Different!!!!  tensor([8.4638], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3706\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3707\n",
      "Different!!!!  tensor([7.5401], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3708\n",
      "Different!!!!  tensor([7.9454], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3709\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3710\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3711\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3712\n",
      "Different!!!!  tensor([8.0868], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3713\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3714\n",
      "Different!!!!  tensor([8.9822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3715\n",
      "Different!!!!  tensor([4.1094], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3716\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-1116c6f90797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutput_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_real_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutput_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalues_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mvalues_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/quantized_neural_networks/models/cifar10/mobilenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compare fake-quantized validation and inference\n",
    "N = 10000\n",
    "error = 0\n",
    "for i in range(N):\n",
    "    #i, (inputs, target) = next(enumerate(val_loader))\n",
    "    inputs = torch.randn(1,3,32,32)\n",
    "    input_var = Variable(inputs.type(ttype))\n",
    "    quantizer.store_and_quantize(training=training)\n",
    "    output_1 = model(input_var)\n",
    "    quantizer.restore_real_value()            \n",
    "    output_2 = quantizer.deployment_model(input_var)\n",
    "    values_1, indices_1 = output_1.max(1)\n",
    "    values_2, indices_2 = output_2.max(1)\n",
    "    if indices_1[0].item() == indices_2[0].item():\n",
    "        pass\n",
    "    else:\n",
    "        print('Different!!!! ', values_1, values_2)\n",
    "        print(i)\n",
    "        error += 1\n",
    "\n",
    "if error == 0:\n",
    "    print('check OKKKK!!!!')\n",
    "else:\n",
    "    print('number of errors', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [1, 1],\n",
       "        [1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x > 0)| (x > -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -2.1960],\n",
       "        [ 0.4333,  0.0000],\n",
       "        [ 0.3283,  0.3488]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.masked_fill_(x>0.5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -2.1960],\n",
       "        [ 0.4333,  0.0000],\n",
       "        [ 0.3283,  0.3488]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_quant_devel(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (22): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (24): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (25): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2], device='cuda:0') tensor([5], device='cuda:0')\n",
      "tensor([[ 1.0216,  1.0125,  2.6960,  1.9408, -1.0364,  2.6033, -0.5207, -0.8645,\n",
      "         -5.0692, -1.7818]], device='cuda:0', grad_fn=<ThAddmmBackward>)\n",
      "tensor([[  7290.0000,   7771.5000,  20460.5000,  15013.5000,  -7461.7500,\n",
      "          20608.2500,  -4499.2500,  -6394.0000, -39368.0000, -13401.0000]],\n",
      "       device='cuda:0', grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(indices_1, indices_2)\n",
    "print(output_1)\n",
    "print(output_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.store_and_quantize(training=training)\n",
    "bias_rr = model.module.fc.bias.data.clone()\n",
    "oo1 = model(input_var)- bias_rr\n",
    "quantizer.restore_real_value()\n",
    "oo2 = quantizer.deployment_model(input_var)  - quantizer.deployment_model.fc[0].bias.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([[ 1.6300,  3.8400,  2.8988, -1.6477, -1.8950, -0.0602,  2.2296, -1.1928,\n",
      "         -3.9970, -1.8142]], device='cuda:0', grad_fn=<ThSubBackward>)\n",
      "tensor([[ 12508.5000,  29883.5000,  21955.5000, -12784.5000, -13882.0000,\n",
      "           -348.2500,  16404.2500,  -8747.5000, -31339.2500, -13704.2500]],\n",
      "       device='cuda:0', grad_fn=<ThSubBackward>)\n",
      "tensor([ 0.0271, -0.6058,  0.3413,  0.7958,  0.3924, -0.1132, -0.1445,  0.3799,\n",
      "        -0.5359, -0.5703], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "oo_values_1, oo_indices_1 = oo1.max(1)\n",
    "oo_values_2, oo_indices_2 = oo2.max(1)\n",
    "print(oo_indices_1, oo_indices_2)\n",
    "print(oo1)\n",
    "print(oo2)\n",
    "print(bias_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([[ 1.6300,  3.8400,  2.8988, -1.6477, -1.8950, -0.0602,  2.2296, -1.1928,\n",
      "         -3.9970, -1.8142]], device='cuda:0', grad_fn=<ThSubBackward>)\n",
      "tensor([[ 12508.5000,  29883.5000,  21955.5000, -12784.5000, -13882.0000,\n",
      "           -348.2500,  16404.2500,  -8747.5000, -31339.2500, -13704.2500]],\n",
      "       device='cuda:0', grad_fn=<ThSubBackward>)\n",
      "tensor([ 0.0271, -0.6058,  0.3413,  0.7958,  0.3924, -0.1132, -0.1445,  0.3799,\n",
      "        -0.5359, -0.5703], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "oo_values_1, oo_indices_1 = oo1.max(1)\n",
    "oo_values_2, oo_indices_2 = oo2.max(1)\n",
    "print(oo_indices_1, oo_indices_2)\n",
    "print(oo1)\n",
    "print(oo2)\n",
    "print(bias_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  207., -4646.,  2615.,  6101.,  3010.,  -870., -1108.,  2911., -4107.,\n",
       "        -4371.], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.fc[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7513, -1.5044,  2.8504,  2.2066, -1.1107, -0.1140,  2.7132, -0.2327,\n",
       "         -3.6545, -2.9618]], device='cuda:0', grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 12914.2500, -10423.2500,  21100.5000,  16114.0000,  -8682.7500,\n",
       "           -955.2500,  21532.7500,  -1240.7500, -28277.2500, -22523.2500]],\n",
       "       device='cuda:0', grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0269, -0.6061,  0.3412,  0.7959,  0.3926, -0.1135, -0.1445,  0.3798,\n",
       "        -0.5357, -0.5702], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.fc.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  207., -4646.,  2615.,  6101.,  3010.,  -870., -1108.,  2911., -4107.,\n",
       "        -4371.], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.fc[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i_l,layer in enumerate(quantizer.param_to_quantize):\n",
    "    print(layer['act'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0269, -0.6061,  0.3412,  0.7959,  0.3926, -0.1135, -0.1445,  0.3798,\n",
       "        -0.5357, -0.5702], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.fc.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_quant_devel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ScaledClippedLinearQuantization(M=0.1803930699825287, clip_val=255)\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "    (3): ScaledClippedLinearQuantization(M=2.054954767227173, clip_val=255)\n",
       "    (4): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (5): ScaledClippedLinearQuantization(M=0.008188934065401554, clip_val=255)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (7): ScaledClippedLinearQuantization(M=0.7202885746955872, clip_val=255)\n",
       "    (8): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (9): ScaledClippedLinearQuantization(M=0.004449482541531324, clip_val=255)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "    (11): ScaledClippedLinearQuantization(M=0.5195009112358093, clip_val=255)\n",
       "    (12): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (13): ScaledClippedLinearQuantization(M=0.0037819177377969027, clip_val=255)\n",
       "    (14): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "    (15): ScaledClippedLinearQuantization(M=0.38205116987228394, clip_val=255)\n",
       "    (16): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (17): ScaledClippedLinearQuantization(M=0.0029082787223160267, clip_val=255)\n",
       "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (19): ScaledClippedLinearQuantization(M=0.538362443447113, clip_val=255)\n",
       "    (20): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (21): ScaledClippedLinearQuantization(M=0.0026687942445278168, clip_val=255)\n",
       "    (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "    (23): ScaledClippedLinearQuantization(M=0.465130478143692, clip_val=255)\n",
       "    (24): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (25): ScaledClippedLinearQuantization(M=0.002053817268460989, clip_val=255)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (27): ScaledClippedLinearQuantization(M=0.4517885148525238, clip_val=255)\n",
       "    (28): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (29): ScaledClippedLinearQuantization(M=0.002201623748987913, clip_val=255)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (31): ScaledClippedLinearQuantization(M=0.41972294449806213, clip_val=255)\n",
       "    (32): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (33): ScaledClippedLinearQuantization(M=0.0022723807487636805, clip_val=255)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (35): ScaledClippedLinearQuantization(M=0.29105091094970703, clip_val=255)\n",
       "    (36): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (37): ScaledClippedLinearQuantization(M=0.0024077112320810556, clip_val=255)\n",
       "    (38): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (39): ScaledClippedLinearQuantization(M=0.11353554576635361, clip_val=255)\n",
       "    (40): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (41): ScaledClippedLinearQuantization(M=0.0020281749311834574, clip_val=255)\n",
       "    (42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (43): ScaledClippedLinearQuantization(M=0.07262083888053894, clip_val=255)\n",
       "    (44): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (45): ScaledClippedLinearQuantization(M=0.002086713444441557, clip_val=255)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "    (47): ScaledClippedLinearQuantization(M=0.0812416598200798, clip_val=255)\n",
       "    (48): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (49): ScaledClippedLinearQuantization(M=0.0020846007391810417, clip_val=255)\n",
       "    (50): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "    (51): ScaledClippedLinearQuantization(M=0.06995085626840591, clip_val=255)\n",
       "    (52): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (53): ScaledClippedLinearQuantization(M=0.002058308804407716, clip_val=255)\n",
       "    (54): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.6592e+00, -9.9213e-01, -1.8538e+00,  ..., -1.6261e+00,\n",
       "            1.5950e+00, -1.5785e+00],\n",
       "          [ 9.5142e-01,  1.1928e+00,  2.8836e+00,  ..., -7.2053e-01,\n",
       "           -7.4531e-02,  3.7682e-01],\n",
       "          [-3.1280e-01,  2.1150e+00, -1.7433e-01,  ...,  7.0935e-01,\n",
       "           -1.6594e+00,  2.1062e-01],\n",
       "          ...,\n",
       "          [ 3.3497e-02,  6.2067e-01,  6.3820e-01,  ...,  4.8994e-01,\n",
       "            2.4570e-01,  6.8401e-01],\n",
       "          [ 1.2982e-01, -1.5967e+00, -3.2398e-01,  ...,  6.2351e-01,\n",
       "            4.4640e-01, -7.6585e-01],\n",
       "          [ 3.9689e-01, -7.4495e-01, -1.2975e+00,  ...,  2.0225e+00,\n",
       "            2.7354e-01, -6.5239e-01]],\n",
       "\n",
       "         [[-3.9841e-02,  7.7911e-01,  1.1855e+00,  ..., -6.1800e-01,\n",
       "            5.0772e-01,  4.5049e-01],\n",
       "          [ 1.1449e+00,  1.2373e+00, -3.3193e-01,  ...,  8.9476e-01,\n",
       "            7.7673e-01,  2.1901e+00],\n",
       "          [-5.0232e-01, -3.0888e-01,  5.3224e-01,  ...,  7.4330e-01,\n",
       "            8.5863e-02, -9.9452e-01],\n",
       "          ...,\n",
       "          [ 3.3065e-01, -7.7455e-01, -1.1916e+00,  ..., -1.4206e+00,\n",
       "           -3.9144e-01,  2.2541e-01],\n",
       "          [-5.2741e-02, -2.9506e-01, -1.5071e+00,  ..., -2.8633e-01,\n",
       "           -1.4177e+00, -2.4069e-01],\n",
       "          [-1.0461e+00,  1.8122e+00, -6.0457e-01,  ..., -1.2118e-01,\n",
       "           -1.0473e+00, -2.4402e-01]],\n",
       "\n",
       "         [[ 4.6992e-01,  3.1880e-01, -1.8890e-01,  ...,  7.8889e-01,\n",
       "            6.2453e-01, -1.4221e+00],\n",
       "          [ 5.2393e-01, -1.5899e+00, -5.0964e-01,  ..., -1.5338e+00,\n",
       "            1.4490e+00,  3.4530e-01],\n",
       "          [ 1.5339e+00, -5.5805e-01,  2.7071e-01,  ..., -2.9869e-01,\n",
       "           -1.1464e+00, -8.9437e-02],\n",
       "          ...,\n",
       "          [ 6.8767e-01, -1.0157e-01,  2.2122e-01,  ..., -8.2075e-01,\n",
       "            2.2601e+00,  1.3912e+00],\n",
       "          [ 4.4290e-01,  8.0562e-01,  2.8213e-02,  ...,  1.0487e+00,\n",
       "            1.5510e+00, -2.6768e+00],\n",
       "          [ 6.9047e-01, -1.7992e+00,  9.2913e-01,  ..., -1.2556e+00,\n",
       "            2.3107e+00,  1.0157e+00]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innn = torch.randn(1,3,224,224)\n",
    "innn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f1a90929208>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 181, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 152, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "store_and_quantize() got an unexpected keyword argument 'update_float_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-50cbb7863535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtarget_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_and_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_quant_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_float_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: store_and_quantize() got an unexpected keyword argument 'update_float_params'"
     ]
    }
   ],
   "source": [
    "# forward and backward\n",
    "for _ in range(100):\n",
    "    i, (inputs, target) = next(enumerate(val_loader))\n",
    "    if gpus is not None:\n",
    "        target = target.cuda(async=True)\n",
    "    input_var = Variable(inputs.type(ttype), volatile=not training)\n",
    "    target_var = Variable(target)\n",
    "    quantizer.store_and_quantize(get_quant_params=not training)\n",
    "    output = model(input_var)\n",
    "    loss = criterion(output, target_var)\n",
    "    print(loss.data)\n",
    "    #print(output.data)\n",
    "    #print(model.model[0][1].weight.data)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    quantizer.restore_real_value()            \n",
    "    quantizer.backprop_quant_gradients()  \n",
    "    optimizer.step()\n",
    "    #print(model.model[0][1].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model[0][1].running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): mobilenet_quant_devel(\n",
       "    (model): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (11): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (12): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (13): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (14): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (15): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (16): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (17): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (18): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (19): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (20): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (21): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (22): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (23): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (24): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (25): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (26): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mOSError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-98084f23ec96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# ensure that the worker exits on process exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0m_update_worker_pids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "i, (inputs, target) = next(enumerate(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "if gpus is not None:\n",
    "    target = target.cuda(async=True)\n",
    "input_var = Variable(inputs.type(ttype), volatile=not training)\n",
    "target_var = Variable(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2973, -1.8580, -0.5975,  ...,  1.3263,  1.2916, -0.4290],\n",
       "          [-1.0215,  0.0675, -0.9857,  ...,  1.3524,  0.6812,  1.5958],\n",
       "          [ 0.0089,  0.3174, -0.6525,  ..., -1.4961,  0.6239, -0.9933],\n",
       "          ...,\n",
       "          [ 0.4883, -0.0547, -0.0974,  ...,  0.1020, -1.3684, -0.1422],\n",
       "          [ 0.2412,  0.2797, -0.2057,  ...,  0.1744, -1.4662,  0.7479],\n",
       "          [-1.8779,  1.6263, -1.6444,  ..., -0.3677,  0.8274,  0.8945]],\n",
       "\n",
       "         [[ 1.6880,  1.1349, -0.0469,  ..., -0.2843,  0.0886, -0.8200],\n",
       "          [-0.4533, -1.3211, -1.0611,  ..., -0.2144,  0.1663,  0.4091],\n",
       "          [ 1.4487, -0.3952,  0.4396,  ...,  0.1303, -0.1160,  0.3651],\n",
       "          ...,\n",
       "          [-0.1381,  0.4277, -0.0694,  ...,  0.1845,  0.2456, -1.9692],\n",
       "          [ 0.5448, -0.5923,  0.0232,  ..., -0.0760,  0.0342, -0.0051],\n",
       "          [ 0.8865, -1.2380,  1.3131,  ...,  0.7514,  0.3448,  0.0068]],\n",
       "\n",
       "         [[-1.6050, -0.4920,  0.0739,  ...,  0.7775,  0.2723, -0.1352],\n",
       "          [-0.0201, -0.0703,  0.2337,  ...,  0.6329,  0.6953, -0.0695],\n",
       "          [ 0.5104,  1.1874, -1.0925,  ..., -0.7873,  0.2734, -0.2888],\n",
       "          ...,\n",
       "          [ 1.7035, -0.2425, -0.0880,  ...,  1.0512, -1.6153, -0.0476],\n",
       "          [-0.6426, -0.2239, -0.2007,  ..., -1.3564, -0.8920, -1.0315],\n",
       "          [ 0.5917, -1.2916,  1.4357,  ...,  0.7011, -0.2120,  1.2757]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_var = Variable(torch.randn(1,3,32,32)).type(ttype)\n",
    "input_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "quantizer.store_and_quantize(training=False, get_quant_params=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1488, -2.6569,  5.3355,  1.3420, -1.5785,  0.7806,  4.1762,  1.8382,\n",
       "         -5.5024, -2.6558]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input_var)\n",
    "quantizer.restore_real_value()\n",
    "output.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.store_and_quantize(training=False, get_quant_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -30.6576,   21.5708,  117.1469,    6.1752, -111.7767,   37.9310,\n",
       "          133.3673,   48.5023, -167.9324,  -54.4490]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.type(ttype)\n",
    "output2 = quantizer.deployment_model(input_var)\n",
    "quantizer.restore_real_value()\n",
    "\n",
    "output2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0227, -0.0039,  0.0264,  0.1494,  0.0201,  0.0206,  0.0225,  0.0312,\n",
       "          0.0266,  0.0338]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.data/output2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(711.0366, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output.data-output2.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.2298e+00, -1.3090e+00, -1.2916e+00,  ..., -1.2027e+00,\n",
       "           -1.1830e+00, -1.1214e+00],\n",
       "          [-1.2267e+00, -1.2917e+00, -1.2363e+00,  ..., -1.1769e+00,\n",
       "           -1.2008e+00, -9.8096e-01],\n",
       "          [-1.1631e+00, -1.1992e+00, -1.2507e+00,  ..., -1.2457e+00,\n",
       "           -1.2062e+00, -1.0203e+00],\n",
       "          ...,\n",
       "          [-1.1890e+00, -1.0999e+00, -1.1926e+00,  ..., -1.2107e+00,\n",
       "           -1.2402e+00, -1.0884e+00],\n",
       "          [-1.1694e+00, -1.0818e+00, -1.2422e+00,  ..., -1.3756e+00,\n",
       "           -1.1469e+00, -1.1982e+00],\n",
       "          [-1.1578e+00, -1.2190e+00, -1.2013e+00,  ..., -1.2658e+00,\n",
       "           -1.2421e+00, -1.2590e+00]],\n",
       "\n",
       "         [[ 1.3645e-02, -9.3586e-02, -8.3263e-02,  ..., -1.5779e-01,\n",
       "            5.7411e-02, -6.6135e-02],\n",
       "          [ 9.7727e-02,  1.1886e-02,  5.1301e-02,  ..., -1.0439e-01,\n",
       "            6.8946e-02, -1.0653e-01],\n",
       "          [-5.2667e-02,  1.0581e-01,  4.1952e-02,  ..., -2.0856e-01,\n",
       "            2.7062e-01, -4.0397e-03],\n",
       "          ...,\n",
       "          [ 4.0488e-02,  1.9030e-01, -1.4606e-01,  ...,  2.5247e-01,\n",
       "           -1.0326e-01, -7.4413e-02],\n",
       "          [-1.8991e-02,  5.6961e-02, -1.3114e-01,  ...,  1.2357e-01,\n",
       "            6.6641e-02, -2.1107e-02],\n",
       "          [-4.9492e-02,  8.1401e-02,  3.4183e-02,  ...,  9.9227e-02,\n",
       "            6.3336e-03, -2.5506e-02]],\n",
       "\n",
       "         [[-1.4930e-01, -2.9414e-01, -2.7980e-01,  ..., -1.7580e-01,\n",
       "           -2.4306e-01, -1.4068e-01],\n",
       "          [-1.9619e-01, -2.0429e-01, -2.0143e-01,  ..., -2.7325e-01,\n",
       "           -1.2954e-01, -1.8017e-02],\n",
       "          [-2.7843e-02,  5.8932e-02, -1.3408e-01,  ..., -2.3310e-01,\n",
       "            4.0310e-02, -3.4693e-03],\n",
       "          ...,\n",
       "          [-8.6351e-02,  5.3724e-03, -9.3770e-02,  ..., -5.4428e-02,\n",
       "           -1.5197e-01, -1.8827e-01],\n",
       "          [-1.4093e-02, -3.6462e-02, -1.0330e-01,  ..., -7.0195e-02,\n",
       "           -4.5040e-02, -1.3007e-01],\n",
       "          [-7.3680e-02, -1.0060e-01, -5.2211e-02,  ..., -1.2701e-01,\n",
       "           -1.0911e-01, -1.7992e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.6009e-01,  2.6931e-01,  2.6136e-01,  ...,  2.5117e-01,\n",
       "            2.6668e-01,  2.5369e-01],\n",
       "          [ 2.4158e-01,  2.6358e-01,  2.7304e-01,  ...,  2.5273e-01,\n",
       "            2.3883e-01,  2.4989e-01],\n",
       "          [ 2.5682e-01,  2.4631e-01,  2.5767e-01,  ...,  2.4888e-01,\n",
       "            2.2210e-01,  2.5733e-01],\n",
       "          ...,\n",
       "          [ 2.4252e-01,  2.5167e-01,  2.5067e-01,  ...,  2.1243e-01,\n",
       "            2.7385e-01,  2.7531e-01],\n",
       "          [ 2.4969e-01,  2.6487e-01,  2.3988e-01,  ...,  2.2648e-01,\n",
       "            2.3806e-01,  2.6165e-01],\n",
       "          [ 2.4563e-01,  2.6747e-01,  2.2758e-01,  ...,  2.3912e-01,\n",
       "            2.5992e-01,  2.4890e-01]],\n",
       "\n",
       "         [[ 7.7269e-01,  1.5988e+00,  2.4018e+00,  ...,  7.8589e-01,\n",
       "            1.0539e+00,  3.3658e-01],\n",
       "          [ 5.5828e-01,  9.5779e-01,  1.8177e+00,  ...,  6.1384e-01,\n",
       "            1.5493e+00,  1.0191e-01],\n",
       "          [-4.1264e-01, -4.7407e-01,  2.6063e-01,  ...,  2.2764e-01,\n",
       "            1.2261e+00, -1.1016e+00],\n",
       "          ...,\n",
       "          [ 2.0218e-01,  6.2887e-01,  5.1546e-01,  ...,  1.3798e+00,\n",
       "            9.1003e-01,  9.9458e-01],\n",
       "          [ 3.0648e-01,  7.4250e-01,  7.8442e-01,  ...,  1.1953e+00,\n",
       "            1.8408e+00,  1.2555e+00],\n",
       "          [ 1.4551e-01,  5.0154e-01,  6.0392e-01,  ...,  5.9098e-01,\n",
       "            4.3648e-01,  4.0840e-01]],\n",
       "\n",
       "         [[-5.9005e-01,  1.3004e-01,  1.0291e-01,  ...,  1.2661e-01,\n",
       "            3.2128e-01,  3.8054e-01],\n",
       "          [-1.0937e+00, -2.3148e-01,  3.3709e-01,  ...,  9.6593e-01,\n",
       "            1.1545e+00,  1.1332e+00],\n",
       "          [-9.3223e-01, -2.4278e-01,  4.5589e-01,  ...,  2.8804e+00,\n",
       "            2.4562e+00,  1.6902e+00],\n",
       "          ...,\n",
       "          [-4.8425e-01, -1.1442e+00, -8.5513e-01,  ..., -1.5815e-01,\n",
       "           -1.0104e+00, -1.3756e-01],\n",
       "          [-1.0418e+00, -1.0104e+00, -4.4722e-01,  ..., -1.1064e-01,\n",
       "           -8.1580e-01, -3.3329e-01],\n",
       "          [-4.0951e-02, -4.4816e-01, -1.2408e-01,  ...,  6.5569e-01,\n",
       "            3.4113e-01,  6.8943e-01]]]],\n",
       "       device='cuda:0', grad_fn=<CudnnBatchNormBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(model.module.model[0][0],model.module.model[0][1] )#,model.module.model[1])\n",
    "output1 = net(input_var)\n",
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr_md = quantizer.deployment_model.model\n",
    "net2= nn.Sequential( ptr_md[0])#,ptr_md[1])#,ptr_md[2],ptr_md[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.8973e+02, -3.0840e+02, -3.0429e+02,  ..., -2.8336e+02,\n",
       "           -2.7872e+02, -2.6420e+02],\n",
       "          [-2.8900e+02, -3.0432e+02, -2.9126e+02,  ..., -2.7727e+02,\n",
       "           -2.8290e+02, -2.3111e+02],\n",
       "          [-2.7401e+02, -2.8253e+02, -2.9466e+02,  ..., -2.9349e+02,\n",
       "           -2.8418e+02, -2.4039e+02],\n",
       "          ...,\n",
       "          [-2.8012e+02, -2.5914e+02, -2.8097e+02,  ..., -2.8525e+02,\n",
       "           -2.9218e+02, -2.5641e+02],\n",
       "          [-2.7550e+02, -2.5487e+02, -2.9266e+02,  ..., -3.2409e+02,\n",
       "           -2.7020e+02, -2.8228e+02],\n",
       "          [-2.7278e+02, -2.8719e+02, -2.8301e+02,  ..., -2.9821e+02,\n",
       "           -2.9263e+02, -2.9661e+02]],\n",
       "\n",
       "         [[ 3.2147e+00, -2.2048e+01, -1.9617e+01,  ..., -3.7174e+01,\n",
       "            1.3526e+01, -1.5581e+01],\n",
       "          [ 2.3024e+01,  2.8003e+00,  1.2086e+01,  ..., -2.4594e+01,\n",
       "            1.6244e+01, -2.5098e+01],\n",
       "          [-1.2408e+01,  2.4930e+01,  9.8837e+00,  ..., -4.9137e+01,\n",
       "            6.3758e+01, -9.5175e-01],\n",
       "          ...,\n",
       "          [ 9.5388e+00,  4.4833e+01, -3.4411e+01,  ...,  5.9480e+01,\n",
       "           -2.4328e+01, -1.7531e+01],\n",
       "          [-4.4742e+00,  1.3420e+01, -3.0896e+01,  ...,  2.9112e+01,\n",
       "            1.5700e+01, -4.9728e+00],\n",
       "          [-1.1660e+01,  1.9178e+01,  8.0534e+00,  ...,  2.3377e+01,\n",
       "            1.4922e+00, -6.0091e+00]],\n",
       "\n",
       "         [[-3.5174e+01, -6.9300e+01, -6.5920e+01,  ..., -4.1419e+01,\n",
       "           -5.7264e+01, -3.3143e+01],\n",
       "          [-4.6221e+01, -4.8129e+01, -4.7456e+01,  ..., -6.4376e+01,\n",
       "           -3.0519e+01, -4.2447e+00],\n",
       "          [-6.5596e+00,  1.3884e+01, -3.1588e+01,  ..., -5.4917e+01,\n",
       "            9.4970e+00, -8.1735e-01],\n",
       "          ...,\n",
       "          [-2.0344e+01,  1.2657e+00, -2.2092e+01,  ..., -1.2823e+01,\n",
       "           -3.5803e+01, -4.4356e+01],\n",
       "          [-3.3203e+00, -8.5903e+00, -2.4336e+01,  ..., -1.6538e+01,\n",
       "           -1.0611e+01, -3.0645e+01],\n",
       "          [-1.7359e+01, -2.3700e+01, -1.2301e+01,  ..., -2.9923e+01,\n",
       "           -2.5705e+01, -4.2389e+01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.1275e+01,  6.3449e+01,  6.1574e+01,  ...,  5.9176e+01,\n",
       "            6.2830e+01,  5.9768e+01],\n",
       "          [ 5.6915e+01,  6.2098e+01,  6.4328e+01,  ...,  5.9543e+01,\n",
       "            5.6267e+01,  5.8874e+01],\n",
       "          [ 6.0505e+01,  5.8030e+01,  6.0706e+01,  ...,  5.8636e+01,\n",
       "            5.2327e+01,  6.0627e+01],\n",
       "          ...,\n",
       "          [ 5.7136e+01,  5.9293e+01,  5.9056e+01,  ...,  5.0048e+01,\n",
       "            6.4518e+01,  6.4863e+01],\n",
       "          [ 5.8827e+01,  6.2403e+01,  5.6514e+01,  ...,  5.3358e+01,\n",
       "            5.6087e+01,  6.1643e+01],\n",
       "          [ 5.7869e+01,  6.3014e+01,  5.3618e+01,  ...,  5.6336e+01,\n",
       "            6.1236e+01,  5.8640e+01]],\n",
       "\n",
       "         [[ 1.8204e+02,  3.7666e+02,  5.6585e+02,  ...,  1.8515e+02,\n",
       "            2.4830e+02,  7.9298e+01],\n",
       "          [ 1.3153e+02,  2.2565e+02,  4.2824e+02,  ...,  1.4462e+02,\n",
       "            3.6502e+02,  2.4009e+01],\n",
       "          [-9.7216e+01, -1.1169e+02,  6.1404e+01,  ...,  5.3632e+01,\n",
       "            2.8886e+02, -2.5953e+02],\n",
       "          ...,\n",
       "          [ 4.7632e+01,  1.4816e+02,  1.2144e+02,  ...,  3.2507e+02,\n",
       "            2.1440e+02,  2.3432e+02],\n",
       "          [ 7.2205e+01,  1.7493e+02,  1.8481e+02,  ...,  2.8160e+02,\n",
       "            4.3369e+02,  2.9578e+02],\n",
       "          [ 3.4282e+01,  1.1816e+02,  1.4228e+02,  ...,  1.3923e+02,\n",
       "            1.0283e+02,  9.6217e+01]],\n",
       "\n",
       "         [[-1.3901e+02,  3.0637e+01,  2.4246e+01,  ...,  2.9828e+01,\n",
       "            7.5693e+01,  8.9654e+01],\n",
       "          [-2.5767e+02, -5.4536e+01,  7.9416e+01,  ...,  2.2757e+02,\n",
       "            2.7200e+02,  2.6698e+02],\n",
       "          [-2.1963e+02, -5.7197e+01,  1.0741e+02,  ...,  6.7862e+02,\n",
       "            5.7868e+02,  3.9822e+02],\n",
       "          ...,\n",
       "          [-1.1409e+02, -2.6957e+02, -2.0147e+02,  ..., -3.7260e+01,\n",
       "           -2.3806e+02, -3.2408e+01],\n",
       "          [-2.4544e+02, -2.3804e+02, -1.0536e+02,  ..., -2.6067e+01,\n",
       "           -1.9220e+02, -7.8522e+01],\n",
       "          [-9.6480e+00, -1.0559e+02, -2.9234e+01,  ...,  1.5448e+02,\n",
       "            8.0370e+01,  1.6243e+02]]]],\n",
       "       device='cuda:0', grad_fn=<CudnnConvolutionBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2 = net2(input_var)\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6668,  0.8130,  0.5452,  ...,  2.5753,  2.7961,  2.4325],\n",
       "        [ 1.5222,  2.2391,  3.4420,  ...,  1.3467,  1.0082,  1.3457],\n",
       "        [ 0.7315, -0.0835, -0.4004,  ...,  1.5123,  1.1432,  0.5446],\n",
       "        ...,\n",
       "        [ 1.2904,  1.1056,  0.8274,  ...,  0.3274,  1.6566,  0.8545],\n",
       "        [ 0.9708,  0.5848,  1.1020,  ...,  0.4696,  1.5043,  1.8956],\n",
       "        [ 1.3714,  1.2828,  0.9303,  ...,  1.4516,  0.2574,  1.8278]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1085, -0.2053, -0.0891],\n",
       "         [ 0.1201,  0.1860,  0.1317],\n",
       "         [ 0.0116,  0.0116,  0.0271]],\n",
       "\n",
       "        [[-0.2867, -0.4184, -0.2053],\n",
       "         [ 0.1627,  0.2944,  0.1976],\n",
       "         [ 0.0349,  0.0465, -0.0155]],\n",
       "\n",
       "        [[-0.0542, -0.1356, -0.0310],\n",
       "         [ 0.0930,  0.1278,  0.0659],\n",
       "         [ 0.0271, -0.0155, -0.0271]]], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.model[0].weight.data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3596, -0.6925, -0.2930],\n",
       "         [ 0.3995,  0.6259,  0.4395],\n",
       "         [ 0.0400,  0.0400,  0.0932]],\n",
       "\n",
       "        [[-0.9588, -1.3983, -0.6925],\n",
       "         [ 0.5460,  0.9855,  0.6659],\n",
       "         [ 0.1199,  0.1598, -0.0533]],\n",
       "\n",
       "        [[-0.1864, -0.4528, -0.1065],\n",
       "         [ 0.3063,  0.4261,  0.2264],\n",
       "         [ 0.0932, -0.0533, -0.0932]]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.model[0][0].weight.data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9430.5840, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output1.data - output2.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2985, -0.3297, -0.2971,  ..., -0.4190, -0.4657, -0.4281],\n",
       "          [-0.4323, -0.4338, -0.4660,  ..., -0.3846, -0.4436, -0.4710],\n",
       "          [-0.3825, -0.3861, -0.3902,  ..., -0.4035, -0.4159, -0.4612],\n",
       "          ...,\n",
       "          [-0.3960, -0.4204, -0.4495,  ..., -0.4364, -0.3823, -0.4304],\n",
       "          [-0.3646, -0.3705, -0.4174,  ..., -0.4267, -0.4123, -0.3685],\n",
       "          [-0.3844, -0.3780, -0.3610,  ..., -0.3673, -0.3633, -0.3704]],\n",
       "\n",
       "         [[-0.3995, -0.3628, -0.2147,  ..., -1.1500, -1.3742, -1.3333],\n",
       "          [-0.6543, -0.8469, -1.1253,  ..., -0.8415, -0.9610, -1.6104],\n",
       "          [-0.7949, -0.8514, -0.9529,  ..., -0.9631, -0.9862, -1.2883],\n",
       "          ...,\n",
       "          [-0.7851, -0.9943, -1.1852,  ..., -1.0356, -1.1369, -1.0480],\n",
       "          [-0.8469, -0.9498, -1.1165,  ..., -1.0756, -1.2138, -1.0943],\n",
       "          [-0.8307, -0.7719, -0.6681,  ..., -0.9480, -0.8255, -0.7907]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]],\n",
       "\n",
       "\n",
       "        [[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2380, -0.2372, -0.3571,  ..., -0.2342, -0.2336, -0.2234],\n",
       "          [-0.2054, -0.2176, -0.3353,  ..., -0.2996, -0.2949, -0.2846],\n",
       "          [-0.1845, -0.2031, -0.3197,  ..., -0.4017, -0.3796, -0.3455],\n",
       "          ...,\n",
       "          [-0.3395, -0.3385, -0.3391,  ..., -0.2992, -0.2882, -0.2872],\n",
       "          [-0.3444, -0.3437, -0.3383,  ..., -0.2679, -0.2623, -0.2629],\n",
       "          [-0.3358, -0.3356, -0.3318,  ..., -0.2523, -0.2533, -0.2518]],\n",
       "\n",
       "         [[-0.3828,  0.0374,  0.0106,  ...,  0.1503,  0.1733,  0.1664],\n",
       "          [-0.2459,  0.3082,  0.1448,  ..., -0.1982, -0.1454, -0.1307],\n",
       "          [-0.1252,  0.4252,  0.2484,  ..., -1.2257, -1.1075, -0.9670],\n",
       "          ...,\n",
       "          [-0.5629, -0.5105, -0.4649,  ..., -0.3677, -0.2916, -0.2247],\n",
       "          [-0.5568, -0.5349, -0.5073,  ..., -0.1558, -0.1605, -0.0816],\n",
       "          [-0.5439, -0.4651, -0.4508,  ..., -0.0514, -0.0843, -0.0840]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]],\n",
       "\n",
       "\n",
       "        [[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2189, -0.2253, -0.1837,  ..., -0.3683, -0.3852, -0.3540],\n",
       "          [-0.1769, -0.2015, -0.1984,  ..., -0.3258, -0.3106, -0.2858],\n",
       "          [-0.2517, -0.2930, -0.1854,  ..., -0.2929, -0.3700, -0.3564],\n",
       "          ...,\n",
       "          [-0.4953, -0.4909, -0.4664,  ..., -0.1944, -0.1949, -0.2029],\n",
       "          [-0.4593, -0.4838, -0.4514,  ..., -0.1942, -0.1934, -0.1959],\n",
       "          [-0.4801, -0.4214, -0.3664,  ..., -0.1895, -0.1928, -0.1942]],\n",
       "\n",
       "         [[-0.1195,  0.1274,  0.2780,  ..., -0.5975, -0.6878, -0.6424],\n",
       "          [-0.0601,  0.5447,  0.2666,  ..., -0.3930, -0.5264, -0.0122],\n",
       "          [-0.2511,  0.2140,  0.4298,  ..., -0.0464, -0.4539, -0.4418],\n",
       "          ...,\n",
       "          [-0.9026, -1.2045, -1.3517,  ...,  0.4517,  0.4260,  0.4005],\n",
       "          [-0.7699, -1.1043, -1.2150,  ...,  0.4663,  0.4430,  0.4430],\n",
       "          [-0.8815, -1.1492, -1.0867,  ...,  0.5064,  0.4726,  0.4484]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3891, -0.2822, -0.2715,  ..., -0.4092, -0.4461, -0.4759],\n",
       "          [-0.3760, -0.2610, -0.2316,  ..., -0.4135, -0.4208, -0.4055],\n",
       "          [-0.3128, -0.2774, -0.2310,  ..., -0.4384, -0.4015, -0.4073],\n",
       "          ...,\n",
       "          [-0.2400, -0.2598, -0.2610,  ..., -0.3478, -0.3701, -0.3571],\n",
       "          [-0.2440, -0.2642, -0.2642,  ..., -0.3143, -0.3378, -0.3634],\n",
       "          [-0.2480, -0.2489, -0.2527,  ..., -0.3248, -0.3240, -0.3325]],\n",
       "\n",
       "         [[-0.5445, -0.5083,  0.1338,  ..., -0.7011, -1.1041, -1.0485],\n",
       "          [-0.7784, -0.2736,  0.2615,  ..., -0.5770, -1.2865, -0.6724],\n",
       "          [-0.4350,  0.0365,  0.3340,  ..., -0.7457, -1.0920, -0.6252],\n",
       "          ...,\n",
       "          [-0.2782,  0.0245, -0.0248,  ..., -0.6087, -0.7991, -0.7929],\n",
       "          [-0.3170, -0.0578, -0.0609,  ..., -0.4377, -0.5591, -0.7303],\n",
       "          [-0.3108,  0.0492,  0.0717,  ..., -0.5224, -0.4622, -0.4739]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]],\n",
       "\n",
       "\n",
       "        [[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.5862, -0.5707, -0.5268,  ..., -0.3219, -0.3154, -0.3117],\n",
       "          [-0.6039, -0.6030, -0.5710,  ..., -0.2930, -0.2937, -0.3163],\n",
       "          [-0.5539, -0.5975, -0.5952,  ..., -0.2721, -0.2629, -0.2868],\n",
       "          ...,\n",
       "          [-0.2466, -0.2643, -0.2819,  ..., -0.3185, -0.3239, -0.2634],\n",
       "          [-0.2322, -0.2719, -0.3040,  ..., -0.2940, -0.2870, -0.2704],\n",
       "          [-0.2988, -0.2982, -0.2772,  ..., -0.2657, -0.2759, -0.2464]],\n",
       "\n",
       "         [[-1.4044, -2.1832, -1.7710,  ..., -0.4852, -0.4524, -0.3524],\n",
       "          [-1.4719, -2.4704, -2.2743,  ..., -0.2724, -0.3692, -0.3233],\n",
       "          [-1.2783, -2.2367, -2.4536,  ..., -0.1583, -0.1073, -0.1712],\n",
       "          ...,\n",
       "          [-0.1642,  0.1229,  0.0438,  ..., -0.0488, -0.3502, -0.2637],\n",
       "          [-0.2106,  0.1288, -0.1659,  ..., -0.2118, -0.1213, -0.0507],\n",
       "          [-0.4123, -0.1540, -0.0578,  ..., -0.1334, -0.0087,  0.0732]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]],\n",
       "\n",
       "\n",
       "        [[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3203, -0.3582, -0.3617,  ..., -0.2470, -0.2518, -0.2551],\n",
       "          [-0.2990, -0.3568, -0.3618,  ..., -0.2503, -0.2438, -0.2445],\n",
       "          [-0.3275, -0.3487, -0.3342,  ..., -0.2506, -0.2360, -0.2356],\n",
       "          ...,\n",
       "          [-0.4716, -0.4534, -0.4413,  ..., -0.5180, -0.5131, -0.5057],\n",
       "          [-0.4706, -0.4602, -0.4467,  ..., -0.5196, -0.5143, -0.5080],\n",
       "          [-0.4636, -0.4561, -0.4438,  ..., -0.5171, -0.5253, -0.5153]],\n",
       "\n",
       "         [[-0.5390, -0.5656, -0.7333,  ...,  0.0511, -0.0168, -0.0277],\n",
       "          [-0.4595, -0.5864, -0.8356,  ...,  0.0127,  0.0278,  0.0778],\n",
       "          [-0.5190, -0.5838, -0.6005,  ..., -0.0273,  0.0642,  0.1318],\n",
       "          ...,\n",
       "          [-1.1563, -1.5301, -1.4282,  ..., -1.9661, -1.9128, -1.8678],\n",
       "          [-1.1563, -1.5664, -1.4725,  ..., -1.9714, -1.9219, -1.8793],\n",
       "          [-1.1298, -1.5281, -1.4504,  ..., -1.9227, -1.9969, -1.9552]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]]],\n",
       "       device='cuda:0', grad_fn=<CudnnConvolutionBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.model[0](input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bias_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-71e778bd7a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bias_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "quantizer.deployment_model.model[0].bias.data = bias_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.2268e-03, -5.6052e-45, -1.3223e-01,  9.5181e-02,  5.6052e-45,\n",
       "         2.7001e-02,  1.2026e-02,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "        -5.6052e-45, -1.5036e-01, -5.6052e-45, -5.6052e-45, -5.6052e-45,\n",
       "         5.6052e-45, -5.6052e-45,  5.6052e-45, -1.6848e-01,  3.7778e-02,\n",
       "         5.6052e-45,  5.6052e-45,  5.6052e-45, -5.6052e-45, -5.6052e-45,\n",
       "         5.6052e-45,  8.0595e-03, -5.6052e-45, -1.1367e-01, -1.0080e-01,\n",
       "         1.2784e-01,  4.8255e-03], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_tensor = model.module.model[0][1].running_mean.clone()\n",
    "mu_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1460e+00, -1.3018e-05,  9.6313e-01,  1.1933e+00, -3.9374e-04,\n",
       "         1.1778e+00, -2.0396e-01, -2.6259e-04,  5.2023e-07, -2.0778e-06,\n",
       "        -2.6015e-05,  1.1760e+00, -3.2594e-06, -1.3979e-05, -1.2620e-07,\n",
       "         5.4683e-07, -1.1149e-05, -2.4389e-05,  8.3711e-01,  1.0547e+00,\n",
       "        -6.0049e-05,  2.1387e-07,  9.6728e-07, -1.0179e-04, -2.4040e-05,\n",
       "         4.7633e-06,  1.2126e+00, -1.0693e-05,  1.0251e+00,  6.2181e-01,\n",
       "         8.9824e-01,  1.1250e+00], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_tensor = mu_tensor.mul(-1)*gamma_over_sigma+beta_tensor\n",
    "bias_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1506e+00, -1.3018e-05,  9.5048e-01,  1.2050e+00, -3.9374e-04,\n",
       "         1.1813e+00, -2.0231e-01, -2.6259e-04,  5.2023e-07, -2.0778e-06,\n",
       "        -2.6015e-05,  1.1557e+00, -3.2594e-06, -1.3979e-05, -1.2620e-07,\n",
       "         5.4683e-07, -1.1149e-05, -2.4389e-05,  8.1169e-01,  1.0597e+00,\n",
       "        -6.0049e-05,  2.1387e-07,  9.6728e-07, -1.0179e-04, -2.4040e-05,\n",
       "         4.7633e-06,  1.2149e+00, -1.0693e-05,  1.0384e+00,  6.1218e-01,\n",
       "         9.1894e-01,  1.1263e+00], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_tensor = model.module.model[0][1].bias.data.clone()\n",
    "beta_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_tensor = model.module.model[0][1].weight.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_tensor = model.module.model[0][1].running_var.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0266,  0.1864,  0.1065],\n",
       "          [-0.1332,  0.1332,  0.2397],\n",
       "          [ 0.2264,  0.5194,  0.5726]],\n",
       "\n",
       "         [[-0.0799,  0.1465,  0.0932],\n",
       "          [-0.1598,  0.0932,  0.1998],\n",
       "          [ 0.2264,  0.4927,  0.4927]],\n",
       "\n",
       "         [[-0.1598,  0.1199,  0.0799],\n",
       "          [-0.1465,  0.1731,  0.3063],\n",
       "          [ 0.0533,  0.3862,  0.4528]]],\n",
       "\n",
       "\n",
       "        [[[-0.0133, -0.0133, -0.0133],\n",
       "          [-0.0133, -0.0133, -0.0133],\n",
       "          [-0.0133, -0.0133, -0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0133,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.4528, -0.5859, -0.4794],\n",
       "          [-0.7058, -0.7857, -0.5460],\n",
       "          [-0.6259, -0.6925, -0.4794]],\n",
       "\n",
       "         [[ 0.0932,  0.0266,  0.0133],\n",
       "          [ 0.0400,  0.0000,  0.0799],\n",
       "          [ 0.0000, -0.0266,  0.0000]],\n",
       "\n",
       "         [[ 0.4661,  0.5593,  0.4661],\n",
       "          [ 0.6126,  0.7058,  0.6659],\n",
       "          [ 0.5060,  0.5726,  0.4395]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4261,  0.4128,  0.3063],\n",
       "          [ 0.4128,  0.3196,  0.2397],\n",
       "          [ 0.3462,  0.3462,  0.3063]],\n",
       "\n",
       "         [[-0.7857, -0.9855, -0.9455],\n",
       "          [-0.9721, -1.2918, -1.2385],\n",
       "          [-0.7857, -1.0387, -1.0787]],\n",
       "\n",
       "         [[ 0.6259,  0.8656,  0.9056],\n",
       "          [ 0.6259,  0.7591,  0.7724],\n",
       "          [ 0.6259,  0.7990,  0.6792]]],\n",
       "\n",
       "\n",
       "        [[[-0.5460, -0.5060, -0.3995],\n",
       "          [-0.7724, -0.7591, -0.4927],\n",
       "          [-0.6925, -0.7058, -0.4528]],\n",
       "\n",
       "         [[ 0.8789,  0.9988,  0.7458],\n",
       "          [ 1.1053,  1.2385,  1.0387],\n",
       "          [ 1.0387,  1.1053,  0.9455]],\n",
       "\n",
       "         [[-0.3196, -0.3995, -0.3729],\n",
       "          [-0.4261, -0.5859, -0.4794],\n",
       "          [-0.2663, -0.5060, -0.4261]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.3596, -0.6925, -0.2930],\n",
       "          [ 0.3995,  0.6259,  0.4395],\n",
       "          [ 0.0400,  0.0400,  0.0932]],\n",
       "\n",
       "         [[-0.9588, -1.3983, -0.6925],\n",
       "          [ 0.5460,  0.9855,  0.6659],\n",
       "          [ 0.1199,  0.1598, -0.0533]],\n",
       "\n",
       "         [[-0.1864, -0.4528, -0.1065],\n",
       "          [ 0.3063,  0.4261,  0.2264],\n",
       "          [ 0.0932, -0.0533, -0.0932]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0133,  0.3596,  0.4661],\n",
       "          [-0.1598, -0.5993,  0.5060],\n",
       "          [ 0.1199, -0.4794, -0.2264]],\n",
       "\n",
       "         [[-0.1598,  0.4661,  1.0920],\n",
       "          [-0.3862, -1.0920,  0.7324],\n",
       "          [ 0.4794, -0.6126, -0.4395]],\n",
       "\n",
       "         [[ 0.0932,  0.2663,  0.2930],\n",
       "          [-0.0799, -0.3196,  0.4794],\n",
       "          [ 0.0799, -0.2397, -0.1065]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5327,  0.8523,  0.6925],\n",
       "          [ 0.9988,  1.3051,  0.9588],\n",
       "          [ 0.8922,  1.0254,  0.7324]],\n",
       "\n",
       "         [[-0.2930, -0.3196, -0.0799],\n",
       "          [-0.3596, -0.3596, -0.2264],\n",
       "          [-0.2131, -0.2930, -0.1598]],\n",
       "\n",
       "         [[-0.2264, -0.4927, -0.3462],\n",
       "          [-0.6925, -0.8523, -0.6126],\n",
       "          [-0.6259, -0.7724, -0.5593]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.3596,  0.8523, -0.1332],\n",
       "          [-0.7191,  1.0787, -0.5726],\n",
       "          [ 0.0133,  0.3063, -0.3329]],\n",
       "\n",
       "         [[-0.7724,  1.3051, -0.2797],\n",
       "          [-0.8123,  1.7845, -1.0121],\n",
       "          [-0.0666,  0.5194, -0.6792]],\n",
       "\n",
       "         [[ 0.0000,  0.3995, -0.0799],\n",
       "          [-0.2930,  0.6392, -0.4261],\n",
       "          [ 0.0932,  0.1465,  0.0133]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0932,  0.0666,  0.1065],\n",
       "          [ 0.0799, -0.4128, -0.1065],\n",
       "          [-0.0000, -0.8257, -0.2264]],\n",
       "\n",
       "         [[ 0.0666, -0.0133,  0.0533],\n",
       "          [-0.0400, -0.5460, -0.1598],\n",
       "          [-0.1998, -0.9855, -0.3329]],\n",
       "\n",
       "         [[ 0.1199,  0.0932,  0.1998],\n",
       "          [ 0.1864, -0.1864,  0.1731],\n",
       "          [ 0.0799, -0.5327,  0.0266]]],\n",
       "\n",
       "\n",
       "        [[[-0.0133,  0.0666, -0.0133],\n",
       "          [-0.0000,  0.2264,  0.1199],\n",
       "          [ 0.1065,  0.3995,  0.2530]],\n",
       "\n",
       "         [[-0.0932,  0.0400, -0.0000],\n",
       "          [-0.1199,  0.1864,  0.0799],\n",
       "          [-0.0133,  0.3729,  0.2264]],\n",
       "\n",
       "         [[-0.0932, -0.0533, -0.0932],\n",
       "          [-0.1598,  0.0533, -0.0533],\n",
       "          [-0.0400,  0.2264,  0.0932]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0799, -0.1199, -0.0666],\n",
       "          [-0.1998, -0.3729, -0.2930],\n",
       "          [-0.1864, -0.3862, -0.2397]],\n",
       "\n",
       "         [[ 0.0666, -0.1065,  0.0000],\n",
       "          [-0.1465, -0.3063, -0.1598],\n",
       "          [-0.1465, -0.2930, -0.0666]],\n",
       "\n",
       "         [[ 0.1864, -0.0533,  0.0133],\n",
       "          [-0.1332, -0.3462, -0.2264],\n",
       "          [-0.2131, -0.4128, -0.2131]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0799,  0.0932,  0.1065],\n",
       "          [-0.6525, -0.9189, -0.5060],\n",
       "          [ 0.2797,  0.5993,  0.4128]],\n",
       "\n",
       "         [[ 0.2663,  0.1864,  0.1199],\n",
       "          [-1.0121, -1.6114, -0.9988],\n",
       "          [ 0.7591,  0.9588,  0.6392]],\n",
       "\n",
       "         [[ 0.0799,  0.1598,  0.0533],\n",
       "          [-0.3462, -0.5593, -0.1065],\n",
       "          [ 0.1332,  0.3063,  0.2397]]],\n",
       "\n",
       "\n",
       "        [[[-0.3596, -0.3995, -0.4661],\n",
       "          [-0.4261, -0.5060, -0.6259],\n",
       "          [-0.3596, -0.3462, -0.5060]],\n",
       "\n",
       "         [[ 0.4528,  0.5593,  0.4395],\n",
       "          [ 0.4927,  0.6126,  0.4794],\n",
       "          [ 0.4528,  0.6925,  0.5859]],\n",
       "\n",
       "         [[ 0.0266,  0.0533, -0.0932],\n",
       "          [ 0.1065,  0.1199, -0.0533],\n",
       "          [ 0.0799,  0.2264,  0.0266]]],\n",
       "\n",
       "\n",
       "        [[[-0.1065, -0.3729, -0.1332],\n",
       "          [-0.0932, -0.4794, -0.2930],\n",
       "          [ 0.0799, -0.1199, -0.0000]],\n",
       "\n",
       "         [[-0.0400, -0.4128, -0.1998],\n",
       "          [-0.0799, -0.5859, -0.3862],\n",
       "          [ 0.1199, -0.1864, -0.0666]],\n",
       "\n",
       "         [[ 0.1065, -0.1998, -0.0533],\n",
       "          [ 0.0666, -0.3729, -0.2397],\n",
       "          [ 0.2397, -0.0533,  0.0400]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0799,  0.0400, -0.1199],\n",
       "          [ 0.3995, -0.0932, -0.6925],\n",
       "          [ 0.4927,  0.1199, -0.5460]],\n",
       "\n",
       "         [[ 0.2264,  0.1199, -0.3196],\n",
       "          [ 0.7324, -0.1998, -1.1985],\n",
       "          [ 0.6792,  0.1998, -0.9056]],\n",
       "\n",
       "         [[-0.0799, -0.0133, -0.0266],\n",
       "          [ 0.1864, -0.0666, -0.4261],\n",
       "          [ 0.2264,  0.1065, -0.3729]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0133,  0.0133,  0.0133],\n",
       "          [ 0.0000,  0.0133,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0133]],\n",
       "\n",
       "         [[ 0.0000,  0.0133,  0.0133],\n",
       "          [ 0.0000,  0.0133,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0533,  0.0266,  0.0266],\n",
       "          [ 0.0799, -0.0533, -0.0400],\n",
       "          [ 0.0400, -0.1465, -0.1731]],\n",
       "\n",
       "         [[ 0.0533,  0.0266,  0.0266],\n",
       "          [ 0.0533, -0.0666, -0.0666],\n",
       "          [-0.0000, -0.1731, -0.1731]],\n",
       "\n",
       "         [[-0.0533, -0.0666, -0.0799],\n",
       "          [-0.0400, -0.0932, -0.1065],\n",
       "          [-0.0533, -0.1598, -0.1598]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1864,  0.1465,  0.0932],\n",
       "          [ 0.0932,  0.0666,  0.0666],\n",
       "          [ 0.0932, -0.0133,  0.1465]],\n",
       "\n",
       "         [[ 0.0133, -0.0000,  0.0799],\n",
       "          [-0.2264, -0.2264, -0.0533],\n",
       "          [-0.2530, -0.3329,  0.0133]],\n",
       "\n",
       "         [[-0.3995, -0.3196, -0.1065],\n",
       "          [-0.6259, -0.5593, -0.2264],\n",
       "          [-0.4661, -0.4661, -0.0533]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = model.module.model[0][0].weight.data.clone()\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0,v1,v2,v3 = weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = model.module.model[0][1].eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_tensor = var_tensor.add(eps).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAST = len(quantizer_load.param_to_quantize) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Same =:  3\n",
      "Conv2d Same =:  7\n",
      "Conv2d Same =:  11\n",
      "Conv2d Same =:  15\n",
      "Conv2d Same =:  19\n",
      "Conv2d Same =:  23\n",
      "Conv2d Same =:  27\n",
      "Conv2d Same =:  31\n",
      "Conv2d Same =:  35\n",
      "Conv2d Same =:  39\n",
      "Conv2d Same =:  43\n",
      "Conv2d Same =:  47\n",
      "Conv2d Same =:  51\n",
      "Conv2d Same =:  55\n",
      "Conv2d Same =:  59\n",
      "Conv2d Same =:  63\n",
      "Conv2d Same =:  67\n",
      "Conv2d Same =:  71\n",
      "Conv2d Same =:  75\n",
      "Conv2d Same =:  79\n",
      "Conv2d Same =:  83\n",
      "Conv2d Same =:  87\n",
      "Conv2d Same =:  91\n",
      "Conv2d Same =:  95\n",
      "Conv2d Same =:  99\n",
      "Conv2d Same =:  103\n",
      "Conv2d Same =:  107\n",
      "Linear =:  111\n",
      "*********** Layer  0 Start **************\n",
      "Layer  0  is OK\n",
      "*********** Layer  1 Start **************\n",
      "Iteration  1 with bits:  8 8\n",
      "Layer  1  is OK\n",
      "*********** Layer  2 Start **************\n",
      "Iteration  1 with bits:  4 8\n",
      "Layer  2  is OK\n",
      "*********** Layer  3 Start **************\n",
      "Layer  3  is OK\n",
      "*********** Layer  4 Start **************\n",
      "Layer  4  is OK\n",
      "*********** Layer  5 Start **************\n",
      "Iteration  1 with bits:  8 8\n",
      "Layer  5  is OK\n",
      "*********** Layer  6 Start **************\n",
      "Layer  6  is OK\n",
      "*********** Layer  7 Start **************\n",
      "Layer  7  is OK\n",
      "*********** Layer  8 Start **************\n",
      "Layer  8  is OK\n",
      "*********** Layer  9 Start **************\n",
      "Layer  9  is OK\n",
      "*********** Layer  10 Start **************\n",
      "Layer  10  is OK\n",
      "*********** Layer  11 Start **************\n",
      "Layer  11  is OK\n",
      "*********** Layer  12 Start **************\n",
      "Layer  12  is OK\n",
      "*********** Layer  13 Start **************\n",
      "Layer  13  is OK\n",
      "*********** Layer  14 Start **************\n",
      "Layer  14  is OK\n",
      "*********** Layer  15 Start **************\n",
      "Layer  15  is OK\n",
      "*********** Layer  16 Start **************\n",
      "Layer  16  is OK\n",
      "*********** Layer  17 Start **************\n",
      "Layer  17  is OK\n",
      "*********** Layer  18 Start **************\n",
      "Layer  18  is OK\n",
      "*********** Layer  19 Start **************\n",
      "Layer  19  is OK\n",
      "*********** Layer  20 Start **************\n",
      "Layer  20  is OK\n",
      "*********** Layer  21 Start **************\n",
      "Layer  21  is OK\n",
      "*********** Layer  22 Start **************\n",
      "Layer  22  is OK\n",
      "*********** Layer  23 Start **************\n",
      "Layer  23  is OK\n",
      "*********** Layer  24 Start **************\n",
      "Layer  24  is OK\n",
      "*********** Layer  25 Start **************\n",
      "Layer  25  is OK\n",
      "*********** Layer  26 Start **************\n",
      "Layer  26  is OK\n",
      "*********** Layer  27 Start **************\n",
      "Layer  27  is OK\n",
      "Input:  [8, 8, 4, 4, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8] \n",
      "  Output: [8, 4, 4, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# x = torch.Tensor(1,3,int(input_size),int(input_size))\n",
    "#compute input dimensions\n",
    "def print_size(model, input, output): \n",
    "    global si, so\n",
    "    si = input[0].size()\n",
    "    so = output[0].size()\n",
    "\n",
    "activation_vector_i = []\n",
    "activation_vector_o = []\n",
    "\n",
    "for i,module in enumerate(model.modules()):\n",
    "    if type(module) is models.linear_quantized_modules.Conv2d_SAME:\n",
    "        print('Conv2d Same =: ',i)\n",
    "        hook = module.register_forward_hook(print_size)\n",
    "        model(x)\n",
    "        hook.remove() \n",
    "        activation_vector_i.append(si)\n",
    "        activation_vector_o.append(so)\n",
    "        \n",
    "    elif type(module) is nn.Linear:\n",
    "        print('Linear =: ',i)\n",
    "        hook = module.register_forward_hook(print_size)\n",
    "        model(x)\n",
    "        hook.remove() \n",
    "        activation_vector_i.append(si)\n",
    "        activation_vector_o.append(so)\n",
    "\n",
    "    elif type(module) is nn.Conv2d:\n",
    "        print('Normal Conv =: ',i)\n",
    "\n",
    "def compute_activation_footprint(param_list):\n",
    "    i_params_mem = []\n",
    "    i_bits_mem = []\n",
    "    o_params_mem = []\n",
    "    o_bits_mem = []\n",
    "\n",
    "    for i,item in enumerate(param_list):\n",
    "        o_size = activation_vector_o[i]\n",
    "        i_size = activation_vector_i[i]\n",
    "        o_params = 1\n",
    "        for v in range(len(o_size)):\n",
    "            o_params *= o_size[v]\n",
    "        i_params = 1\n",
    "        for v in range(len(i_size)):\n",
    "            i_params *= i_size[v] \n",
    "        \n",
    "        if i == 0:\n",
    "            i_bits = 8\n",
    "            o_bits = param_list[i]['act_o_bits']\n",
    "        elif i == LAST:\n",
    "            i_bits = param_list[i-1]['act_o_bits']\n",
    "            o_bits = 8\n",
    "        else:\n",
    "            i_bits = param_list[i-1]['act_o_bits']\n",
    "            o_bits = param_list[i]['act_o_bits']\n",
    "        \n",
    "        i_mem = i_bits * i_params / 32\n",
    "        o_mem = o_bits * o_params / 32\n",
    "            \n",
    "        i_params_mem.append(i_params)\n",
    "        i_bits_mem.append(i_bits)\n",
    "        o_params_mem.append(o_params)\n",
    "        o_bits_mem.append(o_bits)\n",
    "        \n",
    "        #print('Input = ', i_params,'(bits = ', i_bits, ') | Output Params = ', o_params,'(bits = ', o_bits,')')            \n",
    "\n",
    "    return i_params_mem, i_bits_mem, o_params_mem, o_bits_mem\n",
    "\n",
    "def cut_activation_footprint(param_list, act_mem):\n",
    "    MIN_ACT_BITS = 2\n",
    "    i_params_mem, i_bits_mem, o_params_mem, o_bits_mem = compute_activation_footprint(param_list)\n",
    "    errQuant = False\n",
    "    for i in range(len(i_params_mem)):\n",
    "        print('*********** Layer ',i, 'Start **************')\n",
    "        tot_layer_mem = ((i_params_mem[i] * i_bits_mem[i]) + (o_params_mem[i] * o_bits_mem[i]) )/32\n",
    "        n_iter = 1\n",
    "        while tot_layer_mem > act_mem:\n",
    "            \n",
    "            print('Iteration ',n_iter, 'with bits: ',i_bits_mem[i],o_bits_mem[i] )\n",
    "            \n",
    "            i_mem = i_params_mem[i] * i_bits_mem[i]\n",
    "            o_mem = o_params_mem[i] * o_bits_mem[i]\n",
    "\n",
    "            if i == 0: # first layer\n",
    "                if o_bits_mem[i] > MIN_ACT_BITS :\n",
    "                    param_list[i]['act_o_bits'] = int(o_bits_mem[i] / 2)\n",
    "                else:\n",
    "                    errQuant = True\n",
    "                    print('No way to find a solution because of the first layer!')\n",
    "                    break\n",
    "            elif i == LAST: # last layer\n",
    "                if i_bits_mem[i] > MIN_ACT_BITS :\n",
    "                    param_list[i-1]['act_o_bits'] = int(i_bits_mem[i] / 2)\n",
    "                else:\n",
    "                    errQuant = True\n",
    "                    print('No way to find a solution because of the last layer!')\n",
    "                    break                \n",
    "            else:\n",
    "                if i_bits_mem[i] == MIN_ACT_BITS and o_bits_mem[i] == MIN_ACT_BITS :\n",
    "                    errQuant = True\n",
    "                    print('No way to find a solution: layer ',i,'with i_bits,o_bits =',MIN_ACT_BITS)\n",
    "                    break\n",
    "\n",
    "                elif i_bits_mem[i] > o_bits_mem[i] and i_bits_mem[i] > MIN_ACT_BITS:\n",
    "                    param_list[i-1]['act_o_bits'] = int(i_bits_mem[i] / 2)     \n",
    "\n",
    "                elif i_bits_mem[i] < o_bits_mem[i] and o_bits_mem[i] > MIN_ACT_BITS:\n",
    "                    param_list[i]['act_o_bits'] = int(o_bits_mem[i] / 2)  \n",
    "\n",
    "                elif i_bits_mem[i] == o_bits_mem[i]:\n",
    "                    if i_mem > o_mem and i_bits_mem[i] > MIN_ACT_BITS :\n",
    "                        param_list[i-1]['act_o_bits'] = int(i_bits_mem[i] / 2)\n",
    "                    elif o_bits_mem[i] > MIN_ACT_BITS:\n",
    "                        param_list[i]['act_o_bits'] = int(o_bits_mem[i] / 2)\n",
    "                    else:\n",
    "                        print('Corner case!')\n",
    "\n",
    "                elif o_bits_mem[i] > MIN_ACT_BITS :\n",
    "                    param_list[i]['act_o_bits'] = int(o_bits_mem[i] / 2)\n",
    "\n",
    "                elif i_bits_mem[i] > MIN_ACT_BITS :\n",
    "                    param_list[i-1]['act_o_bits'] = int(i_bits_mem[i] / 2)\n",
    "\n",
    "                else: \n",
    "                    errQuant = True\n",
    "                    print('No way to find a solution!')\n",
    "                    break                \n",
    "\n",
    "            i_params_mem, i_bits_mem, o_params_mem, o_bits_mem = compute_activation_footprint(param_list)\n",
    "            tot_layer_mem = ((i_params_mem[i] * i_bits_mem[i]) + (o_params_mem[i] * o_bits_mem[i]) )/32\n",
    "            n_iter +=1\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('Layer ',i,' is OK')\n",
    "            \n",
    "    i_params_mem, i_bits_mem, o_params_mem, o_bits_mem = compute_activation_footprint(param_list)\n",
    "    print('Input: ',i_bits_mem,'\\n  Output:', o_bits_mem)\n",
    "    if errQuant is True:\n",
    "        print('No way for quantization')\n",
    "        \n",
    "        \n",
    "    \n",
    "            \n",
    "\n",
    "            \n",
    "ONLY_READ_MEM = 64*1024\n",
    "param_list = copy.deepcopy(quantizer_load.param_to_quantize)\n",
    "tt = cut_activation_footprint(param_list, ONLY_READ_MEM)\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Footprint:  2568144.0\n",
      "***************************************************************\n",
      "[]\n",
      "most footprint on layer:  27 equal to 0.299048651477 ratio\n",
      "Layer to cut:  27 to  4 bits\n",
      "Total Weights:  2132 kbytes | bias = 38 kbytes ( 1.823689280560256 %)\n",
      "***************************************************************\n",
      "[]\n",
      "most footprint on layer:  26 equal to 0.270048128695 ratio\n",
      "Layer to cut:  26 to  4 bits\n",
      "Total Weights:  1844 kbytes | bias = 41 kbytes ( 2.230324279919036 %)\n",
      "***************************************************************\n",
      "[]\n",
      "most footprint on layer:  27 equal to 0.203257196575 ratio\n",
      "26 0.15610152697\n",
      "24 0.15610152697\n",
      "Layer to cut:  24 to  4 bits\n",
      "Total Weights:  1700 kbytes | bias = 43 kbytes ( 2.551418781749203 %)\n",
      "***************************************************************\n",
      "[]\n",
      "most footprint on layer:  27 equal to 0.220464629206 ratio\n",
      "Layer to cut:  27 to  2 bits\n",
      "Total Weights:  1513 kbytes | bias = 43 kbytes ( 2.867511175808633 %)\n",
      "***************************************************************\n",
      "[1]\n",
      "most footprint on layer:  26 equal to 0.190293306904 ratio\n",
      "Layer to cut:  26 to  2 bits\n",
      "Total Weights:  1369 kbytes | bias = 43 kbytes ( 3.169034171943636 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.105151463289 ratio\n",
      "22 0.105151463289\n",
      "20 0.105151463289\n",
      "18 0.105151463289\n",
      "16 0.105151463289\n",
      "14 0.105151463289\n",
      "Layer to cut:  14 to  4 bits\n",
      "Total Weights:  1297 kbytes | bias = 44 kbytes ( 3.4316027794838444 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.110986668594 ratio\n",
      "22 0.110986668594\n",
      "20 0.110986668594\n",
      "18 0.110986668594\n",
      "16 0.110986668594\n",
      "Layer to cut:  16 to  4 bits\n",
      "Total Weights:  1225 kbytes | bias = 45 kbytes ( 3.725025182011756 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.117507554604 ratio\n",
      "22 0.117507554604\n",
      "20 0.117507554604\n",
      "18 0.117507554604\n",
      "Layer to cut:  18 to  4 bits\n",
      "Total Weights:  1153 kbytes | bias = 46 kbytes ( 4.055079178011677 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.124842524485 ratio\n",
      "22 0.124842524485\n",
      "20 0.124842524485\n",
      "Layer to cut:  20 to  4 bits\n",
      "Total Weights:  1081 kbytes | bias = 47 kbytes ( 4.429081241963215 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.133154176239 ratio\n",
      "22 0.133154176239\n",
      "Layer to cut:  22 to  4 bits\n",
      "Total Weights:  1009 kbytes | bias = 49 kbytes ( 4.85643526042876 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.142651497562 ratio\n",
      "Layer to cut:  24 to  2 bits\n",
      "Total Weights:  937 kbytes | bias = 49 kbytes ( 5.229428138073571 %)\n",
      "***************************************************************\n",
      "[2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.076803840192 ratio\n",
      "20 0.076803840192\n",
      "18 0.076803840192\n",
      "16 0.076803840192\n",
      "12 0.076803840192\n",
      "10 0.038401920096\n",
      "Layer to cut:  10 to  4 bits\n",
      "Total Weights:  919 kbytes | bias = 49 kbytes ( 5.392981561729968 %)\n",
      "***************************************************************\n",
      "[2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0783074177925 ratio\n",
      "20 0.0783074177925\n",
      "18 0.0783074177925\n",
      "16 0.0783074177925\n",
      "12 0.0783074177925\n",
      "Layer to cut:  12 to  4 bits\n",
      "Total Weights:  883 kbytes | bias = 50 kbytes ( 5.7400824180683045 %)\n",
      "***************************************************************\n",
      "[2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0814983817053 ratio\n",
      "20 0.0814983817053\n",
      "18 0.0814983817053\n",
      "16 0.0814983817053\n",
      "14 0.0814983817053\n",
      "12 0.0407491908527\n",
      "Layer to cut:  12 to  2 bits\n",
      "Total Weights:  865 kbytes | bias = 50 kbytes ( 5.859466681109968 %)\n",
      "***************************************************************\n",
      "[10, 2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0831934138547 ratio\n",
      "20 0.0831934138547\n",
      "18 0.0831934138547\n",
      "16 0.0831934138547\n",
      "14 0.0831934138547\n",
      "Layer to cut:  14 to  2 bits\n",
      "Total Weights:  829 kbytes | bias = 50 kbytes ( 6.113779787133842 %)\n",
      "***************************************************************\n",
      "[10, 7, 2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0868041819723 ratio\n",
      "20 0.0868041819723\n",
      "18 0.0868041819723\n",
      "16 0.0868041819723\n",
      "Layer to cut:  16 to  2 bits\n",
      "Total Weights:  793 kbytes | bias = 50 kbytes ( 6.391169925759635 %)\n",
      "***************************************************************\n",
      "[10, 7, 6, 2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.090742600579 ratio\n",
      "20 0.090742600579\n",
      "18 0.090742600579\n",
      "Layer to cut:  18 to  2 bits\n",
      "Total Weights:  757 kbytes | bias = 50 kbytes ( 6.694927491387668 %)\n",
      "***************************************************************\n",
      "[10, 7, 6, 5, 2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0950553870908 ratio\n",
      "20 0.0950553870908\n",
      "Layer to cut:  20 to  2 bits\n",
      "Total Weights:  721 kbytes | bias = 50 kbytes ( 7.028999631819461 %)\n",
      "***************************************************************\n",
      "[10, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0997985835878 ratio\n",
      "Layer to cut:  22 to  2 bits\n",
      "Total Weights:  685 kbytes | bias = 50 kbytes ( 7.398162711709864 %)\n",
      "***************************************************************\n",
      "[10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0262600013677 ratio\n",
      "6 0.0131300006839\n",
      "4 0.00656500034193\n",
      "2 0.00164125008548\n",
      "0 0.000923203173083\n",
      "Layer to cut:  0 to  4 bits\n",
      "Total Weights:  685 kbytes | bias = 50 kbytes ( 7.4118418426979105 %)\n",
      "***************************************************************\n",
      "[10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.026272128624 ratio\n",
      "6 0.013136064312\n",
      "4 0.00656803215599\n",
      "2 0.001642008039\n",
      "0 0.000461814760968\n",
      "Layer to cut:  0 to  2 bits\n",
      "Total Weights:  684 kbytes | bias = 50 kbytes ( 7.413553686959844 %)\n",
      "***************************************************************\n",
      "[27, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0262781964535 ratio\n",
      "6 0.0131390982267\n",
      "4 0.00656954911337\n",
      "2 0.00164238727834\n",
      "1 0.000307947614689\n",
      "Layer to cut:  1 to  4 bits\n",
      "Total Weights:  684 kbytes | bias = 50 kbytes ( 7.424961857096006 %)\n",
      "***************************************************************\n",
      "[26, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0262822432305 ratio\n",
      "6 0.0131411216153\n",
      "4 0.00657056080763\n",
      "2 0.00164264020191\n",
      "1 0.000153997518929\n",
      "Layer to cut:  1 to  2 bits\n",
      "Total Weights:  684 kbytes | bias = 50 kbytes ( 7.425533613972643 %)\n",
      "***************************************************************\n",
      "[26, 27, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0262842670865 ratio\n",
      "6 0.0131421335432\n",
      "4 0.00657106677162\n",
      "2 0.00164276669291\n",
      "Layer to cut:  2 to  4 bits\n",
      "Total Weights:  684 kbytes | bias = 50 kbytes ( 7.452189301821088 %)\n",
      "***************************************************************\n",
      "[26, 27, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0263058742935 ratio\n",
      "6 0.0131529371468\n",
      "4 0.00657646857339\n",
      "2 0.000822058571673\n",
      "Layer to cut:  2 to  2 bits\n",
      "Total Weights:  683 kbytes | bias = 50 kbytes ( 7.455253629396109 %)\n",
      "***************************************************************\n",
      "[26, 27, 25, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0263166912243 ratio\n",
      "6 0.0131583456122\n",
      "4 0.00657917280609\n",
      "3 0.000616797450571\n",
      "Layer to cut:  3 to  4 bits\n",
      "Total Weights:  683 kbytes | bias = 51 kbytes ( 7.478119787024976 %)\n",
      "***************************************************************\n",
      "[26, 27, 24, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0263248097621 ratio\n",
      "6 0.0131624048811\n",
      "4 0.00658120244053\n",
      "3 0.0003084938644\n",
      "Layer to cut:  3 to  2 bits\n",
      "Total Weights:  683 kbytes | bias = 51 kbytes ( 7.47927344200849 %)\n",
      "***************************************************************\n",
      "[25, 27, 24, 26, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0263288709097 ratio\n",
      "6 0.0131644354548\n",
      "4 0.00658221772742\n",
      "Layer to cut:  4 to  4 bits\n",
      "Total Weights:  681 kbytes | bias = 51 kbytes ( 7.54524452393646 %)\n",
      "***************************************************************\n",
      "[25, 27, 24, 26, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0264158082102 ratio\n",
      "6 0.0132079041051\n",
      "4 0.00330197602628\n",
      "Layer to cut:  4 to  2 bits\n",
      "Total Weights:  680 kbytes | bias = 51 kbytes ( 7.557722232749364 %)\n",
      "***************************************************************\n",
      "[25, 27, 24, 26, 21, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0264594925152 ratio\n",
      "6 0.0132297462576\n",
      "5 0.00124028871165\n",
      "Layer to cut:  5 to  4 bits\n",
      "Total Weights:  679 kbytes | bias = 51 kbytes ( 7.603780631445891 %)\n",
      "***************************************************************\n",
      "[25, 27, 24, 26, 21, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0264759114022 ratio\n",
      "6 0.0132379557011\n",
      "5 0.00062052917349\n",
      "Layer to cut:  5 to  2 bits\n",
      "Total Weights:  679 kbytes | bias = 51 kbytes ( 7.606140547499583 %)\n",
      "***************************************************************\n",
      "[25, 27, 23, 26, 21, 24, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0264841284894 ratio\n",
      "6 0.0132420642447\n",
      "Layer to cut:  6 to  4 bits\n",
      "Total Weights:  675 kbytes | bias = 51 kbytes ( 7.698493974160924 %)\n",
      "***************************************************************\n",
      "[25, 27, 23, 26, 21, 24, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0266606495062 ratio\n",
      "6 0.00666516237655\n",
      "Layer to cut:  6 to  2 bits\n",
      "Total Weights:  672 kbytes | bias = 51 kbytes ( 7.724235616470165 %)\n",
      "***************************************************************\n",
      "[25, 27, 23, 26, 21, 24, 18, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.026749795371 ratio\n",
      "7 0.00125389665802\n",
      "Layer to cut:  7 to  4 bits\n",
      "Total Weights:  672 kbytes | bias = 52 kbytes ( 7.770904127094769 %)\n",
      "***************************************************************\n",
      "[25, 27, 23, 26, 21, 24, 18, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0267665766315 ratio\n",
      "7 0.000627341639801\n",
      "Layer to cut:  7 to  2 bits\n",
      "Total Weights:  672 kbytes | bias = 52 kbytes ( 7.773342397778049 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 21, 24, 18, 23, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0267749751599 ratio\n",
      "Layer to cut:  8 to  4 bits\n",
      "Total Weights:  663 kbytes | bias = 52 kbytes ( 7.963627154778942 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 21, 24, 18, 23, 9, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  10 equal to 0.0271382886623 ratio\n",
      "8 0.0135691443312\n",
      "Layer to cut:  8 to  2 bits\n",
      "Total Weights:  658 kbytes | bias = 52 kbytes ( 8.018026031011889 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 21, 24, 18, 23, 11, 9, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  10 equal to 0.0273236680601 ratio\n",
      "9 0.00256159388064\n",
      "Layer to cut:  9 to  4 bits\n",
      "Total Weights:  657 kbytes | bias = 53 kbytes ( 8.11380462984403 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 20, 24, 18, 23, 11, 9, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  10 equal to 0.0273587090109 ratio\n",
      "9 0.00128243948489\n",
      "Layer to cut:  9 to  2 bits\n",
      "Total Weights:  657 kbytes | bias = 53 kbytes ( 8.119010699793847 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 20, 24, 18, 23, 11, 21, 9, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  10 equal to 0.0273762632114 ratio\n",
      "Layer to cut:  10 to  2 bits\n",
      "Total Weights:  648 kbytes | bias = 53 kbytes ( 8.231687116379648 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 20, 24, 18, 23, 11, 21, 9, 8, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104085726161 ratio\n",
      "13 0.00520428630803\n",
      "11 0.00260214315401\n",
      "Layer to cut:  11 to  4 bits\n",
      "Total Weights:  647 kbytes | bias = 53 kbytes ( 8.329262187803451 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 19, 24, 18, 23, 11, 21, 9, 8, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104221325565 ratio\n",
      "13 0.00521106627825\n",
      "11 0.00130276656956\n",
      "Layer to cut:  11 to  2 bits\n",
      "Total Weights:  647 kbytes | bias = 53 kbytes ( 8.334691266378181 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 19, 24, 18, 23, 11, 21, 9, 20, 8, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104289257844 ratio\n",
      "13 0.00521446289222\n",
      "Layer to cut:  13 to  4 bits\n",
      "Total Weights:  645 kbytes | bias = 55 kbytes ( 8.530748331427258 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 19, 24, 17, 23, 11, 20, 9, 21, 8, 4, 5, 6, 3, 7, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104561874853 ratio\n",
      "21 0.00522809374263\n",
      "19 0.00522809374263\n",
      "17 0.00522809374263\n",
      "15 0.00522809374263\n",
      "13 0.00261404687131\n",
      "Layer to cut:  13 to  2 bits\n",
      "Total Weights:  644 kbytes | bias = 55 kbytes ( 8.541912811657427 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 18, 24, 17, 23, 11, 20, 9, 21, 8, 19, 4, 5, 6, 3, 7, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104698718531 ratio\n",
      "21 0.00523493592657\n",
      "19 0.00523493592657\n",
      "17 0.00523493592657\n",
      "15 0.00523493592657\n",
      "Layer to cut:  15 to  4 bits\n",
      "Total Weights:  643 kbytes | bias = 56 kbytes ( 8.739285475630721 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 18, 24, 16, 23, 11, 20, 9, 21, 8, 19, 4, 5, 6, 3, 7, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104973483261 ratio\n",
      "21 0.00524867416304\n",
      "19 0.00524867416304\n",
      "17 0.00524867416304\n",
      "15 0.00262433708152\n",
      "Layer to cut:  15 to  2 bits\n",
      "Total Weights:  642 kbytes | bias = 56 kbytes ( 8.750767958052762 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 17, 24, 16, 23, 11, 20, 9, 21, 8, 18, 4, 19, 5, 6, 3, 7, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0105111407142 ratio\n",
      "21 0.00525557035712\n",
      "19 0.00525557035712\n",
      "17 0.00525557035712\n",
      "Layer to cut:  17 to  4 bits\n",
      "Total Weights:  640 kbytes | bias = 57 kbytes ( 8.949470923672735 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 17, 24, 15, 23, 11, 20, 9, 21, 8, 18, 4, 19, 5, 6, 3, 7, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0105388345074 ratio\n",
      "21 0.00526941725368\n",
      "19 0.00526941725368\n",
      "17 0.00263470862684\n",
      "Layer to cut:  17 to  2 bits\n",
      "Total Weights:  639 kbytes | bias = 57 kbytes ( 8.961276099396027 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 16, 24, 15, 23, 11, 20, 9, 21, 8, 17, 7, 18, 3, 19, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0105527361999 ratio\n",
      "21 0.00527636809996\n",
      "19 0.00527636809996\n",
      "Layer to cut:  19 to  4 bits\n",
      "Total Weights:  637 kbytes | bias = 58 kbytes ( 9.161324295694875 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 16, 24, 14, 23, 11, 20, 9, 21, 8, 17, 7, 18, 3, 19, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0105806499017 ratio\n",
      "21 0.00529032495086\n",
      "19 0.00264516247543\n",
      "Layer to cut:  19 to  2 bits\n",
      "Total Weights:  637 kbytes | bias = 58 kbytes ( 9.173456937725703 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 15, 24, 14, 23, 11, 21, 9, 20, 8, 16, 7, 17, 6, 18, 5, 19, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0105946622032 ratio\n",
      "23 0.00529733110159\n",
      "21 0.00529733110159\n",
      "Layer to cut:  21 to  4 bits\n",
      "Total Weights:  635 kbytes | bias = 59 kbytes ( 9.374865524469936 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 15, 24, 13, 23, 11, 21, 9, 20, 8, 16, 7, 17, 6, 18, 5, 19, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0106227984435 ratio\n",
      "23 0.00531139922173\n",
      "21 0.00265569961087\n",
      "Layer to cut:  21 to  2 bits\n",
      "Total Weights:  634 kbytes | bias = 59 kbytes ( 9.387330489433866 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 14, 24, 13, 23, 11, 21, 9, 20, 8, 15, 7, 16, 6, 17, 5, 18, 4, 19, 3, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0106369226792 ratio\n",
      "23 0.00531846133959\n",
      "Layer to cut:  23 to  4 bits\n",
      "Total Weights:  632 kbytes | bias = 60 kbytes ( 9.590114861653737 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 14, 24, 12, 23, 11, 21, 9, 20, 8, 15, 7, 16, 6, 17, 5, 18, 4, 19, 3, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0106652841298 ratio\n",
      "23 0.00266632103246\n",
      "Layer to cut:  23 to  2 bits\n",
      "Total Weights:  632 kbytes | bias = 60 kbytes ( 9.60291709156083 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 13, 24, 12, 23, 11, 21, 9, 20, 8, 14, 7, 15, 6, 16, 5, 17, 4, 18, 3, 19, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0106795216464 ratio\n",
      "Layer to cut:  25 to  4 bits\n",
      "Total Weights:  628 kbytes | bias = 62 kbytes ( 10.012364779639745 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 13, 24, 12, 23, 10, 21, 9, 20, 8, 14, 7, 15, 6, 16, 5, 17, 4, 18, 3, 19, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.00536842693906 ratio\n",
      "Layer to cut:  25 to  2 bits\n",
      "Total Weights:  626 kbytes | bias = 62 kbytes ( 10.039312437308812 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 13, 24, 11, 23, 10, 21, 9, 20, 8, 14, 7, 15, 6, 16, 5, 17, 4, 18, 3, 19, 2, 12, 1, 0]\n",
      "No way\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_params_footprint(param_list):\n",
    "    LAST = len(param_list)\n",
    "    weight_footprint = [0 for x in range(LAST)]\n",
    "    weight_bits = [0 for x in range(LAST)]\n",
    "    for i,item in enumerate(param_list):\n",
    "        #number of bits\n",
    "        w_bits = item['w_bits']\n",
    "                \n",
    "        # size of convolutional parameters\n",
    "        len_w = len(item['weight'].size())\n",
    "        size_param = 1\n",
    "        for v in range(len_w):\n",
    "            size_param *= item['weight'].size(v)\n",
    "        w_footprint = (size_param * w_bits) / 8 #measure in bytes\n",
    "        weight_footprint[i] = w_footprint\n",
    "        weight_bits[i] = w_bits\n",
    "        \n",
    "        #print('------------------- This is the layer ', i, '---------------')\n",
    "        #print('Weight Bits: ',w_bits)\n",
    "        #print('Number of parameters: ',size_param)\n",
    "        #print('Parameters footprint: ',w_footprint)\n",
    "        #if i is LAST-1:\n",
    "        #    print('no')\n",
    "        #else:\n",
    "        #    print('Activation Bits = ',item['act_o_bits'])\n",
    "    \n",
    "    return weight_footprint,sum(weight_footprint), weight_bits\n",
    "\n",
    "\n",
    "def compute_bias_footprint(param_list):\n",
    "    BIAS_PACT_BITS = 32\n",
    "    BIAS_PACTCH_BITS = 8\n",
    "    BIAS_M0_BITS = 32\n",
    "    BIAS_N0_BITS = 8\n",
    "\n",
    "    LAST = len(param_list)\n",
    "    weight_footprint = [0 for x in range(LAST)]\n",
    "    for i,item in enumerate(param_list):\n",
    "\n",
    "        #number of bits\n",
    "        w_bits = item['w_bits']\n",
    "        n_out_ch = item['weight'].size(0)\n",
    "        \n",
    "        # size of extra parametes\n",
    "        quant_type = item['quant_type']\n",
    "        fold_type = item['fold_type']\n",
    "        if quant_type is None:\n",
    "            quant_type = 'PACT'\n",
    "            fold_type = 'folding_weights'\n",
    "        elif w_bits < 8:\n",
    "            quant_type = 'PACT_CHANNEL'\n",
    "            fold_type = 'fixed_batch'\n",
    "        item['quant_type'] = quant_type\n",
    "        item['fold_type'] = fold_type\n",
    "        \n",
    "        # compute number of bias\n",
    "        if quant_type == 'PACT':\n",
    "            bias_size = int(n_out_ch * BIAS_PACT_BITS / 8 )\n",
    "        elif quant_type == 'PACT_CHANNEL':\n",
    "            bias_size  = int(n_out_ch * BIAS_M0_BITS / 8 ) # M0 \n",
    "            bias_size += int(n_out_ch * BIAS_N0_BITS / 8 ) # N0 \n",
    "            bias_size += int(n_out_ch * BIAS_PACTCH_BITS / 8 ) # B \n",
    "            bias_size += int(n_out_ch * BIAS_PACTCH_BITS / 8 ) # ZW \n",
    "\n",
    "        else:\n",
    "            print('ERROR!!!')\n",
    "        \n",
    "        weight_footprint[i] = bias_size\n",
    "        \n",
    "    return weight_footprint\n",
    "\n",
    "def compute_footprint(param_list):\n",
    "    weight_footprint, _, _ = compute_params_footprint(param_list)\n",
    "    bias_footprint = compute_bias_footprint(param_list)\n",
    "    ratio = np.array(bias_footprint) / np.array(weight_footprint)\n",
    "    weight_footprint = sum(weight_footprint)\n",
    "    bias_footprint = sum(bias_footprint)\n",
    "    print('Total Weights: ',int(weight_footprint/1024),'kbytes | bias =' ,int(bias_footprint/1024),'kbytes ('\\\n",
    "          ,100*bias_footprint/weight_footprint ,'%)')\n",
    "    #print('max:', ratio * 100,'%')\n",
    "    return bias_footprint+weight_footprint\n",
    "        \n",
    "        \n",
    "def compute_next_cut(param_list):\n",
    "    MAX = 0.05\n",
    "\n",
    "    f,_,w_b = compute_params_footprint(param_list)\n",
    "    weight = np.array(f)\n",
    "    total = weight.sum()\n",
    "    perc = weight / total\n",
    "    arr = np.sort(perc)[::-1]\n",
    "    ind = np.argsort(perc)[::-1]\n",
    "    \n",
    "    # remove w_bit = 1 from search\n",
    "    rm = []\n",
    "    for i,item in enumerate(w_b):\n",
    "        if item == MIN_BIT:\n",
    "            for i_x,x in enumerate(ind):\n",
    "                if x == i:\n",
    "                    rm.append(i_x)\n",
    "                    continue\n",
    "    print(rm)\n",
    "    arr = np.delete(arr, rm)\n",
    "    ind = np.delete(ind, rm)\n",
    "    \n",
    "    if len(arr) == 0:\n",
    "        return -1\n",
    "        \n",
    "    #print(arr, ind)\n",
    "    LAST = len(arr)\n",
    "\n",
    "    cut_we = arr[0]\n",
    "    cut_i = ind[0]\n",
    "    print('most footprint on layer: ',cut_i, 'equal to', cut_we, 'ratio' )\n",
    "    thr = cut_we - MAX\n",
    "    for i in range(1, LAST):\n",
    "        if arr[i] > thr and ind[i] < cut_i:\n",
    "            cut_i = ind[i]\n",
    "            print(ind[i],arr[i])\n",
    "    return(cut_i)            \n",
    "\n",
    "    \n",
    "def memory_driven_quantization(param_list, read_only_mem):\n",
    "    _, s, _ = compute_params_footprint(param_list)\n",
    "    print('Total Footprint: ', s )\n",
    "    while (s>read_only_mem):\n",
    "        print('***************************************************************')\n",
    "        c = compute_next_cut(param_list)\n",
    "        if c == -1:\n",
    "            return -1\n",
    "        nb = int(param_list[c]['w_bits'] / 2)\n",
    "        print('Layer to cut: ', c, 'to ', nb  ,'bits')\n",
    "        param_list[c]['w_bits'] = nb\n",
    "        #_, s, _ = compute_params_footprint(param_list)\n",
    "        s = compute_footprint(param_list)\n",
    "    return(param_list)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "ONLY_READ_MEM = 512*1024\n",
    "param_list = copy.deepcopy(quantizer_load.param_to_quantize)\n",
    "param_list = memory_driven_quantization(param_list, ONLY_READ_MEM)\n",
    "if param_list == -1:\n",
    "    print('No way')\n",
    "else:\n",
    "    _, s, w_bits = compute_params_footprint(param_list)\n",
    "    print('-----------  Total Footprint: ', s )\n",
    "    print('Final Quantization: ', w_bits )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0032, 0.0032, 0.0032, 0.0032, 5.1141, 0.1086, 4.4318, 3.4280, 3.2614,\n",
       "        0.0032, 1.4061, 0.0032, 1.3680, 5.7485, 0.0032, 0.0032, 0.0032, 0.0074,\n",
       "        1.0969, 3.7479, 1.9707, 4.7107, 1.5083, 2.8547, 4.3182, 0.0032, 2.3813,\n",
       "        0.0032, 0.0868, 1.5090, 4.2023, 0.0032], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0080,  0.6109,  0.4207,  0.6073,  0.1026,  0.0430,  0.1581,  0.2290,\n",
       "         0.1794,  0.2899,  0.2980,  1.3301,  0.3118,  0.1344,  0.2803, -0.8490,\n",
       "         0.6654, -0.8486, -0.3533,  0.2579,  0.1002,  0.0991,  0.1978,  0.2172,\n",
       "         0.1020,  0.0547,  0.1285,  0.7153, -0.0120,  0.0705,  0.2167, -0.1538],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_over_sigma = gamma_tensor / sigma_tensor\n",
    "gamma_over_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080]],\n",
       "\n",
       "         [[ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080]],\n",
       "\n",
       "         [[ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109]],\n",
       "\n",
       "         [[ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109]],\n",
       "\n",
       "         [[ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207]],\n",
       "\n",
       "         [[ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207]],\n",
       "\n",
       "         [[ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073]],\n",
       "\n",
       "         [[ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073]],\n",
       "\n",
       "         [[ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026]],\n",
       "\n",
       "         [[ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026]],\n",
       "\n",
       "         [[ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430]],\n",
       "\n",
       "         [[ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430]],\n",
       "\n",
       "         [[ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581]],\n",
       "\n",
       "         [[ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581]],\n",
       "\n",
       "         [[ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290]],\n",
       "\n",
       "         [[ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290]],\n",
       "\n",
       "         [[ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794]],\n",
       "\n",
       "         [[ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794]],\n",
       "\n",
       "         [[ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899]],\n",
       "\n",
       "         [[ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899]],\n",
       "\n",
       "         [[ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980]],\n",
       "\n",
       "         [[ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980]],\n",
       "\n",
       "         [[ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301]],\n",
       "\n",
       "         [[ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301]],\n",
       "\n",
       "         [[ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118]],\n",
       "\n",
       "         [[ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118]],\n",
       "\n",
       "         [[ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344]],\n",
       "\n",
       "         [[ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344]],\n",
       "\n",
       "         [[ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803]],\n",
       "\n",
       "         [[ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803]],\n",
       "\n",
       "         [[ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803]]],\n",
       "\n",
       "\n",
       "        [[[-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490]],\n",
       "\n",
       "         [[-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490]],\n",
       "\n",
       "         [[-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654]],\n",
       "\n",
       "         [[ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654]],\n",
       "\n",
       "         [[ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654]]],\n",
       "\n",
       "\n",
       "        [[[-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486]],\n",
       "\n",
       "         [[-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486]],\n",
       "\n",
       "         [[-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486]]],\n",
       "\n",
       "\n",
       "        [[[-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533]],\n",
       "\n",
       "         [[-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533]],\n",
       "\n",
       "         [[-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579]],\n",
       "\n",
       "         [[ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579]],\n",
       "\n",
       "         [[ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002]],\n",
       "\n",
       "         [[ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002]],\n",
       "\n",
       "         [[ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991]],\n",
       "\n",
       "         [[ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991]],\n",
       "\n",
       "         [[ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978]],\n",
       "\n",
       "         [[ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978]],\n",
       "\n",
       "         [[ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172]],\n",
       "\n",
       "         [[ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172]],\n",
       "\n",
       "         [[ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020]],\n",
       "\n",
       "         [[ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020]],\n",
       "\n",
       "         [[ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547]],\n",
       "\n",
       "         [[ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547]],\n",
       "\n",
       "         [[ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285]],\n",
       "\n",
       "         [[ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285]],\n",
       "\n",
       "         [[ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153]],\n",
       "\n",
       "         [[ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153]],\n",
       "\n",
       "         [[ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153]]],\n",
       "\n",
       "\n",
       "        [[[-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120]],\n",
       "\n",
       "         [[-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120]],\n",
       "\n",
       "         [[-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705]],\n",
       "\n",
       "         [[ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705]],\n",
       "\n",
       "         [[ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167]],\n",
       "\n",
       "         [[ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167]],\n",
       "\n",
       "         [[ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167]]],\n",
       "\n",
       "\n",
       "        [[[-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538]],\n",
       "\n",
       "         [[-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538]],\n",
       "\n",
       "         [[-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538]]]], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_over_sigma_tensor = gamma_over_sigma.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand(v0,v1,v2,v3 )\n",
    "gamma_over_sigma_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = weight * gamma_over_sigma_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1085, -0.2053, -0.0891],\n",
       "         [ 0.1201,  0.1860,  0.1317],\n",
       "         [ 0.0116,  0.0116,  0.0271]],\n",
       "\n",
       "        [[-0.2867, -0.4184, -0.2053],\n",
       "         [ 0.1627,  0.2944,  0.1976],\n",
       "         [ 0.0349,  0.0465, -0.0155]],\n",
       "\n",
       "        [[-0.0542, -0.1356, -0.0310],\n",
       "         [ 0.0930,  0.1278,  0.0659],\n",
       "         [ 0.0271, -0.0155, -0.0271]]], device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_tensor[10].div(d255).round().mul(d255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039, device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = weight_tensor.data.max()-weight_tensor.data.min()\n",
    "d255 = delta / 255\n",
    "d255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1085, -0.2053, -0.0891],\n",
       "         [ 0.1201,  0.1860,  0.1317],\n",
       "         [ 0.0116,  0.0116,  0.0271]],\n",
       "\n",
       "        [[-0.2867, -0.4184, -0.2053],\n",
       "         [ 0.1627,  0.2944,  0.1976],\n",
       "         [ 0.0349,  0.0465, -0.0155]],\n",
       "\n",
       "        [[-0.0542, -0.1356, -0.0310],\n",
       "         [ 0.0930,  0.1278,  0.0659],\n",
       "         [ 0.0271, -0.0155, -0.0271]]], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.model[0].weight.data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6001, device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.restore_real_value()            \n",
    "quantizer.backprop_quant_gradients()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReLU6' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-db7f3e6b9759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 366\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReLU6' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "quantizer.deployment_model.model[1].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2.9390e-03\n",
       " 6.2338e-04\n",
       " 5.6052e-45\n",
       " 1.7143e-01\n",
       " 1.7013e-02\n",
       " 4.5158e-02\n",
       " 2.7970e-03\n",
       " 7.9009e-02\n",
       " 1.0388e-01\n",
       " 5.0715e-02\n",
       " 4.5062e-02\n",
       " 3.5231e-01\n",
       " 8.3555e-03\n",
       " 9.0936e-05\n",
       " 2.9523e-02\n",
       " 8.0683e-01\n",
       " 4.9630e-03\n",
       " 5.5362e-01\n",
       " 5.4003e-03\n",
       " 9.3962e-03\n",
       " 5.6052e-45\n",
       " 7.3168e-04\n",
       " 1.3904e-03\n",
       " 8.2610e-03\n",
       " 9.0941e-04\n",
       " 1.1302e-02\n",
       " 5.6052e-45\n",
       " 1.0920e-02\n",
       " 7.0269e-02\n",
       " 1.7943e-03\n",
       " 8.2472e-03\n",
       " 4.1882e-02\n",
       "[torch.cuda.FloatTensor of size 32 (GPU 2)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model[0][1].running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0  , 0  ,.,.) = \n",
       "  0.0022\n",
       "\n",
       "( 0  , 1  ,.,.) = \n",
       "  0.2660\n",
       "\n",
       "( 0  , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 0  ,1021,.,.) = \n",
       "  0.0112\n",
       "\n",
       "( 0  ,1022,.,.) = \n",
       "  0.0024\n",
       "\n",
       "( 0  ,1023,.,.) = \n",
       "  0.0000\n",
       "          \n",
       "\n",
       "( 1  , 0  ,.,.) = \n",
       "  0.0022\n",
       "\n",
       "( 1  , 1  ,.,.) = \n",
       "  0.2753\n",
       "\n",
       "( 1  , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 1  ,1021,.,.) = \n",
       "  0.0117\n",
       "\n",
       "( 1  ,1022,.,.) = \n",
       "  0.0023\n",
       "\n",
       "( 1  ,1023,.,.) = \n",
       "  0.0000\n",
       "          \n",
       "\n",
       "( 2  , 0  ,.,.) = \n",
       "  0.0022\n",
       "\n",
       "( 2  , 1  ,.,.) = \n",
       "  0.2900\n",
       "\n",
       "( 2  , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 2  ,1021,.,.) = \n",
       "  0.0102\n",
       "\n",
       "( 2  ,1022,.,.) = \n",
       "  0.0023\n",
       "\n",
       "( 2  ,1023,.,.) = \n",
       "  0.0000\n",
       " ...      \n",
       "          \n",
       "\n",
       "( 61 , 0  ,.,.) = \n",
       "  0.0021\n",
       "\n",
       "( 61 , 1  ,.,.) = \n",
       "  0.2783\n",
       "\n",
       "( 61 , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 61 ,1021,.,.) = \n",
       "  0.0094\n",
       "\n",
       "( 61 ,1022,.,.) = \n",
       "  0.0023\n",
       "\n",
       "( 61 ,1023,.,.) = \n",
       "  0.0000\n",
       "          \n",
       "\n",
       "( 62 , 0  ,.,.) = \n",
       "  0.0022\n",
       "\n",
       "( 62 , 1  ,.,.) = \n",
       "  0.2816\n",
       "\n",
       "( 62 , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 62 ,1021,.,.) = \n",
       "  0.0087\n",
       "\n",
       "( 62 ,1022,.,.) = \n",
       "  0.0024\n",
       "\n",
       "( 62 ,1023,.,.) = \n",
       "  0.0000\n",
       "          \n",
       "\n",
       "( 63 , 0  ,.,.) = \n",
       "  0.0022\n",
       "\n",
       "( 63 , 1  ,.,.) = \n",
       "  0.2836\n",
       "\n",
       "( 63 , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 63 ,1021,.,.) = \n",
       "  0.0113\n",
       "\n",
       "( 63 ,1022,.,.) = \n",
       "  0.0024\n",
       "\n",
       "( 63 ,1023,.,.) = \n",
       "  0.0000\n",
       "[torch.cuda.FloatTensor of size 64x1024x1x1 (GPU 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2 = quantizer.deployment_model.model(input_var)\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "  0.0000  0.0045  0.0045\n",
       "  0.0045  0.0045  0.0045\n",
       "  0.0000  0.0045  0.0045\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       " -0.0045 -0.0045 -0.0000\n",
       " -0.0045 -0.0000 -0.0045\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       "  0.0000  0.0045  0.0000\n",
       "  0.0000  0.0000  0.0045\n",
       "  0.0000  0.0045  0.0045\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       "  0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       "  0.0180  0.0271 -0.0225\n",
       " -0.0135 -0.0406 -0.0406\n",
       " -0.0361 -0.0406 -0.0000\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       "  0.0406  0.0180 -0.0271\n",
       " -0.0406 -0.0857 -0.0722\n",
       " -0.0496 -0.0496 -0.0000\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       "  0.0406  0.0225 -0.0271\n",
       " -0.0361 -0.0406 -0.0451\n",
       " -0.0406 -0.0271  0.0045\n",
       "\n",
       "(3 ,0 ,.,.) = \n",
       " -0.0000 -0.0135 -0.0000\n",
       " -0.0000 -0.0045 -0.0045\n",
       " -0.0045  0.0045 -0.0045\n",
       "\n",
       "(3 ,1 ,.,.) = \n",
       " -0.0135 -0.0045 -0.0000\n",
       " -0.0135 -0.0000 -0.0045\n",
       "  0.0000  0.0000  0.0045\n",
       "\n",
       "(3 ,2 ,.,.) = \n",
       " -0.0045 -0.0135  0.0000\n",
       " -0.0000 -0.0000 -0.0045\n",
       " -0.0045  0.0045  0.0045\n",
       "\n",
       "(4 ,0 ,.,.) = \n",
       "  0.0045 -0.0135 -0.0045\n",
       "  0.0045 -0.0045  0.0045\n",
       "  0.0000 -0.0045 -0.0045\n",
       "\n",
       "(4 ,1 ,.,.) = \n",
       " -0.0045 -0.0045 -0.0000\n",
       "  0.0135 -0.0045  0.0000\n",
       "  0.0135  0.0135  0.0135\n",
       "\n",
       "(4 ,2 ,.,.) = \n",
       "  0.0135 -0.0045  0.0135\n",
       "  0.0045  0.0135  0.0135\n",
       "  0.0135  0.0135  0.0135\n",
       "\n",
       "(5 ,0 ,.,.) = \n",
       " -0.1759 -0.1894 -0.0541\n",
       "  0.3066  0.2480  0.0857\n",
       " -0.0722 -0.0947  0.0225\n",
       "\n",
       "(5 ,1 ,.,.) = \n",
       " -0.2976 -0.2886 -0.1127\n",
       "  0.4374  0.4194  0.1623\n",
       " -0.0722 -0.1218 -0.0090\n",
       "\n",
       "(5 ,2 ,.,.) = \n",
       " -0.0316 -0.0586  0.0045\n",
       "  0.2165  0.1759  0.0767\n",
       " -0.0586 -0.0767  0.0361\n",
       "\n",
       "(6 ,0 ,.,.) = \n",
       "  0.0045  0.0045 -0.0000\n",
       "  0.0045  0.0000  0.0045\n",
       "  0.0000  0.0045  0.0045\n",
       "\n",
       "(6 ,1 ,.,.) = \n",
       " -0.0000  0.0045  0.0045\n",
       " -0.0000  0.0000 -0.0000\n",
       "  0.0045  0.0045 -0.0000\n",
       "\n",
       "(6 ,2 ,.,.) = \n",
       " -0.0000  0.0000  0.0000\n",
       " -0.0045  0.0000  0.0045\n",
       "  0.0000 -0.0000  0.0045\n",
       "\n",
       "(7 ,0 ,.,.) = \n",
       "  0.0225  0.1443  0.0406\n",
       "  0.0857  0.2300  0.0857\n",
       "  0.1308  0.1939  0.0271\n",
       "\n",
       "(7 ,1 ,.,.) = \n",
       " -0.0406  0.0135 -0.0406\n",
       " -0.0767 -0.0045 -0.0451\n",
       " -0.0361 -0.0180 -0.0631\n",
       "\n",
       "(7 ,2 ,.,.) = \n",
       " -0.0225  0.0135 -0.0225\n",
       " -0.1263 -0.0812 -0.0676\n",
       " -0.0902 -0.0767 -0.0631\n",
       "\n",
       "(8 ,0 ,.,.) = \n",
       "  0.0045  0.0135 -0.0180\n",
       " -0.0135 -0.0000 -0.0180\n",
       " -0.0135  0.0045  0.0045\n",
       "\n",
       "(8 ,1 ,.,.) = \n",
       "  0.0135  0.0135 -0.0135\n",
       "  0.0135  0.0045  0.0225\n",
       "  0.0225  0.0135  0.0225\n",
       "\n",
       "(8 ,2 ,.,.) = \n",
       " -0.0045  0.0045  0.0045\n",
       " -0.0225  0.0135 -0.0000\n",
       " -0.0135 -0.0000  0.0045\n",
       "\n",
       "(9 ,0 ,.,.) = \n",
       " -0.0812 -0.0586 -0.1443\n",
       " -0.1984 -0.1668 -0.2390\n",
       " -0.1353 -0.0857 -0.1939\n",
       "\n",
       "(9 ,1 ,.,.) = \n",
       "  0.0406  0.1533  0.0631\n",
       "  0.0135  0.1353  0.0271\n",
       "  0.0496  0.1488 -0.0045\n",
       "\n",
       "(9 ,2 ,.,.) = \n",
       "  0.0451  0.1894  0.0676\n",
       "  0.1127  0.2435  0.0992\n",
       "  0.1082  0.2119  0.0271\n",
       "\n",
       "(10,0 ,.,.) = \n",
       "  0.0406  0.0586  0.0631\n",
       "  0.0406  0.0496  0.0586\n",
       "  0.0406  0.0451  0.0496\n",
       "\n",
       "(10,1 ,.,.) = \n",
       " -0.0045  0.0045  0.0135\n",
       " -0.0135 -0.0135  0.0000\n",
       " -0.0045 -0.0180 -0.0135\n",
       "\n",
       "(10,2 ,.,.) = \n",
       " -0.0361 -0.0361 -0.0225\n",
       " -0.0451 -0.0586 -0.0406\n",
       " -0.0361 -0.0586 -0.0496\n",
       "\n",
       "(11,0 ,.,.) = \n",
       "  0.0361  0.0180  0.0135\n",
       "  0.0406  0.0271  0.0180\n",
       "  0.0451  0.0361  0.0225\n",
       "\n",
       "(11,1 ,.,.) = \n",
       " -0.0135 -0.0225 -0.0406\n",
       " -0.0045 -0.0180 -0.0361\n",
       " -0.0000 -0.0135 -0.0271\n",
       "\n",
       "(11,2 ,.,.) = \n",
       " -0.0225 -0.0271 -0.0271\n",
       " -0.0135 -0.0225 -0.0361\n",
       " -0.0135 -0.0180 -0.0271\n",
       "\n",
       "(12,0 ,.,.) = \n",
       "  0.0496  0.0676 -0.1037\n",
       "  0.0045  0.1082 -0.0135\n",
       " -0.1037 -0.0045  0.0451\n",
       "\n",
       "(12,1 ,.,.) = \n",
       "  0.1037  0.1398 -0.0451\n",
       "  0.0586  0.1759  0.0631\n",
       " -0.0676  0.0180  0.0902\n",
       "\n",
       "(12,2 ,.,.) = \n",
       "  0.0406  0.0722 -0.0271\n",
       "  0.0045  0.0857  0.0361\n",
       " -0.0812 -0.0135  0.0722\n",
       "\n",
       "(13,0 ,.,.) = \n",
       "  0.0000 -0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(13,1 ,.,.) = \n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(13,2 ,.,.) = \n",
       "  0.0045  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(14,0 ,.,.) = \n",
       "  0.0045  0.0045  0.0000\n",
       "  0.0045  0.0045  0.0000\n",
       "  0.0000  0.0000 -0.0000\n",
       "\n",
       "(14,1 ,.,.) = \n",
       "  0.0045  0.0045  0.0000\n",
       "  0.0000  0.0045  0.0000\n",
       " -0.0000  0.0000 -0.0000\n",
       "\n",
       "(14,2 ,.,.) = \n",
       "  0.0000  0.0045  0.0000\n",
       " -0.0000  0.0000 -0.0000\n",
       " -0.0045 -0.0000 -0.0000\n",
       "\n",
       "(15,0 ,.,.) = \n",
       "  0.1759 -0.0271 -0.1939\n",
       " -0.2255  0.5772  0.5772\n",
       "  0.0541 -0.5727 -0.5727\n",
       "\n",
       "(15,1 ,.,.) = \n",
       " -0.1804  0.0631  0.1578\n",
       " -0.0406  0.5772  0.5772\n",
       "  0.2165 -0.5727 -0.5727\n",
       "\n",
       "(15,2 ,.,.) = \n",
       "  0.2796 -0.0812 -0.2345\n",
       " -0.1939  0.4239  0.5772\n",
       "  0.0722 -0.4104 -0.5727\n",
       "\n",
       "(16,0 ,.,.) = \n",
       "  0.0000  0.0857 -0.0722\n",
       "  0.1037  0.0722 -0.2345\n",
       "  0.0947  0.0496 -0.2570\n",
       "\n",
       "(16,1 ,.,.) = \n",
       "  0.0406  0.1533 -0.1218\n",
       "  0.2435  0.0676 -0.4600\n",
       "  0.1668  0.0451 -0.4149\n",
       "\n",
       "(16,2 ,.,.) = \n",
       "  0.0406  0.1037 -0.0045\n",
       "  0.0676  0.1037 -0.1533\n",
       " -0.0676  0.0361 -0.1488\n",
       "\n",
       "(17,0 ,.,.) = \n",
       " -0.0000  0.0045  0.0045\n",
       "  0.0225  0.0225  0.0225\n",
       "  0.0271  0.0271  0.0271\n",
       "\n",
       "(17,1 ,.,.) = \n",
       "  0.0045  0.0135  0.0135\n",
       "  0.0225  0.0361  0.0361\n",
       "  0.0271  0.0361  0.0361\n",
       "\n",
       "(17,2 ,.,.) = \n",
       " -0.0045 -0.0000  0.0000\n",
       "  0.0045  0.0135  0.0180\n",
       "  0.0135  0.0180  0.0180\n",
       "\n",
       "(18,0 ,.,.) = \n",
       " -0.0586 -0.0451 -0.0451\n",
       " -0.0541 -0.0541 -0.0451\n",
       " -0.0631 -0.0676 -0.0586\n",
       "\n",
       "(18,1 ,.,.) = \n",
       "  0.0812  0.0902  0.0812\n",
       "  0.1037  0.1082  0.0992\n",
       "  0.0812  0.0812  0.0767\n",
       "\n",
       "(18,2 ,.,.) = \n",
       " -0.0586 -0.0406 -0.0180\n",
       " -0.0451 -0.0541 -0.0271\n",
       " -0.0361 -0.0631 -0.0541\n",
       "\n",
       "(19,0 ,.,.) = \n",
       "  0.0000 -0.0000  0.0000\n",
       "  0.0000 -0.0000 -0.0045\n",
       "  0.0000 -0.0000  0.0000\n",
       "\n",
       "(19,1 ,.,.) = \n",
       " -0.0000 -0.0045 -0.0000\n",
       " -0.0000 -0.0045 -0.0045\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(19,2 ,.,.) = \n",
       " -0.0000 -0.0000  0.0000\n",
       "  0.0000 -0.0000 -0.0000\n",
       "  0.0000 -0.0000  0.0000\n",
       "\n",
       "(20,0 ,.,.) = \n",
       " -0.0045 -0.0045  0.0000\n",
       " -0.0000 -0.0045 -0.0000\n",
       " -0.0045 -0.0000 -0.0045\n",
       "\n",
       "(20,1 ,.,.) = \n",
       " -0.0000  0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0045 -0.0000  0.0045\n",
       "\n",
       "(20,2 ,.,.) = \n",
       " -0.0045 -0.0000 -0.0045\n",
       "  0.0045 -0.0000 -0.0000\n",
       " -0.0045  0.0000  0.0000\n",
       "\n",
       "(21,0 ,.,.) = \n",
       "  0.0045  0.0135  0.0135\n",
       "  0.0045  0.0135  0.0180\n",
       "  0.0045  0.0135  0.0180\n",
       "\n",
       "(21,1 ,.,.) = \n",
       "  0.0135  0.0180  0.0180\n",
       "  0.0135  0.0180  0.0180\n",
       "  0.0000  0.0045  0.0135\n",
       "\n",
       "(21,2 ,.,.) = \n",
       "  0.0000  0.0045  0.0045\n",
       "  0.0000  0.0135  0.0135\n",
       " -0.0045  0.0045  0.0135\n",
       "\n",
       "(22,0 ,.,.) = \n",
       "  0.0812  0.0225  0.0722\n",
       "  0.0676  0.0135  0.0722\n",
       "  0.0631 -0.0135  0.0812\n",
       "\n",
       "(22,1 ,.,.) = \n",
       "  0.0812 -0.0361  0.0586\n",
       " -0.0045 -0.1082  0.0180\n",
       " -0.0406 -0.1443  0.0225\n",
       "\n",
       "(22,2 ,.,.) = \n",
       " -0.0135 -0.1127  0.0135\n",
       " -0.1263 -0.2165 -0.0361\n",
       " -0.1488 -0.2165 -0.0225\n",
       "\n",
       "(23,0 ,.,.) = \n",
       "  0.0496  0.0676  0.0586\n",
       "  0.1218  0.1263  0.0676\n",
       "  0.0812  0.1488  0.1037\n",
       "\n",
       "(23,1 ,.,.) = \n",
       " -0.2751 -0.3202 -0.2931\n",
       " -0.3157 -0.3562 -0.3608\n",
       " -0.3202 -0.2751 -0.2570\n",
       "\n",
       "(23,2 ,.,.) = \n",
       "  0.2390  0.2119  0.1759\n",
       "  0.2119  0.2029  0.1668\n",
       "  0.1398  0.2029  0.1894\n",
       "\n",
       "(24,0 ,.,.) = \n",
       "  0.0045 -0.0045 -0.0000\n",
       "  0.0045  0.0045  0.0000\n",
       " -0.0045 -0.0045 -0.0045\n",
       "\n",
       "(24,1 ,.,.) = \n",
       " -0.0045  0.0000 -0.0000\n",
       " -0.0000 -0.0045 -0.0045\n",
       " -0.0000 -0.0000  0.0000\n",
       "\n",
       "(24,2 ,.,.) = \n",
       " -0.0000 -0.0000  0.0000\n",
       " -0.0045  0.0000  0.0045\n",
       " -0.0045  0.0045 -0.0045\n",
       "\n",
       "(25,0 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0045  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(25,1 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(25,2 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(26,0 ,.,.) = \n",
       "  0.1398  0.1443  0.0496\n",
       "  0.1939  0.2255  0.1533\n",
       "  0.1488  0.1804  0.1533\n",
       "\n",
       "(26,1 ,.,.) = \n",
       " -0.0676 -0.0767 -0.0767\n",
       " -0.0902 -0.0992 -0.0902\n",
       " -0.0767 -0.0992 -0.0812\n",
       "\n",
       "(26,2 ,.,.) = \n",
       " -0.0767 -0.0586 -0.0000\n",
       " -0.1037 -0.1263 -0.0676\n",
       " -0.0496 -0.0902 -0.0451\n",
       "\n",
       "(27,0 ,.,.) = \n",
       " -0.0406 -0.1714 -0.0271\n",
       " -0.1714 -0.2931 -0.1037\n",
       " -0.0451 -0.1533 -0.0180\n",
       "\n",
       "(27,1 ,.,.) = \n",
       "  0.0271 -0.0812  0.0496\n",
       " -0.0722 -0.1759  0.0135\n",
       "  0.0180 -0.0586  0.0857\n",
       "\n",
       "(27,2 ,.,.) = \n",
       "  0.1082  0.0045  0.0586\n",
       "  0.0496 -0.0406  0.0722\n",
       "  0.0857  0.0225  0.0812\n",
       "\n",
       "(28,0 ,.,.) = \n",
       "  0.0271  0.0767  0.0857\n",
       "  0.0225  0.0361  0.0992\n",
       "  0.0406 -0.0406  0.0406\n",
       "\n",
       "(28,1 ,.,.) = \n",
       " -0.1037 -0.0857 -0.0767\n",
       " -0.1263 -0.1533 -0.0857\n",
       " -0.0857 -0.2029 -0.1218\n",
       "\n",
       "(28,2 ,.,.) = \n",
       "  0.0676  0.1037  0.0676\n",
       "  0.0812  0.0857  0.0992\n",
       "  0.0992  0.0271  0.0767\n",
       "\n",
       "(29,0 ,.,.) = \n",
       "  0.0000  0.0000  0.0045\n",
       " -0.0045 -0.0000  0.0000\n",
       " -0.0045 -0.0045 -0.0000\n",
       "\n",
       "(29,1 ,.,.) = \n",
       " -0.0045 -0.0045  0.0000\n",
       " -0.0045 -0.0045 -0.0000\n",
       " -0.0045 -0.0045 -0.0000\n",
       "\n",
       "(29,2 ,.,.) = \n",
       " -0.0000  0.0000  0.0000\n",
       " -0.0000 -0.0000  0.0000\n",
       " -0.0045 -0.0000  0.0000\n",
       "\n",
       "(30,0 ,.,.) = \n",
       " -0.0947  0.1172 -0.0361\n",
       "  0.0135  0.1714 -0.0045\n",
       " -0.0135  0.1714 -0.1263\n",
       "\n",
       "(30,1 ,.,.) = \n",
       " -0.0676  0.1308  0.0180\n",
       "  0.0180  0.2255  0.0631\n",
       " -0.0180  0.1849 -0.1127\n",
       "\n",
       "(30,2 ,.,.) = \n",
       " -0.0812  0.1263 -0.0045\n",
       "  0.0180  0.1488 -0.0045\n",
       " -0.0135  0.1353 -0.0947\n",
       "\n",
       "(31,0 ,.,.) = \n",
       "  0.0000 -0.0045 -0.0045\n",
       " -0.0045 -0.0180 -0.0090\n",
       " -0.0045 -0.0000 -0.0090\n",
       "\n",
       "(31,1 ,.,.) = \n",
       " -0.0045  0.0045  0.0045\n",
       " -0.0045 -0.0045 -0.0045\n",
       "  0.0000 -0.0045  0.0000\n",
       "\n",
       "(31,2 ,.,.) = \n",
       "  0.0000 -0.0045 -0.0000\n",
       " -0.0045 -0.0045  0.0000\n",
       "  0.0000 -0.0045 -0.0000\n",
       "[torch.cuda.FloatTensor of size 32x3x3x3 (GPU 2)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.model[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "  0.0000  0.0838  0.0838\n",
       "  0.0838  0.0838  0.0838\n",
       "  0.0000  0.0838  0.0838\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       " -0.0838 -0.0838 -0.0000\n",
       " -0.0838 -0.0000 -0.0838\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       "  0.0000  0.0838  0.0000\n",
       "  0.0000  0.0000  0.0838\n",
       "  0.0000  0.0838  0.0838\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       "  0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       "  0.1329  0.1994 -0.1662\n",
       " -0.0997 -0.2991 -0.2991\n",
       " -0.2659 -0.2991 -0.0000\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       "  0.2991  0.1329 -0.1994\n",
       " -0.2991 -0.6314 -0.5317\n",
       " -0.3656 -0.3656 -0.0000\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       "  0.2991  0.1662 -0.1994\n",
       " -0.2659 -0.2991 -0.3323\n",
       " -0.2991 -0.1994  0.0332\n",
       "\n",
       "(3 ,0 ,.,.) = \n",
       " -0.0000 -0.0540 -0.0000\n",
       " -0.0000 -0.0180 -0.0180\n",
       " -0.0180  0.0180 -0.0180\n",
       "\n",
       "(3 ,1 ,.,.) = \n",
       " -0.0540 -0.0180 -0.0000\n",
       " -0.0540 -0.0000 -0.0180\n",
       "  0.0000  0.0000  0.0180\n",
       "\n",
       "(3 ,2 ,.,.) = \n",
       " -0.0180 -0.0540  0.0000\n",
       " -0.0000 -0.0000 -0.0180\n",
       " -0.0180  0.0180  0.0180\n",
       "\n",
       "(4 ,0 ,.,.) = \n",
       "  0.0076 -0.0228 -0.0076\n",
       "  0.0076 -0.0076  0.0076\n",
       "  0.0000 -0.0076 -0.0076\n",
       "\n",
       "(4 ,1 ,.,.) = \n",
       " -0.0076 -0.0076 -0.0000\n",
       "  0.0228 -0.0076  0.0000\n",
       "  0.0228  0.0228  0.0228\n",
       "\n",
       "(4 ,2 ,.,.) = \n",
       "  0.0228 -0.0076  0.0228\n",
       "  0.0076  0.0228  0.0228\n",
       "  0.0228  0.0228  0.0228\n",
       "\n",
       "(5 ,0 ,.,.) = \n",
       " -0.7093 -0.7638 -0.2182\n",
       "  1.2367  1.0003  0.3455\n",
       " -0.2910 -0.3819  0.0909\n",
       "\n",
       "(5 ,1 ,.,.) = \n",
       " -1.2003 -1.1639 -0.4547\n",
       "  1.7641  1.6914  0.6547\n",
       " -0.2910 -0.4910 -0.0364\n",
       "\n",
       "(5 ,2 ,.,.) = \n",
       " -0.1273 -0.2364  0.0182\n",
       "  0.8730  0.7093  0.3092\n",
       " -0.2364 -0.3092  0.1455\n",
       "\n",
       "(6 ,0 ,.,.) = \n",
       " -0.0226 -0.0226  0.0000\n",
       " -0.0226 -0.0000 -0.0226\n",
       " -0.0000 -0.0226 -0.0226\n",
       "\n",
       "(6 ,1 ,.,.) = \n",
       "  0.0000 -0.0226 -0.0226\n",
       "  0.0000 -0.0000  0.0000\n",
       " -0.0226 -0.0226  0.0000\n",
       "\n",
       "(6 ,2 ,.,.) = \n",
       "  0.0000 -0.0000 -0.0000\n",
       "  0.0226 -0.0000 -0.0226\n",
       " -0.0000  0.0000 -0.0226\n",
       "\n",
       "(7 ,0 ,.,.) = \n",
       "  0.1238  0.7921  0.2228\n",
       "  0.4703  1.2625  0.4703\n",
       "  0.7179  1.0644  0.1485\n",
       "\n",
       "(7 ,1 ,.,.) = \n",
       " -0.2228  0.0743 -0.2228\n",
       " -0.4208 -0.0248 -0.2475\n",
       " -0.1980 -0.0990 -0.3466\n",
       "\n",
       "(7 ,2 ,.,.) = \n",
       " -0.1238  0.0743 -0.1238\n",
       " -0.6931 -0.4456 -0.3713\n",
       " -0.4951 -0.4208 -0.3466\n",
       "\n",
       "(8 ,0 ,.,.) = \n",
       "  0.0058  0.0174 -0.0233\n",
       " -0.0174 -0.0000 -0.0233\n",
       " -0.0174  0.0058  0.0058\n",
       "\n",
       "(8 ,1 ,.,.) = \n",
       "  0.0174  0.0174 -0.0174\n",
       "  0.0174  0.0058  0.0291\n",
       "  0.0291  0.0174  0.0291\n",
       "\n",
       "(8 ,2 ,.,.) = \n",
       " -0.0058  0.0058  0.0058\n",
       " -0.0291  0.0174 -0.0000\n",
       " -0.0174 -0.0000  0.0058\n",
       "\n",
       "(9 ,0 ,.,.) = \n",
       " -0.2433 -0.1757 -0.4326\n",
       " -0.5948 -0.5002 -0.7165\n",
       " -0.4056 -0.2568 -0.5813\n",
       "\n",
       "(9 ,1 ,.,.) = \n",
       "  0.1217  0.4596  0.1893\n",
       "  0.0406  0.4056  0.0811\n",
       "  0.1487  0.4461 -0.0135\n",
       "\n",
       "(9 ,2 ,.,.) = \n",
       "  0.1352  0.5678  0.2028\n",
       "  0.3380  0.7300  0.2974\n",
       "  0.3244  0.6354  0.0811\n",
       "\n",
       "(10,0 ,.,.) = \n",
       "  0.2705  0.3907  0.4207\n",
       "  0.2705  0.3306  0.3907\n",
       "  0.2705  0.3005  0.3306\n",
       "\n",
       "(10,1 ,.,.) = \n",
       " -0.0301  0.0301  0.0902\n",
       " -0.0902 -0.0902  0.0000\n",
       " -0.0301 -0.1202 -0.0902\n",
       "\n",
       "(10,2 ,.,.) = \n",
       " -0.2404 -0.2404 -0.1503\n",
       " -0.3005 -0.3907 -0.2705\n",
       " -0.2404 -0.3907 -0.3306\n",
       "\n",
       "(11,0 ,.,.) = \n",
       "  0.2536  0.1268  0.0951\n",
       "  0.2853  0.1902  0.1268\n",
       "  0.3170  0.2536  0.1585\n",
       "\n",
       "(11,1 ,.,.) = \n",
       " -0.0951 -0.1585 -0.2853\n",
       " -0.0317 -0.1268 -0.2536\n",
       " -0.0000 -0.0951 -0.1902\n",
       "\n",
       "(11,2 ,.,.) = \n",
       " -0.1585 -0.1902 -0.1902\n",
       " -0.0951 -0.1585 -0.2536\n",
       " -0.0951 -0.1268 -0.1902\n",
       "\n",
       "(12,0 ,.,.) = \n",
       "  0.2250  0.3068 -0.4704\n",
       "  0.0205  0.4909 -0.0614\n",
       " -0.4704 -0.0205  0.2045\n",
       "\n",
       "(12,1 ,.,.) = \n",
       "  0.4704  0.6341 -0.2045\n",
       "  0.2659  0.7977  0.2864\n",
       " -0.3068  0.0818  0.4091\n",
       "\n",
       "(12,2 ,.,.) = \n",
       "  0.1841  0.3273 -0.1227\n",
       "  0.0205  0.3886  0.1636\n",
       " -0.3682 -0.0614  0.3273\n",
       "\n",
       "(13,0 ,.,.) = \n",
       "  0.0000 -0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(13,1 ,.,.) = \n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(13,2 ,.,.) = \n",
       "  0.0760  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(14,0 ,.,.) = \n",
       "  0.2159  0.2159  0.0000\n",
       "  0.2159  0.2159  0.0000\n",
       "  0.0000  0.0000 -0.0000\n",
       "\n",
       "(14,1 ,.,.) = \n",
       "  0.2159  0.2159  0.0000\n",
       "  0.0000  0.2159  0.0000\n",
       " -0.0000  0.0000 -0.0000\n",
       "\n",
       "(14,2 ,.,.) = \n",
       "  0.0000  0.2159  0.0000\n",
       " -0.0000  0.0000 -0.0000\n",
       " -0.2159 -0.0000 -0.0000\n",
       "\n",
       "(15,0 ,.,.) = \n",
       "  0.1760 -0.0271 -0.1941\n",
       " -0.2257  0.5777  0.5777\n",
       "  0.0542 -0.5732 -0.5732\n",
       "\n",
       "(15,1 ,.,.) = \n",
       " -0.1805  0.0632  0.1580\n",
       " -0.0406  0.5777  0.5777\n",
       "  0.2167 -0.5732 -0.5732\n",
       "\n",
       "(15,2 ,.,.) = \n",
       "  0.2798 -0.0812 -0.2347\n",
       " -0.1941  0.4243  0.5777\n",
       "  0.0722 -0.4107 -0.5732\n",
       "\n",
       "(16,0 ,.,.) = \n",
       "  0.0000  0.2458 -0.2070\n",
       "  0.2975  0.2070 -0.6726\n",
       "  0.2716  0.1423 -0.7373\n",
       "\n",
       "(16,1 ,.,.) = \n",
       "  0.1164  0.4398 -0.3492\n",
       "  0.6985  0.1940 -1.3194\n",
       "  0.4786  0.1293 -1.1900\n",
       "\n",
       "(16,2 ,.,.) = \n",
       "  0.1164  0.2975 -0.0129\n",
       "  0.1940  0.2975 -0.4398\n",
       " -0.1940  0.1035 -0.4269\n",
       "\n",
       "(17,0 ,.,.) = \n",
       " -0.0000  0.0280  0.0280\n",
       "  0.1401  0.1401  0.1401\n",
       "  0.1681  0.1681  0.1681\n",
       "\n",
       "(17,1 ,.,.) = \n",
       "  0.0280  0.0840  0.0840\n",
       "  0.1401  0.2241  0.2241\n",
       "  0.1681  0.2241  0.2241\n",
       "\n",
       "(17,2 ,.,.) = \n",
       " -0.0280 -0.0000  0.0000\n",
       "  0.0280  0.0840  0.1120\n",
       "  0.0840  0.1120  0.1120\n",
       "\n",
       "(18,0 ,.,.) = \n",
       " -0.7227 -0.5560 -0.5560\n",
       " -0.6671 -0.6671 -0.5560\n",
       " -0.7783 -0.8339 -0.7227\n",
       "\n",
       "(18,1 ,.,.) = \n",
       "  1.0007  1.1119  1.0007\n",
       "  1.2787  1.3343  1.2231\n",
       "  1.0007  1.0007  0.9451\n",
       "\n",
       "(18,2 ,.,.) = \n",
       " -0.7227 -0.5004 -0.2224\n",
       " -0.5560 -0.6671 -0.3336\n",
       " -0.4448 -0.7783 -0.6671\n",
       "\n",
       "(19,0 ,.,.) = \n",
       "  0.0000 -0.0000  0.0000\n",
       "  0.0000 -0.0000 -0.1031\n",
       "  0.0000 -0.0000  0.0000\n",
       "\n",
       "(19,1 ,.,.) = \n",
       " -0.0000 -0.1031 -0.0000\n",
       " -0.0000 -0.1031 -0.1031\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(19,2 ,.,.) = \n",
       " -0.0000 -0.0000  0.0000\n",
       "  0.0000 -0.0000 -0.0000\n",
       "  0.0000 -0.0000  0.0000\n",
       "\n",
       "(20,0 ,.,.) = \n",
       "  0.0278  0.0278 -0.0000\n",
       "  0.0000  0.0278  0.0000\n",
       "  0.0278  0.0000  0.0278\n",
       "\n",
       "(20,1 ,.,.) = \n",
       "  0.0000 -0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0278  0.0000 -0.0278\n",
       "\n",
       "(20,2 ,.,.) = \n",
       "  0.0278  0.0000  0.0278\n",
       " -0.0278  0.0000  0.0000\n",
       "  0.0278 -0.0000 -0.0000\n",
       "\n",
       "(21,0 ,.,.) = \n",
       "  0.0715  0.2144  0.2144\n",
       "  0.0715  0.2144  0.2859\n",
       "  0.0715  0.2144  0.2859\n",
       "\n",
       "(21,1 ,.,.) = \n",
       "  0.2144  0.2859  0.2859\n",
       "  0.2144  0.2859  0.2859\n",
       "  0.0000  0.0715  0.2144\n",
       "\n",
       "(21,2 ,.,.) = \n",
       "  0.0000  0.0715  0.0715\n",
       "  0.0000  0.2144  0.2144\n",
       " -0.0715  0.0715  0.2144\n",
       "\n",
       "(22,0 ,.,.) = \n",
       "  0.2948  0.0819  0.2620\n",
       "  0.2457  0.0491  0.2620\n",
       "  0.2293 -0.0491  0.2948\n",
       "\n",
       "(22,1 ,.,.) = \n",
       "  0.2948 -0.1310  0.2129\n",
       " -0.0164 -0.3931  0.0655\n",
       " -0.1474 -0.5241  0.0819\n",
       "\n",
       "(22,2 ,.,.) = \n",
       " -0.0491 -0.4095  0.0491\n",
       " -0.4586 -0.7861 -0.1310\n",
       " -0.5405 -0.7861 -0.0819\n",
       "\n",
       "(23,0 ,.,.) = \n",
       "  0.1562  0.2130  0.1846\n",
       "  0.3835  0.3977  0.2130\n",
       "  0.2556  0.4687  0.3266\n",
       "\n",
       "(23,1 ,.,.) = \n",
       " -0.8663 -1.0083 -0.9231\n",
       " -0.9941 -1.1220 -1.1362\n",
       " -1.0083 -0.8663 -0.8095\n",
       "\n",
       "(23,2 ,.,.) = \n",
       "  0.7527  0.6675  0.5539\n",
       "  0.6675  0.6391  0.5255\n",
       "  0.4403  0.6391  0.5965\n",
       "\n",
       "(24,0 ,.,.) = \n",
       "  0.0173 -0.0173 -0.0000\n",
       "  0.0173  0.0173  0.0000\n",
       " -0.0173 -0.0173 -0.0173\n",
       "\n",
       "(24,1 ,.,.) = \n",
       " -0.0173  0.0000 -0.0000\n",
       " -0.0000 -0.0173 -0.0173\n",
       " -0.0000 -0.0000  0.0000\n",
       "\n",
       "(24,2 ,.,.) = \n",
       " -0.0000 -0.0000  0.0000\n",
       " -0.0173  0.0000  0.0173\n",
       " -0.0173  0.0173 -0.0173\n",
       "\n",
       "(25,0 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.1789  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(25,1 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(25,2 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(26,0 ,.,.) = \n",
       "  0.8173  0.8437  0.2900\n",
       "  1.1337  1.3183  0.8964\n",
       "  0.8701  1.0546  0.8964\n",
       "\n",
       "(26,1 ,.,.) = \n",
       " -0.3955 -0.4482 -0.4482\n",
       " -0.5273 -0.5800 -0.5273\n",
       " -0.4482 -0.5800 -0.4746\n",
       "\n",
       "(26,2 ,.,.) = \n",
       " -0.4482 -0.3427 -0.0000\n",
       " -0.6064 -0.7382 -0.3955\n",
       " -0.2900 -0.5273 -0.2637\n",
       "\n",
       "(27,0 ,.,.) = \n",
       " -0.1273 -0.5376 -0.0849\n",
       " -0.5376 -0.9195 -0.3254\n",
       " -0.1415 -0.4810 -0.0566\n",
       "\n",
       "(27,1 ,.,.) = \n",
       "  0.0849 -0.2546  0.1556\n",
       " -0.2263 -0.5517  0.0424\n",
       "  0.0566 -0.1839  0.2688\n",
       "\n",
       "(27,2 ,.,.) = \n",
       "  0.3395  0.0141  0.1839\n",
       "  0.1556 -0.1273  0.2263\n",
       "  0.2688  0.0707  0.2546\n",
       "\n",
       "(28,0 ,.,.) = \n",
       "  0.1519  0.4304  0.4810\n",
       "  0.1266  0.2025  0.5569\n",
       "  0.2278 -0.2278  0.2278\n",
       "\n",
       "(28,1 ,.,.) = \n",
       " -0.5823 -0.4810 -0.4304\n",
       " -0.7088 -0.8607 -0.4810\n",
       " -0.4810 -1.1392 -0.6835\n",
       "\n",
       "(28,2 ,.,.) = \n",
       "  0.3797  0.5823  0.3797\n",
       "  0.4557  0.4810  0.5569\n",
       "  0.5569  0.1519  0.4304\n",
       "\n",
       "(29,0 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0830\n",
       "  0.0830  0.0000 -0.0000\n",
       "  0.0830  0.0830  0.0000\n",
       "\n",
       "(29,1 ,.,.) = \n",
       "  0.0830  0.0830 -0.0000\n",
       "  0.0830  0.0830  0.0000\n",
       "  0.0830  0.0830  0.0000\n",
       "\n",
       "(29,2 ,.,.) = \n",
       "  0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000 -0.0000\n",
       "  0.0830  0.0000 -0.0000\n",
       "\n",
       "(30,0 ,.,.) = \n",
       " -0.1820  0.2254 -0.0693\n",
       "  0.0260  0.3294 -0.0087\n",
       " -0.0260  0.3294 -0.2427\n",
       "\n",
       "(30,1 ,.,.) = \n",
       " -0.1300  0.2514  0.0347\n",
       "  0.0347  0.4334  0.1214\n",
       " -0.0347  0.3554 -0.2167\n",
       "\n",
       "(30,2 ,.,.) = \n",
       " -0.1560  0.2427 -0.0087\n",
       "  0.0347  0.2861 -0.0087\n",
       " -0.0260  0.2601 -0.1820\n",
       "\n",
       "(31,0 ,.,.) = \n",
       "  0.0000 -0.0152 -0.0152\n",
       " -0.0152 -0.0608 -0.0304\n",
       " -0.0152 -0.0000 -0.0304\n",
       "\n",
       "(31,1 ,.,.) = \n",
       " -0.0152  0.0152  0.0152\n",
       " -0.0152 -0.0152 -0.0152\n",
       "  0.0000 -0.0152  0.0000\n",
       "\n",
       "(31,2 ,.,.) = \n",
       "  0.0000 -0.0152 -0.0000\n",
       " -0.0152 -0.0152  0.0000\n",
       "  0.0000 -0.0152 -0.0000\n",
       "[torch.cuda.FloatTensor of size 32x3x3x3 (GPU 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model[0][0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.FloatTensor' object has no attribute 'zeros'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3dcf64850002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.FloatTensor' object has no attribute 'zeros'"
     ]
    }
   ],
   "source": [
    "cc.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-476d1b94c172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clip' is not defined"
     ]
    }
   ],
   "source": [
    "torch.clamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "  2.5504e+00  2.9247e+00  2.8491e+00  ...   1.3634e+00  2.1798e+00  2.3784e+00\n",
       "  2.5581e+00  2.8316e+00  2.8244e+00  ...   1.3436e+00  1.9383e+00  2.1978e+00\n",
       "  2.5185e+00  2.7827e+00  2.7916e+00  ...   1.1708e+00  1.3936e+00  1.2473e+00\n",
       "                 ...                                      ...                \n",
       "  1.4324e+00  1.1519e+00  1.1440e+00  ...   2.3858e+00  2.6389e+00  2.6338e+00\n",
       "  1.2796e+00  1.0198e+00  9.3218e-01  ...   2.3512e+00  2.3562e+00  2.3163e+00\n",
       "  1.3625e+00  1.1382e+00  1.0658e+00  ...   2.4910e+00  2.4283e+00  2.4607e+00\n",
       "    ... \n",
       "\n",
       "( 0 ,29 ,.,.) = \n",
       " -3.9294e-01 -3.1780e-01 -3.2797e-01  ...  -4.4508e-01 -4.1689e-01 -3.8600e-01\n",
       " -3.9389e-01 -3.2135e-01 -3.1451e-01  ...  -4.6912e-01 -4.4305e-01 -4.1408e-01\n",
       " -3.9460e-01 -3.2656e-01 -3.1425e-01  ...  -4.6941e-01 -4.8057e-01 -4.7490e-01\n",
       "                 ...                                      ...                \n",
       " -4.2900e-01 -4.8440e-01 -4.8981e-01  ...  -3.7408e-01 -3.5938e-01 -3.3601e-01\n",
       " -4.2923e-01 -4.8513e-01 -4.9904e-01  ...  -3.5546e-01 -4.0375e-01 -3.6996e-01\n",
       " -4.2520e-01 -4.7567e-01 -4.8224e-01  ...  -3.4522e-01 -3.8132e-01 -3.7620e-01\n",
       "\n",
       "( 0 ,30 ,.,.) = \n",
       " -7.2077e-01 -9.8162e-01 -5.0869e-01  ...   1.6688e+00 -2.7374e-01 -2.6053e-01\n",
       " -1.1289e+00 -1.0253e+00 -6.9074e-01  ...   2.0373e+00 -1.2818e-01 -3.0190e-02\n",
       " -1.0781e+00 -8.5870e-01 -1.0816e+00  ...   2.0079e+00  6.1596e-01  1.5991e+00\n",
       "                 ...                                      ...                \n",
       "  1.5206e+00  1.3369e+00  1.4081e+00  ...  -3.3409e-01 -6.2011e-01 -8.2409e-01\n",
       "  1.8912e+00  1.7455e+00  1.8824e+00  ...  -1.0218e-01 -6.2166e-01 -4.0202e-01\n",
       "  1.7334e+00  1.5949e+00  1.7162e+00  ...  -6.4006e-01 -2.4545e-01 -3.2626e-01\n",
       "\n",
       "( 0 ,31 ,.,.) = \n",
       "  1.2349e-01  1.6336e-01  1.5356e-01  ...  -4.0085e-02  7.4508e-02  1.0540e-01\n",
       "  1.2065e-01  1.6922e-01  1.4449e-01  ...  -6.9298e-02  4.8290e-02  7.5447e-02\n",
       "  1.1660e-01  1.5824e-01  1.5772e-01  ...  -1.1698e-01 -5.7831e-02 -7.2172e-02\n",
       "                 ...                                      ...                \n",
       " -1.1397e-01 -1.5083e-01 -1.5543e-01  ...   1.1547e-01  1.2634e-01  1.1947e-01\n",
       " -1.2286e-01 -1.7222e-01 -1.9095e-01  ...   6.9490e-02  1.1199e-01  1.0913e-01\n",
       " -1.1504e-01 -1.6062e-01 -1.7840e-01  ...   1.2450e-01  9.7319e-02  1.1543e-01\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       "  2.1652e+00  2.2983e+00  2.2912e+00  ...   2.4805e+00  2.5144e+00  2.5117e+00\n",
       "  2.1053e+00  2.1079e+00  2.0917e+00  ...   2.3739e+00  2.3881e+00  2.4226e+00\n",
       "  2.0647e+00  2.0649e+00  2.0259e+00  ...   2.3347e+00  2.3915e+00  2.4468e+00\n",
       "                 ...                                      ...                \n",
       "  1.2073e+00  9.7454e-01  1.0287e+00  ...   1.7552e+00  1.5021e+00  1.5719e+00\n",
       "  1.2190e+00  9.8558e-01  9.7137e-01  ...   1.9867e+00  1.7107e+00  1.6225e+00\n",
       "  1.2294e+00  9.4958e-01  9.2410e-01  ...   2.2029e+00  1.9933e+00  1.8875e+00\n",
       "    ... \n",
       "\n",
       "( 1 ,29 ,.,.) = \n",
       " -4.0551e-01 -3.8159e-01 -3.8464e-01  ...  -3.6606e-01 -3.6252e-01 -3.6064e-01\n",
       " -4.0693e-01 -3.9337e-01 -3.9808e-01  ...  -3.6676e-01 -3.6630e-01 -3.6205e-01\n",
       " -4.0859e-01 -3.9647e-01 -4.0212e-01  ...  -3.7311e-01 -3.6770e-01 -3.6227e-01\n",
       "                 ...                                      ...                \n",
       " -4.3327e-01 -4.9934e-01 -4.9838e-01  ...  -4.0645e-01 -4.4181e-01 -4.5547e-01\n",
       " -4.3279e-01 -4.9839e-01 -5.0357e-01  ...  -3.8711e-01 -4.1968e-01 -4.4275e-01\n",
       " -4.3303e-01 -4.9839e-01 -5.0617e-01  ...  -3.7574e-01 -3.9323e-01 -4.1493e-01\n",
       "\n",
       "( 1 ,30 ,.,.) = \n",
       " -5.3574e-02 -6.9420e-02 -3.7330e-02  ...  -2.7473e-01 -3.1808e-01 -3.3157e-01\n",
       " -1.6205e-01  4.7503e-02  8.3373e-02  ...  -3.1430e-01 -3.5270e-01 -4.0825e-01\n",
       " -7.6118e-02  1.3931e-01  1.9085e-01  ...  -2.5542e-01 -3.3549e-01 -4.4369e-01\n",
       "                 ...                                      ...                \n",
       "  2.0368e+00  1.8447e+00  1.6947e+00  ...   5.4610e-01  1.0388e+00  9.9141e-01\n",
       "  2.0761e+00  1.8284e+00  1.7804e+00  ...   3.2040e-01  8.0358e-01  9.8177e-01\n",
       "  2.0537e+00  1.8634e+00  1.8729e+00  ...  -7.9313e-02  3.3124e-01  5.8786e-01\n",
       "\n",
       "( 1 ,31 ,.,.) = \n",
       "  3.4404e-02  3.6159e-02  3.6809e-02  ...   7.2958e-02  7.8270e-02  7.7025e-02\n",
       "  1.9428e-02  1.3376e-02  9.6745e-03  ...   6.5767e-02  6.8895e-02  7.4208e-02\n",
       "  1.5038e-02  7.1684e-03 -1.2726e-03  ...   6.2950e-02  6.8573e-02  7.7942e-02\n",
       "                 ...                                      ...                \n",
       " -1.2820e-01 -1.6600e-01 -1.6282e-01  ...  -3.1952e-02 -7.6030e-02 -6.5094e-02\n",
       " -1.2849e-01 -1.6817e-01 -1.6721e-01  ...  -2.5948e-03 -5.5375e-02 -6.6666e-02\n",
       " -1.2755e-01 -1.7098e-01 -1.7440e-01  ...   4.1455e-02 -1.3279e-03 -2.7268e-02\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       "  2.2496e+00  2.3510e+00  2.3291e+00  ...   6.7004e-01  1.1983e+00  1.4919e+00\n",
       "  2.1008e+00  2.2463e+00  2.2566e+00  ...   1.1697e+00  1.1578e+00  1.5957e+00\n",
       "  2.1304e+00  2.1689e+00  2.3702e+00  ...   1.6473e+00  1.5865e+00  1.2911e+00\n",
       "                 ...                                      ...                \n",
       "  1.4625e+00  1.2392e+00  1.2121e+00  ...   9.7569e-01  9.5290e-01  9.8694e-01\n",
       "  1.4615e+00  1.2611e+00  1.2429e+00  ...   9.7201e-01  9.5837e-01  9.7155e-01\n",
       "  1.3139e+00  1.1112e+00  1.1468e+00  ...   1.0134e+00  9.8011e-01  1.0025e+00\n",
       "    ... \n",
       "\n",
       "( 2 ,29 ,.,.) = \n",
       " -4.0646e-01 -3.7256e-01 -3.7631e-01  ...  -5.2206e-01 -4.7631e-01 -4.4726e-01\n",
       " -4.1547e-01 -3.7634e-01 -3.6733e-01  ...  -4.8224e-01 -4.9611e-01 -4.4367e-01\n",
       " -4.1476e-01 -4.0899e-01 -3.6123e-01  ...  -4.2125e-01 -4.7022e-01 -4.7136e-01\n",
       "                 ...                                      ...                \n",
       " -4.2781e-01 -4.7723e-01 -4.8500e-01  ...  -5.0650e-01 -5.0814e-01 -5.0908e-01\n",
       " -4.3066e-01 -4.8334e-01 -4.8499e-01  ...  -5.0837e-01 -5.0648e-01 -5.1121e-01\n",
       " -4.3279e-01 -4.9444e-01 -4.9186e-01  ...  -5.0084e-01 -5.0626e-01 -5.0577e-01\n",
       "\n",
       "( 2 ,30 ,.,.) = \n",
       " -1.2701e-01 -6.9721e-02 -5.7072e-01  ...   1.9633e+00  1.0849e+00  5.8860e-01\n",
       "  1.4136e-01  1.3164e-01 -5.2330e-01  ...   1.3851e+00  1.4043e+00  7.0251e-01\n",
       "  6.5687e-02  1.1454e-01 -5.9608e-01  ...   4.9523e-01  9.0405e-01  1.5742e+00\n",
       "                 ...                                      ...                \n",
       "  1.4688e+00  1.3851e+00  1.3727e+00  ...   1.8729e+00  1.8840e+00  1.8327e+00\n",
       "  1.4141e+00  1.3880e+00  1.4260e+00  ...   1.8611e+00  1.8451e+00  1.8601e+00\n",
       "  1.7501e+00  1.6533e+00  1.5474e+00  ...   1.8275e+00  1.8545e+00  1.8574e+00\n",
       "\n",
       "( 2 ,31 ,.,.) = \n",
       "  7.2959e-02  8.0582e-02  5.4170e-02  ...  -1.4746e-01 -9.5906e-02 -4.6827e-02\n",
       "  5.7294e-02  7.1486e-02  8.1921e-02  ...  -1.3144e-01 -8.5359e-02 -2.5183e-02\n",
       "  2.9537e-02  8.6823e-02  8.5989e-02  ...  -6.5810e-02 -5.3646e-02 -8.8732e-02\n",
       "                 ...                                      ...                \n",
       " -7.7768e-02 -1.1186e-01 -1.2056e-01  ...  -1.5721e-01 -1.5752e-01 -1.5003e-01\n",
       " -7.8057e-02 -1.0682e-01 -1.1154e-01  ...  -1.5065e-01 -1.5533e-01 -1.5189e-01\n",
       " -1.0247e-01 -1.2873e-01 -1.2591e-01  ...  -1.5254e-01 -1.5253e-01 -1.5129e-01\n",
       "...     \n",
       "        \n",
       "\n",
       "(61 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "(61 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "(61 , 2 ,.,.) = \n",
       "  1.5708e+00  1.3416e+00  1.4072e+00  ...   2.2121e+00  2.4150e+00  2.3901e+00\n",
       "  1.6689e+00  1.6019e+00  1.6494e+00  ...   1.6353e+00  2.2411e+00  2.2824e+00\n",
       "  1.8206e+00  1.7893e+00  1.7690e+00  ...   1.0356e+00  1.7503e+00  2.1911e+00\n",
       "                 ...                                      ...                \n",
       "  1.6527e+00  1.5038e+00  1.4176e+00  ...   2.6727e+00  2.8532e+00  2.8119e+00\n",
       "  1.7102e+00  1.5718e+00  1.4929e+00  ...   2.8211e+00  2.8511e+00  2.7318e+00\n",
       "  1.7792e+00  1.6739e+00  1.5890e+00  ...   2.8234e+00  2.7951e+00  2.7456e+00\n",
       "    ... \n",
       "\n",
       "(61 ,29 ,.,.) = \n",
       " -4.2235e-01 -4.5395e-01 -4.5087e-01  ...  -4.0272e-01 -3.6956e-01 -3.6676e-01\n",
       " -4.1832e-01 -4.3554e-01 -4.3574e-01  ...  -4.8388e-01 -3.9165e-01 -3.7216e-01\n",
       " -4.1358e-01 -4.1712e-01 -4.2181e-01  ...  -5.3716e-01 -4.6307e-01 -3.8769e-01\n",
       "                 ...                                      ...                \n",
       " -4.2045e-01 -4.4563e-01 -4.5413e-01  ...  -3.4084e-01 -3.2224e-01 -3.1943e-01\n",
       " -4.1761e-01 -4.3549e-01 -4.4894e-01  ...  -3.2577e-01 -3.2152e-01 -3.2012e-01\n",
       " -4.1547e-01 -4.2840e-01 -4.3877e-01  ...  -3.1753e-01 -3.2366e-01 -3.2224e-01\n",
       "\n",
       "(61 ,30 ,.,.) = \n",
       "  9.3591e-01  1.1438e+00  1.0571e+00  ...  -4.0473e-02 -2.0707e-01 -1.8155e-01\n",
       "  9.5338e-01  9.9405e-01  8.9433e-01  ...   5.6920e-01 -2.6011e-01 -1.8759e-01\n",
       "  6.2776e-01  6.5024e-01  7.1498e-01  ...   1.7962e+00  3.5022e-01 -7.1436e-02\n",
       "                 ...                                      ...                \n",
       "  9.8077e-01  1.0403e+00  1.2054e+00  ...  -7.7697e-01 -1.0226e+00 -9.9059e-01\n",
       "  8.3786e-01  9.3452e-01  1.1210e+00  ...  -1.0030e+00 -1.0667e+00 -8.2549e-01\n",
       "  6.5009e-01  7.8957e-01  9.6934e-01  ...  -1.0333e+00 -9.6132e-01 -9.6053e-01\n",
       "\n",
       "(61 ,31 ,.,.) = \n",
       " -4.9593e-02 -6.6271e-02 -5.7191e-02  ...   4.2072e-02  5.8598e-02  5.0152e-02\n",
       " -2.9905e-02 -4.4387e-02 -4.2143e-02  ...  -2.2878e-02  5.2058e-02  4.9846e-02\n",
       " -7.4161e-03 -1.7486e-02 -2.6517e-02  ...  -1.5162e-01 -8.1797e-03  3.5170e-02\n",
       "                 ...                                      ...                \n",
       " -4.2314e-02 -6.3009e-02 -7.7385e-02  ...   1.3060e-01  1.5214e-01  1.4654e-01\n",
       " -3.1372e-02 -5.4285e-02 -6.8344e-02  ...   1.4279e-01  1.5746e-01  1.3684e-01\n",
       " -2.2609e-02 -4.2060e-02 -5.4229e-02  ...   1.4903e-01  1.4809e-01  1.3313e-01\n",
       "        \n",
       "\n",
       "(62 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "(62 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "(62 , 2 ,.,.) = \n",
       "  2.0530e+00  2.1560e+00  2.1500e+00  ...   2.3509e+00  2.3618e+00  2.3384e+00\n",
       "  2.0574e+00  2.1166e+00  2.1205e+00  ...   2.3176e+00  2.2913e+00  2.2815e+00\n",
       "  2.0617e+00  2.1111e+00  2.1124e+00  ...   2.3138e+00  2.3151e+00  2.3036e+00\n",
       "                 ...                                      ...                \n",
       "  1.9062e+00  2.0593e+00  2.1218e+00  ...   2.1058e+00  2.1276e+00  2.0752e+00\n",
       "  1.8466e+00  1.8920e+00  1.9952e+00  ...   1.8102e+00  1.7463e+00  1.9646e+00\n",
       "  1.7386e+00  1.7690e+00  1.8136e+00  ...   1.9423e+00  2.2012e+00  2.1660e+00\n",
       "    ... \n",
       "\n",
       "(62 ,29 ,.,.) = \n",
       " -4.0907e-01 -4.0020e-01 -4.0020e-01  ...  -3.7974e-01 -3.7551e-01 -3.7786e-01\n",
       " -4.0907e-01 -4.0089e-01 -4.0066e-01  ...  -3.7741e-01 -3.7669e-01 -3.7905e-01\n",
       " -4.0907e-01 -4.0020e-01 -4.0044e-01  ...  -3.7644e-01 -3.7597e-01 -3.7739e-01\n",
       "                 ...                                      ...                \n",
       " -4.1358e-01 -4.0963e-01 -4.0113e-01  ...  -4.0256e-01 -3.9837e-01 -4.0520e-01\n",
       " -4.1761e-01 -4.3032e-01 -4.1690e-01  ...  -4.2595e-01 -4.3366e-01 -4.1769e-01\n",
       " -4.1832e-01 -4.3626e-01 -4.3155e-01  ...  -4.1351e-01 -3.9675e-01 -3.9319e-01\n",
       "\n",
       "(62 ,30 ,.,.) = \n",
       "  1.4599e-01  1.3789e-01  1.3259e-01  ...  -1.0759e-01 -1.1539e-01 -1.0290e-01\n",
       " -1.1982e-02  5.8786e-02  4.8112e-02  ...  -2.0992e-01 -1.7741e-01 -1.4798e-01\n",
       " -2.7444e-02  6.0444e-02  3.3983e-02  ...  -2.3328e-01 -2.1408e-01 -1.8571e-01\n",
       "                 ...                                      ...                \n",
       "  5.2489e-01  2.0243e-01  3.6006e-02  ...   4.5301e-02 -6.0187e-03  9.6876e-02\n",
       "  5.6642e-01  3.5019e-01  1.6788e-01  ...   4.4052e-01  5.5325e-01  3.6909e-01\n",
       "  8.3402e-01  5.8485e-01  4.6268e-01  ...   5.6204e-01  5.3625e-02  7.6164e-02\n",
       "\n",
       "(62 ,31 ,.,.) = \n",
       "  5.8131e-03  4.1781e-03  3.2169e-03  ...   5.4580e-02  5.3308e-02  5.1747e-02\n",
       "  7.0524e-03  6.6456e-03  9.1572e-03  ...   5.7070e-02  5.3630e-02  5.1130e-02\n",
       "  6.7578e-03  3.2446e-03  2.6222e-03  ...   5.5820e-02  5.6453e-02  5.5814e-02\n",
       "                 ...                                      ...                \n",
       " -1.9215e-02 -1.0220e-02  5.9900e-03  ...   2.4017e-02  2.3711e-02  1.1591e-02\n",
       " -3.6031e-02 -2.4540e-02 -1.0187e-02  ...  -4.1755e-02 -3.7393e-02 -1.2420e-02\n",
       " -5.2607e-02 -5.9199e-02 -4.5762e-02  ...  -2.0305e-02  2.3117e-02  2.2761e-02\n",
       "        \n",
       "\n",
       "(63 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "(63 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "(63 , 2 ,.,.) = \n",
       "  1.4460e+00  1.2233e+00  1.2054e+00  ...   9.3768e-01  9.2162e-01  9.6274e-01\n",
       "  1.4197e+00  1.2717e+00  1.2962e+00  ...   1.1285e+00  1.2022e+00  1.1709e+00\n",
       "  1.4379e+00  1.2739e+00  1.2397e+00  ...   1.1359e+00  1.1321e+00  1.1755e+00\n",
       "                 ...                                      ...                \n",
       "  1.3002e+00  1.1626e+00  1.1957e+00  ...   2.2888e+00  2.2868e+00  2.3737e+00\n",
       "  1.2839e+00  1.1005e+00  1.1694e+00  ...   2.4097e+00  2.3781e+00  2.4555e+00\n",
       "  1.3568e+00  1.1071e+00  1.0814e+00  ...   2.4332e+00  2.4362e+00  2.5110e+00\n",
       "    ... \n",
       "\n",
       "(63 ,29 ,.,.) = \n",
       " -4.2828e-01 -4.6923e-01 -4.6853e-01  ...  -4.8784e-01 -4.9161e-01 -4.8877e-01\n",
       " -4.2947e-01 -4.6970e-01 -4.7159e-01  ...  -4.8523e-01 -4.7770e-01 -4.7984e-01\n",
       " -4.2781e-01 -4.7017e-01 -4.7348e-01  ...  -4.8855e-01 -4.8857e-01 -4.8405e-01\n",
       "                 ...                                      ...                \n",
       " -4.3327e-01 -4.8454e-01 -4.8264e-01  ...  -3.7134e-01 -3.7015e-01 -3.6638e-01\n",
       " -4.3113e-01 -4.8529e-01 -4.8433e-01  ...  -3.5743e-01 -3.6166e-01 -3.6405e-01\n",
       " -4.2947e-01 -4.8315e-01 -4.8926e-01  ...  -3.5717e-01 -3.5763e-01 -3.4964e-01\n",
       "\n",
       "(63 ,30 ,.,.) = \n",
       "  1.2247e+00  1.2233e+00  1.2452e+00  ...   1.5468e+00  1.6306e+00  1.5151e+00\n",
       "  1.6462e+00  1.3764e+00  1.3464e+00  ...   1.6173e+00  1.5193e+00  1.5568e+00\n",
       "  1.6058e+00  1.3622e+00  1.4667e+00  ...   1.5889e+00  1.5688e+00  1.4517e+00\n",
       "                 ...                                      ...                \n",
       "  1.9219e+00  1.5256e+00  1.5724e+00  ...  -1.2842e-01 -1.1948e-01 -2.1381e-01\n",
       "  1.9438e+00  1.7440e+00  1.3929e+00  ...  -3.1828e-01 -2.1845e-01 -3.8648e-01\n",
       "  1.8020e+00  1.6733e+00  1.5826e+00  ...  -3.4447e-01 -3.9936e-01 -4.4304e-01\n",
       "\n",
       "(63 ,31 ,.,.) = \n",
       " -7.6761e-02 -9.1150e-02 -9.2089e-02  ...  -1.4390e-01 -1.4360e-01 -1.3891e-01\n",
       " -7.8011e-02 -9.6445e-02 -9.3961e-02  ...  -1.2609e-01 -1.2202e-01 -1.2391e-01\n",
       " -7.7372e-02 -9.6462e-02 -1.0179e-01  ...  -1.2235e-01 -1.2048e-01 -1.1609e-01\n",
       "                 ...                                      ...                \n",
       " -1.0018e-01 -1.1766e-01 -1.1334e-01  ...   6.5890e-02  7.1236e-02  8.2466e-02\n",
       " -1.0083e-01 -1.2866e-01 -1.1797e-01  ...   8.5928e-02  8.3078e-02  9.8993e-02\n",
       " -9.3954e-02 -1.2395e-01 -1.2489e-01  ...   8.9329e-02  1.0120e-01  1.0184e-01\n",
       "[torch.cuda.FloatTensor of size 64x32x112x112 (GPU 2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = nn.Sequential(model.model[0][0],model.model[0][1]) (input_var)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function FloatTensor.clone>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi gpus\n",
    "if gpus and len(gpus) > 1:\n",
    "    model_dist = torch.nn.DataParallel(model, gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_dist(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.store_and_quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "  1.3363e+00  1.3145e+00  1.3089e+00  ...   1.3162e+00  1.3258e+00  1.3259e+00\n",
       "  1.3862e+00  1.3369e+00  1.3300e+00  ...   1.3595e+00  1.3505e+00  1.3573e+00\n",
       "  1.3731e+00  1.3275e+00  1.3182e+00  ...   1.3518e+00  1.3675e+00  1.3635e+00\n",
       "                 ...                                      ...                \n",
       "  9.3571e-01  9.7869e-01  8.4257e-01  ...   8.7363e-01  8.7672e-01  8.8933e-01\n",
       "  9.8799e-01  8.2247e-01  6.9976e-01  ...   8.5056e-01  8.6704e-01  8.7351e-01\n",
       "  8.6465e-01  7.6496e-01  7.3176e-01  ...   8.5165e-01  8.8781e-01  8.9473e-01\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  2.3497e+00  2.5492e+00  2.5428e+00  ...   2.5717e+00  2.5678e+00  2.5981e+00\n",
       "  2.3835e+00  2.7356e+00  2.7238e+00  ...   2.7793e+00  2.7772e+00  2.7869e+00\n",
       "  2.3540e+00  2.7192e+00  2.6982e+00  ...   2.7431e+00  2.7900e+00  2.8017e+00\n",
       "                 ...                                      ...                \n",
       "  1.8327e+00  2.1647e+00  2.1655e+00  ...   1.4009e+00  1.4049e+00  1.4236e+00\n",
       "  1.8569e+00  2.0481e+00  1.7447e+00  ...   1.3416e+00  1.3727e+00  1.3950e+00\n",
       "  1.7091e+00  1.8087e+00  1.7261e+00  ...   1.3323e+00  1.3966e+00  1.4636e+00\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -2.9540e-01  1.0573e+00  1.0665e+00  ...   1.0822e+00  1.0001e+00  1.0400e+00\n",
       " -1.0241e+00  6.3840e-01  6.0864e-01  ...   5.6922e-01  6.3254e-01  5.3445e-01\n",
       " -9.7486e-01  6.5131e-01  6.3060e-01  ...   5.5343e-01  5.5851e-01  5.9261e-01\n",
       "                 ...                                      ...                \n",
       "  9.5400e-02  5.6090e-01  1.4976e+00  ...   1.7165e+00  1.7006e+00  1.6348e+00\n",
       " -3.9346e-02  1.4902e+00  1.4729e+00  ...   1.7516e+00  1.6988e+00  1.7114e+00\n",
       "  4.9181e-01  1.3599e+00  1.3940e+00  ...   1.7428e+00  1.6041e+00  1.6787e+00\n",
       "    ... \n",
       "\n",
       "( 0 ,29 ,.,.) = \n",
       "  4.2042e-01  3.5728e-01  3.5831e-01  ...   3.5116e-01  3.5103e-01  3.4401e-01\n",
       "  2.2544e-01  9.6014e-02  9.6531e-02  ...   7.2965e-02  7.3764e-02  6.8168e-02\n",
       "  2.3335e-01  1.0520e-01  1.0831e-01  ...   9.0141e-02  7.6894e-02  7.0810e-02\n",
       "                 ...                                      ...                \n",
       "  3.5686e-01  2.2027e-01  2.3697e-01  ...   6.0537e-01  5.9801e-01  5.8463e-01\n",
       "  3.6141e-01  2.7081e-01  3.6512e-01  ...   6.2921e-01  6.2110e-01  6.1151e-01\n",
       "  4.2083e-01  3.7330e-01  3.8912e-01  ...   6.3206e-01  6.0753e-01  5.8898e-01\n",
       "\n",
       "( 0 ,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "( 0 ,31 ,.,.) = \n",
       " -2.0977e-01 -2.3717e-01 -2.3711e-01  ...  -2.3642e-01 -2.3623e-01 -2.3572e-01\n",
       " -2.1054e-01 -2.4709e-01 -2.4631e-01  ...  -2.4525e-01 -2.4634e-01 -2.4494e-01\n",
       " -2.1124e-01 -2.4669e-01 -2.4617e-01  ...  -2.4492e-01 -2.4590e-01 -2.4559e-01\n",
       "                 ...                                      ...                \n",
       " -2.1461e-01 -2.2768e-01 -2.3397e-01  ...  -2.3732e-01 -2.3758e-01 -2.3733e-01\n",
       " -2.1244e-01 -2.3255e-01 -2.2908e-01  ...  -2.3685e-01 -2.3674e-01 -2.3772e-01\n",
       " -2.1651e-01 -2.2924e-01 -2.2671e-01  ...  -2.3672e-01 -2.3604e-01 -2.3754e-01\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  8.3131e-01  7.8908e-01  8.7961e-01  ...   7.5248e-01  7.9315e-01  7.9698e-01\n",
       "  7.7897e-01  6.6795e-01  6.7889e-01  ...   7.6038e-01  7.5238e-01  7.4445e-01\n",
       "  8.1730e-01  7.8493e-01  8.5419e-01  ...   7.4464e-01  7.3595e-01  7.3606e-01\n",
       "                 ...                                      ...                \n",
       "  8.9419e-01  7.2514e-01  7.1808e-01  ...   8.0047e-01  8.4330e-01  7.1594e-01\n",
       "  8.7780e-01  8.5376e-01  8.6655e-01  ...   6.0681e-01  5.4292e-01  7.6247e-01\n",
       "  9.2695e-01  8.5097e-01  7.9564e-01  ...   7.9114e-01  6.6468e-01  8.1315e-01\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  1.5638e+00  1.5843e+00  1.7393e+00  ...   1.4499e+00  1.5804e+00  1.6271e+00\n",
       "  1.5042e+00  1.4509e+00  1.4689e+00  ...   1.5928e+00  1.5659e+00  1.5814e+00\n",
       "  1.5345e+00  1.6321e+00  1.7235e+00  ...   1.6557e+00  1.4912e+00  1.5507e+00\n",
       "                 ...                                      ...                \n",
       "  1.8289e+00  1.7166e+00  1.6912e+00  ...   1.8219e+00  1.8876e+00  1.7520e+00\n",
       "  1.7491e+00  1.9443e+00  1.9293e+00  ...   1.6282e+00  1.2381e+00  1.5218e+00\n",
       "  1.7561e+00  1.9501e+00  1.9596e+00  ...   1.8954e+00  1.6364e+00  1.5480e+00\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       "  1.1972e+00  1.5116e+00  1.2873e+00  ...   1.4876e+00  1.4338e+00  1.4194e+00\n",
       "  1.1599e+00  1.6326e+00  1.2523e+00  ...   1.6396e+00  1.3317e+00  1.3729e+00\n",
       "  1.0994e+00  1.3474e+00  9.3165e-01  ...   1.6560e+00  1.5862e+00  1.5972e+00\n",
       "                 ...                                      ...                \n",
       "  3.7627e-01  1.6095e+00  1.3602e+00  ...   1.3841e+00  1.1779e+00  1.5181e+00\n",
       "  2.3871e-01  1.1470e+00  1.1713e+00  ...   2.0181e+00  1.6768e+00  7.9595e-01\n",
       "  9.4456e-02  1.2601e+00  1.3693e+00  ...   1.3730e+00  1.8557e+00  9.5447e-01\n",
       "    ... \n",
       "\n",
       "( 1 ,29 ,.,.) = \n",
       "  5.5152e-01  5.3105e-01  5.0575e-01  ...   5.6361e-01  5.3392e-01  5.2342e-01\n",
       "  5.1486e-01  5.0663e-01  4.7854e-01  ...   4.7973e-01  4.6125e-01  4.4458e-01\n",
       "  5.1918e-01  4.8860e-01  4.3774e-01  ...   4.7369e-01  5.0584e-01  4.9786e-01\n",
       "                 ...                                      ...                \n",
       "  4.2398e-01  4.3470e-01  4.6495e-01  ...   3.9276e-01  3.8013e-01  4.0864e-01\n",
       "  3.9689e-01  3.7400e-01  3.5181e-01  ...   4.4075e-01  5.5313e-01  4.7324e-01\n",
       "  4.0170e-01  2.8708e-01  3.2910e-01  ...   3.6952e-01  4.8934e-01  4.8759e-01\n",
       "\n",
       "( 1 ,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "( 1 ,31 ,.,.) = \n",
       " -2.2389e-01 -2.2964e-01 -2.2485e-01  ...  -2.2940e-01 -2.2900e-01 -2.2992e-01\n",
       " -2.2478e-01 -2.3176e-01 -2.2630e-01  ...  -2.3269e-01 -2.3107e-01 -2.3224e-01\n",
       " -2.2408e-01 -2.2977e-01 -2.2513e-01  ...  -2.3609e-01 -2.3389e-01 -2.3105e-01\n",
       "                 ...                                      ...                \n",
       " -2.1899e-01 -2.3482e-01 -2.2886e-01  ...  -2.3768e-01 -2.2908e-01 -2.3388e-01\n",
       " -2.1912e-01 -2.3194e-01 -2.3420e-01  ...  -2.4526e-01 -2.3593e-01 -2.2242e-01\n",
       " -2.1645e-01 -2.3448e-01 -2.3283e-01  ...  -2.2856e-01 -2.3461e-01 -2.2614e-01\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       "  9.5676e-01  9.6177e-01  9.4148e-01  ...   6.8269e-01  6.8790e-01  6.8975e-01\n",
       "  9.7636e-01  9.9796e-01  9.8153e-01  ...   6.8260e-01  6.8533e-01  7.1026e-01\n",
       "  9.5745e-01  9.8253e-01  9.6418e-01  ...   6.9254e-01  6.9679e-01  7.3371e-01\n",
       "                 ...                                      ...                \n",
       "  1.3722e+00  1.3794e+00  1.3725e+00  ...   7.8317e-01  8.2560e-01  8.8911e-01\n",
       "  1.3776e+00  1.3886e+00  1.3768e+00  ...   7.7355e-01  8.1016e-01  8.5319e-01\n",
       "  1.3733e+00  1.3828e+00  1.3788e+00  ...   7.1207e-01  6.9996e-01  6.5855e-01\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  1.5073e+00  1.4934e+00  1.4401e+00  ...   1.0988e+00  1.1275e+00  1.1223e+00\n",
       "  1.5034e+00  1.5348e+00  1.4766e+00  ...   1.0339e+00  1.0566e+00  1.1142e+00\n",
       "  1.4608e+00  1.4715e+00  1.4262e+00  ...   1.0520e+00  1.0622e+00  1.1186e+00\n",
       "                 ...                                      ...                \n",
       "  2.1624e+00  2.4553e+00  2.4687e+00  ...   1.1136e+00  1.2212e+00  1.3332e+00\n",
       "  2.1677e+00  2.4767e+00  2.4858e+00  ...   1.0822e+00  1.2117e+00  1.2810e+00\n",
       "  2.1619e+00  2.4757e+00  2.4897e+00  ...   1.0063e+00  1.0277e+00  9.8312e-01\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       "  1.5745e+00  1.7956e+00  1.8270e+00  ...   1.7582e+00  1.8772e+00  1.8731e+00\n",
       "  1.5884e+00  1.7660e+00  1.8058e+00  ...   1.9559e+00  1.9290e+00  1.8274e+00\n",
       "  1.7271e+00  1.7944e+00  1.8627e+00  ...   1.9592e+00  1.9454e+00  1.7449e+00\n",
       "                 ...                                      ...                \n",
       " -1.2700e-01  9.2114e-01  9.2117e-01  ...   1.7901e+00  1.7611e+00  1.5655e+00\n",
       " -1.5502e-01  8.9004e-01  9.2635e-01  ...   1.7931e+00  1.7920e+00  1.6919e+00\n",
       " -1.5450e-01  8.8763e-01  9.2364e-01  ...   1.9667e+00  2.0113e+00  2.1385e+00\n",
       "    ... \n",
       "\n",
       "( 2 ,29 ,.,.) = \n",
       "  6.1150e-01  6.1391e-01  6.2726e-01  ...   6.6254e-01  6.5845e-01  6.6131e-01\n",
       "  5.8783e-01  5.9001e-01  6.1242e-01  ...   7.2569e-01  7.1473e-01  6.9863e-01\n",
       "  6.0799e-01  6.1383e-01  6.3362e-01  ...   7.2648e-01  7.2103e-01  6.8705e-01\n",
       "                 ...                                      ...                \n",
       "  3.5423e-01  2.4857e-01  2.4060e-01  ...   7.1171e-01  6.7676e-01  6.3090e-01\n",
       "  3.5031e-01  2.3936e-01  2.3345e-01  ...   7.1854e-01  6.7920e-01  6.4686e-01\n",
       "  3.5044e-01  2.3787e-01  2.3183e-01  ...   7.4327e-01  7.2416e-01  7.2906e-01\n",
       "\n",
       "( 2 ,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "( 2 ,31 ,.,.) = \n",
       " -2.3286e-01 -2.4111e-01 -2.4070e-01  ...  -2.3116e-01 -2.3309e-01 -2.3354e-01\n",
       " -2.3432e-01 -2.4441e-01 -2.4381e-01  ...  -2.3225e-01 -2.3265e-01 -2.3132e-01\n",
       " -2.3559e-01 -2.4387e-01 -2.4411e-01  ...  -2.3270e-01 -2.3297e-01 -2.3157e-01\n",
       "                 ...                                      ...                \n",
       " -2.1944e-01 -2.4972e-01 -2.4960e-01  ...  -2.3386e-01 -2.3609e-01 -2.3562e-01\n",
       " -2.1925e-01 -2.4960e-01 -2.4986e-01  ...  -2.3366e-01 -2.3537e-01 -2.3598e-01\n",
       " -2.1925e-01 -2.4953e-01 -2.4973e-01  ...  -2.3434e-01 -2.3473e-01 -2.3535e-01\n",
       "...     \n",
       "        \n",
       "\n",
       "(125, 0 ,.,.) = \n",
       "  7.2241e-01  7.7881e-01  7.7837e-01  ...   8.6145e-01  8.6191e-01  8.5368e-01\n",
       "  7.4920e-01  8.1740e-01  8.1359e-01  ...   8.8993e-01  8.8272e-01  8.7428e-01\n",
       "  7.5849e-01  8.2216e-01  8.1277e-01  ...   8.8647e-01  8.8113e-01  8.7510e-01\n",
       "                 ...                                      ...                \n",
       "  1.1385e+00  1.1218e+00  1.1221e+00  ...   1.1212e+00  1.1272e+00  1.1228e+00\n",
       "  1.1466e+00  1.1341e+00  1.1327e+00  ...   1.1385e+00  1.1437e+00  1.1263e+00\n",
       "  1.1279e+00  1.1252e+00  1.1431e+00  ...   1.1582e+00  1.1562e+00  1.1455e+00\n",
       "\n",
       "(125, 1 ,.,.) = \n",
       "  1.0857e+00  1.0484e+00  1.0624e+00  ...   1.4510e+00  1.4397e+00  1.4178e+00\n",
       "  1.1159e+00  1.0721e+00  1.0865e+00  ...   1.5135e+00  1.4937e+00  1.4677e+00\n",
       "  1.1246e+00  1.0806e+00  1.0913e+00  ...   1.4914e+00  1.4894e+00  1.4719e+00\n",
       "                 ...                                      ...                \n",
       "  1.9896e+00  2.2442e+00  2.2431e+00  ...   2.2809e+00  2.2791e+00  2.2718e+00\n",
       "  1.9962e+00  2.2584e+00  2.2564e+00  ...   2.3372e+00  2.3230e+00  2.2987e+00\n",
       "  1.9948e+00  2.2203e+00  2.2968e+00  ...   2.3553e+00  2.3468e+00  2.3429e+00\n",
       "\n",
       "(125, 2 ,.,.) = \n",
       "  2.3710e+00  1.9561e+00  1.9502e+00  ...   1.7155e+00  1.7373e+00  1.7468e+00\n",
       "  2.5481e+00  2.0020e+00  1.9879e+00  ...   1.6543e+00  1.6822e+00  1.7033e+00\n",
       "  2.5278e+00  2.0035e+00  1.9942e+00  ...   1.6508e+00  1.6760e+00  1.6921e+00\n",
       "                 ...                                      ...                \n",
       " -4.0175e-02  9.7561e-01  9.8335e-01  ...   1.0057e+00  9.7448e-01  9.8974e-01\n",
       " -1.7879e-02  9.7444e-01  1.0152e+00  ...   9.7014e-01  9.2425e-01  1.0002e+00\n",
       "  5.7168e-02  8.9145e-01  9.2321e-01  ...   8.9899e-01  9.3065e-01  1.0191e+00\n",
       "    ... \n",
       "\n",
       "(125,29 ,.,.) = \n",
       "  6.8941e-01  7.0962e-01  7.0669e-01  ...   6.0013e-01  6.0340e-01  6.0916e-01\n",
       "  7.2954e-01  7.5939e-01  7.5042e-01  ...   5.6228e-01  5.7088e-01  5.7851e-01\n",
       "  7.2851e-01  7.5566e-01  7.4828e-01  ...   5.6869e-01  5.6966e-01  5.7695e-01\n",
       "                 ...                                      ...                \n",
       "  3.5766e-01  2.6171e-01  2.5760e-01  ...   2.5401e-01  2.5900e-01  2.6030e-01\n",
       "  3.6337e-01  2.6892e-01  2.6906e-01  ...   2.3793e-01  2.3819e-01  2.3931e-01\n",
       "  3.6743e-01  2.7364e-01  2.5339e-01  ...   2.2726e-01  2.2559e-01  2.3441e-01\n",
       "\n",
       "(125,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "(125,31 ,.,.) = \n",
       " -2.4284e-01 -2.3958e-01 -2.3946e-01  ...  -2.3597e-01 -2.3623e-01 -2.3641e-01\n",
       " -2.4265e-01 -2.3903e-01 -2.3892e-01  ...  -2.3743e-01 -2.3788e-01 -2.3774e-01\n",
       " -2.4253e-01 -2.3929e-01 -2.3904e-01  ...  -2.3775e-01 -2.3775e-01 -2.3749e-01\n",
       "                 ...                                      ...                \n",
       " -2.1690e-01 -2.4011e-01 -2.4030e-01  ...  -2.4077e-01 -2.3980e-01 -2.4039e-01\n",
       " -2.1690e-01 -2.3947e-01 -2.4012e-01  ...  -2.4047e-01 -2.4015e-01 -2.4149e-01\n",
       " -2.1740e-01 -2.3862e-01 -2.3994e-01  ...  -2.3996e-01 -2.4093e-01 -2.4117e-01\n",
       "        \n",
       "\n",
       "(126, 0 ,.,.) = \n",
       "  5.5948e-01  4.8506e-01  4.5144e-01  ...   4.0062e-01  3.9923e-01  3.9925e-01\n",
       "  5.6471e-01  4.7860e-01  4.3835e-01  ...   3.4301e-01  3.4773e-01  3.5194e-01\n",
       "  6.1252e-01  5.0088e-01  4.5962e-01  ...   3.1194e-01  3.1892e-01  3.2039e-01\n",
       "                 ...                                      ...                \n",
       "  1.2519e+00  1.0541e+00  1.0084e+00  ...   6.6774e-01  6.6110e-01  6.8109e-01\n",
       "  1.2180e+00  1.0337e+00  9.9561e-01  ...   6.5247e-01  6.7875e-01  6.9388e-01\n",
       "  1.1038e+00  9.9428e-01  9.7650e-01  ...   6.4553e-01  6.8654e-01  6.8063e-01\n",
       "\n",
       "(126, 1 ,.,.) = \n",
       "  1.1894e+00  1.0485e+00  9.1149e-01  ...   9.7312e-01  9.8110e-01  9.7896e-01\n",
       "  1.2410e+00  1.0624e+00  9.4383e-01  ...   7.1789e-01  7.4981e-01  7.8308e-01\n",
       "  1.3195e+00  1.1414e+00  1.0057e+00  ...   4.6331e-01  4.9957e-01  5.3321e-01\n",
       "                 ...                                      ...                \n",
       "  2.3262e+00  2.4031e+00  2.1653e+00  ...   1.4004e+00  1.3846e+00  1.3820e+00\n",
       "  2.2669e+00  2.2955e+00  2.1180e+00  ...   1.3697e+00  1.4041e+00  1.4102e+00\n",
       "  2.0874e+00  2.0937e+00  2.0148e+00  ...   1.3548e+00  1.3950e+00  1.4057e+00\n",
       "\n",
       "(126, 2 ,.,.) = \n",
       "  1.8829e+00  2.0110e+00  2.0969e+00  ...   1.8371e+00  1.8411e+00  1.8414e+00\n",
       "  1.8305e+00  2.0788e+00  2.2359e+00  ...   2.0618e+00  2.0344e+00  1.9928e+00\n",
       "  1.6333e+00  2.0560e+00  2.0850e+00  ...   2.3129e+00  2.2413e+00  2.1894e+00\n",
       "                 ...                                      ...                \n",
       " -9.0999e-01  1.3555e+00  1.2973e+00  ...   1.6383e+00  1.6554e+00  1.5852e+00\n",
       " -6.7618e-01  1.3797e+00  1.2895e+00  ...   1.6241e+00  1.5545e+00  1.7075e+00\n",
       " -1.4015e-01  1.3249e+00  1.3459e+00  ...   1.6256e+00  1.5438e+00  1.7766e+00\n",
       "    ... \n",
       "\n",
       "(126,29 ,.,.) = \n",
       "  6.0646e-01  6.3609e-01  6.7197e-01  ...   6.2984e-01  6.2746e-01  6.2797e-01\n",
       "  6.1119e-01  6.6845e-01  7.2643e-01  ...   7.4713e-01  7.3800e-01  7.2570e-01\n",
       "  5.8517e-01  6.3729e-01  6.8537e-01  ...   8.6298e-01  8.4634e-01  8.2597e-01\n",
       "                 ...                                      ...                \n",
       "  2.3031e-01  1.9393e-01  2.8543e-01  ...   5.4280e-01  5.5172e-01  5.4806e-01\n",
       "  2.4393e-01  2.2959e-01  3.0374e-01  ...   5.4907e-01  5.4306e-01  5.4797e-01\n",
       "  3.1414e-01  3.0207e-01  3.4114e-01  ...   5.5470e-01  5.3891e-01  5.5165e-01\n",
       "\n",
       "(126,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "(126,31 ,.,.) = \n",
       " -2.3025e-01 -2.2843e-01 -2.2965e-01  ...  -2.2115e-01 -2.2103e-01 -2.2122e-01\n",
       " -2.2898e-01 -2.2655e-01 -2.2598e-01  ...  -2.1800e-01 -2.1788e-01 -2.1693e-01\n",
       " -2.2714e-01 -2.2780e-01 -2.2556e-01  ...  -2.2052e-01 -2.1963e-01 -2.1913e-01\n",
       "                 ...                                      ...                \n",
       " -2.0755e-01 -2.4464e-01 -2.4035e-01  ...  -2.2813e-01 -2.2806e-01 -2.2755e-01\n",
       " -2.1034e-01 -2.4349e-01 -2.3923e-01  ...  -2.2755e-01 -2.2672e-01 -2.3025e-01\n",
       " -2.1492e-01 -2.4032e-01 -2.3899e-01  ...  -2.2690e-01 -2.2755e-01 -2.3095e-01\n",
       "        \n",
       "\n",
       "(127, 0 ,.,.) = \n",
       "  5.6952e-01  5.5051e-01  4.7313e-01  ...   6.9723e-01  5.2132e-01  3.6818e-01\n",
       "  6.0446e-01  6.5989e-01  5.6126e-01  ...   6.3035e-01  5.0671e-01  3.5863e-01\n",
       "  6.0447e-01  6.3926e-01  6.3272e-01  ...   5.6210e-01  5.6562e-01  4.4077e-01\n",
       "                 ...                                      ...                \n",
       "  6.3400e-01  6.0464e-01  3.9064e-01  ...   4.2518e-01  3.1850e-01  4.6805e-01\n",
       "  5.6201e-01  3.5155e-01  2.4482e-01  ...   3.6370e-01  3.5393e-01  4.4989e-01\n",
       "  6.3648e-01  4.0841e-01  2.6657e-01  ...   4.2617e-01  3.8547e-01  4.6055e-01\n",
       "\n",
       "(127, 1 ,.,.) = \n",
       "  1.0695e+00  9.7406e-01  8.7613e-01  ...   1.3276e+00  1.0638e+00  4.3637e-01\n",
       "  1.1132e+00  1.1445e+00  1.0758e+00  ...   1.1822e+00  1.0212e+00  4.5041e-01\n",
       "  1.1111e+00  1.1302e+00  1.1412e+00  ...   1.1153e+00  9.2242e-01  8.1351e-01\n",
       "                 ...                                      ...                \n",
       "  1.2980e+00  1.3554e+00  1.0904e+00  ...   1.2253e+00  1.0292e+00  1.0997e+00\n",
       "  1.1969e+00  1.0145e+00  8.5266e-01  ...   1.0890e+00  1.0484e+00  1.0712e+00\n",
       "  1.3343e+00  1.2195e+00  7.8553e-01  ...   1.1540e+00  1.1057e+00  1.0743e+00\n",
       "\n",
       "(127, 2 ,.,.) = \n",
       "  2.1643e+00  1.9123e+00  2.1987e+00  ...   1.7024e+00  2.9007e+00  2.5232e+00\n",
       "  2.1147e+00  1.7878e+00  2.2491e+00  ...   1.7470e+00  2.8368e+00  3.0559e+00\n",
       "  2.1037e+00  1.7989e+00  1.9838e+00  ...   2.3179e+00  1.9393e+00  2.7971e+00\n",
       "                 ...                                      ...                \n",
       "  1.5594e+00  1.4400e+00  2.0165e+00  ...   1.7506e+00  1.7448e+00  1.4940e+00\n",
       "  1.7360e+00  2.0204e+00  1.8572e+00  ...   1.5995e+00  1.7173e+00  1.4202e+00\n",
       "  1.5577e+00  2.0817e+00  1.8742e+00  ...   1.5673e+00  1.8529e+00  1.6596e+00\n",
       "    ... \n",
       "\n",
       "(127,29 ,.,.) = \n",
       "  6.4761e-01  6.7156e-01  6.9681e-01  ...   5.9703e-01  6.8615e-01  8.1323e-01\n",
       "  6.7200e-01  6.9117e-01  7.3370e-01  ...   6.2434e-01  7.7623e-01  9.6629e-01\n",
       "  6.6544e-01  6.7045e-01  6.8428e-01  ...   6.8616e-01  7.5749e-01  8.5234e-01\n",
       "                 ...                                      ...                \n",
       "  5.4438e-01  5.2780e-01  5.6654e-01  ...   5.9752e-01  6.4927e-01  6.3614e-01\n",
       "  6.4085e-01  6.4203e-01  7.0738e-01  ...   6.0705e-01  6.1946e-01  5.8435e-01\n",
       "  5.9406e-01  6.4404e-01  7.4352e-01  ...   6.2029e-01  6.3476e-01  6.4983e-01\n",
       "\n",
       "(127,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "(127,31 ,.,.) = \n",
       " -2.3725e-01 -2.3306e-01 -2.3636e-01  ...  -2.3119e-01 -2.4462e-01 -2.3746e-01\n",
       " -2.3629e-01 -2.2979e-01 -2.3381e-01  ...  -2.3116e-01 -2.3909e-01 -2.3804e-01\n",
       " -2.3604e-01 -2.3035e-01 -2.3260e-01  ...  -2.3543e-01 -2.2886e-01 -2.3851e-01\n",
       "                 ...                                      ...                \n",
       " -2.2942e-01 -2.2467e-01 -2.3406e-01  ...  -2.2847e-01 -2.2577e-01 -2.2080e-01\n",
       " -2.3197e-01 -2.3150e-01 -2.2499e-01  ...  -2.2723e-01 -2.2489e-01 -2.2118e-01\n",
       " -2.2949e-01 -2.3159e-01 -2.2482e-01  ...  -2.2562e-01 -2.2980e-01 -2.2047e-01\n",
       "[torch.cuda.FloatTensor of size 128x32x112x112 (GPU 2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant = torch.nn.DataParallel(quantizer.deployment_model, gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = model_quant(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.1552e+00  9.5017e-01  8.7872e-01  ...  -1.8631e-01  1.9957e+00  9.9282e-01\n",
       "-3.9337e+00 -4.2530e+00 -1.2152e-01  ...  -2.8191e+00 -7.4017e-01  1.6144e+00\n",
       "-5.5881e+00  3.0396e+00 -5.6240e-01  ...  -7.4701e-01 -1.7197e+00  4.4811e+00\n",
       "                ...                                      ...                \n",
       " 1.9808e+00  2.0554e+00  1.9562e+00  ...   6.3680e-01  1.1966e+00 -4.0874e-01\n",
       " 4.5948e-01 -1.6623e+00 -3.7264e-01  ...  -1.6432e+00  9.9374e-01  2.9577e+00\n",
       "-9.2918e-01 -3.6385e+00  2.4017e+00  ...  -1.0772e+00 -1.9507e-01 -2.8710e-02\n",
       "[torch.cuda.FloatTensor of size 128x1000 (GPU 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.9494e+00 -3.9579e-01  3.2307e-01  ...   4.3272e-01  2.4295e+00  3.2377e-01\n",
       "-5.2903e+00 -5.2021e+00  9.0071e-03  ...  -3.1000e+00 -1.9988e+00  2.0122e+00\n",
       "-6.1509e+00  2.0534e+00 -1.4551e+00  ...  -3.8637e-01 -2.2865e+00  4.8162e+00\n",
       "                ...                                      ...                \n",
       " 1.6031e+00 -1.6777e+00 -1.1769e+00  ...   1.7391e+00 -1.1841e-01 -1.9975e+00\n",
       " 2.4302e+00 -2.1855e+00  4.6637e-01  ...  -2.0303e+00  1.1533e+00  5.9466e-01\n",
       "-6.8305e-01 -4.2784e+00  1.8464e+00  ...  -1.1086e+00  1.0207e-01 -1.4453e+00\n",
       "[torch.cuda.FloatTensor of size 128x1000 (GPU 2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data_loader, model, criterion, epoch=0, training=True, optimizer=None, quantizer=None, get_quant_params=False):\n",
    "\n",
    "#    if args.gpus and len(args.gpus) > 1:\n",
    "#        model = torch.nn.DataParallel(model, args.gpus)\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    for i, (inputs, target) in enumerate(data_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if gpus is not None:\n",
    "            target = target.cuda(async=True)\n",
    "        input_var = Variable(inputs.type(ttype), volatile=not training)\n",
    "        target_var = Variable(target)\n",
    "\n",
    "\n",
    "        # quantization before computing output\n",
    "        if quantizer is not None:\n",
    "            quantizer.store_and_quantize(training=training, get_quant_params=False)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "\n",
    "        loss = criterion(output, target_var)\n",
    "        if type(output) is list:\n",
    "            output = output[0]\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        top5.update(prec5[0], inputs.size(0))\n",
    "\n",
    "        if training:\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # restore real value parameters before update\n",
    "            if quantizer is not None:\n",
    "                quantizer.restore_real_value()            \n",
    "                quantizer.backprop_quant_gradients()    \n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        elif quantizer is not None:\n",
    "            quantizer.restore_real_value()\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            logging.info('{phase} - Epoch: [{0}][{1}/{2}]\\t'\n",
    "                         'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                         'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                         'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                         'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                         'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                             epoch, i, len(data_loader),\n",
    "                             phase='TRAINING' if training else 'EVALUATING',\n",
    "                             batch_time=batch_time,\n",
    "                             data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg\n",
    "\n",
    "\n",
    "def train(data_loader, model, criterion, epoch, optimizer, quantizer):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    return forward(data_loader, model, criterion, epoch,\n",
    "                   training=True, optimizer=optimizer, quantizer=quantizer, get_quant_params=False)\n",
    "\n",
    "\n",
    "def validate(data_loader, model, criterion, epoch, quantizer, get_quant_params=False):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    return forward(data_loader, model, criterion, epoch,\n",
    "                   training=False, optimizer=None, quantizer=quantizer, get_quant_params=get_quant_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:20: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:38: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "EVALUATING - Epoch: [0][0/5000]\tTime 1.692 (1.692)\tData 1.321 (1.321)\tLoss 1.8149 (1.8149)\tPrec@1 60.000 (60.000)\tPrec@5 60.000 (60.000)\n",
      "EVALUATING - Epoch: [0][10/5000]\tTime 0.061 (0.206)\tData 0.000 (0.120)\tLoss 2.1649 (2.1501)\tPrec@1 40.000 (55.455)\tPrec@5 80.000 (71.818)\n",
      "EVALUATING - Epoch: [0][20/5000]\tTime 0.058 (0.140)\tData 0.000 (0.063)\tLoss 2.1839 (2.0736)\tPrec@1 60.000 (47.143)\tPrec@5 80.000 (79.524)\n",
      "EVALUATING - Epoch: [0][30/5000]\tTime 0.060 (0.114)\tData 0.000 (0.043)\tLoss 3.6015 (2.2907)\tPrec@1 10.000 (44.839)\tPrec@5 50.000 (75.484)\n",
      "EVALUATING - Epoch: [0][40/5000]\tTime 0.049 (0.098)\tData 0.000 (0.033)\tLoss 3.0640 (2.3257)\tPrec@1 30.000 (44.390)\tPrec@5 70.000 (73.902)\n",
      "EVALUATING - Epoch: [0][50/5000]\tTime 0.053 (0.091)\tData 0.000 (0.026)\tLoss 2.3611 (2.4076)\tPrec@1 50.000 (44.118)\tPrec@5 90.000 (72.549)\n",
      "EVALUATING - Epoch: [0][60/5000]\tTime 0.080 (0.087)\tData 0.000 (0.022)\tLoss 2.4272 (2.3161)\tPrec@1 40.000 (45.902)\tPrec@5 80.000 (73.115)\n",
      "EVALUATING - Epoch: [0][70/5000]\tTime 0.072 (0.083)\tData 0.000 (0.019)\tLoss 1.8615 (2.2721)\tPrec@1 50.000 (45.915)\tPrec@5 80.000 (73.803)\n",
      "EVALUATING - Epoch: [0][80/5000]\tTime 0.051 (0.079)\tData 0.000 (0.017)\tLoss 1.3479 (2.1763)\tPrec@1 70.000 (48.395)\tPrec@5 90.000 (74.815)\n",
      "EVALUATING - Epoch: [0][90/5000]\tTime 0.059 (0.077)\tData 0.000 (0.015)\tLoss 1.8275 (2.1140)\tPrec@1 60.000 (50.110)\tPrec@5 90.000 (75.495)\n",
      "EVALUATING - Epoch: [0][100/5000]\tTime 0.076 (0.075)\tData 0.000 (0.014)\tLoss 0.6850 (2.0989)\tPrec@1 90.000 (51.386)\tPrec@5 90.000 (75.743)\n",
      "EVALUATING - Epoch: [0][110/5000]\tTime 0.049 (0.074)\tData 0.000 (0.012)\tLoss 2.4065 (2.1108)\tPrec@1 30.000 (51.441)\tPrec@5 80.000 (76.126)\n",
      "EVALUATING - Epoch: [0][120/5000]\tTime 0.049 (0.072)\tData 0.000 (0.011)\tLoss 2.3020 (2.1581)\tPrec@1 60.000 (49.504)\tPrec@5 70.000 (75.785)\n",
      "EVALUATING - Epoch: [0][130/5000]\tTime 0.052 (0.070)\tData 0.000 (0.011)\tLoss 4.3341 (2.1174)\tPrec@1 10.000 (51.069)\tPrec@5 20.000 (76.183)\n",
      "EVALUATING - Epoch: [0][140/5000]\tTime 0.052 (0.069)\tData 0.000 (0.010)\tLoss 3.4103 (2.2674)\tPrec@1 40.000 (48.936)\tPrec@5 60.000 (73.688)\n",
      "EVALUATING - Epoch: [0][150/5000]\tTime 0.070 (0.069)\tData 0.000 (0.009)\tLoss 2.7749 (2.3319)\tPrec@1 40.000 (48.013)\tPrec@5 70.000 (73.046)\n",
      "EVALUATING - Epoch: [0][160/5000]\tTime 0.060 (0.068)\tData 0.000 (0.009)\tLoss 5.0807 (2.3307)\tPrec@1 0.000 (47.888)\tPrec@5 20.000 (72.919)\n",
      "EVALUATING - Epoch: [0][170/5000]\tTime 0.057 (0.068)\tData 0.000 (0.008)\tLoss 4.1278 (2.4486)\tPrec@1 20.000 (46.199)\tPrec@5 20.000 (70.760)\n",
      "EVALUATING - Epoch: [0][180/5000]\tTime 0.055 (0.067)\tData 0.000 (0.008)\tLoss 2.8795 (2.5211)\tPrec@1 20.000 (44.807)\tPrec@5 80.000 (69.724)\n",
      "EVALUATING - Epoch: [0][190/5000]\tTime 0.066 (0.067)\tData 0.000 (0.007)\tLoss 4.9767 (2.5089)\tPrec@1 10.000 (44.660)\tPrec@5 30.000 (70.052)\n",
      "EVALUATING - Epoch: [0][200/5000]\tTime 0.053 (0.066)\tData 0.000 (0.007)\tLoss 1.3633 (2.5641)\tPrec@1 40.000 (43.781)\tPrec@5 100.000 (69.055)\n",
      "EVALUATING - Epoch: [0][210/5000]\tTime 0.059 (0.066)\tData 0.000 (0.007)\tLoss 5.9563 (2.6176)\tPrec@1 20.000 (42.938)\tPrec@5 30.000 (68.389)\n",
      "EVALUATING - Epoch: [0][220/5000]\tTime 0.059 (0.066)\tData 0.000 (0.006)\tLoss 4.7591 (2.7323)\tPrec@1 10.000 (41.312)\tPrec@5 30.000 (66.154)\n",
      "EVALUATING - Epoch: [0][230/5000]\tTime 0.058 (0.065)\tData 0.000 (0.006)\tLoss 3.2207 (2.8175)\tPrec@1 0.000 (39.913)\tPrec@5 40.000 (64.675)\n",
      "EVALUATING - Epoch: [0][240/5000]\tTime 0.047 (0.064)\tData 0.000 (0.006)\tLoss 2.1971 (2.8298)\tPrec@1 30.000 (39.087)\tPrec@5 70.000 (64.647)\n",
      "EVALUATING - Epoch: [0][250/5000]\tTime 0.048 (0.064)\tData 0.000 (0.006)\tLoss 3.8623 (2.8185)\tPrec@1 10.000 (39.203)\tPrec@5 50.000 (65.060)\n",
      "EVALUATING - Epoch: [0][260/5000]\tTime 0.047 (0.064)\tData 0.000 (0.006)\tLoss 3.3093 (2.8452)\tPrec@1 30.000 (38.812)\tPrec@5 60.000 (64.636)\n",
      "EVALUATING - Epoch: [0][270/5000]\tTime 0.061 (0.064)\tData 0.000 (0.005)\tLoss 5.4381 (2.8924)\tPrec@1 0.000 (38.376)\tPrec@5 20.000 (63.801)\n",
      "EVALUATING - Epoch: [0][280/5000]\tTime 0.055 (0.063)\tData 0.000 (0.005)\tLoss 3.6334 (2.9090)\tPrec@1 10.000 (37.616)\tPrec@5 40.000 (63.274)\n",
      "EVALUATING - Epoch: [0][290/5000]\tTime 0.050 (0.063)\tData 0.000 (0.005)\tLoss 3.9019 (2.9341)\tPrec@1 10.000 (37.216)\tPrec@5 40.000 (62.887)\n",
      "EVALUATING - Epoch: [0][300/5000]\tTime 0.051 (0.063)\tData 0.000 (0.005)\tLoss 4.2774 (2.9561)\tPrec@1 0.000 (36.711)\tPrec@5 40.000 (62.458)\n",
      "EVALUATING - Epoch: [0][310/5000]\tTime 0.045 (0.063)\tData 0.000 (0.005)\tLoss 4.6751 (3.0102)\tPrec@1 10.000 (35.852)\tPrec@5 30.000 (61.608)\n",
      "EVALUATING - Epoch: [0][320/5000]\tTime 0.054 (0.062)\tData 0.000 (0.005)\tLoss 1.1879 (3.0406)\tPrec@1 70.000 (35.296)\tPrec@5 90.000 (61.277)\n",
      "EVALUATING - Epoch: [0][330/5000]\tTime 0.052 (0.062)\tData 0.000 (0.005)\tLoss 5.0008 (3.0472)\tPrec@1 0.000 (35.136)\tPrec@5 10.000 (61.239)\n",
      "EVALUATING - Epoch: [0][340/5000]\tTime 0.052 (0.062)\tData 0.000 (0.005)\tLoss 5.1908 (3.1101)\tPrec@1 0.000 (34.194)\tPrec@5 20.000 (60.029)\n",
      "EVALUATING - Epoch: [0][350/5000]\tTime 0.031 (0.062)\tData 0.000 (0.005)\tLoss 2.3808 (3.1157)\tPrec@1 40.000 (34.217)\tPrec@5 70.000 (60.028)\n",
      "EVALUATING - Epoch: [0][360/5000]\tTime 0.031 (0.061)\tData 0.000 (0.005)\tLoss 0.7359 (3.1023)\tPrec@1 90.000 (34.571)\tPrec@5 100.000 (60.305)\n",
      "EVALUATING - Epoch: [0][370/5000]\tTime 0.049 (0.061)\tData 0.000 (0.005)\tLoss 2.1286 (3.0944)\tPrec@1 40.000 (34.690)\tPrec@5 80.000 (60.323)\n",
      "EVALUATING - Epoch: [0][380/5000]\tTime 0.028 (0.060)\tData 0.000 (0.005)\tLoss 2.6456 (3.0602)\tPrec@1 60.000 (35.407)\tPrec@5 70.000 (60.787)\n",
      "EVALUATING - Epoch: [0][390/5000]\tTime 0.027 (0.060)\tData 0.000 (0.005)\tLoss 3.0381 (3.0459)\tPrec@1 40.000 (35.627)\tPrec@5 40.000 (60.972)\n",
      "EVALUATING - Epoch: [0][400/5000]\tTime 0.142 (0.060)\tData 0.000 (0.005)\tLoss 1.3888 (3.0768)\tPrec@1 80.000 (35.187)\tPrec@5 90.000 (60.549)\n",
      "EVALUATING - Epoch: [0][410/5000]\tTime 0.051 (0.060)\tData 0.000 (0.005)\tLoss 3.1172 (3.0611)\tPrec@1 30.000 (35.450)\tPrec@5 50.000 (60.730)\n",
      "EVALUATING - Epoch: [0][420/5000]\tTime 0.055 (0.059)\tData 0.000 (0.005)\tLoss 4.0700 (3.0451)\tPrec@1 40.000 (35.796)\tPrec@5 40.000 (60.998)\n",
      "EVALUATING - Epoch: [0][430/5000]\tTime 0.055 (0.059)\tData 0.000 (0.005)\tLoss 1.3421 (3.0108)\tPrec@1 50.000 (36.497)\tPrec@5 90.000 (61.578)\n",
      "EVALUATING - Epoch: [0][440/5000]\tTime 0.051 (0.059)\tData 0.000 (0.004)\tLoss 2.0673 (2.9872)\tPrec@1 50.000 (36.961)\tPrec@5 80.000 (61.973)\n",
      "EVALUATING - Epoch: [0][450/5000]\tTime 0.065 (0.059)\tData 0.000 (0.004)\tLoss 0.7397 (2.9459)\tPrec@1 70.000 (37.871)\tPrec@5 100.000 (62.616)\n",
      "EVALUATING - Epoch: [0][460/5000]\tTime 0.052 (0.059)\tData 0.000 (0.004)\tLoss 1.1827 (2.9150)\tPrec@1 60.000 (38.308)\tPrec@5 90.000 (63.102)\n",
      "EVALUATING - Epoch: [0][470/5000]\tTime 0.051 (0.059)\tData 0.000 (0.004)\tLoss 1.0216 (2.8836)\tPrec@1 60.000 (38.832)\tPrec@5 100.000 (63.673)\n",
      "EVALUATING - Epoch: [0][480/5000]\tTime 0.051 (0.059)\tData 0.000 (0.004)\tLoss 1.1806 (2.8536)\tPrec@1 70.000 (39.459)\tPrec@5 90.000 (64.179)\n",
      "EVALUATING - Epoch: [0][490/5000]\tTime 0.049 (0.059)\tData 0.000 (0.004)\tLoss 2.3980 (2.8275)\tPrec@1 40.000 (39.959)\tPrec@5 70.000 (64.562)\n",
      "EVALUATING - Epoch: [0][500/5000]\tTime 0.049 (0.059)\tData 0.000 (0.004)\tLoss 0.7963 (2.8127)\tPrec@1 100.000 (40.200)\tPrec@5 100.000 (64.870)\n",
      "EVALUATING - Epoch: [0][510/5000]\tTime 0.054 (0.059)\tData 0.000 (0.004)\tLoss 1.4483 (2.7947)\tPrec@1 50.000 (40.489)\tPrec@5 90.000 (65.264)\n",
      "EVALUATING - Epoch: [0][520/5000]\tTime 0.052 (0.058)\tData 0.000 (0.004)\tLoss 2.7946 (2.7801)\tPrec@1 30.000 (40.614)\tPrec@5 80.000 (65.605)\n",
      "EVALUATING - Epoch: [0][530/5000]\tTime 0.053 (0.058)\tData 0.000 (0.004)\tLoss 5.0378 (2.8036)\tPrec@1 0.000 (40.301)\tPrec@5 20.000 (65.122)\n",
      "EVALUATING - Epoch: [0][540/5000]\tTime 0.057 (0.058)\tData 0.000 (0.004)\tLoss 2.0687 (2.8226)\tPrec@1 40.000 (40.129)\tPrec@5 70.000 (64.787)\n",
      "EVALUATING - Epoch: [0][550/5000]\tTime 0.054 (0.058)\tData 0.000 (0.004)\tLoss 3.6695 (2.8129)\tPrec@1 20.000 (40.109)\tPrec@5 60.000 (64.991)\n",
      "EVALUATING - Epoch: [0][560/5000]\tTime 0.048 (0.058)\tData 0.000 (0.004)\tLoss 3.2118 (2.8152)\tPrec@1 50.000 (40.000)\tPrec@5 60.000 (65.045)\n",
      "EVALUATING - Epoch: [0][570/5000]\tTime 0.052 (0.058)\tData 0.000 (0.004)\tLoss 4.9135 (2.8378)\tPrec@1 10.000 (39.667)\tPrec@5 20.000 (64.676)\n",
      "EVALUATING - Epoch: [0][580/5000]\tTime 0.055 (0.058)\tData 0.000 (0.004)\tLoss 2.3143 (2.8488)\tPrec@1 40.000 (39.449)\tPrec@5 80.000 (64.509)\n",
      "EVALUATING - Epoch: [0][590/5000]\tTime 0.050 (0.058)\tData 0.000 (0.003)\tLoss 3.0146 (2.8411)\tPrec@1 30.000 (39.662)\tPrec@5 70.000 (64.619)\n",
      "EVALUATING - Epoch: [0][600/5000]\tTime 0.051 (0.058)\tData 0.000 (0.003)\tLoss 2.1911 (2.8642)\tPrec@1 60.000 (39.201)\tPrec@5 60.000 (64.193)\n",
      "EVALUATING - Epoch: [0][610/5000]\tTime 0.054 (0.058)\tData 0.000 (0.003)\tLoss 2.4565 (2.8734)\tPrec@1 30.000 (38.953)\tPrec@5 90.000 (64.141)\n",
      "EVALUATING - Epoch: [0][620/5000]\tTime 0.055 (0.058)\tData 0.000 (0.003)\tLoss 4.2569 (2.8809)\tPrec@1 10.000 (38.808)\tPrec@5 20.000 (63.929)\n",
      "EVALUATING - Epoch: [0][630/5000]\tTime 0.050 (0.058)\tData 0.000 (0.003)\tLoss 2.7224 (2.9020)\tPrec@1 30.000 (38.352)\tPrec@5 80.000 (63.582)\n",
      "EVALUATING - Epoch: [0][640/5000]\tTime 0.150 (0.058)\tData 0.000 (0.003)\tLoss 2.2833 (2.9015)\tPrec@1 30.000 (38.393)\tPrec@5 80.000 (63.635)\n",
      "EVALUATING - Epoch: [0][650/5000]\tTime 0.051 (0.058)\tData 0.000 (0.003)\tLoss 2.0972 (2.8928)\tPrec@1 50.000 (38.387)\tPrec@5 80.000 (63.810)\n",
      "EVALUATING - Epoch: [0][660/5000]\tTime 0.055 (0.058)\tData 0.000 (0.003)\tLoss 1.6599 (2.8806)\tPrec@1 50.000 (38.563)\tPrec@5 90.000 (64.054)\n",
      "EVALUATING - Epoch: [0][670/5000]\tTime 0.050 (0.058)\tData 0.000 (0.003)\tLoss 1.8945 (2.8713)\tPrec@1 40.000 (38.703)\tPrec@5 80.000 (64.262)\n",
      "EVALUATING - Epoch: [0][680/5000]\tTime 0.064 (0.058)\tData 0.000 (0.003)\tLoss 1.9386 (2.8618)\tPrec@1 60.000 (38.781)\tPrec@5 90.000 (64.435)\n",
      "EVALUATING - Epoch: [0][690/5000]\tTime 0.056 (0.058)\tData 0.000 (0.003)\tLoss 2.0200 (2.8437)\tPrec@1 50.000 (39.088)\tPrec@5 80.000 (64.660)\n",
      "EVALUATING - Epoch: [0][700/5000]\tTime 0.054 (0.058)\tData 0.000 (0.003)\tLoss 3.2536 (2.8318)\tPrec@1 20.000 (39.358)\tPrec@5 50.000 (64.793)\n",
      "EVALUATING - Epoch: [0][710/5000]\tTime 0.053 (0.058)\tData 0.000 (0.003)\tLoss 2.7202 (2.8347)\tPrec@1 30.000 (39.269)\tPrec@5 50.000 (64.740)\n",
      "EVALUATING - Epoch: [0][720/5000]\tTime 0.053 (0.058)\tData 0.000 (0.003)\tLoss 1.3678 (2.8261)\tPrec@1 80.000 (39.473)\tPrec@5 90.000 (64.854)\n",
      "EVALUATING - Epoch: [0][730/5000]\tTime 0.060 (0.058)\tData 0.000 (0.003)\tLoss 3.0544 (2.8123)\tPrec@1 40.000 (39.808)\tPrec@5 70.000 (65.048)\n",
      "EVALUATING - Epoch: [0][740/5000]\tTime 0.052 (0.058)\tData 0.000 (0.003)\tLoss 1.2699 (2.8033)\tPrec@1 70.000 (40.013)\tPrec@5 100.000 (65.304)\n",
      "EVALUATING - Epoch: [0][750/5000]\tTime 0.059 (0.058)\tData 0.000 (0.003)\tLoss 2.7748 (2.7851)\tPrec@1 40.000 (40.346)\tPrec@5 50.000 (65.553)\n",
      "EVALUATING - Epoch: [0][760/5000]\tTime 0.053 (0.058)\tData 0.000 (0.003)\tLoss 2.1046 (2.7881)\tPrec@1 0.000 (40.184)\tPrec@5 100.000 (65.532)\n",
      "EVALUATING - Epoch: [0][770/5000]\tTime 0.050 (0.062)\tData 0.000 (0.007)\tLoss 2.6217 (2.7803)\tPrec@1 60.000 (40.130)\tPrec@5 60.000 (65.694)\n",
      "EVALUATING - Epoch: [0][780/5000]\tTime 0.051 (0.063)\tData 0.000 (0.009)\tLoss 0.7580 (2.7759)\tPrec@1 80.000 (40.064)\tPrec@5 100.000 (65.762)\n",
      "EVALUATING - Epoch: [0][790/5000]\tTime 0.053 (0.063)\tData 0.000 (0.009)\tLoss 3.9747 (2.7542)\tPrec@1 10.000 (40.518)\tPrec@5 50.000 (66.106)\n",
      "EVALUATING - Epoch: [0][800/5000]\tTime 0.047 (0.063)\tData 0.000 (0.009)\tLoss 2.8832 (2.7572)\tPrec@1 20.000 (40.449)\tPrec@5 70.000 (66.092)\n",
      "EVALUATING - Epoch: [0][810/5000]\tTime 0.053 (0.063)\tData 0.000 (0.009)\tLoss 2.0077 (2.7478)\tPrec@1 30.000 (40.617)\tPrec@5 80.000 (66.264)\n",
      "EVALUATING - Epoch: [0][820/5000]\tTime 0.050 (0.063)\tData 0.000 (0.008)\tLoss 2.3401 (2.7558)\tPrec@1 50.000 (40.402)\tPrec@5 80.000 (66.139)\n",
      "EVALUATING - Epoch: [0][830/5000]\tTime 0.047 (0.063)\tData 0.000 (0.008)\tLoss 2.6808 (2.7654)\tPrec@1 40.000 (40.060)\tPrec@5 60.000 (66.005)\n",
      "EVALUATING - Epoch: [0][840/5000]\tTime 0.053 (0.063)\tData 0.000 (0.008)\tLoss 3.0407 (2.7725)\tPrec@1 0.000 (39.750)\tPrec@5 80.000 (65.898)\n",
      "EVALUATING - Epoch: [0][850/5000]\tTime 0.053 (0.063)\tData 0.000 (0.008)\tLoss 5.2933 (2.7718)\tPrec@1 10.000 (39.530)\tPrec@5 20.000 (65.993)\n",
      "EVALUATING - Epoch: [0][860/5000]\tTime 0.057 (0.063)\tData 0.000 (0.008)\tLoss 3.8541 (2.7799)\tPrec@1 0.000 (39.419)\tPrec@5 40.000 (65.865)\n",
      "EVALUATING - Epoch: [0][870/5000]\tTime 0.056 (0.063)\tData 0.000 (0.008)\tLoss 2.7516 (2.7862)\tPrec@1 50.000 (39.323)\tPrec@5 70.000 (65.706)\n",
      "EVALUATING - Epoch: [0][880/5000]\tTime 0.051 (0.062)\tData 0.000 (0.008)\tLoss 2.8433 (2.7979)\tPrec@1 30.000 (39.103)\tPrec@5 60.000 (65.494)\n",
      "EVALUATING - Epoch: [0][890/5000]\tTime 0.054 (0.062)\tData 0.000 (0.008)\tLoss 1.3622 (2.8055)\tPrec@1 70.000 (38.956)\tPrec@5 90.000 (65.320)\n",
      "EVALUATING - Epoch: [0][900/5000]\tTime 0.051 (0.062)\tData 0.000 (0.008)\tLoss 2.5493 (2.8031)\tPrec@1 10.000 (38.946)\tPrec@5 60.000 (65.283)\n",
      "EVALUATING - Epoch: [0][910/5000]\tTime 0.052 (0.062)\tData 0.000 (0.008)\tLoss 2.8636 (2.8072)\tPrec@1 30.000 (38.760)\tPrec@5 50.000 (65.115)\n",
      "EVALUATING - Epoch: [0][920/5000]\tTime 0.057 (0.062)\tData 0.000 (0.008)\tLoss 2.4900 (2.8076)\tPrec@1 50.000 (38.817)\tPrec@5 70.000 (65.092)\n",
      "EVALUATING - Epoch: [0][930/5000]\tTime 0.146 (0.062)\tData 0.000 (0.007)\tLoss 3.3358 (2.8100)\tPrec@1 10.000 (38.711)\tPrec@5 40.000 (65.038)\n",
      "EVALUATING - Epoch: [0][940/5000]\tTime 0.060 (0.062)\tData 0.000 (0.007)\tLoss 2.8228 (2.8170)\tPrec@1 40.000 (38.459)\tPrec@5 80.000 (64.973)\n",
      "EVALUATING - Epoch: [0][950/5000]\tTime 0.059 (0.062)\tData 0.000 (0.007)\tLoss 2.6489 (2.8278)\tPrec@1 20.000 (38.212)\tPrec@5 80.000 (64.795)\n",
      "EVALUATING - Epoch: [0][960/5000]\tTime 0.059 (0.062)\tData 0.000 (0.007)\tLoss 3.4103 (2.8325)\tPrec@1 30.000 (38.106)\tPrec@5 60.000 (64.703)\n",
      "EVALUATING - Epoch: [0][970/5000]\tTime 0.060 (0.062)\tData 0.000 (0.007)\tLoss 3.6374 (2.8350)\tPrec@1 20.000 (38.002)\tPrec@5 50.000 (64.655)\n",
      "EVALUATING - Epoch: [0][980/5000]\tTime 0.061 (0.062)\tData 0.000 (0.007)\tLoss 2.0029 (2.8328)\tPrec@1 60.000 (37.992)\tPrec@5 80.000 (64.689)\n",
      "EVALUATING - Epoch: [0][990/5000]\tTime 0.059 (0.062)\tData 0.000 (0.007)\tLoss 1.9613 (2.8312)\tPrec@1 30.000 (37.962)\tPrec@5 70.000 (64.743)\n",
      "EVALUATING - Epoch: [0][1000/5000]\tTime 0.060 (0.062)\tData 0.000 (0.007)\tLoss 3.8148 (2.8362)\tPrec@1 0.000 (37.742)\tPrec@5 50.000 (64.695)\n",
      "EVALUATING - Epoch: [0][1010/5000]\tTime 0.050 (0.062)\tData 0.000 (0.007)\tLoss 4.0168 (2.8386)\tPrec@1 10.000 (37.606)\tPrec@5 40.000 (64.629)\n",
      "EVALUATING - Epoch: [0][1020/5000]\tTime 0.048 (0.062)\tData 0.000 (0.007)\tLoss 1.9999 (2.8367)\tPrec@1 60.000 (37.591)\tPrec@5 90.000 (64.643)\n",
      "EVALUATING - Epoch: [0][1030/5000]\tTime 0.053 (0.062)\tData 0.000 (0.007)\tLoss 3.8601 (2.8323)\tPrec@1 0.000 (37.633)\tPrec@5 30.000 (64.685)\n",
      "EVALUATING - Epoch: [0][1040/5000]\tTime 0.093 (0.062)\tData 0.000 (0.007)\tLoss 2.4125 (2.8391)\tPrec@1 30.000 (37.512)\tPrec@5 70.000 (64.553)\n",
      "EVALUATING - Epoch: [0][1050/5000]\tTime 0.057 (0.062)\tData 0.000 (0.007)\tLoss 3.9415 (2.8434)\tPrec@1 30.000 (37.393)\tPrec@5 40.000 (64.462)\n",
      "EVALUATING - Epoch: [0][1060/5000]\tTime 0.061 (0.062)\tData 0.000 (0.007)\tLoss 2.0490 (2.8477)\tPrec@1 60.000 (37.191)\tPrec@5 80.000 (64.383)\n",
      "EVALUATING - Epoch: [0][1070/5000]\tTime 0.051 (0.062)\tData 0.000 (0.007)\tLoss 2.9855 (2.8442)\tPrec@1 60.000 (37.227)\tPrec@5 80.000 (64.444)\n",
      "EVALUATING - Epoch: [0][1080/5000]\tTime 0.053 (0.062)\tData 0.000 (0.007)\tLoss 2.1567 (2.8421)\tPrec@1 60.000 (37.188)\tPrec@5 60.000 (64.505)\n",
      "EVALUATING - Epoch: [0][1090/5000]\tTime 0.052 (0.062)\tData 0.000 (0.006)\tLoss 1.8208 (2.8296)\tPrec@1 40.000 (37.461)\tPrec@5 100.000 (64.748)\n",
      "EVALUATING - Epoch: [0][1100/5000]\tTime 0.049 (0.062)\tData 0.000 (0.006)\tLoss 3.6961 (2.8300)\tPrec@1 30.000 (37.275)\tPrec@5 60.000 (64.768)\n",
      "EVALUATING - Epoch: [0][1110/5000]\tTime 0.053 (0.062)\tData 0.000 (0.006)\tLoss 1.8584 (2.8270)\tPrec@1 50.000 (37.318)\tPrec@5 90.000 (64.851)\n",
      "EVALUATING - Epoch: [0][1120/5000]\tTime 0.063 (0.062)\tData 0.000 (0.006)\tLoss 2.8078 (2.8269)\tPrec@1 40.000 (37.306)\tPrec@5 70.000 (64.888)\n",
      "EVALUATING - Epoch: [0][1130/5000]\tTime 0.056 (0.062)\tData 0.000 (0.006)\tLoss 4.8854 (2.8243)\tPrec@1 10.000 (37.409)\tPrec@5 30.000 (64.951)\n",
      "EVALUATING - Epoch: [0][1140/5000]\tTime 0.057 (0.061)\tData 0.000 (0.006)\tLoss 3.6886 (2.8326)\tPrec@1 30.000 (37.309)\tPrec@5 60.000 (64.812)\n",
      "EVALUATING - Epoch: [0][1150/5000]\tTime 0.056 (0.061)\tData 0.000 (0.006)\tLoss 1.3710 (2.8288)\tPrec@1 50.000 (37.341)\tPrec@5 90.000 (64.857)\n",
      "EVALUATING - Epoch: [0][1160/5000]\tTime 0.061 (0.061)\tData 0.000 (0.006)\tLoss 2.5221 (2.8218)\tPrec@1 50.000 (37.347)\tPrec@5 60.000 (64.961)\n",
      "EVALUATING - Epoch: [0][1170/5000]\tTime 0.059 (0.061)\tData 0.000 (0.006)\tLoss 2.4675 (2.8273)\tPrec@1 50.000 (37.199)\tPrec@5 70.000 (64.842)\n",
      "EVALUATING - Epoch: [0][1180/5000]\tTime 0.058 (0.061)\tData 0.000 (0.006)\tLoss 3.1728 (2.8266)\tPrec@1 30.000 (37.172)\tPrec@5 60.000 (64.936)\n",
      "EVALUATING - Epoch: [0][1190/5000]\tTime 0.064 (0.061)\tData 0.000 (0.006)\tLoss 2.8026 (2.8265)\tPrec@1 20.000 (37.145)\tPrec@5 60.000 (64.962)\n",
      "EVALUATING - Epoch: [0][1200/5000]\tTime 0.053 (0.062)\tData 0.000 (0.006)\tLoss 1.6055 (2.8201)\tPrec@1 70.000 (37.202)\tPrec@5 80.000 (65.096)\n",
      "EVALUATING - Epoch: [0][1210/5000]\tTime 0.060 (0.062)\tData 0.000 (0.006)\tLoss 2.8998 (2.8146)\tPrec@1 30.000 (37.168)\tPrec@5 70.000 (65.260)\n",
      "EVALUATING - Epoch: [0][1220/5000]\tTime 0.066 (0.062)\tData 0.000 (0.006)\tLoss 1.9009 (2.8073)\tPrec@1 30.000 (37.224)\tPrec@5 100.000 (65.430)\n",
      "EVALUATING - Epoch: [0][1230/5000]\tTime 0.050 (0.061)\tData 0.000 (0.006)\tLoss 3.3335 (2.8036)\tPrec@1 30.000 (37.303)\tPrec@5 60.000 (65.508)\n",
      "EVALUATING - Epoch: [0][1240/5000]\tTime 0.052 (0.061)\tData 0.000 (0.006)\tLoss 3.1304 (2.7982)\tPrec@1 10.000 (37.381)\tPrec@5 60.000 (65.608)\n",
      "EVALUATING - Epoch: [0][1250/5000]\tTime 0.056 (0.061)\tData 0.000 (0.006)\tLoss 2.3120 (2.7987)\tPrec@1 40.000 (37.154)\tPrec@5 80.000 (65.659)\n",
      "EVALUATING - Epoch: [0][1260/5000]\tTime 0.052 (0.061)\tData 0.000 (0.006)\tLoss 1.1467 (2.7878)\tPrec@1 90.000 (37.431)\tPrec@5 90.000 (65.813)\n",
      "EVALUATING - Epoch: [0][1270/5000]\tTime 0.061 (0.061)\tData 0.000 (0.006)\tLoss 2.6129 (2.7823)\tPrec@1 30.000 (37.537)\tPrec@5 70.000 (65.924)\n",
      "EVALUATING - Epoch: [0][1280/5000]\tTime 0.056 (0.061)\tData 0.000 (0.006)\tLoss 2.5443 (2.7735)\tPrec@1 50.000 (37.760)\tPrec@5 70.000 (66.066)\n",
      "EVALUATING - Epoch: [0][1290/5000]\tTime 0.056 (0.061)\tData 0.000 (0.006)\tLoss 1.4263 (2.7711)\tPrec@1 90.000 (37.769)\tPrec@5 90.000 (66.112)\n",
      "EVALUATING - Epoch: [0][1300/5000]\tTime 0.052 (0.061)\tData 0.000 (0.006)\tLoss 1.5408 (2.7591)\tPrec@1 60.000 (37.978)\tPrec@5 100.000 (66.326)\n",
      "EVALUATING - Epoch: [0][1310/5000]\tTime 0.056 (0.061)\tData 0.000 (0.005)\tLoss 3.2572 (2.7513)\tPrec@1 30.000 (38.146)\tPrec@5 60.000 (66.453)\n",
      "EVALUATING - Epoch: [0][1320/5000]\tTime 0.056 (0.061)\tData 0.000 (0.005)\tLoss 3.8337 (2.7525)\tPrec@1 0.000 (38.070)\tPrec@5 40.000 (66.382)\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_prec1, val_prec5 = validate(val_loader, model, criterion, 0, quantizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09921024739742279"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model[0][0].weight.data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:32: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:33: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "EVALUATING - Epoch: [0][0/1563]\tTime 12.467 (12.467)\tData 0.731 (0.731)\tLoss 0.7861 (0.7861)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\n",
      "EVALUATING - Epoch: [0][10/1563]\tTime 0.890 (2.030)\tData 0.000 (0.067)\tLoss 3.5623 (2.1899)\tPrec@1 9.375 (48.295)\tPrec@5 46.875 (76.705)\n",
      "EVALUATING - Epoch: [0][20/1563]\tTime 0.820 (1.532)\tData 0.000 (0.035)\tLoss 2.4930 (2.4698)\tPrec@1 56.250 (47.173)\tPrec@5 65.625 (72.024)\n",
      "EVALUATING - Epoch: [0][30/1563]\tTime 0.770 (1.369)\tData 0.000 (0.024)\tLoss 3.3504 (2.4294)\tPrec@1 21.875 (47.581)\tPrec@5 43.750 (72.379)\n",
      "EVALUATING - Epoch: [0][40/1563]\tTime 0.820 (1.286)\tData 0.000 (0.018)\tLoss 1.9706 (2.5265)\tPrec@1 62.500 (47.104)\tPrec@5 71.875 (70.351)\n",
      "EVALUATING - Epoch: [0][50/1563]\tTime 1.312 (1.241)\tData 0.000 (0.015)\tLoss 5.4116 (2.8164)\tPrec@1 0.000 (42.157)\tPrec@5 15.625 (66.544)\n",
      "EVALUATING - Epoch: [0][60/1563]\tTime 0.824 (1.194)\tData 0.000 (0.012)\tLoss 4.7464 (2.9200)\tPrec@1 15.625 (39.805)\tPrec@5 25.000 (64.805)\n",
      "EVALUATING - Epoch: [0][70/1563]\tTime 0.877 (1.166)\tData 0.000 (0.011)\tLoss 3.8762 (3.1567)\tPrec@1 18.750 (35.431)\tPrec@5 50.000 (59.683)\n",
      "EVALUATING - Epoch: [0][80/1563]\tTime 0.951 (1.148)\tData 0.000 (0.009)\tLoss 5.8859 (3.2523)\tPrec@1 12.500 (34.066)\tPrec@5 18.750 (58.333)\n",
      "EVALUATING - Epoch: [0][90/1563]\tTime 0.876 (1.130)\tData 0.000 (0.009)\tLoss 4.2909 (3.3466)\tPrec@1 12.500 (31.834)\tPrec@5 31.250 (56.628)\n",
      "EVALUATING - Epoch: [0][100/1563]\tTime 1.190 (1.110)\tData 0.000 (0.008)\tLoss 4.2443 (3.4538)\tPrec@1 12.500 (29.765)\tPrec@5 43.750 (54.765)\n",
      "EVALUATING - Epoch: [0][110/1563]\tTime 1.337 (1.100)\tData 0.000 (0.007)\tLoss 3.1694 (3.4570)\tPrec@1 37.500 (29.307)\tPrec@5 53.125 (54.758)\n",
      "EVALUATING - Epoch: [0][120/1563]\tTime 1.048 (1.089)\tData 0.000 (0.007)\tLoss 2.8059 (3.4458)\tPrec@1 34.375 (29.778)\tPrec@5 65.625 (54.804)\n",
      "EVALUATING - Epoch: [0][130/1563]\tTime 0.774 (1.083)\tData 0.000 (0.006)\tLoss 1.8274 (3.4530)\tPrec@1 59.375 (29.461)\tPrec@5 78.125 (54.532)\n",
      "EVALUATING - Epoch: [0][140/1563]\tTime 0.805 (1.074)\tData 0.000 (0.006)\tLoss 2.5287 (3.3638)\tPrec@1 50.000 (31.228)\tPrec@5 81.250 (56.095)\n",
      "EVALUATING - Epoch: [0][150/1563]\tTime 1.438 (1.072)\tData 0.000 (0.005)\tLoss 3.3961 (3.3301)\tPrec@1 18.750 (31.602)\tPrec@5 62.500 (56.850)\n",
      "EVALUATING - Epoch: [0][160/1563]\tTime 0.898 (1.065)\tData 0.000 (0.005)\tLoss 1.2440 (3.3036)\tPrec@1 71.875 (32.220)\tPrec@5 90.625 (57.531)\n",
      "EVALUATING - Epoch: [0][170/1563]\tTime 1.671 (1.064)\tData 0.000 (0.005)\tLoss 2.2097 (3.3189)\tPrec@1 43.750 (32.036)\tPrec@5 68.750 (56.999)\n",
      "EVALUATING - Epoch: [0][180/1563]\tTime 0.720 (1.056)\tData 0.000 (0.005)\tLoss 4.1735 (3.3612)\tPrec@1 9.375 (31.319)\tPrec@5 37.500 (56.336)\n",
      "EVALUATING - Epoch: [0][190/1563]\tTime 0.827 (1.051)\tData 0.000 (0.004)\tLoss 3.0070 (3.3936)\tPrec@1 15.625 (30.481)\tPrec@5 65.625 (55.759)\n",
      "EVALUATING - Epoch: [0][200/1563]\tTime 0.798 (1.049)\tData 0.000 (0.004)\tLoss 4.0025 (3.4041)\tPrec@1 12.500 (30.239)\tPrec@5 53.125 (55.535)\n",
      "EVALUATING - Epoch: [0][210/1563]\tTime 0.758 (1.044)\tData 0.000 (0.004)\tLoss 5.6285 (3.4173)\tPrec@1 6.250 (29.887)\tPrec@5 21.875 (55.687)\n",
      "EVALUATING - Epoch: [0][220/1563]\tTime 1.341 (1.043)\tData 0.000 (0.004)\tLoss 2.7478 (3.3697)\tPrec@1 50.000 (30.953)\tPrec@5 71.875 (56.533)\n",
      "EVALUATING - Epoch: [0][230/1563]\tTime 0.788 (1.037)\tData 0.000 (0.004)\tLoss 3.1757 (3.3423)\tPrec@1 43.750 (31.521)\tPrec@5 65.625 (57.035)\n",
      "EVALUATING - Epoch: [0][240/1563]\tTime 1.234 (1.035)\tData 0.000 (0.003)\tLoss 3.2396 (3.3382)\tPrec@1 25.000 (31.406)\tPrec@5 53.125 (56.963)\n",
      "Process Process-8:\n",
      "Process Process-1:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Process Process-2:\n",
      "Process Process-6:\n",
      "Traceback (most recent call last):\n",
      "Process Process-4:\n",
      "Process Process-7:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-abfda8629711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_prec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_prec5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_prec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_prec5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-cbe724eb105c>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(data_loader, model, criterion, epoch, quantizer, gpus)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     return forward(data_loader, model, criterion, epoch,\n\u001b[0;32m---> 85\u001b[0;31m                    training=False, optimizer=None, quantizer=quantizer)\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-cbe724eb105c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(data_loader, model, criterion, epoch, training, optimizer, quantizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# quantization before computing output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_and_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_quant_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_float_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/quantized_neural_networks/quantization/quantop.py\u001b[0m in \u001b[0;36mstore_and_quantize\u001b[0;34m(self, get_quant_params, update_float_params)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;31m# defold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_norm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_fold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mweight_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_defold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_quant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;31m# copy back into floating point model, not needed if not defold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/quantized_neural_networks/quantization/quantop.py\u001b[0m in \u001b[0;36m_batch_defold\u001b[0;34m(self, weight_tensor, batch_layer)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn_out_channel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_tensor_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mgamma_over_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma_over_sigma_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_out_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mgamma_over_sigma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mweight_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_out_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gamma over sigma'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run validation\n",
    "val_loss, val_prec1, val_prec5 = validate(val_loader, model, criterion, 0, quantizer, gpus)\n",
    "print(val_loss, val_prec1, val_prec5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.2059,  5.9497,  1.0535, 10.2415, 28.1148, 12.4720,  3.4225,  2.6865,\n",
       "        11.5736,  1.9509,  5.7577,  3.0689,  5.4406,  1.8761, 16.6637,  0.1006,\n",
       "         7.2505, 17.0050,  3.0287, 10.2209,  2.5811,  2.2394, 10.0781, 19.4257,\n",
       "         0.2328,  6.2219,  0.2076,  7.0416,  3.6740, 17.6075,  9.2285,  0.3068],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.model[0][1].running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0109,  0.0000, -0.0055],\n",
       "          [ 0.0109, -0.0000, -0.0073],\n",
       "          [ 0.0109,  0.0000, -0.0073]],\n",
       "\n",
       "         [[ 0.0000, -0.0036, -0.0036],\n",
       "          [ 0.0073,  0.0000, -0.0091],\n",
       "          [ 0.0128,  0.0055,  0.0000]],\n",
       "\n",
       "         [[-0.0273, -0.0292, -0.0255],\n",
       "          [-0.0219, -0.0219, -0.0292],\n",
       "          [-0.0201, -0.0164, -0.0164]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0055,  0.0729,  0.0474],\n",
       "          [-0.0182, -0.0018,  0.0036],\n",
       "          [-0.0456, -0.1130, -0.0547]],\n",
       "\n",
       "         [[ 0.1039,  0.1823,  0.1294],\n",
       "          [-0.0219, -0.0292,  0.0073],\n",
       "          [-0.1659, -0.2826, -0.1513]],\n",
       "\n",
       "         [[ 0.0018,  0.0547,  0.0401],\n",
       "          [-0.0036,  0.0073,  0.0146],\n",
       "          [-0.0438, -0.0674, -0.0182]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0201,  0.0438,  0.0346],\n",
       "          [ 0.0201,  0.0219, -0.0018],\n",
       "          [ 0.0000,  0.0164,  0.0055]],\n",
       "\n",
       "         [[-0.0109,  0.0128, -0.0073],\n",
       "          [-0.0036, -0.0109, -0.0255],\n",
       "          [-0.0164, -0.0292, -0.0365]],\n",
       "\n",
       "         [[-0.0273, -0.0292, -0.0310],\n",
       "          [-0.0292, -0.0255, -0.0237],\n",
       "          [-0.0219, -0.0073, -0.0255]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0602,  0.0529,  0.0456],\n",
       "          [ 0.0656,  0.0583,  0.0365],\n",
       "          [ 0.0401,  0.0310,  0.0146]],\n",
       "\n",
       "         [[-0.0456, -0.0620, -0.0766],\n",
       "          [-0.0820, -0.1094, -0.1276],\n",
       "          [-0.1203, -0.1331, -0.1349]],\n",
       "\n",
       "         [[ 0.0346,  0.0529,  0.0766],\n",
       "          [ 0.0820,  0.1112,  0.1021],\n",
       "          [ 0.0984,  0.1076,  0.0966]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0346,  0.1112,  0.1258],\n",
       "          [ 0.0820,  0.1604,  0.1659],\n",
       "          [ 0.0911,  0.1294,  0.1313]],\n",
       "\n",
       "         [[-0.0711, -0.0747, -0.0802],\n",
       "          [-0.0747, -0.0656, -0.0602],\n",
       "          [-0.0474, -0.0656, -0.0729]],\n",
       "\n",
       "         [[ 0.0237,  0.0000, -0.0055],\n",
       "          [-0.0091, -0.0328, -0.0292],\n",
       "          [-0.0346, -0.0620, -0.0620]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0146,  0.0456,  0.0456],\n",
       "          [ 0.0565,  0.0930,  0.0693],\n",
       "          [ 0.0711,  0.0948,  0.0711]],\n",
       "\n",
       "         [[-0.0292, -0.0237, -0.0146],\n",
       "          [ 0.0000,  0.0146, -0.0018],\n",
       "          [ 0.0292,  0.0474,  0.0073]],\n",
       "\n",
       "         [[-0.0255, -0.0219, -0.0273],\n",
       "          [-0.0109,  0.0273,  0.0164],\n",
       "          [ 0.0018,  0.0164, -0.0109]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0182,  0.0109, -0.0164],\n",
       "          [ 0.0018,  0.0565, -0.0018],\n",
       "          [-0.0164,  0.0529,  0.0164]],\n",
       "\n",
       "         [[ 0.0109,  0.0328,  0.0237],\n",
       "          [-0.0055,  0.0583,  0.0237],\n",
       "          [ 0.0036,  0.0930,  0.0711]],\n",
       "\n",
       "         [[-0.0036,  0.0182, -0.0456],\n",
       "          [-0.0091,  0.0529, -0.0383],\n",
       "          [ 0.0073,  0.0383, -0.0164]]],\n",
       "\n",
       "\n",
       "        [[[-0.0328, -0.0164, -0.0219],\n",
       "          [-0.0055, -0.0201, -0.0128],\n",
       "          [-0.0073, -0.0164, -0.0164]],\n",
       "\n",
       "         [[-0.0091,  0.0018, -0.0146],\n",
       "          [-0.0000,  0.0073,  0.0091],\n",
       "          [ 0.0146, -0.0073,  0.0055]],\n",
       "\n",
       "         [[-0.0073, -0.0182, -0.0201],\n",
       "          [-0.0091, -0.0182, -0.0219],\n",
       "          [ 0.0073, -0.0055, -0.0055]]],\n",
       "\n",
       "\n",
       "        [[[-0.0018, -0.0036, -0.0018],\n",
       "          [-0.0036, -0.0055, -0.0036],\n",
       "          [-0.0055, -0.0055, -0.0055]],\n",
       "\n",
       "         [[-0.0000, -0.0036,  0.0018],\n",
       "          [-0.0055, -0.0036, -0.0055],\n",
       "          [-0.0055, -0.0036, -0.0055]],\n",
       "\n",
       "         [[ 0.0018, -0.0018, -0.0018],\n",
       "          [-0.0018, -0.0036, -0.0036],\n",
       "          [-0.0018, -0.0055, -0.0073]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0018,  0.0000,  0.0036],\n",
       "          [-0.0018,  0.0018,  0.0036],\n",
       "          [-0.0036,  0.0055,  0.0073]],\n",
       "\n",
       "         [[ 0.0036, -0.0000,  0.0000],\n",
       "          [-0.0018,  0.0018,  0.0055],\n",
       "          [ 0.0000,  0.0073,  0.0055]],\n",
       "\n",
       "         [[ 0.0000,  0.0036,  0.0055],\n",
       "          [ 0.0036,  0.0036,  0.0055],\n",
       "          [ 0.0073,  0.0036,  0.0036]]],\n",
       "\n",
       "\n",
       "        [[[-0.0273, -0.0182, -0.0073],\n",
       "          [ 0.0055,  0.0146, -0.0055],\n",
       "          [ 0.0383,  0.0310, -0.0164]],\n",
       "\n",
       "         [[-0.0273, -0.0128,  0.0109],\n",
       "          [ 0.0164,  0.0273,  0.0091],\n",
       "          [ 0.0529,  0.0456, -0.0346]],\n",
       "\n",
       "         [[-0.0128,  0.0055,  0.0219],\n",
       "          [ 0.0091,  0.0383,  0.0055],\n",
       "          [ 0.0419,  0.0401, -0.0328]]],\n",
       "\n",
       "\n",
       "        [[[-0.0036,  0.0182,  0.0839],\n",
       "          [ 0.0346,  0.0146,  0.0620],\n",
       "          [ 0.0000,  0.0018,  0.0219]],\n",
       "\n",
       "         [[-0.0602, -0.0036,  0.0164],\n",
       "          [-0.0383, -0.0055,  0.0000],\n",
       "          [-0.0201, -0.0219, -0.0292]],\n",
       "\n",
       "         [[-0.0875, -0.0638,  0.0219],\n",
       "          [-0.0547, -0.0474, -0.0255],\n",
       "          [-0.0711, -0.0583, -0.0456]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0018, -0.0091, -0.0018],\n",
       "          [-0.0091, -0.0000, -0.0073],\n",
       "          [-0.0091, -0.0091, -0.0128]],\n",
       "\n",
       "         [[ 0.0036, -0.0109, -0.0018],\n",
       "          [ 0.0036, -0.0073, -0.0109],\n",
       "          [-0.0018, -0.0036, -0.0091]],\n",
       "\n",
       "         [[-0.0036, -0.0055, -0.0164],\n",
       "          [-0.0146, -0.0164, -0.0164],\n",
       "          [-0.0146, -0.0128, -0.0219]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0036,  0.0073,  0.0000],\n",
       "          [ 0.0018,  0.0073,  0.0000],\n",
       "          [-0.0000,  0.0073, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0055,  0.0018],\n",
       "          [ 0.0055,  0.0073, -0.0000],\n",
       "          [ 0.0018,  0.0055, -0.0000]],\n",
       "\n",
       "         [[ 0.0036,  0.0018, -0.0000],\n",
       "          [ 0.0000,  0.0018,  0.0073],\n",
       "          [ 0.0055,  0.0073,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0182, -0.0091, -0.0255],\n",
       "          [ 0.0255, -0.0164, -0.0784],\n",
       "          [ 0.0164, -0.0036, -0.0456]],\n",
       "\n",
       "         [[ 0.0419, -0.0146, -0.0383],\n",
       "          [ 0.0583, -0.0401, -0.1221],\n",
       "          [ 0.0383, -0.0237, -0.0784]],\n",
       "\n",
       "         [[ 0.0146, -0.0036, -0.0018],\n",
       "          [ 0.0182, -0.0036, -0.0365],\n",
       "          [ 0.0109,  0.0091, -0.0128]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0128,  0.0036,  0.0857],\n",
       "          [-0.0547,  0.0565,  0.0018],\n",
       "          [ 0.0346,  0.0529,  0.0620]],\n",
       "\n",
       "         [[ 0.0292,  0.0201, -0.0273],\n",
       "          [-0.0383,  0.0802, -0.0474],\n",
       "          [-0.0273,  0.0055, -0.0182]],\n",
       "\n",
       "         [[-0.0383,  0.0182, -0.0438],\n",
       "          [-0.0766, -0.0201, -0.0547],\n",
       "          [-0.0948, -0.0438, -0.0073]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0018,  0.0036,  0.0036],\n",
       "          [-0.0000, -0.0018,  0.0018],\n",
       "          [ 0.0055,  0.0055,  0.0055]],\n",
       "\n",
       "         [[ 0.0128,  0.0055,  0.0036],\n",
       "          [ 0.0073,  0.0073,  0.0109],\n",
       "          [ 0.0055,  0.0036,  0.0146]],\n",
       "\n",
       "         [[ 0.0055,  0.0091,  0.0073],\n",
       "          [ 0.0091,  0.0073,  0.0128],\n",
       "          [ 0.0128,  0.0128,  0.0164]]],\n",
       "\n",
       "\n",
       "        [[[-0.0365, -0.0273, -0.0255],\n",
       "          [-0.0565, -0.0438, -0.0273],\n",
       "          [-0.0383, -0.0747, -0.0729]],\n",
       "\n",
       "         [[ 0.0091,  0.0492,  0.0674],\n",
       "          [-0.0036,  0.0565,  0.0948],\n",
       "          [ 0.0292,  0.0365,  0.0456]],\n",
       "\n",
       "         [[-0.0036,  0.0438,  0.0383],\n",
       "          [ 0.0055,  0.0674,  0.0729],\n",
       "          [ 0.0237,  0.0474,  0.0474]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0128,  0.0073],\n",
       "          [ 0.0073,  0.0018,  0.0109],\n",
       "          [-0.0055, -0.0091,  0.0018]],\n",
       "\n",
       "         [[ 0.0073,  0.0201,  0.0219],\n",
       "          [-0.0073,  0.0128,  0.0201],\n",
       "          [ 0.0182,  0.0000,  0.0073]],\n",
       "\n",
       "         [[-0.0036,  0.0328,  0.0310],\n",
       "          [ 0.0018,  0.0109,  0.0219],\n",
       "          [ 0.0182,  0.0000,  0.0164]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0383,  0.0492,  0.0510],\n",
       "          [ 0.0310,  0.0237,  0.0365],\n",
       "          [ 0.0237,  0.0237,  0.0419]],\n",
       "\n",
       "         [[-0.1003, -0.0875, -0.0602],\n",
       "          [-0.1513, -0.1385, -0.1076],\n",
       "          [-0.1604, -0.1513, -0.0966]],\n",
       "\n",
       "         [[ 0.0036, -0.0018, -0.0073],\n",
       "          [ 0.0820,  0.0711,  0.0529],\n",
       "          [ 0.1130,  0.1076,  0.0839]]],\n",
       "\n",
       "\n",
       "        [[[-0.0036,  0.0091, -0.0091],\n",
       "          [ 0.0055,  0.0055, -0.0109],\n",
       "          [ 0.0091, -0.0055, -0.0146]],\n",
       "\n",
       "         [[-0.0146, -0.0365, -0.0292],\n",
       "          [-0.0201, -0.0310, -0.0164],\n",
       "          [-0.0237, -0.0000, -0.0219]],\n",
       "\n",
       "         [[-0.0292, -0.0036,  0.0000],\n",
       "          [-0.0164, -0.0255, -0.0219],\n",
       "          [ 0.0091, -0.0128, -0.0036]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0018,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0018,  0.0000],\n",
       "          [ 0.0018,  0.0018,  0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0018,  0.0000,  0.0018],\n",
       "          [ 0.0000,  0.0000,  0.0018]],\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0018]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0036,  0.0018,  0.0018],\n",
       "          [ 0.0000,  0.0018, -0.0018]],\n",
       "\n",
       "         [[ 0.0036,  0.0036,  0.0055],\n",
       "          [ 0.0036,  0.0036,  0.0055],\n",
       "          [ 0.0055,  0.0055,  0.0055]],\n",
       "\n",
       "         [[ 0.0036,  0.0036,  0.0036],\n",
       "          [ 0.0036,  0.0036,  0.0036],\n",
       "          [ 0.0073,  0.0036,  0.0055]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0164,  0.0128,  0.0109],\n",
       "          [ 0.0109,  0.0146,  0.0073],\n",
       "          [ 0.0109,  0.0164,  0.0146]],\n",
       "\n",
       "         [[ 0.0091,  0.0109,  0.0036],\n",
       "          [ 0.0073,  0.0036,  0.0036],\n",
       "          [ 0.0055,  0.0109,  0.0036]],\n",
       "\n",
       "         [[ 0.0109,  0.0128,  0.0055],\n",
       "          [ 0.0146,  0.0109,  0.0055],\n",
       "          [ 0.0091,  0.0055,  0.0055]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0036, -0.0000, -0.0018],\n",
       "          [-0.0018, -0.0000,  0.0018],\n",
       "          [-0.0000, -0.0036, -0.0018]],\n",
       "\n",
       "         [[-0.0018, -0.0036, -0.0018],\n",
       "          [-0.0018, -0.0018, -0.0018],\n",
       "          [ 0.0018, -0.0018, -0.0018]],\n",
       "\n",
       "         [[-0.0036, -0.0018, -0.0036],\n",
       "          [ 0.0018, -0.0018, -0.0018],\n",
       "          [ 0.0018,  0.0018,  0.0018]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0036, -0.0036,  0.0109],\n",
       "          [-0.0128, -0.0201,  0.0091],\n",
       "          [-0.0091, -0.0091,  0.0164]],\n",
       "\n",
       "         [[-0.0000, -0.0109,  0.0164],\n",
       "          [-0.0255, -0.0310, -0.0000],\n",
       "          [-0.0146, -0.0091,  0.0109]],\n",
       "\n",
       "         [[-0.0073, -0.0073,  0.0146],\n",
       "          [-0.0219, -0.0255, -0.0036],\n",
       "          [-0.0164, -0.0128,  0.0018]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0438, -0.0273,  0.0383],\n",
       "          [-0.0273, -0.0164,  0.0292],\n",
       "          [ 0.0510, -0.0365, -0.0456]],\n",
       "\n",
       "         [[ 0.0146,  0.0656,  0.0091],\n",
       "          [-0.0000,  0.0656, -0.0474],\n",
       "          [ 0.0602,  0.0529,  0.0255]],\n",
       "\n",
       "         [[-0.0201,  0.0438,  0.0620],\n",
       "          [ 0.0310,  0.0328,  0.0510],\n",
       "          [-0.0492, -0.0292, -0.0164]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0747,  0.0456,  0.0474],\n",
       "          [ 0.0328, -0.1130, -0.1112],\n",
       "          [ 0.0802, -0.0839, -0.0310]],\n",
       "\n",
       "         [[ 0.0128, -0.0164, -0.0237],\n",
       "          [ 0.0091, -0.1622, -0.1586],\n",
       "          [ 0.0237, -0.1094, -0.0857]],\n",
       "\n",
       "         [[ 0.0255,  0.0401,  0.0602],\n",
       "          [ 0.0328, -0.0911, -0.0674],\n",
       "          [ 0.0109, -0.0802, -0.0273]]],\n",
       "\n",
       "\n",
       "        [[[-0.0693, -0.0346, -0.0073],\n",
       "          [-0.0820, -0.0547, -0.0383],\n",
       "          [-0.0602, -0.0055,  0.0128]],\n",
       "\n",
       "         [[-0.0109,  0.0273,  0.0510],\n",
       "          [-0.0201,  0.0091,  0.0547],\n",
       "          [ 0.0310,  0.0565,  0.0784]],\n",
       "\n",
       "         [[ 0.0164,  0.0383,  0.0729],\n",
       "          [ 0.0201,  0.0401,  0.0474],\n",
       "          [ 0.0201,  0.0438,  0.0583]]],\n",
       "\n",
       "\n",
       "        [[[-0.0365, -0.1039, -0.0638],\n",
       "          [-0.1185, -0.1805, -0.1148],\n",
       "          [-0.0857, -0.1422, -0.0510]],\n",
       "\n",
       "         [[ 0.0492, -0.0146,  0.0055],\n",
       "          [ 0.0182, -0.0602, -0.0055],\n",
       "          [ 0.0018, -0.0419,  0.0328]],\n",
       "\n",
       "         [[ 0.0456,  0.0109,  0.0328],\n",
       "          [ 0.0182, -0.0346,  0.0383],\n",
       "          [ 0.0128, -0.0237,  0.0438]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0073,  0.0000,  0.0018],\n",
       "          [-0.0036, -0.0091,  0.0018],\n",
       "          [-0.0036, -0.0036, -0.0018]],\n",
       "\n",
       "         [[ 0.0109,  0.0073,  0.0073],\n",
       "          [-0.0036, -0.0018,  0.0036],\n",
       "          [-0.0036, -0.0036, -0.0036]],\n",
       "\n",
       "         [[-0.0128, -0.0146, -0.0201],\n",
       "          [-0.0201, -0.0273, -0.0201],\n",
       "          [-0.0237, -0.0201, -0.0219]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0036,  0.0073, -0.0073],\n",
       "          [-0.0055, -0.0091, -0.0055],\n",
       "          [-0.0055, -0.0018,  0.0000]],\n",
       "\n",
       "         [[ 0.0073, -0.0036, -0.0018],\n",
       "          [ 0.0036, -0.0128, -0.0036],\n",
       "          [-0.0109, -0.0128, -0.0128]],\n",
       "\n",
       "         [[ 0.0091,  0.0109,  0.0073],\n",
       "          [ 0.0018, -0.0036, -0.0036],\n",
       "          [-0.0018,  0.0073,  0.0018]]]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "quantizer.deployment_model.model[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][0/782]\tTime 5.019 (5.019)\tData 3.639 (3.639)\tLoss 8.2795 (8.2795)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][10/782]\tTime 0.965 (1.337)\tData 0.000 (0.331)\tLoss 8.7712 (9.9373)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][20/782]\tTime 0.964 (1.161)\tData 0.000 (0.174)\tLoss 10.0164 (9.0190)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][30/782]\tTime 0.952 (1.098)\tData 0.000 (0.118)\tLoss 8.3803 (9.0293)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][40/782]\tTime 0.968 (1.065)\tData 0.000 (0.089)\tLoss 7.8637 (9.0348)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][50/782]\tTime 0.975 (1.043)\tData 0.000 (0.072)\tLoss 8.8594 (9.0806)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][60/782]\tTime 0.970 (1.029)\tData 0.000 (0.060)\tLoss 8.8920 (9.0228)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][70/782]\tTime 0.964 (1.020)\tData 0.000 (0.052)\tLoss 6.5390 (8.8604)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.022)\n",
      "EVALUATING - Epoch: [0][80/782]\tTime 0.979 (1.012)\tData 0.000 (0.045)\tLoss 9.0713 (8.7113)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.965)\n",
      "EVALUATING - Epoch: [0][90/782]\tTime 0.984 (1.006)\tData 0.000 (0.040)\tLoss 8.4570 (8.7363)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.859)\n",
      "EVALUATING - Epoch: [0][100/782]\tTime 0.984 (1.002)\tData 0.000 (0.037)\tLoss 6.0219 (8.6701)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.774)\n",
      "EVALUATING - Epoch: [0][110/782]\tTime 0.963 (0.999)\tData 0.000 (0.033)\tLoss 10.2985 (8.6976)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.704)\n",
      "EVALUATING - Epoch: [0][120/782]\tTime 0.965 (0.994)\tData 0.000 (0.031)\tLoss 7.6817 (8.6362)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.646)\n",
      "EVALUATING - Epoch: [0][130/782]\tTime 0.952 (0.991)\tData 0.000 (0.028)\tLoss 8.5773 (8.5547)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.596)\n",
      "EVALUATING - Epoch: [0][140/782]\tTime 0.958 (0.990)\tData 0.000 (0.026)\tLoss 7.9203 (8.5285)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.554)\n",
      "EVALUATING - Epoch: [0][150/782]\tTime 0.989 (0.986)\tData 0.000 (0.025)\tLoss 8.4802 (8.4927)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.517)\n",
      "EVALUATING - Epoch: [0][160/782]\tTime 0.947 (0.985)\tData 0.000 (0.023)\tLoss 8.0160 (8.4403)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.485)\n",
      "EVALUATING - Epoch: [0][170/782]\tTime 0.959 (0.984)\tData 0.000 (0.022)\tLoss 8.0115 (8.3631)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.457)\n",
      "EVALUATING - Epoch: [0][180/782]\tTime 0.955 (0.982)\tData 0.000 (0.021)\tLoss 7.4375 (8.3259)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.432)\n",
      "EVALUATING - Epoch: [0][190/782]\tTime 0.965 (0.981)\tData 0.000 (0.020)\tLoss 8.8382 (8.2868)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.409)\n",
      "EVALUATING - Epoch: [0][200/782]\tTime 0.983 (0.980)\tData 0.000 (0.019)\tLoss 7.5416 (8.2407)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.389)\n",
      "EVALUATING - Epoch: [0][210/782]\tTime 0.960 (0.978)\tData 0.000 (0.018)\tLoss 7.8985 (8.1980)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.370)\n",
      "EVALUATING - Epoch: [0][220/782]\tTime 0.956 (0.978)\tData 0.000 (0.017)\tLoss 6.3647 (8.2327)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.354)\n",
      "EVALUATING - Epoch: [0][230/782]\tTime 0.963 (0.977)\tData 0.000 (0.016)\tLoss 8.4014 (8.2697)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.338)\n",
      "EVALUATING - Epoch: [0][240/782]\tTime 0.978 (0.976)\tData 0.000 (0.016)\tLoss 7.7805 (8.2895)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.324)\n",
      "EVALUATING - Epoch: [0][250/782]\tTime 0.979 (0.975)\tData 0.000 (0.015)\tLoss 11.5204 (8.2900)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.311)\n",
      "EVALUATING - Epoch: [0][260/782]\tTime 0.983 (0.975)\tData 0.001 (0.014)\tLoss 8.0245 (8.3403)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.299)\n",
      "EVALUATING - Epoch: [0][270/782]\tTime 0.973 (0.975)\tData 0.000 (0.014)\tLoss 7.3110 (8.3539)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.288)\n",
      "EVALUATING - Epoch: [0][280/782]\tTime 0.897 (0.974)\tData 0.000 (0.013)\tLoss 8.5295 (8.3346)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.278)\n",
      "EVALUATING - Epoch: [0][290/782]\tTime 0.982 (0.974)\tData 0.000 (0.013)\tLoss 8.6974 (8.3433)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.268)\n",
      "EVALUATING - Epoch: [0][300/782]\tTime 0.980 (0.973)\tData 0.000 (0.013)\tLoss 8.3680 (8.3466)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.260)\n",
      "EVALUATING - Epoch: [0][310/782]\tTime 0.974 (0.973)\tData 0.000 (0.012)\tLoss 7.5611 (8.3284)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.251)\n",
      "EVALUATING - Epoch: [0][320/782]\tTime 0.965 (0.972)\tData 0.000 (0.012)\tLoss 7.0433 (8.3075)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.243)\n",
      "EVALUATING - Epoch: [0][330/782]\tTime 0.937 (0.972)\tData 0.000 (0.012)\tLoss 7.8173 (8.2711)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.236)\n",
      "EVALUATING - Epoch: [0][340/782]\tTime 0.968 (0.972)\tData 0.000 (0.011)\tLoss 6.1415 (8.2422)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.229)\n",
      "EVALUATING - Epoch: [0][350/782]\tTime 0.969 (0.971)\tData 0.000 (0.011)\tLoss 6.1353 (8.2056)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.223)\n",
      "EVALUATING - Epoch: [0][360/782]\tTime 0.971 (0.971)\tData 0.000 (0.011)\tLoss 6.6858 (8.1787)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.216)\n",
      "EVALUATING - Epoch: [0][370/782]\tTime 0.968 (0.970)\tData 0.000 (0.010)\tLoss 4.7814 (8.1504)\tPrec@1 31.250 (0.211)\tPrec@5 31.250 (0.421)\n",
      "EVALUATING - Epoch: [0][380/782]\tTime 0.967 (0.970)\tData 0.000 (0.010)\tLoss 7.5536 (8.1284)\tPrec@1 0.000 (0.205)\tPrec@5 0.000 (0.410)\n",
      "EVALUATING - Epoch: [0][390/782]\tTime 0.948 (0.970)\tData 0.000 (0.010)\tLoss 7.9802 (8.1136)\tPrec@1 0.000 (0.200)\tPrec@5 0.000 (0.400)\n",
      "EVALUATING - Epoch: [0][400/782]\tTime 0.955 (0.969)\tData 0.000 (0.010)\tLoss 5.6896 (8.0823)\tPrec@1 0.000 (0.195)\tPrec@5 0.000 (0.390)\n",
      "EVALUATING - Epoch: [0][410/782]\tTime 0.960 (0.969)\tData 0.000 (0.009)\tLoss 9.1765 (8.0617)\tPrec@1 0.000 (0.190)\tPrec@5 15.625 (0.570)\n",
      "EVALUATING - Epoch: [0][420/782]\tTime 0.981 (0.969)\tData 0.000 (0.009)\tLoss 7.5616 (8.0485)\tPrec@1 0.000 (0.186)\tPrec@5 0.000 (0.557)\n",
      "EVALUATING - Epoch: [0][430/782]\tTime 0.976 (0.969)\tData 0.000 (0.009)\tLoss 6.6741 (8.0393)\tPrec@1 0.000 (0.181)\tPrec@5 0.000 (0.544)\n",
      "EVALUATING - Epoch: [0][440/782]\tTime 0.970 (0.969)\tData 0.000 (0.009)\tLoss 6.6410 (8.0271)\tPrec@1 0.000 (0.177)\tPrec@5 0.000 (0.531)\n",
      "EVALUATING - Epoch: [0][450/782]\tTime 0.998 (0.968)\tData 0.000 (0.009)\tLoss 8.5591 (8.0068)\tPrec@1 0.000 (0.173)\tPrec@5 0.000 (0.662)\n",
      "EVALUATING - Epoch: [0][460/782]\tTime 0.994 (0.968)\tData 0.000 (0.008)\tLoss 7.0639 (7.9915)\tPrec@1 0.000 (0.169)\tPrec@5 0.000 (0.647)\n",
      "EVALUATING - Epoch: [0][470/782]\tTime 0.963 (0.968)\tData 0.000 (0.008)\tLoss 6.4562 (7.9615)\tPrec@1 0.000 (0.166)\tPrec@5 0.000 (0.634)\n",
      "EVALUATING - Epoch: [0][480/782]\tTime 0.973 (0.968)\tData 0.000 (0.008)\tLoss 6.0982 (7.9456)\tPrec@1 0.000 (0.162)\tPrec@5 0.000 (0.620)\n",
      "EVALUATING - Epoch: [0][490/782]\tTime 0.936 (0.968)\tData 0.000 (0.008)\tLoss 8.1502 (7.9357)\tPrec@1 0.000 (0.159)\tPrec@5 0.000 (0.608)\n",
      "EVALUATING - Epoch: [0][500/782]\tTime 0.956 (0.968)\tData 0.000 (0.008)\tLoss 11.9681 (7.9225)\tPrec@1 0.000 (0.156)\tPrec@5 0.000 (0.596)\n",
      "EVALUATING - Epoch: [0][510/782]\tTime 0.950 (0.968)\tData 0.000 (0.008)\tLoss 8.0935 (7.9170)\tPrec@1 0.000 (0.153)\tPrec@5 0.000 (0.584)\n",
      "EVALUATING - Epoch: [0][520/782]\tTime 0.978 (0.967)\tData 0.000 (0.007)\tLoss 6.5244 (7.9036)\tPrec@1 0.000 (0.150)\tPrec@5 0.000 (0.573)\n",
      "EVALUATING - Epoch: [0][530/782]\tTime 0.974 (0.967)\tData 0.000 (0.007)\tLoss 6.3035 (7.8902)\tPrec@1 0.000 (0.147)\tPrec@5 0.000 (0.562)\n",
      "EVALUATING - Epoch: [0][540/782]\tTime 0.964 (0.967)\tData 0.000 (0.007)\tLoss 7.2725 (7.8823)\tPrec@1 0.000 (0.144)\tPrec@5 0.000 (0.552)\n",
      "EVALUATING - Epoch: [0][550/782]\tTime 0.977 (0.967)\tData 0.000 (0.007)\tLoss 7.7163 (7.8736)\tPrec@1 0.000 (0.142)\tPrec@5 0.000 (0.542)\n",
      "EVALUATING - Epoch: [0][560/782]\tTime 0.993 (0.967)\tData 0.001 (0.007)\tLoss 6.8248 (7.8597)\tPrec@1 0.000 (0.139)\tPrec@5 0.000 (0.663)\n",
      "EVALUATING - Epoch: [0][570/782]\tTime 0.986 (0.967)\tData 0.000 (0.007)\tLoss 9.1024 (7.8551)\tPrec@1 0.000 (0.137)\tPrec@5 0.000 (0.651)\n",
      "EVALUATING - Epoch: [0][580/782]\tTime 0.943 (0.967)\tData 0.000 (0.007)\tLoss 7.7637 (7.8511)\tPrec@1 0.000 (0.134)\tPrec@5 0.000 (0.640)\n",
      "EVALUATING - Epoch: [0][590/782]\tTime 0.941 (0.966)\tData 0.000 (0.007)\tLoss 8.3272 (7.8422)\tPrec@1 0.000 (0.132)\tPrec@5 0.000 (0.629)\n",
      "EVALUATING - Epoch: [0][600/782]\tTime 0.972 (0.966)\tData 0.000 (0.007)\tLoss 7.0134 (7.8276)\tPrec@1 0.000 (0.130)\tPrec@5 0.000 (0.619)\n",
      "EVALUATING - Epoch: [0][610/782]\tTime 0.952 (0.966)\tData 0.000 (0.006)\tLoss 7.9760 (7.8119)\tPrec@1 0.000 (0.128)\tPrec@5 0.000 (0.609)\n",
      "EVALUATING - Epoch: [0][620/782]\tTime 0.993 (0.966)\tData 0.000 (0.006)\tLoss 7.8771 (7.7980)\tPrec@1 0.000 (0.126)\tPrec@5 0.000 (0.599)\n",
      "EVALUATING - Epoch: [0][630/782]\tTime 0.971 (0.965)\tData 0.000 (0.006)\tLoss 7.7878 (7.7941)\tPrec@1 0.000 (0.124)\tPrec@5 0.000 (0.589)\n",
      "EVALUATING - Epoch: [0][640/782]\tTime 0.976 (0.965)\tData 0.000 (0.006)\tLoss 6.3292 (7.7890)\tPrec@1 0.000 (0.122)\tPrec@5 0.000 (0.580)\n",
      "EVALUATING - Epoch: [0][650/782]\tTime 0.962 (0.965)\tData 0.000 (0.006)\tLoss 8.6536 (7.7830)\tPrec@1 0.000 (0.120)\tPrec@5 0.000 (0.595)\n",
      "EVALUATING - Epoch: [0][660/782]\tTime 0.961 (0.965)\tData 0.000 (0.006)\tLoss 5.8704 (7.7698)\tPrec@1 0.000 (0.118)\tPrec@5 0.000 (0.586)\n",
      "EVALUATING - Epoch: [0][670/782]\tTime 0.951 (0.965)\tData 0.000 (0.006)\tLoss 7.5885 (7.7697)\tPrec@1 0.000 (0.116)\tPrec@5 0.000 (0.577)\n",
      "EVALUATING - Epoch: [0][680/782]\tTime 0.848 (0.965)\tData 0.000 (0.006)\tLoss 8.5749 (7.7752)\tPrec@1 0.000 (0.115)\tPrec@5 0.000 (0.569)\n",
      "EVALUATING - Epoch: [0][690/782]\tTime 0.965 (0.965)\tData 0.000 (0.006)\tLoss 6.9612 (7.7677)\tPrec@1 0.000 (0.113)\tPrec@5 0.000 (0.561)\n",
      "EVALUATING - Epoch: [0][700/782]\tTime 0.981 (0.965)\tData 0.000 (0.006)\tLoss 7.0836 (7.7589)\tPrec@1 0.000 (0.111)\tPrec@5 0.000 (0.553)\n",
      "EVALUATING - Epoch: [0][710/782]\tTime 0.967 (0.965)\tData 0.000 (0.006)\tLoss 7.8591 (7.7499)\tPrec@1 0.000 (0.110)\tPrec@5 0.000 (0.545)\n",
      "EVALUATING - Epoch: [0][720/782]\tTime 0.956 (0.965)\tData 0.000 (0.006)\tLoss 9.9092 (7.7424)\tPrec@1 0.000 (0.108)\tPrec@5 0.000 (0.537)\n",
      "EVALUATING - Epoch: [0][730/782]\tTime 0.968 (0.965)\tData 0.000 (0.005)\tLoss 7.8352 (7.7622)\tPrec@1 0.000 (0.107)\tPrec@5 0.000 (0.530)\n",
      "EVALUATING - Epoch: [0][740/782]\tTime 0.969 (0.965)\tData 0.000 (0.005)\tLoss 9.1154 (7.7767)\tPrec@1 0.000 (0.105)\tPrec@5 0.000 (0.523)\n",
      "EVALUATING - Epoch: [0][750/782]\tTime 0.959 (0.965)\tData 0.000 (0.005)\tLoss 9.8188 (7.7907)\tPrec@1 0.000 (0.104)\tPrec@5 0.000 (0.516)\n",
      "EVALUATING - Epoch: [0][760/782]\tTime 0.984 (0.965)\tData 0.000 (0.005)\tLoss 8.8584 (7.8005)\tPrec@1 0.000 (0.103)\tPrec@5 0.000 (0.509)\n",
      "EVALUATING - Epoch: [0][770/782]\tTime 0.958 (0.965)\tData 0.000 (0.005)\tLoss 7.8593 (7.8218)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.503)\n",
      "EVALUATING - Epoch: [0][780/782]\tTime 0.960 (0.964)\tData 0.000 (0.005)\tLoss 8.7775 (7.8398)\tPrec@1 0.000 (0.100)\tPrec@5 0.000 (0.496)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.839918360290527 0.1 0.496\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_prec1, val_prec5 = validate(val_loader, quantizer.deployment_model, criterion, 0, quantizer, gpus)\n",
    "print(val_loss, val_prec1, val_prec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.0\n",
      "model.1.0\n",
      "model.1.3\n",
      "model.2.0\n",
      "model.2.3\n",
      "model.3.0\n",
      "model.3.3\n",
      "model.4.0\n",
      "model.4.3\n",
      "model.5.0\n",
      "model.5.3\n",
      "model.6.0\n",
      "model.6.3\n",
      "model.7.0\n",
      "model.7.3\n",
      "model.8.0\n",
      "model.8.3\n",
      "model.9.0\n",
      "model.9.3\n",
      "model.10.0\n",
      "model.10.3\n",
      "model.11.0\n",
      "model.11.3\n",
      "model.12.0\n",
      "model.12.3\n",
      "model.13.0\n",
      "model.13.3\n",
      "fc\n"
     ]
    }
   ],
   "source": [
    "def has_children(module):\n",
    "    try:\n",
    "        next(module.children())\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "\n",
    "for module_full_name, module in model.named_modules():\n",
    "    if has_children(module) is False:\n",
    "        if type(module) in [nn.Conv2d, nn.Linear]:\n",
    "            print(module_full_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNorm Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other module type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mobilenet_real(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d (3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d (32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Conv2d (32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Conv2d (64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d (64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d (128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "    (15): ReLU(inplace)\n",
       "    (16): Conv2d (128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (17): ReLU(inplace)\n",
       "    (18): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (19): ReLU(inplace)\n",
       "    (20): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (21): ReLU(inplace)\n",
       "    (22): Conv2d (256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "    (23): ReLU(inplace)\n",
       "    (24): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (31): ReLU(inplace)\n",
       "    (32): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (33): ReLU(inplace)\n",
       "    (34): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (35): ReLU(inplace)\n",
       "    (36): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (37): ReLU(inplace)\n",
       "    (38): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (39): ReLU(inplace)\n",
       "    (40): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (41): ReLU(inplace)\n",
       "    (42): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (43): ReLU(inplace)\n",
       "    (44): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (45): ReLU(inplace)\n",
       "    (46): Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "    (47): ReLU(inplace)\n",
       "    (48): Conv2d (512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (49): ReLU(inplace)\n",
       "    (50): Conv2d (1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "    (51): ReLU(inplace)\n",
       "    (52): Conv2d (1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (53): ReLU(inplace)\n",
       "    (54): AvgPool2d(kernel_size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=1000)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment_model = None\n",
    "deployment_modules = []\n",
    "conv_module = []\n",
    "batch_module = []\n",
    "act_module = []\n",
    "\n",
    "\n",
    "def batch_fold(conv_layer, batch_layer):\n",
    "    weight_tensor = conv_layer.weight.data\n",
    "    weight_tensor_size = weight_tensor.size()\n",
    "\n",
    "    eps = batch_layer.eps\n",
    "    gamma_tensor = batch_layer.weight.data \n",
    "    beta_tensor = batch_layer.bias.data \n",
    "    mu_tensor = batch_layer.running_mean\n",
    "    var_tensor = batch_layer.running_var\n",
    "    \n",
    "    if conv_layer.bias is None:\n",
    "        bias_tensor = - mu_tensor \n",
    "    else:\n",
    "        bias_tensor = conv_layer.bias.data - mu_tensor \n",
    "\n",
    "    #folded weight\n",
    "    for n_out_channel in range(weight_tensor_size[0]):\n",
    "        gamma = gamma_tensor[n_out_channel]\n",
    "        sigma = math.sqrt(var_tensor[n_out_channel] + eps)\n",
    "        beta = beta_tensor[n_out_channel]\n",
    "        \n",
    "        weight_per_channel = weight_tensor[n_out_channel]\n",
    "        weight_per_channel =(weight_per_channel*gamma)/sigma\n",
    "        weight_tensor[n_out_channel] =  weight_per_channel\n",
    "        \n",
    "        bias = bias_tensor[n_out_channel]\n",
    "        bias = ((bias*gamma)/sigma)+beta\n",
    "        bias_tensor[n_out_channel] = bias\n",
    "        \n",
    "    conv_layer.bias = nn.Parameter(bias_tensor)\n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "def find_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Sequential):\n",
    "            find_layers(child) \n",
    "        elif isinstance(child, nn.Conv2d):\n",
    "            conv_module.append(child)\n",
    "        elif isinstance(child, nn.BatchNorm2d):\n",
    "            batch_module.append(child)\n",
    "        elif isinstance(child, nn.ReLU):\n",
    "            act_module.append(child)\n",
    "        else:\n",
    "            print('Other module type')\n",
    "            deployment_modules.append(child)\n",
    "\n",
    "            \n",
    "        #print(child.__class__.__name__)\n",
    "        if len(conv_module) > 0 and len(batch_module) > 0 and len(act_module) > 0 :\n",
    "            conv_layer = copy.deepcopy(conv_module.pop())\n",
    "            batch_layer = copy.deepcopy(batch_module.pop() )\n",
    "            act_layer = copy.deepcopy(act_module.pop() )\n",
    "            conv_layer = batch_fold(conv_layer, batch_layer)\n",
    "            deployment_modules.append(conv_layer )\n",
    "            #deployment_modules.append(batch_layer )\n",
    "            deployment_modules.append( act_layer )\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "find_layers(model.model)\n",
    "deployment_model = copy.deepcopy(model)\n",
    "deployment_model.model = nn.Sequential(*deployment_modules)    \n",
    "deployment_model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][0/391]\tTime 9.409 (9.409)\tData 2.844 (2.844)\tLoss 0.6119 (0.6119)\tPrec@1 82.812 (82.812)\tPrec@5 96.094 (96.094)\n",
      "EVALUATING - Epoch: [0][10/391]\tTime 0.047 (0.900)\tData 0.000 (0.259)\tLoss 1.1026 (0.6558)\tPrec@1 72.656 (82.812)\tPrec@5 91.406 (95.455)\n",
      "EVALUATING - Epoch: [0][20/391]\tTime 0.050 (0.564)\tData 0.000 (0.206)\tLoss 0.9791 (0.9172)\tPrec@1 78.906 (76.228)\tPrec@5 92.188 (93.192)\n",
      "EVALUATING - Epoch: [0][30/391]\tTime 0.046 (0.455)\tData 0.000 (0.197)\tLoss 1.5945 (1.0715)\tPrec@1 73.438 (72.681)\tPrec@5 86.719 (92.011)\n",
      "EVALUATING - Epoch: [0][40/391]\tTime 0.049 (0.395)\tData 0.000 (0.188)\tLoss 0.9665 (0.9725)\tPrec@1 76.562 (75.248)\tPrec@5 92.969 (92.683)\n",
      "EVALUATING - Epoch: [0][50/391]\tTime 0.300 (0.395)\tData 0.243 (0.219)\tLoss 0.3795 (0.9922)\tPrec@1 85.938 (75.107)\tPrec@5 99.219 (92.341)\n",
      "EVALUATING - Epoch: [0][60/391]\tTime 0.180 (0.366)\tData 0.131 (0.211)\tLoss 1.0166 (0.9407)\tPrec@1 75.000 (76.422)\tPrec@5 91.406 (92.649)\n",
      "EVALUATING - Epoch: [0][70/391]\tTime 0.047 (0.346)\tData 0.000 (0.206)\tLoss 1.1106 (0.9732)\tPrec@1 64.844 (75.209)\tPrec@5 92.969 (92.628)\n",
      "EVALUATING - Epoch: [0][80/391]\tTime 0.060 (0.331)\tData 0.010 (0.198)\tLoss 1.0620 (0.9957)\tPrec@1 70.312 (74.277)\tPrec@5 92.188 (92.554)\n",
      "EVALUATING - Epoch: [0][90/391]\tTime 0.144 (0.330)\tData 0.093 (0.206)\tLoss 1.0373 (1.0032)\tPrec@1 65.625 (74.021)\tPrec@5 93.750 (92.591)\n",
      "EVALUATING - Epoch: [0][100/391]\tTime 0.048 (0.319)\tData 0.000 (0.203)\tLoss 0.8347 (0.9944)\tPrec@1 80.469 (73.948)\tPrec@5 93.750 (92.837)\n",
      "EVALUATING - Epoch: [0][110/391]\tTime 0.344 (0.312)\tData 0.297 (0.202)\tLoss 1.2768 (0.9970)\tPrec@1 58.594 (73.705)\tPrec@5 93.750 (92.934)\n",
      "EVALUATING - Epoch: [0][120/391]\tTime 0.048 (0.303)\tData 0.000 (0.199)\tLoss 0.8282 (0.9894)\tPrec@1 78.125 (74.115)\tPrec@5 94.531 (93.046)\n",
      "EVALUATING - Epoch: [0][130/391]\tTime 0.445 (0.306)\tData 0.395 (0.205)\tLoss 0.4969 (0.9820)\tPrec@1 88.281 (74.416)\tPrec@5 96.875 (93.112)\n",
      "EVALUATING - Epoch: [0][140/391]\tTime 0.261 (0.300)\tData 0.210 (0.203)\tLoss 1.1970 (0.9811)\tPrec@1 60.938 (74.330)\tPrec@5 94.531 (93.146)\n",
      "EVALUATING - Epoch: [0][150/391]\tTime 0.261 (0.298)\tData 0.209 (0.205)\tLoss 0.9869 (0.9895)\tPrec@1 61.719 (74.064)\tPrec@5 94.531 (93.031)\n",
      "EVALUATING - Epoch: [0][160/391]\tTime 0.808 (0.296)\tData 0.756 (0.205)\tLoss 1.1045 (1.0027)\tPrec@1 69.531 (73.942)\tPrec@5 89.062 (92.833)\n",
      "EVALUATING - Epoch: [0][170/391]\tTime 0.047 (0.292)\tData 0.000 (0.204)\tLoss 1.8531 (1.0406)\tPrec@1 58.594 (73.195)\tPrec@5 85.156 (92.329)\n",
      "EVALUATING - Epoch: [0][180/391]\tTime 0.152 (0.288)\tData 0.103 (0.202)\tLoss 2.4478 (1.0762)\tPrec@1 47.656 (72.514)\tPrec@5 75.781 (91.790)\n",
      "EVALUATING - Epoch: [0][190/391]\tTime 0.086 (0.286)\tData 0.033 (0.202)\tLoss 2.2388 (1.1103)\tPrec@1 49.219 (71.850)\tPrec@5 80.469 (91.320)\n",
      "EVALUATING - Epoch: [0][200/391]\tTime 0.050 (0.286)\tData 0.001 (0.203)\tLoss 1.3808 (1.1454)\tPrec@1 57.812 (71.140)\tPrec@5 88.281 (90.827)\n",
      "EVALUATING - Epoch: [0][210/391]\tTime 0.050 (0.284)\tData 0.000 (0.203)\tLoss 1.4181 (1.1672)\tPrec@1 67.188 (70.812)\tPrec@5 87.500 (90.510)\n",
      "EVALUATING - Epoch: [0][220/391]\tTime 0.052 (0.286)\tData 0.000 (0.206)\tLoss 0.9056 (1.1795)\tPrec@1 80.469 (70.687)\tPrec@5 89.062 (90.229)\n",
      "EVALUATING - Epoch: [0][230/391]\tTime 0.047 (0.283)\tData 0.000 (0.204)\tLoss 2.1866 (1.1949)\tPrec@1 50.000 (70.417)\tPrec@5 75.000 (90.006)\n",
      "EVALUATING - Epoch: [0][240/391]\tTime 0.197 (0.281)\tData 0.151 (0.204)\tLoss 1.4121 (1.2047)\tPrec@1 67.188 (70.348)\tPrec@5 87.500 (89.818)\n",
      "EVALUATING - Epoch: [0][250/391]\tTime 0.469 (0.280)\tData 0.418 (0.204)\tLoss 1.1659 (1.2288)\tPrec@1 72.656 (69.814)\tPrec@5 86.719 (89.486)\n",
      "EVALUATING - Epoch: [0][260/391]\tTime 0.594 (0.279)\tData 0.544 (0.204)\tLoss 1.6096 (1.2474)\tPrec@1 60.938 (69.453)\tPrec@5 87.500 (89.278)\n",
      "EVALUATING - Epoch: [0][270/391]\tTime 0.166 (0.278)\tData 0.115 (0.204)\tLoss 2.1620 (1.2598)\tPrec@1 50.000 (69.194)\tPrec@5 76.562 (89.097)\n",
      "EVALUATING - Epoch: [0][280/391]\tTime 0.048 (0.276)\tData 0.000 (0.203)\tLoss 1.4343 (1.2680)\tPrec@1 61.719 (69.073)\tPrec@5 89.062 (89.012)\n",
      "EVALUATING - Epoch: [0][290/391]\tTime 0.049 (0.273)\tData 0.000 (0.201)\tLoss 2.0138 (1.2839)\tPrec@1 43.750 (68.798)\tPrec@5 83.594 (88.754)\n",
      "EVALUATING - Epoch: [0][300/391]\tTime 0.605 (0.273)\tData 0.553 (0.202)\tLoss 1.3191 (1.2958)\tPrec@1 71.875 (68.620)\tPrec@5 85.938 (88.543)\n",
      "EVALUATING - Epoch: [0][310/391]\tTime 0.048 (0.272)\tData 0.000 (0.202)\tLoss 1.6978 (1.3065)\tPrec@1 64.844 (68.474)\tPrec@5 79.688 (88.374)\n",
      "EVALUATING - Epoch: [0][320/391]\tTime 0.194 (0.272)\tData 0.139 (0.202)\tLoss 0.8506 (1.3148)\tPrec@1 81.250 (68.322)\tPrec@5 92.188 (88.264)\n",
      "EVALUATING - Epoch: [0][330/391]\tTime 0.050 (0.270)\tData 0.000 (0.201)\tLoss 2.0751 (1.3347)\tPrec@1 49.219 (67.917)\tPrec@5 77.344 (87.981)\n",
      "EVALUATING - Epoch: [0][340/391]\tTime 0.050 (0.270)\tData 0.000 (0.201)\tLoss 1.2242 (1.3409)\tPrec@1 63.281 (67.753)\tPrec@5 90.625 (87.901)\n",
      "EVALUATING - Epoch: [0][350/391]\tTime 0.050 (0.269)\tData 0.000 (0.200)\tLoss 1.2724 (1.3474)\tPrec@1 72.656 (67.642)\tPrec@5 88.281 (87.812)\n",
      "EVALUATING - Epoch: [0][360/391]\tTime 0.048 (0.268)\tData 0.000 (0.200)\tLoss 1.6824 (1.3569)\tPrec@1 57.031 (67.458)\tPrec@5 86.719 (87.719)\n",
      "EVALUATING - Epoch: [0][370/391]\tTime 0.260 (0.268)\tData 0.212 (0.200)\tLoss 1.1868 (1.3508)\tPrec@1 63.281 (67.571)\tPrec@5 92.188 (87.807)\n",
      "EVALUATING - Epoch: [0][380/391]\tTime 0.372 (0.269)\tData 0.319 (0.202)\tLoss 1.0566 (1.3527)\tPrec@1 71.875 (67.538)\tPrec@5 93.750 (87.787)\n",
      "EVALUATING - Epoch: [0][390/391]\tTime 0.390 (0.268)\tData 0.000 (0.201)\tLoss 2.6144 (1.3458)\tPrec@1 37.500 (67.652)\tPrec@5 72.500 (87.878)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3458160171508788 67.652 87.878\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_prec1, val_prec5 = validate(val_loader, deployment_model.cuda(), criterion, 0, None, gpus)\n",
    "print(val_loss, val_prec1, val_prec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Training Quantization of weights as\n",
    "https://github.com/ARM-software/ML-KWS-for-MCU/blob/master/quant_test.py\n",
    "\n",
    "https://github.com/ARM-software/ML-KWS-for-MCU/blob/master/quant_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_point_linear_quant(min_value, max_value, n_bits, signed=True):\n",
    "    range_values = max_value- min_value\n",
    "\n",
    "    max_range = max(abs(min_value),abs(max_value))\n",
    "    int_bits = int(np.ceil(np.log2(max_range)))\n",
    "    \n",
    "    if int_bits < 0:\n",
    "        int_bits = 0\n",
    "    elif int_bits>(n_bits-1):\n",
    "        int_bits = (n_bits-1)\n",
    "        \n",
    "    if signed:\n",
    "        frac_bits = n_bits-1-int_bits #remaining bits are fractional bits (1-bit for sign)\n",
    "    else:\n",
    "        frac_bits = n_bits-int_bits\n",
    "        \n",
    "    return int_bits, frac_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558258593082428 -0.8026384711265564 1.3608970642089844 0 7\n",
      "4.027159690856934 -4.618956565856934 8.646116256713867 3 4\n",
      "1.2754887342453003 -1.4489389657974243 2.7244277000427246 1 6\n",
      "5.171591758728027 -3.461690664291382 8.63328242301941 3 4\n",
      "0.8815251588821411 -0.831538736820221 1.713063895702362 0 7\n",
      "4.645492076873779 -4.746695041656494 9.392187118530273 3 4\n",
      "0.9018105864524841 -0.8606885075569153 1.7624990940093994 0 7\n",
      "2.1124305725097656 -2.2831485271453857 4.395579099655151 2 5\n",
      "0.571179986000061 -0.4470636546611786 1.0182436406612396 0 7\n",
      "6.458991527557373 -5.2962141036987305 11.755205631256104 3 4\n",
      "0.5835903286933899 -0.5659104585647583 1.1495007872581482 0 7\n",
      "22.159364700317383 -2.932857036590576 25.09222173690796 5 2\n",
      "0.3646238148212433 -0.40404146909713745 0.7686652839183807 0 7\n",
      "19.831918716430664 -6.3695831298828125 26.201501846313477 5 2\n",
      "0.45581403374671936 -0.3711283206939697 0.8269423544406891 0 7\n",
      "9.509058952331543 -5.71013879776001 15.219197750091553 4 3\n",
      "0.401159405708313 -0.3822166621685028 0.7833760678768158 0 7\n",
      "4.265459060668945 -5.392614364624023 9.658073425292969 3 4\n",
      "0.4583653509616852 -0.3846471905708313 0.8430125415325165 0 7\n",
      "5.717052936553955 -4.4894795417785645 10.20653247833252 3 4\n",
      "0.4761676490306854 -0.4442507326602936 0.920418381690979 0 7\n",
      "2.4391751289367676 -3.9615747928619385 6.400749921798706 2 5\n",
      "0.37422895431518555 -0.4431194067001343 0.8173483610153198 0 7\n",
      "2.249077081680298 -2.4493234157562256 4.698400497436523 2 5\n",
      "0.500937819480896 -0.4916362166404724 0.9925740361213684 0 7\n",
      "12.539684295654297 -14.799212455749512 27.33889675140381 4 3\n",
      "8.922979354858398 -7.055124282836914 15.978103637695312 4 3\n",
      "0.9389712810516357 -0.3607997000217438 1.2997709810733795 0 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.558258593082428, -0.8026384711265564, 1.3608970642089844, 0, 7],\n",
       " [4.027159690856934, -4.618956565856934, 8.646116256713867, 3, 4],\n",
       " [1.2754887342453003, -1.4489389657974243, 2.7244277000427246, 1, 6],\n",
       " [5.171591758728027, -3.461690664291382, 8.63328242301941, 3, 4],\n",
       " [0.8815251588821411, -0.831538736820221, 1.713063895702362, 0, 7],\n",
       " [4.645492076873779, -4.746695041656494, 9.392187118530273, 3, 4],\n",
       " [0.9018105864524841, -0.8606885075569153, 1.7624990940093994, 0, 7],\n",
       " [2.1124305725097656, -2.2831485271453857, 4.395579099655151, 2, 5],\n",
       " [0.571179986000061, -0.4470636546611786, 1.0182436406612396, 0, 7],\n",
       " [6.458991527557373, -5.2962141036987305, 11.755205631256104, 3, 4],\n",
       " [0.5835903286933899, -0.5659104585647583, 1.1495007872581482, 0, 7],\n",
       " [22.159364700317383, -2.932857036590576, 25.09222173690796, 5, 2],\n",
       " [0.3646238148212433, -0.40404146909713745, 0.7686652839183807, 0, 7],\n",
       " [19.831918716430664, -6.3695831298828125, 26.201501846313477, 5, 2],\n",
       " [0.45581403374671936, -0.3711283206939697, 0.8269423544406891, 0, 7],\n",
       " [9.509058952331543, -5.71013879776001, 15.219197750091553, 4, 3],\n",
       " [0.401159405708313, -0.3822166621685028, 0.7833760678768158, 0, 7],\n",
       " [4.265459060668945, -5.392614364624023, 9.658073425292969, 3, 4],\n",
       " [0.4583653509616852, -0.3846471905708313, 0.8430125415325165, 0, 7],\n",
       " [5.717052936553955, -4.4894795417785645, 10.20653247833252, 3, 4],\n",
       " [0.4761676490306854, -0.4442507326602936, 0.920418381690979, 0, 7],\n",
       " [2.4391751289367676, -3.9615747928619385, 6.400749921798706, 2, 5],\n",
       " [0.37422895431518555, -0.4431194067001343, 0.8173483610153198, 0, 7],\n",
       " [2.249077081680298, -2.4493234157562256, 4.698400497436523, 2, 5],\n",
       " [0.500937819480896, -0.4916362166404724, 0.9925740361213684, 0, 7],\n",
       " [12.539684295654297, -14.799212455749512, 27.33889675140381, 4, 3],\n",
       " [8.922979354858398, -7.055124282836914, 15.978103637695312, 4, 3],\n",
       " [0.9389712810516357, -0.3607997000217438, 1.2997709810733795, 0, 7]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_conv2d = []\n",
    "N_BITS = 8\n",
    "def f_quant_weight(model, symmetric_range=True):\n",
    "    if symmetric_range is not True: # asymmetric to be implemented\n",
    "        print('Error')\n",
    "        return -1\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Sequential):\n",
    "            f_quant_weight(child, symmetric_range) \n",
    "        elif isinstance(child, nn.Conv2d) or isinstance(child, nn.Linear):\n",
    "            max_weights = child.weight.data.max()\n",
    "            min_weights = child.weight.data.min()\n",
    "            range_weights = max_weights- min_weights\n",
    "            int_bits, frac_bits = fixed_point_linear_quant(min_weights, max_weights, N_BITS, signed=True)\n",
    "            \n",
    "#            max_range = max(abs(min_weights),abs(max_weights))\n",
    "#            int_bits = int(np.ceil(np.log2(max_range)))\n",
    "#            if int_bits<0:\n",
    "#                int_bits = 0\n",
    "#            elif int_bits>(N_BITS-1):\n",
    "#                int_bits = (N_BITS-1)\n",
    "#            frac_bits = N_BITS-1-int_bits #remaining bits are fractional bits (1-bit for sign)           \n",
    "            \n",
    "            stats_conv2d.append([max_weights, min_weights, range_weights, int_bits, frac_bits])\n",
    "            print(max_weights, min_weights, range_weights, int_bits, frac_bits)\n",
    "            if int_bits > 0:\n",
    "                max_value = (2**(int_bits-1))+1-2**(-frac_bits)\n",
    "            else:\n",
    "                max_value = 1-2**(-frac_bits)\n",
    "            #print(int_bits, -2**int_bits, (2**int_bits)-1, max_value, 2**(-frac_bits))\n",
    "            clipped_weight = child.weight.data.clamp( -max_value, max_value)\n",
    "            quant_weight = clipped_weight.mul(2**frac_bits).round().div(2**frac_bits)\n",
    "            #print(quant_weight)\n",
    "            child.weight.data = quant_weight\n",
    "            \n",
    "            \n",
    "f_quant_weight(deployment_model, symmetric_range=True)\n",
    "stats_conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Training Quantization of activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.1 ReLU(inplace)\n",
      "model.3 ReLU(inplace)\n",
      "model.5 ReLU(inplace)\n",
      "model.7 ReLU(inplace)\n",
      "model.9 ReLU(inplace)\n",
      "model.11 ReLU(inplace)\n",
      "model.13 ReLU(inplace)\n",
      "model.15 ReLU(inplace)\n",
      "model.17 ReLU(inplace)\n",
      "model.19 ReLU(inplace)\n",
      "model.21 ReLU(inplace)\n",
      "model.23 ReLU(inplace)\n",
      "model.25 ReLU(inplace)\n",
      "model.27 ReLU(inplace)\n",
      "model.29 ReLU(inplace)\n",
      "model.31 ReLU(inplace)\n",
      "model.33 ReLU(inplace)\n",
      "model.35 ReLU(inplace)\n",
      "model.37 ReLU(inplace)\n",
      "model.39 ReLU(inplace)\n",
      "model.41 ReLU(inplace)\n",
      "model.43 ReLU(inplace)\n",
      "model.45 ReLU(inplace)\n",
      "model.47 ReLU(inplace)\n",
      "model.49 ReLU(inplace)\n",
      "model.51 ReLU(inplace)\n",
      "model.53 ReLU(inplace)\n",
      "5.781166076660156 0.0 0.3834649622440338 0.4645973742008209\n",
      "11.030117988586426 0.0 0.4068128168582916 0.5873093605041504\n",
      "15.173482894897461 0.0 0.29163435101509094 0.34446993470191956\n",
      "9.82027530670166 0.0 0.38282033801078796 0.4403855800628662\n",
      "7.053549289703369 0.0 0.24418221414089203 0.2565935552120209\n",
      "8.497363090515137 0.0 0.31330856680870056 0.41559818387031555\n",
      "6.900217056274414 0.0 0.16909408569335938 0.25600773096084595\n",
      "5.829596996307373 0.0 0.36365532875061035 0.40665629506111145\n",
      "4.300732612609863 0.0 0.23592238128185272 0.22172459959983826\n",
      "6.044707775115967 0.0 0.2059078961610794 0.3030555248260498\n",
      "5.0792388916015625 0.0 0.12782810628414154 0.1951996237039566\n",
      "5.930695533752441 0.0 0.25347980856895447 0.33783596754074097\n",
      "2.746802568435669 0.0 0.17686402797698975 0.19882994890213013\n",
      "5.156436920166016 0.0 0.16627545654773712 0.24499720335006714\n",
      "2.577873468399048 0.0 0.11860686540603638 0.16618749499320984\n",
      "5.190664768218994 0.0 0.1889183670282364 0.2715107798576355\n",
      "2.6644299030303955 0.0 0.09938447177410126 0.1590992659330368\n",
      "6.229292869567871 0.0 0.1911730319261551 0.2737148404121399\n",
      "3.028366804122925 0.0 0.09505829215049744 0.16001670062541962\n",
      "9.23070240020752 0.0 0.21005034446716309 0.285238116979599\n",
      "3.0311944484710693 0.0 0.0819159746170044 0.15400920808315277\n",
      "8.21544361114502 0.0 0.21989963948726654 0.2998955249786377\n",
      "3.0023880004882812 0.0 0.08230133354663849 0.14688849449157715\n",
      "5.690117359161377 0.0 0.24297726154327393 0.317352831363678\n",
      "2.495088577270508 0.0 0.03746156021952629 0.10500804334878922\n",
      "5.377908229827881 0.0 0.19883793592453003 0.26117825508117676\n",
      "24.16253662109375 0.0 0.5525522828102112 1.0916551351547241\n",
      "6.664950370788574 0.0 0.3872512876987457 0.4662327170372009\n",
      "10.988555908203125 0.0 0.4079631567001343 0.5896613597869873\n",
      "16.35980987548828 0.0 0.29030683636665344 0.3429514169692993\n",
      "11.518256187438965 0.0 0.3817547857761383 0.4368402361869812\n",
      "7.814406871795654 0.0 0.24423319101333618 0.25419312715530396\n",
      "10.22216796875 0.0 0.31167978048324585 0.4136788845062256\n",
      "9.11392879486084 0.0 0.16812920570373535 0.2537979483604431\n",
      "6.719604015350342 0.0 0.3632097542285919 0.4052547514438629\n",
      "4.476060390472412 0.0 0.23494665324687958 0.22002099454402924\n",
      "6.0306315422058105 0.0 0.20608185231685638 0.3025068938732147\n",
      "4.231258392333984 0.0 0.1275200992822647 0.19453935325145721\n",
      "6.0801310539245605 0.0 0.2530222535133362 0.33593520522117615\n",
      "2.443739891052246 0.0 0.17674794793128967 0.1979517638683319\n",
      "4.307379245758057 0.0 0.1660032868385315 0.24437858164310455\n",
      "3.3671813011169434 0.0 0.11883580684661865 0.16617974638938904\n",
      "6.2998504638671875 0.0 0.18906453251838684 0.27161580324172974\n",
      "3.031012535095215 0.0 0.09959936141967773 0.15934967994689941\n",
      "8.671957969665527 0.0 0.1915571093559265 0.274165540933609\n",
      "3.3287761211395264 0.0 0.09549186378717422 0.16049756109714508\n",
      "8.809027671813965 0.0 0.2100968211889267 0.28544867038726807\n",
      "3.3182079792022705 0.0 0.08226322382688522 0.15440939366817474\n",
      "6.460849285125732 0.0 0.22035157680511475 0.30053311586380005\n",
      "2.7975456714630127 0.0 0.08244195580482483 0.14737065136432648\n",
      "6.1071696281433105 0.0 0.24390345811843872 0.3191078007221222\n",
      "2.2566850185394287 0.0 0.037500228732824326 0.10552773624658585\n",
      "4.138452529907227 0.0 0.19875772297382355 0.26104578375816345\n",
      "31.04730224609375 0.0 0.5449451208114624 1.077134370803833\n",
      "5.767683506011963 0.0 0.38724377751350403 0.46766579151153564\n",
      "10.204055786132812 0.0 0.4079686403274536 0.5865800976753235\n",
      "15.62868881225586 0.0 0.2880960702896118 0.3386620879173279\n",
      "10.739806175231934 0.0 0.38048693537712097 0.4351388216018677\n",
      "6.5554046630859375 0.0 0.24347269535064697 0.25357621908187866\n",
      "8.813451766967773 0.0 0.3105151653289795 0.41203972697257996\n",
      "7.263717174530029 0.0 0.16673660278320312 0.25254738330841064\n",
      "6.283617973327637 0.0 0.36254173517227173 0.4054209291934967\n",
      "4.251223564147949 0.0 0.23401430249214172 0.2201586365699768\n",
      "5.846908092498779 0.0 0.20581437647342682 0.303046315908432\n",
      "4.30446195602417 0.0 0.12711821496486664 0.19467449188232422\n",
      "6.6130828857421875 0.0 0.2527199685573578 0.3365621864795685\n",
      "3.2688236236572266 0.0 0.17593637108802795 0.19782163202762604\n",
      "4.782138347625732 0.0 0.16580620408058167 0.24443469941616058\n",
      "2.7686214447021484 0.0 0.11852842569351196 0.1658501774072647\n",
      "6.857495307922363 0.0 0.18915903568267822 0.2719874978065491\n",
      "2.988759994506836 0.0 0.09957278519868851 0.1596129685640335\n",
      "9.359879493713379 0.0 0.19146034121513367 0.2744625210762024\n",
      "3.228980302810669 0.0 0.0953095331788063 0.1607336401939392\n",
      "10.603116035461426 0.0 0.21037551760673523 0.28620612621307373\n",
      "3.723951578140259 0.0 0.08191580325365067 0.1547054797410965\n",
      "7.182102203369141 0.0 0.22057069838047028 0.30119332671165466\n",
      "3.1340975761413574 0.0 0.08221153169870377 0.14758746325969696\n",
      "7.306548118591309 0.0 0.24456804990768433 0.31971198320388794\n",
      "2.1038150787353516 0.0 0.037643976509571075 0.1064058244228363\n",
      "7.007523536682129 0.0 0.19876748323440552 0.2614665627479553\n",
      "29.268627166748047 0.0 0.5484708547592163 1.0891704559326172\n",
      "5.969430446624756 0.0 0.3853878378868103 0.46774816513061523\n",
      "10.100621223449707 0.0 0.40873637795448303 0.5886038541793823\n",
      "15.736034393310547 0.0 0.28963354229927063 0.3423806130886078\n",
      "10.070354461669922 0.0 0.38071736693382263 0.4358086585998535\n",
      "6.405888080596924 0.0 0.24355089664459229 0.2547966241836548\n",
      "9.56824779510498 0.0 0.31269964575767517 0.4142134189605713\n",
      "5.946002960205078 0.0 0.16803981363773346 0.2537473738193512\n",
      "5.691372871398926 0.0 0.3611658811569214 0.402597576379776\n",
      "4.861944198608398 0.0 0.23492030799388885 0.2192559391260147\n",
      "5.838006973266602 0.0 0.204440638422966 0.30015093088150024\n",
      "4.067049026489258 0.0 0.12686938047409058 0.1933504194021225\n",
      "5.871992588043213 0.0 0.25232771039009094 0.3348012864589691\n",
      "2.654642105102539 0.0 0.1762882024049759 0.19789248704910278\n",
      "4.488038539886475 0.0 0.1650201976299286 0.24246379733085632\n",
      "3.945125102996826 0.0 0.11806107312440872 0.16488268971443176\n",
      "6.540256977081299 0.0 0.18830972909927368 0.26961803436279297\n",
      "2.728891611099243 0.0 0.09893817454576492 0.15833094716072083\n",
      "8.544553756713867 0.0 0.19086843729019165 0.2726295292377472\n",
      "3.9182915687561035 0.0 0.09499049931764603 0.15995067358016968\n",
      "9.709424018859863 0.0 0.21043376624584198 0.2853914201259613\n",
      "3.2372515201568604 0.0 0.08146795630455017 0.15375357866287231\n",
      "7.010584831237793 0.0 0.22057704627513885 0.3008340895175934\n",
      "2.7842295169830322 0.0 0.081698477268219 0.1469714343547821\n",
      "6.114715099334717 0.0 0.24381721019744873 0.318882018327713\n",
      "2.728534460067749 0.0 0.037461057305336 0.10566697269678116\n",
      "4.378849983215332 0.0 0.19856387376785278 0.26135730743408203\n",
      "25.820993423461914 0.0 0.5418849587440491 1.076187014579773\n",
      "5.7389349937438965 0.0 0.38931119441986084 0.4707522988319397\n",
      "11.5618314743042 0.0 0.4057389199733734 0.5872696042060852\n",
      "16.483959197998047 0.0 0.2896914482116699 0.3430219292640686\n",
      "9.122922897338867 0.0 0.38277867436408997 0.43825194239616394\n",
      "6.949329853057861 0.0 0.24493840336799622 0.25608962774276733\n",
      "8.746262550354004 0.0 0.31168118119239807 0.41390132904052734\n",
      "6.758269786834717 0.0 0.16735686361789703 0.25363337993621826\n",
      "5.6861138343811035 0.0 0.36337682604789734 0.40586450695991516\n",
      "3.9971959590911865 0.0 0.23508593440055847 0.2206311672925949\n",
      "5.638060092926025 0.0 0.20561550557613373 0.3022677004337311\n",
      "4.120280742645264 0.0 0.12708967924118042 0.19449345767498016\n",
      "5.966485977172852 0.0 0.2541050314903259 0.3378106355667114\n",
      "2.514887809753418 0.0 0.17635010182857513 0.1987503618001938\n",
      "4.84092903137207 0.0 0.1660451740026474 0.24406737089157104\n",
      "2.702420949935913 0.0 0.11838028579950333 0.1656891405582428\n",
      "5.249781608581543 0.0 0.1886080652475357 0.2705487012863159\n",
      "2.4960405826568604 0.0 0.09880774468183517 0.1583433896303177\n",
      "7.493415355682373 0.0 0.1908179670572281 0.27272945642471313\n",
      "3.2901570796966553 0.0 0.09478703141212463 0.15926629304885864\n",
      "10.087891578674316 0.0 0.21032480895519257 0.28544890880584717\n",
      "3.4092931747436523 0.0 0.08149620145559311 0.15359991788864136\n",
      "7.027744770050049 0.0 0.22086268663406372 0.30092427134513855\n",
      "2.683464288711548 0.0 0.0816037729382515 0.14693336188793182\n",
      "6.2671613693237305 0.0 0.24425023794174194 0.31950780749320984\n",
      "2.4426631927490234 0.0 0.037558719515800476 0.10625260323286057\n",
      "4.369529724121094 0.0 0.19925929605960846 0.2618064880371094\n",
      "33.942787170410156 0.0 0.5526525378227234 1.097456932067871\n",
      "6.265329360961914 0.0 0.38849449157714844 0.47099149227142334\n",
      "13.286332130432129 0.0 0.4057871699333191 0.5854570865631104\n",
      "15.5068359375 0.0 0.28933316469192505 0.33952683210372925\n",
      "10.981025695800781 0.0 0.3819543421268463 0.4368765652179718\n",
      "7.011906147003174 0.0 0.24467183649539948 0.25533583760261536\n",
      "9.05483627319336 0.0 0.31126174330711365 0.41254761815071106\n",
      "7.086294651031494 0.0 0.16814205050468445 0.2536892890930176\n",
      "7.003238201141357 0.0 0.36242273449897766 0.40484511852264404\n",
      "4.015329837799072 0.0 0.2347562611103058 0.22057360410690308\n",
      "5.846207618713379 0.0 0.2059193253517151 0.3024962544441223\n",
      "3.634432792663574 0.0 0.1273060292005539 0.194541797041893\n",
      "7.402890205383301 0.0 0.25346988439559937 0.3365992605686188\n",
      "2.5735719203948975 0.0 0.17581391334533691 0.19792959094047546\n",
      "5.480377674102783 0.0 0.16607365012168884 0.24425289034843445\n",
      "4.021682262420654 0.0 0.11848624795675278 0.16565629839897156\n",
      "6.608579158782959 0.0 0.18902117013931274 0.27147623896598816\n",
      "2.855058431625366 0.0 0.09935259073972702 0.15912334620952606\n",
      "7.009454250335693 0.0 0.1912030577659607 0.2737687826156616\n",
      "2.9869165420532227 0.0 0.0953017920255661 0.16059397161006927\n",
      "11.047274589538574 0.0 0.21059156954288483 0.2868184745311737\n",
      "3.5444271564483643 0.0 0.08188551664352417 0.15471000969409943\n",
      "7.714627742767334 0.0 0.22077952325344086 0.3013904094696045\n",
      "5.036674499511719 0.0 0.08229091018438339 0.14796766638755798\n",
      "6.056582450866699 0.0 0.2442735731601715 0.3198987543582916\n",
      "2.178928852081299 0.0 0.03823716193437576 0.10761307924985886\n",
      "3.861360549926758 0.0 0.19887512922286987 0.2615945041179657\n",
      "25.35779571533203 0.0 0.5559198260307312 1.10194993019104\n",
      "5.909363746643066 0.0 0.38511136174201965 0.46720510721206665\n",
      "9.986863136291504 0.0 0.41103655099868774 0.5956870317459106\n",
      "14.219680786132812 0.0 0.2910098135471344 0.3457166254520416\n",
      "9.084871292114258 0.0 0.38353943824768066 0.44226545095443726\n",
      "6.149734973907471 0.0 0.24452978372573853 0.25634828209877014\n",
      "8.0183744430542 0.0 0.31228107213974 0.4153136610984802\n",
      "6.236149787902832 0.0 0.16891181468963623 0.2558910846710205\n",
      "5.157678604125977 0.0 0.36269229650497437 0.4061799645423889\n",
      "3.896655321121216 0.0 0.2347375899553299 0.2209833562374115\n",
      "5.807025909423828 0.0 0.20568059384822845 0.30356329679489136\n",
      "4.648247241973877 0.0 0.1277662068605423 0.1951795071363449\n",
      "6.688112735748291 0.0 0.2529548704624176 0.3365786075592041\n",
      "3.044114112854004 0.0 0.17582735419273376 0.19758908450603485\n",
      "5.706474304199219 0.0 0.1662878841161728 0.2451874315738678\n",
      "2.9484164714813232 0.0 0.11877602338790894 0.16623805463314056\n",
      "5.21544075012207 0.0 0.18956446647644043 0.2723267376422882\n",
      "3.2019569873809814 0.0 0.09948976337909698 0.15952633321285248\n",
      "5.80686616897583 0.0 0.1913485825061798 0.27431702613830566\n",
      "4.049004077911377 0.0 0.09562313556671143 0.16108274459838867\n",
      "7.922708034515381 0.0 0.21048852801322937 0.2866356670856476\n",
      "3.8861351013183594 0.0 0.08186722546815872 0.15460219979286194\n",
      "8.371297836303711 0.0 0.2213996797800064 0.301884263753891\n",
      "4.070579528808594 0.0 0.08236993849277496 0.14801351726055145\n",
      "6.0577497482299805 0.0 0.24404741823673248 0.31924161314964294\n",
      "2.967348098754883 0.0 0.03833664208650589 0.1072695404291153\n",
      "6.595295429229736 0.0 0.19844798743724823 0.26100262999534607\n",
      "25.294723510742188 0.0 0.5586786270141602 1.1038320064544678\n",
      "5.844448566436768 0.0 0.38782280683517456 0.4676472842693329\n",
      "10.842126846313477 0.0 0.4053933322429657 0.5890145897865295\n",
      "15.13398551940918 0.0 0.2910056412220001 0.3447411358356476\n",
      "9.586301803588867 0.0 0.3840760290622711 0.4424612820148468\n",
      "6.090952396392822 0.0 0.24587245285511017 0.2579963803291321\n",
      "7.612849235534668 0.0 0.31376948952674866 0.4178102910518646\n",
      "6.108648300170898 0.0 0.1708398163318634 0.25804853439331055\n",
      "5.3065338134765625 0.0 0.36518990993499756 0.40755191445350647\n",
      "3.794229030609131 0.0 0.23693619668483734 0.22251306474208832\n",
      "5.042410850524902 0.0 0.20725716650485992 0.3051253855228424\n",
      "4.707599639892578 0.0 0.1294940710067749 0.19677385687828064\n",
      "6.290925979614258 0.0 0.2539892792701721 0.3371850848197937\n",
      "2.5567727088928223 0.0 0.1778959035873413 0.19902528822422028\n",
      "4.233060359954834 0.0 0.16710779070854187 0.24634882807731628\n",
      "3.4775238037109375 0.0 0.1196117103099823 0.16729220747947693\n",
      "6.907040119171143 0.0 0.18967729806900024 0.2728407382965088\n",
      "2.871995210647583 0.0 0.10007896274328232 0.160182923078537\n",
      "7.614438056945801 0.0 0.1913563311100006 0.2749495208263397\n",
      "3.5761263370513916 0.0 0.09597920626401901 0.16122552752494812\n",
      "8.064541816711426 0.0 0.2103610783815384 0.2868732213973999\n",
      "3.0820233821868896 0.0 0.0824316143989563 0.1551540046930313\n",
      "6.225557327270508 0.0 0.22038784623146057 0.30094924569129944\n",
      "2.542156934738159 0.0 0.08218783140182495 0.14731299877166748\n",
      "5.841694355010986 0.0 0.24311351776123047 0.3176997900009155\n",
      "2.1115713119506836 0.0 0.037893228232860565 0.1061650738120079\n",
      "3.9283928871154785 0.0 0.19840826094150543 0.2609136700630188\n",
      "33.53582000732422 0.0 0.5502418279647827 1.0851693153381348\n",
      "5.661237716674805 0.0 0.38766470551490784 0.46748095750808716\n",
      "11.83582878112793 0.0 0.40471288561820984 0.5828987956047058\n",
      "18.945629119873047 0.0 0.2876698076725006 0.33761027455329895\n",
      "11.47564697265625 0.0 0.3793642818927765 0.43342089653015137\n",
      "7.71937370300293 0.0 0.2436412274837494 0.2539370059967041\n",
      "9.750901222229004 0.0 0.31142452359199524 0.41369080543518066\n",
      "6.5113067626953125 0.0 0.16786344349384308 0.25310781598091125\n",
      "5.4035964012146 0.0 0.3627488613128662 0.403978168964386\n",
      "4.314685344696045 0.0 0.2343718260526657 0.22016407549381256\n",
      "6.167637348175049 0.0 0.20551256835460663 0.3012770116329193\n",
      "4.46516752243042 0.0 0.1277286261320114 0.19425225257873535\n",
      "6.783105850219727 0.0 0.2520386576652527 0.33454737067222595\n",
      "2.6339664459228516 0.0 0.17695751786231995 0.19807764887809753\n",
      "6.2744317054748535 0.0 0.16583004593849182 0.24447733163833618\n",
      "3.18927264213562 0.0 0.1190718337893486 0.1661561131477356\n",
      "4.920074939727783 0.0 0.1890660524368286 0.2712991237640381\n",
      "3.1828625202178955 0.0 0.09982894361019135 0.15934446454048157\n",
      "6.603978633880615 0.0 0.190804123878479 0.2733577489852905\n",
      "3.2678122520446777 0.0 0.0954243540763855 0.16013915836811066\n",
      "7.181917190551758 0.0 0.21004055440425873 0.2856161892414093\n",
      "3.390014410018921 0.0 0.08220672607421875 0.15428784489631653\n",
      "7.203535556793213 0.0 0.21989592909812927 0.2999536693096161\n",
      "2.8746843338012695 0.0 0.08216226100921631 0.1467757374048233\n",
      "6.375483989715576 0.0 0.2433827519416809 0.3176104426383972\n",
      "2.3035173416137695 0.0 0.03762286901473999 0.10550283640623093\n",
      "5.104689121246338 0.0 0.19909590482711792 0.26091015338897705\n",
      "38.55585861206055 0.0 0.548650324344635 1.0798920392990112\n",
      "5.623178958892822 0.0 0.3876851499080658 0.4708155691623688\n",
      "11.642446517944336 0.0 0.40537595748901367 0.5885428190231323\n",
      "16.603351593017578 0.0 0.2918592691421509 0.34699547290802\n",
      "10.775076866149902 0.0 0.38571760058403015 0.4464004337787628\n",
      "7.286225318908691 0.0 0.24639467895030975 0.2599391043186188\n",
      "8.626455307006836 0.0 0.3143174648284912 0.41913512349128723\n",
      "7.169100284576416 0.0 0.16993610560894012 0.2582634687423706\n",
      "7.056463241577148 0.0 0.36465081572532654 0.40853261947631836\n",
      "4.7752275466918945 0.0 0.23735776543617249 0.2227226048707962\n",
      "6.119517803192139 0.0 0.20717722177505493 0.30522724986076355\n",
      "4.928838729858398 0.0 0.12840601801872253 0.19644229114055634\n",
      "6.761790752410889 0.0 0.25502175092697144 0.34057721495628357\n",
      "2.741333484649658 0.0 0.17732666432857513 0.20103879272937775\n",
      "5.1860222816467285 0.0 0.16713808476924896 0.24702005088329315\n",
      "3.5990800857543945 0.0 0.11914175748825073 0.1670798510313034\n",
      "5.807942867279053 0.0 0.18985402584075928 0.27293410897254944\n",
      "2.7619049549102783 0.0 0.09972768276929855 0.15977682173252106\n",
      "8.922192573547363 0.0 0.19165560603141785 0.2745321989059448\n",
      "3.461829423904419 0.0 0.09555225074291229 0.1607576310634613\n",
      "8.8489351272583 0.0 0.21130219101905823 0.28714466094970703\n",
      "3.866086006164551 0.0 0.08193142712116241 0.154922753572464\n",
      "7.371682643890381 0.0 0.22095640003681183 0.3017602562904358\n",
      "3.02559494972229 0.0 0.08188841491937637 0.14749696850776672\n",
      "5.711442470550537 0.0 0.24450066685676575 0.31926727294921875\n",
      "2.248440980911255 0.0 0.03789600729942322 0.10675227642059326\n",
      "5.4514312744140625 0.0 0.19899679720401764 0.2618192434310913\n",
      "42.723777770996094 0.0 0.5535233020782471 1.1020148992538452\n",
      "Variable containing:\n",
      " 3.3336e+00 -3.3298e-02 -2.7162e+00  ...   3.2468e+00  4.5482e+00 -9.0792e-02\n",
      "-5.6986e+00 -3.3322e+00 -5.6047e-01  ...  -5.8076e+00  2.2899e-01  5.8702e+00\n",
      " 4.7371e+00  1.0636e+00  6.9245e-02  ...   7.0995e+00  3.4443e+00  3.0970e+00\n",
      "                ...                                      ...                \n",
      " 1.4816e+00 -2.5806e+00 -4.4353e+00  ...   4.9790e+00  4.5386e+00 -9.4509e-02\n",
      "-4.2827e-01 -1.0509e+00 -1.0286e-01  ...  -5.0183e+00  1.6008e+00  7.5282e+00\n",
      " 5.8482e-01  1.2717e+00 -2.5426e+00  ...  -2.1912e+00  3.4187e+00  6.0799e+00\n",
      "[torch.cuda.FloatTensor of size 128x1000 (GPU 2)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_range_min =  []\n",
    "output_range_max =  []\n",
    "output_range_mean =  []\n",
    "output_range_std =  []\n",
    "output_range_values = []\n",
    "def get_io_shape_3(self, input, output):\n",
    "    output_range_min[self.mark].append(output.data.min())\n",
    "    output_range_max[self.mark].append(output.data.max())\n",
    "    output_range_mean[self.mark].append(output.data.mean())\n",
    "    output_range_std[self.mark].append(output.data.std())\n",
    "    print(output.data.max(),output.data.min(),output.data.mean(),output.data.std())\n",
    "\n",
    "    \n",
    "    \n",
    "def calibrate(data_loader, model, n_batch, gpus):\n",
    "    training = False\n",
    "    \n",
    "    counter = 0\n",
    "    for n,l in model.named_modules():\n",
    "        if isinstance(l,nn.ReLU):\n",
    "            print(n,l)\n",
    "            l.mark = counter\n",
    "            output_range_min.append([])\n",
    "            output_range_max.append([])\n",
    "            output_range_std.append([])\n",
    "            output_range_mean.append([])\n",
    "            counter += 1 \n",
    "            l.register_forward_hook(get_io_shape_3)\n",
    "        \n",
    "    #v = model.model[1].register_forward_hook(get_layer8)\n",
    "\n",
    "    #if gpus and len(gpus) > 1:\n",
    "    #    model = torch.nn.DataParallel(model, gpus)\n",
    "    model.eval()\n",
    "    for i, (inputs, target) in enumerate(data_loader):\n",
    "        if i >= n_batch:\n",
    "            break\n",
    "            \n",
    "        input_var = Variable(inputs.type(ttype), volatile=not training)\n",
    "        target_var = Variable(target)\n",
    "        \n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        \n",
    "        #print(v) \n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Run validation\n",
    "output = calibrate(train_loader, deployment_model.cuda(), 10, gpus)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1d861d683a6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_range_min\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "mean(output_range_min[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine precision per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_act_layer = len(output_range_min)\n",
    "precision_layer = [[] for x in range(n_act_layer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 7\n",
      "1 7\n",
      "1 7\n",
      "1 7\n",
      "0 8\n",
      "1 7\n",
      "0 8\n",
      "1 7\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "1 7\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "2 6\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_act_layer):\n",
    "    min_value = min(output_range_min[i])\n",
    "    max_value = max(output_range_max[i])\n",
    "    std_value = max(output_range_std[i])\n",
    "    int_bits, frac_bits = fixed_point_linear_quant(min_value, 3*std_value, N_BITS, signed=False)\n",
    "    print(int_bits, frac_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuantizeFixed(real_activation, numBits=8, min_value=0, max_value=6, int_bits=1):\n",
    "    tensor = real_activation.clamp( 0, max_value )\n",
    "    eps = (max_value)/((2**numBits)-1)\n",
    "    shift_int = 2**int_bits\n",
    "    tensor=tensor.div(eps).floor().mul(eps)\n",
    "    #tensor = tensor.div(shift_int)\n",
    "    #print(eps,shift_int )\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class QuantizationReLU(torch.autograd.Function):\n",
    "    '''\n",
    "    Quantize the input activations and calculate the mean across channel dimension.\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bits, min_value, max_value, int_bits):\n",
    "        ctx.save_for_backward(input)\n",
    "        output = QuantizeFixed(input, numBits=bits, min_value=min_value, max_value=max_value, int_bits=int_bits)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input.ge(1)] = 0\n",
    "        grad_input[input.le(0)] = 0\n",
    "        return grad_input, None, None, None, None\n",
    "\n",
    "\n",
    "class QuantReLU(nn.Module):\n",
    "    def __init__(self, min_value, max_value, num_of_bits, int_bits):\n",
    "        super(QuantReLU,self).__init__()\n",
    "        self.bits = num_of_bits\n",
    "        self.max_value = max_value\n",
    "        self.min_value = min_value\n",
    "        self.int_bits = int_bits\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return QuantizationReLU.apply(input, self.bits, self.min_value,self.max_value, self.int_bits )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2969404458999634\n",
      "4.1698092222213745\n",
      "2.42896831035614\n",
      "3.1248030364513397\n",
      "1.8195737302303314\n",
      "2.9339458644390106\n",
      "1.8078442811965942\n",
      "2.8597283363342285\n",
      "1.5590582340955734\n",
      "2.136590749025345\n",
      "1.3774169981479645\n",
      "2.384040504693985\n",
      "1.4072715491056442\n",
      "1.729140356183052\n",
      "1.1710454523563385\n",
      "1.910538762807846\n",
      "1.121280461549759\n",
      "1.924646645784378\n",
      "1.1285786926746368\n",
      "2.010012626647949\n",
      "1.0860780328512192\n",
      "2.113189846277237\n",
      "1.0360946208238602\n",
      "2.2392912805080414\n",
      "0.753291554749012\n",
      "1.8327347040176392\n",
      "7.726824045181274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mobilenet_real(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d (3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): QuantReLU(\n",
       "    )\n",
       "    (2): Conv2d (32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "    (3): QuantReLU(\n",
       "    )\n",
       "    (4): Conv2d (32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (5): QuantReLU(\n",
       "    )\n",
       "    (6): Conv2d (64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (7): QuantReLU(\n",
       "    )\n",
       "    (8): Conv2d (64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (9): QuantReLU(\n",
       "    )\n",
       "    (10): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "    (11): QuantReLU(\n",
       "    )\n",
       "    (12): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (13): QuantReLU(\n",
       "    )\n",
       "    (14): Conv2d (128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "    (15): QuantReLU(\n",
       "    )\n",
       "    (16): Conv2d (128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (17): QuantReLU(\n",
       "    )\n",
       "    (18): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (19): QuantReLU(\n",
       "    )\n",
       "    (20): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (21): QuantReLU(\n",
       "    )\n",
       "    (22): Conv2d (256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "    (23): QuantReLU(\n",
       "    )\n",
       "    (24): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (25): QuantReLU(\n",
       "    )\n",
       "    (26): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (27): QuantReLU(\n",
       "    )\n",
       "    (28): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (29): QuantReLU(\n",
       "    )\n",
       "    (30): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (31): QuantReLU(\n",
       "    )\n",
       "    (32): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (33): QuantReLU(\n",
       "    )\n",
       "    (34): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (35): QuantReLU(\n",
       "    )\n",
       "    (36): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (37): QuantReLU(\n",
       "    )\n",
       "    (38): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (39): QuantReLU(\n",
       "    )\n",
       "    (40): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (41): QuantReLU(\n",
       "    )\n",
       "    (42): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (43): QuantReLU(\n",
       "    )\n",
       "    (44): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (45): QuantReLU(\n",
       "    )\n",
       "    (46): Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "    (47): QuantReLU(\n",
       "    )\n",
       "    (48): Conv2d (512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (49): QuantReLU(\n",
       "    )\n",
       "    (50): Conv2d (1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "    (51): QuantReLU(\n",
       "    )\n",
       "    (52): Conv2d (1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (53): QuantReLU(\n",
       "    )\n",
       "    (54): AvgPool2d(kernel_size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=1000)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model = None\n",
    "quantized_modules = []\n",
    "counter = 0\n",
    "\n",
    "def find_activations_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Sequential):\n",
    "            find_activations_layers(child) \n",
    "        elif isinstance(child, nn.ReLU):\n",
    "            global counter\n",
    "            min_value = min(output_range_min[counter])\n",
    "            max_value = max(output_range_max[counter])\n",
    "            std_value = max(output_range_std[counter])\n",
    "            counter += 1\n",
    "            max_value = 7*std_value\n",
    "            print(max_value)\n",
    "            int_bits, frac_bits = fixed_point_linear_quant(min_value, max_value, N_BITS, signed=False)\n",
    "            quantized_modules.append(QuantReLU(min_value, max_value, N_BITS, int_bits ) )\n",
    "        else:\n",
    "            quantized_modules.append(child)\n",
    "            \n",
    "            \n",
    "find_activations_layers(deployment_model.model)\n",
    "quantized_model = copy.deepcopy(deployment_model)\n",
    "quantized_model.model = nn.Sequential(*quantized_modules)    \n",
    "quantized_model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][0/391]\tTime 3.287 (3.287)\tData 3.211 (3.211)\tLoss 0.6898 (0.6898)\tPrec@1 83.594 (83.594)\tPrec@5 92.188 (92.188)\n",
      "EVALUATING - Epoch: [0][10/391]\tTime 0.542 (0.509)\tData 0.473 (0.440)\tLoss 1.3616 (0.9342)\tPrec@1 64.062 (76.989)\tPrec@5 85.938 (91.051)\n",
      "EVALUATING - Epoch: [0][20/391]\tTime 0.213 (0.357)\tData 0.143 (0.288)\tLoss 1.5703 (1.1213)\tPrec@1 64.844 (71.429)\tPrec@5 82.812 (90.141)\n",
      "EVALUATING - Epoch: [0][30/391]\tTime 0.067 (0.327)\tData 0.000 (0.257)\tLoss 1.8310 (1.2872)\tPrec@1 66.406 (67.288)\tPrec@5 82.031 (88.659)\n",
      "EVALUATING - Epoch: [0][40/391]\tTime 0.232 (0.303)\tData 0.160 (0.234)\tLoss 1.0482 (1.1945)\tPrec@1 74.219 (69.798)\tPrec@5 89.844 (89.348)\n",
      "EVALUATING - Epoch: [0][50/391]\tTime 0.070 (0.300)\tData 0.000 (0.230)\tLoss 0.6157 (1.2641)\tPrec@1 82.812 (68.903)\tPrec@5 94.531 (88.251)\n",
      "EVALUATING - Epoch: [0][60/391]\tTime 0.310 (0.288)\tData 0.240 (0.219)\tLoss 1.2655 (1.2117)\tPrec@1 69.531 (70.338)\tPrec@5 87.500 (88.781)\n",
      "EVALUATING - Epoch: [0][70/391]\tTime 0.144 (0.283)\tData 0.073 (0.213)\tLoss 1.5444 (1.2282)\tPrec@1 53.906 (69.443)\tPrec@5 90.625 (88.974)\n",
      "EVALUATING - Epoch: [0][80/391]\tTime 0.068 (0.280)\tData 0.001 (0.210)\tLoss 1.3246 (1.2529)\tPrec@1 61.719 (68.470)\tPrec@5 89.844 (88.860)\n",
      "EVALUATING - Epoch: [0][90/391]\tTime 0.349 (0.277)\tData 0.280 (0.208)\tLoss 1.2517 (1.2611)\tPrec@1 59.375 (68.218)\tPrec@5 91.406 (88.856)\n",
      "EVALUATING - Epoch: [0][100/391]\tTime 0.068 (0.270)\tData 0.000 (0.200)\tLoss 1.1961 (1.2477)\tPrec@1 71.875 (68.201)\tPrec@5 88.281 (89.233)\n",
      "EVALUATING - Epoch: [0][110/391]\tTime 0.264 (0.275)\tData 0.194 (0.206)\tLoss 1.4441 (1.2443)\tPrec@1 54.688 (68.039)\tPrec@5 89.062 (89.372)\n",
      "EVALUATING - Epoch: [0][120/391]\tTime 0.069 (0.270)\tData 0.001 (0.201)\tLoss 1.1183 (1.2308)\tPrec@1 71.875 (68.472)\tPrec@5 90.625 (89.573)\n",
      "EVALUATING - Epoch: [0][130/391]\tTime 0.293 (0.273)\tData 0.224 (0.203)\tLoss 0.5791 (1.2285)\tPrec@1 85.156 (68.726)\tPrec@5 95.312 (89.522)\n",
      "EVALUATING - Epoch: [0][140/391]\tTime 0.411 (0.271)\tData 0.344 (0.201)\tLoss 1.5002 (1.2259)\tPrec@1 51.562 (68.761)\tPrec@5 89.062 (89.633)\n",
      "EVALUATING - Epoch: [0][150/391]\tTime 0.067 (0.271)\tData 0.001 (0.201)\tLoss 1.1172 (1.2278)\tPrec@1 64.062 (68.703)\tPrec@5 91.406 (89.637)\n",
      "EVALUATING - Epoch: [0][160/391]\tTime 0.168 (0.267)\tData 0.095 (0.197)\tLoss 1.4304 (1.2376)\tPrec@1 60.938 (68.668)\tPrec@5 85.156 (89.431)\n",
      "EVALUATING - Epoch: [0][170/391]\tTime 0.068 (0.267)\tData 0.001 (0.197)\tLoss 1.9976 (1.2756)\tPrec@1 55.469 (67.978)\tPrec@5 81.250 (88.916)\n",
      "EVALUATING - Epoch: [0][180/391]\tTime 0.069 (0.265)\tData 0.000 (0.195)\tLoss 2.8174 (1.3121)\tPrec@1 37.500 (67.218)\tPrec@5 71.094 (88.445)\n",
      "EVALUATING - Epoch: [0][190/391]\tTime 0.067 (0.263)\tData 0.000 (0.193)\tLoss 2.4280 (1.3375)\tPrec@1 47.656 (66.754)\tPrec@5 72.656 (88.040)\n",
      "EVALUATING - Epoch: [0][200/391]\tTime 0.069 (0.262)\tData 0.000 (0.192)\tLoss 2.2488 (1.3761)\tPrec@1 44.531 (65.990)\tPrec@5 71.875 (87.496)\n",
      "EVALUATING - Epoch: [0][210/391]\tTime 0.151 (0.262)\tData 0.080 (0.192)\tLoss 1.9601 (1.4034)\tPrec@1 59.375 (65.558)\tPrec@5 76.562 (87.089)\n",
      "EVALUATING - Epoch: [0][220/391]\tTime 0.069 (0.262)\tData 0.000 (0.192)\tLoss 1.2940 (1.4229)\tPrec@1 68.750 (65.222)\tPrec@5 84.375 (86.740)\n",
      "EVALUATING - Epoch: [0][230/391]\tTime 0.068 (0.261)\tData 0.000 (0.191)\tLoss 2.3712 (1.4393)\tPrec@1 47.656 (65.013)\tPrec@5 72.656 (86.462)\n",
      "EVALUATING - Epoch: [0][240/391]\tTime 0.333 (0.259)\tData 0.261 (0.189)\tLoss 1.6326 (1.4488)\tPrec@1 67.969 (64.931)\tPrec@5 84.375 (86.268)\n",
      "EVALUATING - Epoch: [0][250/391]\tTime 0.377 (0.257)\tData 0.301 (0.187)\tLoss 1.5558 (1.4702)\tPrec@1 66.406 (64.436)\tPrec@5 80.469 (86.000)\n",
      "EVALUATING - Epoch: [0][260/391]\tTime 0.074 (0.259)\tData 0.000 (0.189)\tLoss 2.0896 (1.4882)\tPrec@1 51.562 (64.071)\tPrec@5 78.906 (85.794)\n",
      "EVALUATING - Epoch: [0][270/391]\tTime 0.067 (0.257)\tData 0.000 (0.187)\tLoss 2.3109 (1.5007)\tPrec@1 46.875 (63.855)\tPrec@5 72.656 (85.606)\n",
      "EVALUATING - Epoch: [0][280/391]\tTime 0.067 (0.256)\tData 0.000 (0.186)\tLoss 1.5986 (1.5099)\tPrec@1 57.812 (63.648)\tPrec@5 86.719 (85.504)\n",
      "EVALUATING - Epoch: [0][290/391]\tTime 0.067 (0.254)\tData 0.000 (0.184)\tLoss 2.0242 (1.5240)\tPrec@1 42.969 (63.418)\tPrec@5 87.500 (85.285)\n",
      "EVALUATING - Epoch: [0][300/391]\tTime 0.434 (0.255)\tData 0.363 (0.185)\tLoss 1.6379 (1.5377)\tPrec@1 68.750 (63.276)\tPrec@5 80.469 (85.068)\n",
      "EVALUATING - Epoch: [0][310/391]\tTime 0.149 (0.254)\tData 0.074 (0.184)\tLoss 1.5625 (1.5535)\tPrec@1 63.281 (63.010)\tPrec@5 88.281 (84.870)\n",
      "EVALUATING - Epoch: [0][320/391]\tTime 0.069 (0.254)\tData 0.000 (0.184)\tLoss 1.3095 (1.5610)\tPrec@1 71.094 (62.902)\tPrec@5 87.500 (84.760)\n",
      "EVALUATING - Epoch: [0][330/391]\tTime 0.069 (0.253)\tData 0.000 (0.183)\tLoss 2.2447 (1.5824)\tPrec@1 49.219 (62.488)\tPrec@5 77.344 (84.467)\n",
      "EVALUATING - Epoch: [0][340/391]\tTime 0.266 (0.254)\tData 0.200 (0.183)\tLoss 1.5371 (1.5893)\tPrec@1 67.969 (62.314)\tPrec@5 85.938 (84.386)\n",
      "EVALUATING - Epoch: [0][350/391]\tTime 0.068 (0.253)\tData 0.000 (0.183)\tLoss 1.4917 (1.5978)\tPrec@1 63.281 (62.148)\tPrec@5 85.938 (84.235)\n",
      "EVALUATING - Epoch: [0][360/391]\tTime 0.355 (0.253)\tData 0.285 (0.182)\tLoss 1.7082 (1.6024)\tPrec@1 57.812 (62.059)\tPrec@5 82.031 (84.191)\n",
      "EVALUATING - Epoch: [0][370/391]\tTime 0.387 (0.253)\tData 0.312 (0.182)\tLoss 1.4691 (1.5977)\tPrec@1 53.906 (62.089)\tPrec@5 87.500 (84.272)\n",
      "EVALUATING - Epoch: [0][380/391]\tTime 0.242 (0.252)\tData 0.168 (0.182)\tLoss 1.3470 (1.6002)\tPrec@1 67.188 (62.071)\tPrec@5 88.281 (84.240)\n",
      "EVALUATING - Epoch: [0][390/391]\tTime 0.044 (0.252)\tData 0.000 (0.181)\tLoss 2.2472 (1.5935)\tPrec@1 45.000 (62.202)\tPrec@5 73.750 (84.306)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5934641359710693 62.202 84.306\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_prec1, val_prec5 = validate(val_loader, model, criterion, 0, None, gpus)\n",
    "print(val_loss, val_prec1, val_prec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------- Extra ----------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3250339 ,  0.5       ,  0.21670543,  0.23417938,  0.5       ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(5)\n",
    "np.clip(a,0,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 3, 3])\n",
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([64, 32, 1, 1])\n",
      "torch.Size([64, 1, 3, 3])\n",
      "torch.Size([128, 64, 1, 1])\n",
      "torch.Size([128, 1, 3, 3])\n",
      "torch.Size([128, 128, 1, 1])\n",
      "torch.Size([128, 1, 3, 3])\n",
      "torch.Size([256, 128, 1, 1])\n",
      "torch.Size([256, 1, 3, 3])\n",
      "torch.Size([256, 256, 1, 1])\n",
      "torch.Size([256, 1, 3, 3])\n",
      "torch.Size([512, 256, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([1024, 512, 1, 1])\n",
      "torch.Size([1024, 1, 3, 3])\n",
      "torch.Size([1024, 1024, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_fold_vector = [\n",
    "['module.model.0.0','module.model.0.1'],\n",
    "['module.model.1.0','module.model.1.1'],\n",
    "['module.model.1.3','module.model.1.4'],\n",
    "['module.model.2.0','module.model.2.1'],\n",
    "['module.model.2.3','module.model.2.4'],\n",
    "['module.model.3.0','module.model.3.1'],\n",
    "['module.model.3.3','module.model.3.4'],\n",
    "['module.model.4.0','module.model.4.1'],\n",
    "['module.model.4.3','module.model.4.4'],\n",
    "['module.model.5.0','module.model.5.1'],\n",
    "['module.model.5.3','module.model.5.4'],\n",
    "['module.model.6.0','module.model.6.1'],\n",
    "['module.model.6.3','module.model.6.4'],\n",
    "['module.model.7.0','module.model.7.1'],\n",
    "['module.model.7.3','module.model.7.4'],\n",
    "['module.model.8.0','module.model.8.1'],\n",
    "['module.model.8.3','module.model.8.4'],\n",
    "['module.model.9.0','module.model.9.1'],\n",
    "['module.model.9.3','module.model.9.4'],\n",
    "['module.model.10.0','module.model.10.1'],\n",
    "['module.model.10.3','module.model.10.4'],\n",
    "['module.model.11.0','module.model.11.1'],\n",
    "['module.model.11.3','module.model.11.4'],\n",
    "['module.model.12.0','module.model.12.1'],\n",
    "['module.model.12.3','module.model.12.4'],\n",
    "['module.model.13.0','module.model.13.1'],\n",
    "['module.model.13.3','module.model.13.4']\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_tensor_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.quant_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index =  0\n",
      "Index =  1\n",
      "Index =  2\n",
      "Conv module  2 quantized!\n",
      "Index =  3\n",
      "Index =  4\n",
      "Index =  5\n",
      "Index =  6\n",
      "Index =  7\n",
      "Conv module  7 quantized!\n",
      "Index =  8\n",
      "Index =  9\n",
      "Index =  10\n",
      "Index =  11\n",
      "Index =  12\n",
      "Conv module  12 quantized!\n",
      "Index =  13\n",
      "Index =  14\n",
      "Index =  15\n",
      "Index =  16\n",
      "Conv module  16 quantized!\n",
      "Index =  17\n",
      "Index =  18\n",
      "Index =  19\n",
      "Index =  20\n",
      "Conv module  20 quantized!\n",
      "Index =  21\n",
      "Index =  22\n",
      "Index =  23\n",
      "Index =  24\n",
      "Index =  25\n",
      "Linear module:  25  quantized!\n",
      "Index =  26\n",
      "Index =  27\n",
      "Index =  28\n",
      "Linear module:  28  quantized!\n",
      "Index =  29\n",
      "Index =  30\n",
      "Index =  31\n",
      "Linear module:  31  quantized!\n",
      "Index =  32\n",
      "************************\n",
      "Tot to quantized:  8\n"
     ]
    }
   ],
   "source": [
    "index = -1\n",
    "index_conv2d = []\n",
    "index_lin = []\n",
    "\n",
    "for idx,m in enumerate(model.modules()):\n",
    "    print('Index = ', idx)\n",
    "    weight_quant = getattr(m, 'quant_weight', None)\n",
    "    if weight_quant != None:\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            index = index + 1\n",
    "            index_conv2d.append(index)\n",
    "            print('Conv module ', idx, 'quantized!')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            index = index + 1\n",
    "            index_lin.append(index)\n",
    "            print('Linear module: ', idx, ' quantized!')\n",
    "        else: \n",
    "            print('Module: ', idx, ' NOT quantized!')\n",
    "        \n",
    "print('************************')\n",
    "print('Tot to quantized: ', len(index_conv2d) + len(index_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = model.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MaxPool2d' object has no attribute 'quant_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d4f503ecf526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 366\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaxPool2d' object has no attribute 'quant_weight'"
     ]
    }
   ],
   "source": [
    "model.features[1].quant_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trianing \n",
    "epoch = 0\n",
    "model.train()\n",
    "optimizer = adjust_optimizer(optimizer, epoch, regime)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.adam.Adam at 0x7f13cb465128>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,target = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = Variable(inputs, volatile=False)\n",
    "target_var = Variable(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.binarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_var)\n",
    "loss = criterion(output, target_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(quantizer.num_of_params):\n",
    "    quantizer.target_modules[index].data.copy_(quantizer.saved_params[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.updateBinaryGradWeight2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1.8127e-02  1.5027e-03  3.2594e-03 -1.2902e-02  1.4510e-02\n",
       "  7.1123e-03  1.9530e-02 -1.0884e-02 -2.8592e-03 -1.3259e-02\n",
       "  2.1212e-02 -2.1252e-02 -8.8512e-03  5.5863e-03  2.5938e-02\n",
       "  2.4559e-02 -1.9748e-02  2.2135e-02  1.5002e-03 -1.0777e-02\n",
       " -1.0942e-02  3.0251e-03 -8.2488e-03 -2.0687e-02 -3.1814e-03\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -2.3760e-03 -2.6540e-03 -1.1723e-02 -1.5072e-02 -1.2241e-03\n",
       " -1.3689e-02 -2.1758e-02  1.1396e-02  2.5437e-02 -8.7456e-04\n",
       "  2.2533e-03 -1.3677e-02 -5.6864e-03  2.1138e-02  4.9958e-03\n",
       "  1.5956e-03 -3.7952e-03 -6.2803e-03  1.3870e-02  1.3949e-02\n",
       "  1.2288e-02  1.0312e-02 -8.1034e-04  1.1017e-02 -1.1564e-02\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -9.1109e-03 -9.9038e-04  6.6380e-03 -2.5215e-02 -1.5876e-03\n",
       " -1.5147e-02 -3.6442e-03  1.6910e-02  4.4561e-04 -1.8616e-02\n",
       " -2.3990e-02  2.0563e-03 -2.4307e-02 -8.7227e-03  1.0326e-02\n",
       "  2.3016e-02 -2.1399e-02  2.1827e-02  1.4622e-02 -1.6246e-02\n",
       " -1.4855e-02 -4.9985e-03 -1.8892e-02 -1.4951e-03  4.6154e-03\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -7.0762e-03  4.7802e-03  7.1773e-03  2.0799e-02 -1.8840e-03\n",
       " -1.0092e-02 -1.4496e-02  3.5899e-03 -2.2817e-02 -1.8218e-02\n",
       "  2.5225e-02 -2.6470e-02 -1.2509e-02  9.9451e-03 -2.1741e-02\n",
       "  9.1894e-03 -4.6840e-03 -1.1146e-02  2.0051e-02  2.2290e-02\n",
       " -2.1867e-02 -1.3247e-02  1.2491e-02 -1.2545e-02 -1.1102e-02\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  3.1433e-03 -2.2180e-02 -2.0657e-02  4.6139e-03 -1.0855e-02\n",
       " -2.1155e-02 -1.9463e-02  1.2641e-02  6.9894e-03  1.3142e-02\n",
       " -1.1386e-03 -1.1071e-02 -2.8952e-03  1.7722e-02  2.3034e-02\n",
       "  4.7652e-03 -1.6204e-02 -1.7498e-02 -9.6184e-03 -1.8215e-02\n",
       " -4.4003e-03 -1.5078e-02  1.8350e-02 -3.9738e-03 -7.5132e-03\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -8.4719e-03 -1.8920e-02  2.5539e-02 -6.3212e-03 -1.1142e-02\n",
       " -1.6250e-02  1.4741e-02 -1.1612e-02  1.6739e-03 -1.6748e-02\n",
       "  1.5673e-03 -2.2873e-02 -6.9813e-03  6.2617e-05  2.1669e-02\n",
       " -7.5196e-03  1.4789e-02 -1.1447e-02  2.1471e-02 -3.5458e-03\n",
       " -2.7225e-03 -1.6787e-02 -2.1236e-02 -2.1042e-02  7.3566e-03\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  1.9096e-02 -1.7414e-02  2.2140e-02 -1.5287e-02  1.4455e-02\n",
       "  2.0397e-03 -1.4509e-02 -1.6617e-03  1.5224e-02  1.9241e-02\n",
       "  1.5679e-02  2.1376e-02  5.1866e-03  1.3742e-03  5.6530e-03\n",
       " -1.3192e-02  3.4915e-03  2.0605e-02  1.9053e-02 -5.1003e-03\n",
       " -9.3828e-03 -3.5060e-03  6.4312e-03  2.0911e-02  7.4230e-03\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  2.4834e-02  1.5733e-02 -1.7126e-02 -2.3076e-02 -8.4893e-03\n",
       "  2.3981e-02 -1.9572e-02  3.4260e-03  2.5698e-03 -1.1628e-02\n",
       "  1.5161e-03 -1.6879e-02  6.2190e-03  4.4115e-03  1.4936e-02\n",
       "  1.1733e-02 -1.6696e-02 -1.1506e-02  1.5677e-02  2.2522e-02\n",
       "  6.7536e-03 -6.0812e-03 -2.2076e-02 -1.1564e-02  1.1740e-02\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1.9174e-02  1.9563e-02  6.8029e-04  9.0326e-03  2.4305e-02\n",
       " -2.0872e-02  1.5029e-02  1.8903e-02 -2.7500e-03 -2.4397e-02\n",
       " -3.5883e-03 -2.4260e-03  5.9608e-03 -9.6108e-03 -1.9486e-02\n",
       " -6.4282e-04  1.3236e-02 -6.3805e-03  4.3614e-03 -1.9481e-02\n",
       "  2.5316e-03  6.9512e-03 -5.6784e-03  2.0049e-02 -1.2546e-02\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  2.1178e-02  9.2475e-03 -1.3356e-02  2.3237e-02  1.7591e-02\n",
       " -7.8461e-04  2.3762e-02 -1.1373e-04  2.5877e-02  1.9244e-02\n",
       " -1.7386e-02  8.1954e-03  2.3153e-02 -1.1749e-02  2.2543e-02\n",
       " -5.1184e-03 -1.2765e-02 -2.7481e-03 -1.5597e-02  4.5053e-03\n",
       " -2.3091e-03  3.5104e-03  1.6740e-02  2.0176e-02  4.9362e-03\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  2.1149e-02  1.2407e-02  1.5939e-02  2.5849e-03  1.1430e-02\n",
       "  9.7003e-03 -1.8499e-02 -1.8851e-02 -1.0847e-02  3.4258e-03\n",
       " -1.7196e-02 -5.4654e-03 -6.4304e-03 -5.8260e-03 -3.4130e-03\n",
       " -1.9647e-02  3.5472e-03 -1.9833e-02 -1.9377e-02  6.4316e-03\n",
       "  1.1275e-02 -1.9409e-02  5.7234e-03  2.1985e-02  9.7657e-03\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -1.6058e-03  3.7541e-03 -1.6566e-02  2.3257e-02 -1.4757e-02\n",
       " -9.1012e-03 -1.0380e-02 -1.7606e-02 -1.4127e-02  4.5094e-03\n",
       "  1.0568e-02  1.3536e-02  2.3153e-02 -1.5670e-03  1.9681e-02\n",
       " -2.2910e-02 -1.8209e-02 -2.3108e-02 -1.3555e-02 -7.6443e-03\n",
       " -1.2070e-02 -2.2582e-02  2.2177e-02 -9.1846e-04  1.0176e-02\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -1.1585e-02 -7.6666e-03 -9.1613e-03 -1.5271e-02  1.6936e-02\n",
       " -7.5146e-03  1.2522e-02 -1.4715e-02 -2.2066e-02  9.0287e-03\n",
       "  1.7321e-02  2.1467e-02 -8.5637e-03  9.4318e-03 -2.1875e-02\n",
       " -6.8519e-03 -1.8958e-02 -1.4568e-02 -2.4060e-02 -7.4957e-03\n",
       "  2.2308e-02  1.8909e-02  1.5447e-02 -4.4835e-03  5.2158e-03\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  9.7157e-03  3.0865e-04  1.1476e-02  5.8029e-03 -1.4323e-02\n",
       " -1.5687e-02 -2.2458e-02 -1.3379e-02 -1.4305e-02  7.6678e-03\n",
       "  7.1823e-03  1.2023e-02  2.5851e-02  1.8209e-02  8.7983e-03\n",
       "  2.5959e-02  1.5203e-02 -9.4223e-03  1.1792e-02 -1.5215e-02\n",
       "  1.4800e-02 -6.8187e-03  5.4734e-03  7.9059e-03  1.0551e-02\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1.6353e-02 -5.7692e-03  4.3811e-03 -8.4465e-03 -1.5122e-02\n",
       " -2.7007e-03 -1.5615e-02 -1.8432e-02 -9.0274e-03 -2.0761e-02\n",
       "  2.3247e-02 -2.4744e-02  1.1459e-02  5.7309e-03 -1.4396e-02\n",
       " -1.8722e-02 -5.6219e-04  7.1841e-03  1.6912e-02  1.7856e-02\n",
       "  1.5941e-02  9.0543e-03  1.3205e-02  1.0449e-02 -2.0351e-03\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1.8643e-02 -6.6896e-04 -9.5753e-03  3.6254e-03  2.1763e-02\n",
       "  1.0189e-02 -1.1745e-04  1.2482e-02  1.2609e-02 -1.1771e-02\n",
       " -2.9821e-03 -1.0488e-02 -1.3929e-02  2.3947e-02  1.9473e-02\n",
       " -1.4677e-02 -1.5780e-02 -2.3211e-02 -6.2103e-03  7.4194e-03\n",
       " -9.8016e-03 -1.4882e-02 -2.0435e-02 -1.9943e-02 -7.5823e-03\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1.8786e-02  1.0936e-02  2.0851e-02  1.5438e-02 -1.1014e-02\n",
       "  1.3392e-02 -1.8632e-02  2.1880e-02  2.6382e-02 -1.0817e-02\n",
       "  1.9406e-02  5.6681e-03  1.6735e-02 -1.0116e-02  5.9607e-03\n",
       " -3.0752e-03  9.2660e-03  5.0495e-03  1.4882e-02  1.7354e-02\n",
       "  5.3769e-03  2.0343e-02 -1.1393e-04 -1.6952e-02  1.1258e-02\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1.4544e-02 -2.3295e-02  6.3029e-03  1.4665e-02 -1.8535e-02\n",
       " -2.2719e-02  1.7705e-02  9.2022e-03  1.7612e-02  4.5667e-04\n",
       " -5.9746e-03  2.1752e-02 -5.1201e-03 -2.1248e-02  1.7188e-02\n",
       "  2.3872e-02 -2.0435e-02 -1.7739e-02  2.2524e-02 -1.0802e-02\n",
       "  2.3962e-02  2.4012e-02 -1.1178e-02 -8.2167e-03 -7.4922e-03\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1.7364e-02 -2.3632e-02 -1.6631e-02 -5.5243e-03 -1.5761e-02\n",
       "  1.7652e-02  1.1065e-02 -8.6269e-03 -1.9450e-02  2.0392e-02\n",
       "  2.1468e-02 -8.9295e-03  1.5543e-02  2.0302e-02  8.3006e-03\n",
       " -2.1280e-03 -1.2588e-02  7.7211e-04 -9.8156e-03  2.0207e-02\n",
       "  4.6332e-03  1.4387e-02 -1.1070e-02 -1.6499e-02  1.0041e-02\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1.4068e-02  1.2109e-02  7.8176e-03  1.5914e-02  3.0466e-03\n",
       "  1.8128e-02  6.7775e-03 -1.1536e-02 -1.4469e-02  1.4715e-02\n",
       "  1.0304e-02  1.7005e-02 -1.1996e-02 -1.4766e-03  4.8024e-04\n",
       "  8.6332e-03 -8.7581e-03 -1.3530e-02 -1.2634e-02  2.0204e-02\n",
       "  5.3275e-04  1.5249e-02 -1.9053e-02  6.2446e-03  1.6458e-03\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1.3839e-02 -6.6901e-03 -1.7259e-02  1.0888e-02 -1.1756e-02\n",
       " -2.0550e-02 -2.0470e-02 -6.7177e-03  2.7262e-03  2.3528e-02\n",
       " -3.3724e-03 -2.0359e-02  8.7641e-03  1.4151e-02  2.4517e-02\n",
       "  7.7960e-03  1.9392e-02  2.2572e-02 -2.0742e-02 -1.0075e-02\n",
       " -1.9978e-03 -1.3593e-02  4.3226e-03  1.9941e-02 -1.2358e-02\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  1.0589e-02 -6.6963e-03  2.4210e-02  1.1408e-02 -2.1294e-03\n",
       "  7.7296e-03 -9.2406e-03  8.1714e-03  1.9367e-02 -7.2199e-03\n",
       " -2.1214e-02  1.8880e-02 -7.4127e-03  1.9251e-03 -1.1957e-02\n",
       " -2.2092e-02 -8.0984e-03  1.9220e-02  1.9847e-02 -2.2047e-02\n",
       "  2.0403e-02 -3.5536e-03  2.4226e-02 -2.4024e-02 -2.4730e-02\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -4.2391e-03 -5.3030e-03  1.0658e-02 -1.2108e-02 -5.4902e-03\n",
       "  9.7204e-04  1.3947e-02  2.1941e-02 -5.2715e-03 -7.0353e-03\n",
       "  1.2066e-02 -2.0121e-02 -1.4200e-04 -5.7222e-03  2.5184e-02\n",
       " -7.6840e-03 -8.9304e-03 -8.6936e-03 -1.9946e-02 -2.1174e-02\n",
       "  9.4660e-03  1.3340e-02 -2.2505e-02  2.5334e-02  1.3314e-02\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1.2388e-02 -1.2473e-02  1.0232e-02  4.0885e-03  1.9520e-02\n",
       "  2.5037e-02 -1.7132e-02 -2.0942e-02 -3.6316e-03 -1.2958e-02\n",
       " -2.1506e-02  6.6534e-03 -1.0212e-02  2.1433e-02 -1.0499e-02\n",
       " -9.0928e-03  1.5628e-02  1.1886e-02  9.6382e-03 -1.8932e-02\n",
       "  6.7430e-03 -1.7735e-02  1.2968e-02  1.3950e-02 -1.2412e-02\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -7.3033e-03 -9.6375e-03 -1.6765e-02  5.9203e-03  1.2102e-02\n",
       " -1.8605e-02  2.5505e-02  9.5010e-03 -3.6182e-03 -2.1026e-02\n",
       "  6.9440e-03 -1.4352e-02 -1.9271e-02 -1.1019e-04  2.0428e-03\n",
       "  1.9763e-02 -4.9082e-03  1.3405e-03  6.9979e-04 -2.0806e-02\n",
       "  1.1378e-02  2.3315e-02 -1.7659e-02 -1.8258e-03 -4.3315e-04\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1.7995e-02 -4.9583e-03  6.3116e-03 -1.3536e-02  1.4433e-02\n",
       "  2.4653e-02  5.6016e-03  1.9553e-02 -2.0848e-02 -7.4392e-03\n",
       " -1.8006e-02  1.7567e-02  5.3860e-03 -1.9548e-02  1.9358e-02\n",
       " -1.9094e-02 -5.6246e-03 -1.7039e-02 -5.1723e-03 -1.0268e-02\n",
       "  2.3830e-02 -1.2603e-02 -1.2014e-02  8.4320e-03 -2.1335e-02\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  3.3651e-03 -2.9374e-03  5.6431e-03  9.1055e-03  1.8099e-02\n",
       "  2.3847e-02  1.1458e-02 -9.1567e-03 -1.6221e-02  1.7251e-02\n",
       " -1.3793e-02  2.4402e-02  5.0026e-03  1.2061e-02  2.2677e-02\n",
       " -1.3772e-03  2.0489e-02  4.4757e-04  1.3686e-03 -1.0282e-02\n",
       " -6.7459e-03 -4.5834e-03 -1.8638e-02 -1.6804e-02  1.6340e-02\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  2.2726e-03 -1.7781e-02 -1.6517e-02  9.7338e-03 -1.2821e-03\n",
       " -2.3312e-02 -1.7351e-02  1.9172e-02  1.7839e-02  2.2680e-02\n",
       " -1.0755e-02 -2.0241e-02 -2.3159e-02 -2.1281e-02  1.6821e-02\n",
       " -1.0286e-02 -1.8717e-03 -1.3905e-02  1.1224e-02  1.8342e-02\n",
       " -5.6575e-03 -2.4588e-02 -1.1709e-02  1.7231e-02 -5.8803e-03\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  8.9722e-03  7.6994e-03 -6.1710e-03 -1.7346e-02 -1.0553e-02\n",
       " -1.9065e-02 -4.7555e-03  1.8818e-03  2.3298e-03  4.8624e-03\n",
       " -5.3030e-03  1.5726e-02  1.9602e-02 -1.0429e-02  8.7251e-03\n",
       " -1.5885e-02  1.5797e-02  7.0807e-03  6.3033e-03  1.9082e-03\n",
       "  3.0532e-03 -1.6404e-02  1.7278e-02 -2.3992e-02 -1.4893e-02\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  2.2734e-03  8.6821e-03  1.0706e-02  1.2268e-02 -4.1850e-03\n",
       " -1.8091e-02 -8.4180e-03 -7.9240e-03  5.9634e-03 -1.7016e-02\n",
       " -9.0636e-03  1.9540e-02  2.0140e-02  5.4666e-03 -2.4706e-02\n",
       "  8.3691e-03 -1.5574e-02  2.3885e-02  2.2704e-02  1.8427e-02\n",
       " -2.0067e-02  4.3661e-04 -4.2112e-03  8.2481e-03 -2.2887e-02\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1.4308e-02 -9.5898e-03  2.3750e-02  2.3004e-02 -1.6307e-02\n",
       "  1.5765e-02  2.3169e-02  1.0679e-02 -1.1085e-02  2.0628e-02\n",
       "  8.4566e-03 -2.1378e-02 -1.1902e-02 -6.9342e-03 -2.2068e-02\n",
       " -6.3314e-03 -2.2011e-02  5.2397e-03  1.3538e-02  1.3860e-02\n",
       "  7.8030e-03 -2.3762e-02  1.4535e-02  5.1957e-03 -2.1936e-02\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  3.7074e-03  1.6220e-02  2.4980e-02 -1.4291e-02  5.7010e-03\n",
       " -1.4965e-02 -3.0771e-03  4.3778e-03 -2.4586e-02  6.5280e-03\n",
       " -1.4934e-02 -2.1475e-02  2.0766e-02  1.1532e-02  5.2004e-03\n",
       "  1.4769e-02  3.5629e-03  1.7762e-02 -2.9013e-03  5.2643e-04\n",
       " -1.6940e-02 -1.3351e-02 -6.3491e-03  2.3796e-02 -2.1506e-03\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1.2541e-02  1.1167e-02  1.1739e-02  2.3201e-02 -3.5539e-03\n",
       " -2.0052e-02 -7.7975e-03  1.3294e-02  2.2005e-02  9.3777e-03\n",
       "  1.1195e-02  7.4881e-03 -1.7157e-02  9.1117e-03  2.6419e-02\n",
       " -1.3660e-02  1.3795e-02 -8.1635e-03 -2.2401e-03  1.7745e-02\n",
       " -5.8353e-03  5.6632e-03  1.7998e-02 -2.0141e-02  2.5952e-03\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  7.4122e-04  9.2812e-03 -7.9120e-03 -2.4929e-02 -1.9314e-02\n",
       " -2.1690e-02 -1.9924e-02  1.6077e-02  1.4468e-03  2.0931e-02\n",
       " -4.9944e-03 -1.2125e-02 -1.2787e-02  1.9119e-02  2.0771e-02\n",
       " -4.4511e-03  1.4258e-02  1.6398e-02 -1.6357e-02  4.7478e-03\n",
       " -2.0676e-02  1.5762e-02  2.8027e-03  1.5717e-02  6.5052e-03\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1.6821e-02 -1.9790e-03  9.8683e-04  2.4800e-03 -9.8744e-03\n",
       " -1.9654e-02  6.3028e-03  8.2439e-03 -1.9898e-03  7.1746e-03\n",
       "  2.2315e-02 -2.4642e-02  6.8330e-03 -1.5569e-02  2.3157e-02\n",
       "  3.2500e-04  4.6957e-03 -2.0091e-02  6.0148e-03  2.4852e-03\n",
       "  4.3062e-03  2.1994e-02  1.4914e-02 -1.3887e-02  1.3293e-02\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1.9145e-02 -1.2170e-02  1.3510e-02 -1.9991e-02  5.8897e-03\n",
       "  1.9570e-02  2.5780e-02 -5.0493e-04 -2.4282e-02 -2.0446e-02\n",
       " -1.4590e-02 -2.9095e-03  3.5906e-03  1.8663e-02 -1.6835e-02\n",
       " -6.7832e-03  8.4137e-03  5.5342e-03 -4.4324e-03 -8.0609e-03\n",
       "  7.0907e-03  1.4839e-02 -1.4372e-02 -5.1883e-03  6.1437e-03\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       " -0.0305 -0.0396 -0.0279 -0.0134 -0.0152\n",
       " -0.0114 -0.0266 -0.0116 -0.0244 -0.0115\n",
       " -0.0183 -0.0288 -0.0244 -0.0335 -0.0331\n",
       "  0.0048  0.0138 -0.0234 -0.0050 -0.0150\n",
       " -0.0089  0.0039  0.0133  0.0012 -0.0098\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -0.0149 -0.0453 -0.0115  0.0112  0.0002\n",
       " -0.0263 -0.0172 -0.0210  0.0020 -0.0010\n",
       " -0.0262 -0.0166 -0.0302 -0.0196 -0.0029\n",
       " -0.0282 -0.0147 -0.0169 -0.0244 -0.0162\n",
       " -0.0065 -0.0139 -0.0212  0.0036  0.0018\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -0.0039 -0.0094 -0.0027  0.0085  0.0162\n",
       " -0.0215 -0.0199  0.0074 -0.0156  0.0015\n",
       " -0.0125 -0.0276 -0.0038 -0.0135 -0.0058\n",
       "  0.0034  0.0195 -0.0195  0.0120 -0.0050\n",
       " -0.0144  0.0106  0.0295  0.0133  0.0012\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -0.0010 -0.0220 -0.0113 -0.0068 -0.0008\n",
       " -0.0075 -0.0106 -0.0159  0.0005 -0.0021\n",
       " -0.0197 -0.0172 -0.0132 -0.0122  0.0000\n",
       " -0.0088 -0.0091 -0.0031 -0.0062  0.0006\n",
       "  0.0066 -0.0009 -0.0075 -0.0113 -0.0007\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  0.0064  0.0128 -0.0052 -0.0077 -0.0012\n",
       " -0.0130 -0.0098 -0.0072  0.0094 -0.0054\n",
       " -0.0076  0.0050  0.0007  0.0026 -0.0190\n",
       " -0.0067 -0.0046  0.0013 -0.0050  0.0063\n",
       " -0.0083 -0.0124  0.0210  0.0165  0.0036\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -0.0107 -0.0242 -0.0173 -0.0013 -0.0090\n",
       " -0.0099 -0.0190 -0.0328 -0.0113 -0.0145\n",
       " -0.0035 -0.0110 -0.0057  0.0008 -0.0148\n",
       " -0.0205 -0.0231  0.0059  0.0081 -0.0180\n",
       " -0.0194 -0.0010 -0.0043 -0.0024 -0.0125\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       " -0.0095 -0.0099 -0.0243 -0.0168  0.0040\n",
       " -0.0139 -0.0295 -0.0125 -0.0115 -0.0106\n",
       " -0.0140 -0.0079 -0.0269 -0.0076 -0.0144\n",
       " -0.0062 -0.0131  0.0047 -0.0059  0.0010\n",
       " -0.0068  0.0152 -0.0129 -0.0195 -0.0056\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       " -0.0216 -0.0113 -0.0087  0.0076 -0.0025\n",
       " -0.0283  0.0046 -0.0094  0.0076  0.0188\n",
       " -0.0136  0.0188 -0.0007 -0.0031 -0.0182\n",
       "  0.0018  0.0314 -0.0090 -0.0072  0.0076\n",
       " -0.0030  0.0034 -0.0110 -0.0048 -0.0024\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -0.0227 -0.0091  0.0084  0.0208  0.0105\n",
       " -0.0350 -0.0281 -0.0081 -0.0046 -0.0056\n",
       " -0.0109 -0.0315 -0.0129  0.0072  0.0007\n",
       " -0.0118 -0.0172 -0.0207 -0.0025  0.0148\n",
       "  0.0003 -0.0152 -0.0295  0.0014  0.0118\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  0.0150  0.0138  0.0137  0.0085  0.0069\n",
       "  0.0074  0.0108  0.0012  0.0017  0.0070\n",
       "  0.0091  0.0124  0.0074  0.0040  0.0055\n",
       " -0.0005  0.0061  0.0042  0.0014  0.0003\n",
       "  0.0038  0.0020  0.0042  0.0003 -0.0033\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       " -0.0180 -0.0027 -0.0069 -0.0228  0.0115\n",
       "  0.0087 -0.0145 -0.0168 -0.0021 -0.0084\n",
       "  0.0018 -0.0103 -0.0002  0.0095  0.0118\n",
       " -0.0029 -0.0120 -0.0177 -0.0152 -0.0119\n",
       " -0.0194 -0.0045 -0.0158 -0.0178 -0.0224\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -0.0099  0.0030  0.0054  0.0022 -0.0061\n",
       "  0.0051 -0.0039 -0.0092 -0.0068 -0.0031\n",
       "  0.0180  0.0273 -0.0091 -0.0150  0.0015\n",
       "  0.0094  0.0249 -0.0108 -0.0161  0.0055\n",
       " -0.0011 -0.0022 -0.0090  0.0026 -0.0254\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       "  0.0026  0.0023  0.0018 -0.0022  0.0005\n",
       "  0.0004 -0.0002  0.0026  0.0010  0.0011\n",
       " -0.0002  0.0002  0.0017  0.0025 -0.0012\n",
       " -0.0009  0.0018  0.0000 -0.0003 -0.0005\n",
       "  0.0008  0.0029  0.0022 -0.0017  0.0003\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       " -0.0011 -0.0023  0.0002 -0.0002 -0.0013\n",
       " -0.0012 -0.0015  0.0009 -0.0022 -0.0058\n",
       " -0.0032 -0.0008 -0.0016 -0.0021 -0.0014\n",
       "  0.0002 -0.0036 -0.0021 -0.0035 -0.0016\n",
       "  0.0005 -0.0021 -0.0020 -0.0028 -0.0032\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       "  0.0037  0.0019 -0.0008  0.0009  0.0032\n",
       "  0.0038  0.0032  0.0044  0.0030  0.0000\n",
       "  0.0025  0.0011  0.0005 -0.0009 -0.0035\n",
       "  0.0034  0.0051  0.0004 -0.0009  0.0006\n",
       "  0.0031  0.0066  0.0034  0.0001 -0.0002\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -0.0009 -0.0017 -0.0015 -0.0003 -0.0003\n",
       " -0.0028 -0.0014 -0.0011 -0.0009 -0.0005\n",
       " -0.0014 -0.0018  0.0005 -0.0010 -0.0007\n",
       " -0.0008 -0.0007 -0.0013 -0.0019 -0.0022\n",
       " -0.0007 -0.0021 -0.0008 -0.0013 -0.0001\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  0.0038  0.0055  0.0049  0.0017  0.0031\n",
       "  0.0050  0.0057  0.0023  0.0011  0.0010\n",
       "  0.0007  0.0008 -0.0007 -0.0018 -0.0036\n",
       "  0.0007  0.0052  0.0036  0.0047 -0.0001\n",
       "  0.0008  0.0014  0.0032  0.0013  0.0016\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  0.0027  0.0029  0.0038  0.0009 -0.0003\n",
       "  0.0011  0.0010 -0.0001 -0.0024 -0.0004\n",
       "  0.0011 -0.0008 -0.0031 -0.0028 -0.0005\n",
       " -0.0018 -0.0023 -0.0008  0.0022  0.0017\n",
       " -0.0027  0.0001 -0.0015  0.0023  0.0025\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       " -0.0163 -0.0056 -0.0087 -0.0157 -0.0132\n",
       " -0.0167 -0.0109 -0.0019 -0.0096 -0.0097\n",
       " -0.0118  0.0006 -0.0084 -0.0043 -0.0098\n",
       " -0.0103 -0.0084  0.0035  0.0042 -0.0070\n",
       " -0.0058  0.0004 -0.0045 -0.0039 -0.0049\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -0.0216 -0.0011 -0.0153 -0.0093 -0.0117\n",
       " -0.0232 -0.0043 -0.0020 -0.0031 -0.0139\n",
       " -0.0223 -0.0087 -0.0116 -0.0103 -0.0130\n",
       " -0.0130 -0.0003 -0.0036 -0.0162 -0.0122\n",
       " -0.0174 -0.0087  0.0022 -0.0063 -0.0168\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -0.0167 -0.0004 -0.0128 -0.0140 -0.0139\n",
       " -0.0225 -0.0131 -0.0103 -0.0056 -0.0137\n",
       " -0.0190  0.0029 -0.0124 -0.0210 -0.0194\n",
       " -0.0134  0.0006  0.0072 -0.0039 -0.0050\n",
       "  0.0061  0.0110  0.0039  0.0159  0.0136\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       " -0.0140 -0.0096 -0.0188 -0.0156 -0.0106\n",
       " -0.0113 -0.0050 -0.0059 -0.0090 -0.0152\n",
       " -0.0134 -0.0095 -0.0072 -0.0129 -0.0064\n",
       " -0.0120 -0.0013 -0.0003 -0.0133 -0.0108\n",
       " -0.0179 -0.0065 -0.0036 -0.0113 -0.0050\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -0.0127 -0.0078 -0.0087 -0.0068 -0.0012\n",
       " -0.0050 -0.0189 -0.0180 -0.0200 -0.0100\n",
       " -0.0083 -0.0118  0.0047 -0.0121 -0.0110\n",
       " -0.0054 -0.0008  0.0021  0.0105  0.0021\n",
       " -0.0168 -0.0005  0.0002 -0.0129 -0.0028\n",
       "\n",
       "(189,63 ,.,.) = \n",
       " -0.0254 -0.0152 -0.0041 -0.0163 -0.0121\n",
       " -0.0153 -0.0093 -0.0092 -0.0117 -0.0158\n",
       " -0.0104 -0.0079 -0.0071 -0.0092 -0.0093\n",
       " -0.0151 -0.0076 -0.0159 -0.0040 -0.0010\n",
       " -0.0037 -0.0088 -0.0148 -0.0088 -0.0037\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       "  0.0080  0.0084  0.0001 -0.0073 -0.0227\n",
       " -0.0082  0.0150 -0.0007 -0.0198 -0.0045\n",
       " -0.0079 -0.0206 -0.0097 -0.0123 -0.0168\n",
       " -0.0215 -0.0164 -0.0147 -0.0176 -0.0266\n",
       " -0.0285 -0.0160 -0.0233 -0.0117 -0.0205\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       " -0.0383 -0.0143  0.0003 -0.0203  0.0038\n",
       " -0.0167 -0.0216 -0.0185 -0.0238 -0.0055\n",
       " -0.0110 -0.0153 -0.0276 -0.0214 -0.0086\n",
       " -0.0012 -0.0302 -0.0133 -0.0066  0.0071\n",
       " -0.0146 -0.0169 -0.0106 -0.0047 -0.0007\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  0.0144  0.0065 -0.0132 -0.0053 -0.0029\n",
       " -0.0003  0.0124  0.0006 -0.0038 -0.0175\n",
       " -0.0096 -0.0132 -0.0102 -0.0072 -0.0036\n",
       " -0.0132 -0.0090 -0.0089 -0.0114  0.0026\n",
       " -0.0129 -0.0201 -0.0066  0.0013 -0.0142\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       " -0.0182 -0.0152 -0.0062 -0.0067 -0.0067\n",
       " -0.0042 -0.0014 -0.0004 -0.0053 -0.0104\n",
       " -0.0066 -0.0060 -0.0006  0.0002 -0.0076\n",
       " -0.0120 -0.0026 -0.0041 -0.0040 -0.0099\n",
       " -0.0118 -0.0069 -0.0012 -0.0055 -0.0093\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  0.0006  0.0098 -0.0210 -0.0128 -0.0169\n",
       "  0.0021 -0.0011  0.0040 -0.0211 -0.0226\n",
       "  0.0013  0.0028  0.0104 -0.0033 -0.0133\n",
       " -0.0198 -0.0072 -0.0112 -0.0087 -0.0255\n",
       " -0.0121  0.0040  0.0095  0.0027 -0.0146\n",
       "\n",
       "(190,63 ,.,.) = \n",
       " -0.0103  0.0020 -0.0055 -0.0109 -0.0099\n",
       " -0.0212 -0.0080 -0.0146 -0.0088 -0.0177\n",
       "  0.0007 -0.0037 -0.0117 -0.0048 -0.0104\n",
       "  0.0022 -0.0024 -0.0130  0.0136 -0.0030\n",
       "  0.0012 -0.0008  0.0102  0.0165 -0.0013\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -0.0130  0.0098 -0.0047 -0.0243 -0.0040\n",
       "  0.0308  0.0091 -0.0070 -0.0171  0.0026\n",
       "  0.0041  0.0054  0.0049 -0.0061 -0.0180\n",
       " -0.0194  0.0015 -0.0122 -0.0081 -0.0209\n",
       " -0.0033 -0.0173 -0.0211  0.0155 -0.0133\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  0.0016 -0.0458 -0.0290  0.0074 -0.0102\n",
       " -0.0196 -0.0453 -0.0218  0.0368  0.0228\n",
       " -0.0114 -0.0123 -0.0081 -0.0005  0.0057\n",
       " -0.0169  0.0045 -0.0193  0.0344  0.0030\n",
       " -0.0307 -0.0322  0.0290  0.0280  0.0128\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       " -0.0131 -0.0315 -0.0400 -0.0101 -0.0079\n",
       " -0.0121 -0.0402 -0.0180 -0.0532 -0.0330\n",
       " -0.0246 -0.0273 -0.0388 -0.0098 -0.0030\n",
       " -0.0222 -0.0063  0.0074 -0.0336 -0.0307\n",
       " -0.0198 -0.0204 -0.0155 -0.0343 -0.0071\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  0.0026  0.0041 -0.0108  0.0135  0.0073\n",
       " -0.0052  0.0062  0.0124  0.0251  0.0077\n",
       " -0.0026  0.0043  0.0019  0.0065  0.0156\n",
       " -0.0033 -0.0133  0.0058  0.0059  0.0115\n",
       " -0.0150 -0.0076  0.0024 -0.0008  0.0045\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  0.0088 -0.0082 -0.0180 -0.0274 -0.0209\n",
       " -0.0221 -0.0099 -0.0162 -0.0216 -0.0091\n",
       " -0.0189 -0.0123 -0.0014 -0.0367 -0.0449\n",
       " -0.0164 -0.0280  0.0198 -0.0062 -0.0368\n",
       " -0.0423 -0.0337 -0.0199 -0.0208 -0.0177\n",
       "\n",
       "(191,63 ,.,.) = \n",
       " -0.0267 -0.0395 -0.0140 -0.0011  0.0010\n",
       " -0.0123 -0.0253 -0.0245 -0.0256 -0.0057\n",
       "  0.0054 -0.0044 -0.0222 -0.0134 -0.0357\n",
       "  0.0078 -0.0066  0.0117 -0.0053 -0.0158\n",
       " -0.0101 -0.0183 -0.0315 -0.0158 -0.0442\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1.8670e-02  2.2519e-03  4.0718e-03 -1.2053e-02  1.5346e-02\n",
       "  7.2466e-03  2.0200e-02 -1.0133e-02 -2.1329e-03 -1.2672e-02\n",
       "  2.1838e-02 -2.0518e-02 -8.0581e-03  6.3540e-03  2.6678e-02\n",
       "  2.4403e-02 -2.0051e-02  2.2867e-02  2.3579e-03 -9.9237e-03\n",
       " -1.0284e-02  2.7908e-03 -8.8978e-03 -2.0199e-02 -2.3274e-03\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -1.7704e-03 -1.8924e-03 -1.0922e-02 -1.5883e-02 -1.8224e-03\n",
       " -1.2829e-02 -2.0913e-02  1.2217e-02  2.5780e-02 -1.2984e-04\n",
       "  2.9267e-03 -1.2853e-02 -5.1088e-03  2.1482e-02  5.8533e-03\n",
       "  2.2906e-03 -3.0871e-03 -5.7004e-03  1.4502e-02  1.4483e-02\n",
       "  1.2501e-02  1.0770e-02 -1.9770e-04  1.1162e-02 -1.2295e-02\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -8.3275e-03 -1.1672e-03  6.1495e-03 -2.6007e-02 -2.4467e-03\n",
       " -1.4519e-02 -3.3815e-03  1.6122e-02  8.5915e-04 -1.9005e-02\n",
       " -2.3674e-02  2.4657e-03 -2.3547e-02 -7.8640e-03  1.0768e-02\n",
       "  2.3381e-02 -2.2082e-02  2.2670e-02  1.4836e-02 -1.5474e-02\n",
       " -1.4576e-02 -5.6797e-03 -1.9072e-02 -2.1535e-03  4.0057e-03\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -7.3399e-03  5.5056e-03  7.6906e-03  2.1133e-02 -2.3271e-03\n",
       " -9.2341e-03 -1.3682e-02  4.1928e-03 -2.3184e-02 -1.8525e-02\n",
       "  2.5907e-02 -2.5945e-02 -1.2084e-02  1.0320e-02 -2.2313e-02\n",
       "  9.4249e-03 -4.0526e-03 -1.1258e-02  2.0531e-02  2.1654e-02\n",
       " -2.2652e-02 -1.3649e-02  1.2735e-02 -1.2371e-02 -1.1598e-02\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  2.2827e-03 -2.3022e-02 -2.1071e-02  4.2460e-03 -1.1342e-02\n",
       " -2.0824e-02 -1.9459e-02  1.2625e-02  6.2654e-03  1.3067e-02\n",
       " -1.3808e-03 -1.1724e-02 -3.0852e-03  1.6897e-02  2.2999e-02\n",
       "  5.5358e-03 -1.5501e-02 -1.7133e-02 -8.7850e-03 -1.8876e-02\n",
       " -3.9984e-03 -1.4232e-02  1.7678e-02 -4.6650e-03 -7.2793e-03\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -8.2504e-03 -1.8230e-02  2.6378e-02 -5.5046e-03 -1.0436e-02\n",
       " -1.5695e-02  1.5343e-02 -1.0780e-02  2.4873e-03 -1.5936e-02\n",
       "  2.3924e-03 -2.2019e-02 -6.1866e-03  6.0553e-04  2.2464e-02\n",
       " -6.9490e-03  1.5363e-02 -1.1436e-02  2.1486e-02 -2.7997e-03\n",
       " -2.4099e-03 -1.7311e-02 -2.1271e-02 -2.0207e-02  7.5329e-03\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  1.9105e-02 -1.6569e-02  2.2941e-02 -1.4427e-02  1.3617e-02\n",
       "  2.1528e-03 -1.3728e-02 -8.1956e-04  1.6026e-02  1.9993e-02\n",
       "  1.5530e-02  2.2236e-02  5.8993e-03  2.2328e-03  6.4849e-03\n",
       " -1.3503e-02  3.4639e-03  1.9793e-02  1.9670e-02 -4.5792e-03\n",
       " -9.5303e-03 -4.2561e-03  6.6372e-03  2.1478e-02  7.4338e-03\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  2.5559e-02  1.6592e-02 -1.6284e-02 -2.3023e-02 -7.6557e-03\n",
       "  2.4712e-02 -1.9364e-02  4.1592e-03  2.0739e-03 -1.2089e-02\n",
       "  2.0948e-03 -1.7468e-02  6.8909e-03  5.2689e-03  1.5606e-02\n",
       "  1.2102e-02 -1.7167e-02 -1.0661e-02  1.6532e-02  2.2462e-02\n",
       "  7.5405e-03 -5.8228e-03 -2.1221e-02 -1.0767e-02  1.2513e-02\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1.8566e-02  1.9733e-02 -1.7857e-04  8.4009e-03  2.3603e-02\n",
       " -2.0286e-02  1.5572e-02  1.9252e-02 -2.5934e-03 -2.4064e-02\n",
       " -3.3300e-03 -1.9621e-03  6.8216e-03 -1.0468e-02 -2.0141e-02\n",
       " -8.0692e-04  1.3554e-02 -5.5379e-03  5.0553e-03 -1.9428e-02\n",
       "  1.9428e-03  7.6142e-03 -4.9818e-03  2.0506e-02 -1.2991e-02\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  2.0637e-02  8.5368e-03 -1.3934e-02  2.2987e-02  1.7503e-02\n",
       " -1.1677e-03  2.3216e-02  3.2018e-04  2.6247e-02  1.9151e-02\n",
       " -1.8164e-02  7.4293e-03  2.2689e-02 -1.1687e-02  2.2509e-02\n",
       " -5.6037e-03 -1.3010e-02 -2.5217e-03 -1.5104e-02  5.0626e-03\n",
       " -2.1716e-03  2.8965e-03  1.6445e-02  2.0724e-02  5.6710e-03\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  2.1546e-02  1.1953e-02  1.5979e-02  2.9715e-03  1.0598e-02\n",
       "  8.8745e-03 -1.8374e-02 -1.8471e-02 -1.1304e-02  3.6013e-03\n",
       " -1.7834e-02 -5.2165e-03 -5.8118e-03 -6.3218e-03 -4.2718e-03\n",
       " -2.0055e-02  4.2288e-03 -1.9146e-02 -1.9066e-02  6.7234e-03\n",
       "  1.1631e-02 -1.8696e-02  6.5212e-03  2.2672e-02  1.0341e-02\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -1.4405e-03  3.8690e-03 -1.6284e-02  2.3745e-02 -1.4042e-02\n",
       " -9.0710e-03 -9.6603e-03 -1.6747e-02 -1.3406e-02  5.2242e-03\n",
       "  9.7888e-03  1.2946e-02  2.4013e-02 -7.3156e-04  2.0200e-02\n",
       " -2.3753e-02 -1.8835e-02 -2.2833e-02 -1.2771e-02 -8.0316e-03\n",
       " -1.2512e-02 -2.1876e-02  2.2577e-02 -5.9588e-04  1.0818e-02\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -1.2320e-02 -8.2558e-03 -1.0018e-02 -1.4543e-02  1.7418e-02\n",
       " -7.1733e-03  1.3230e-02 -1.5497e-02 -2.1842e-02  9.3744e-03\n",
       "  1.7635e-02  2.1739e-02 -9.4136e-03  8.7187e-03 -2.1039e-02\n",
       " -6.6724e-03 -1.9121e-02 -1.5147e-02 -2.4555e-02 -7.7806e-03\n",
       "  2.1512e-02  1.8051e-02  1.4592e-02 -3.8636e-03  4.4773e-03\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  1.0419e-02  1.1175e-03  1.2022e-02  6.3997e-03 -1.3627e-02\n",
       " -1.4988e-02 -2.1716e-02 -1.2959e-02 -1.3515e-02  8.5271e-03\n",
       "  7.9967e-03  1.2694e-02  2.6667e-02  1.8991e-02  9.5414e-03\n",
       "  2.6516e-02  1.6057e-02 -8.6534e-03  1.2635e-02 -1.4491e-02\n",
       "  1.5318e-02 -6.0597e-03  6.2457e-03  8.7008e-03  1.1348e-02\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1.7211e-02 -6.5809e-03  4.3338e-03 -9.2861e-03 -1.5834e-02\n",
       " -3.5359e-03 -1.6427e-02 -1.9155e-02 -9.8790e-03 -2.1348e-02\n",
       "  2.2434e-02 -2.5540e-02  1.0802e-02  5.2642e-03 -1.4406e-02\n",
       " -1.9447e-02 -1.2026e-03  6.4859e-03  1.6601e-02  1.7098e-02\n",
       "  1.5083e-02  8.2247e-03  1.2362e-02  9.8584e-03 -2.5765e-03\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1.7969e-02  1.0552e-04 -8.8331e-03  4.2374e-03  2.2378e-02\n",
       "  1.0969e-02  6.1506e-04  1.3202e-02  1.3283e-02 -1.1140e-02\n",
       " -2.2678e-03 -9.7112e-03 -1.3434e-02  2.4659e-02  2.0132e-02\n",
       " -1.4021e-02 -1.5115e-02 -2.2483e-02 -5.4310e-03  8.2147e-03\n",
       " -9.1615e-03 -1.4111e-02 -1.9775e-02 -1.9246e-02 -6.9944e-03\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1.8559e-02  1.0649e-02  2.0254e-02  1.5598e-02 -1.1228e-02\n",
       "  1.2857e-02 -1.8859e-02  2.1911e-02  2.6598e-02 -1.0640e-02\n",
       "  1.9808e-02  6.1613e-03  1.7403e-02 -9.2771e-03  6.8081e-03\n",
       " -2.6568e-03  9.1057e-03  5.0440e-03  1.4531e-02  1.7950e-02\n",
       "  5.8364e-03  2.0602e-02 -1.4418e-04 -1.6905e-02  1.1065e-02\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1.4518e-02 -2.3477e-02  6.0982e-03  1.4966e-02 -1.7902e-02\n",
       " -2.2894e-02  1.7831e-02  9.8696e-03  1.8387e-02  1.1258e-03\n",
       " -6.6692e-03  2.2610e-02 -4.6073e-03 -2.1262e-02  1.6892e-02\n",
       "  2.4691e-02 -1.9755e-02 -1.7883e-02  2.1821e-02 -1.1197e-02\n",
       "  2.4450e-02  2.3402e-02 -1.1416e-02 -9.0451e-03 -8.0720e-03\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1.8046e-02 -2.3537e-02 -1.6262e-02 -4.8368e-03 -1.5394e-02\n",
       "  1.8241e-02  1.1182e-02 -9.0376e-03 -1.8592e-02  2.1043e-02\n",
       "  2.1950e-02 -9.5889e-03  1.5639e-02  2.0315e-02  9.1451e-03\n",
       " -2.3267e-03 -1.2284e-02 -7.1756e-05 -1.0619e-02  2.0109e-02\n",
       "  4.4275e-03  1.3744e-02 -1.1429e-02 -1.6875e-02  9.7686e-03\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1.3246e-02  1.2735e-02  8.6130e-03  1.6734e-02  3.9059e-03\n",
       "  1.8977e-02  7.5253e-03 -1.0851e-02 -1.3775e-02  1.5560e-02\n",
       "  1.1163e-02  1.7825e-02 -1.1147e-02 -6.2173e-04  1.3400e-03\n",
       "  9.4686e-03 -8.1568e-03 -1.2844e-02 -1.1887e-02  2.1063e-02\n",
       "  1.3916e-03  1.6072e-02 -1.8647e-02  7.0476e-03  2.5042e-03\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1.3556e-02 -7.2452e-03 -1.7225e-02  1.1344e-02 -1.1336e-02\n",
       " -2.0284e-02 -2.0633e-02 -6.4459e-03  3.4400e-03  2.4381e-02\n",
       " -3.0923e-03 -2.0966e-02  8.9185e-03  1.4266e-02  2.4969e-02\n",
       "  7.9520e-03  1.8698e-02  2.1713e-02 -2.1202e-02 -1.0432e-02\n",
       " -2.7984e-03 -1.4452e-02  3.4756e-03  1.9380e-02 -1.2840e-02\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  1.1448e-02 -5.8377e-03  2.5046e-02  1.2264e-02 -1.3176e-03\n",
       "  8.5886e-03 -8.4665e-03  8.9383e-03  2.0220e-02 -6.3645e-03\n",
       " -2.0364e-02  1.9690e-02 -6.6556e-03  2.7742e-03 -1.1188e-02\n",
       " -2.1234e-02 -7.4628e-03  1.9806e-02  2.0683e-02 -2.1219e-02\n",
       "  2.1238e-02 -2.7510e-03  2.4952e-02 -2.3170e-02 -2.3968e-02\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -3.4050e-03 -5.1940e-03  1.1263e-02 -1.1307e-02 -4.8074e-03\n",
       "  1.8097e-03  1.4678e-02  2.2424e-02 -4.6559e-03 -6.1757e-03\n",
       "  1.2877e-02 -1.9268e-02 -5.7676e-04 -5.7181e-03  2.6044e-02\n",
       " -6.9507e-03 -8.2867e-03 -8.2182e-03 -2.0289e-02 -2.0671e-02\n",
       "  1.0301e-02  1.3992e-02 -2.1944e-02  2.6177e-02  1.3970e-02\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1.2837e-02 -1.2078e-02  1.1061e-02  4.9365e-03  1.9722e-02\n",
       "  2.5484e-02 -1.6645e-02 -2.0138e-02 -3.0087e-03 -1.2823e-02\n",
       " -2.0875e-02  7.4963e-03 -9.3639e-03  2.2183e-02 -9.6396e-03\n",
       " -8.8622e-03  1.5942e-02  1.2628e-02  1.0482e-02 -1.8276e-02\n",
       "  7.5114e-03 -1.6877e-02  1.3392e-02  1.4190e-02 -1.1644e-02\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -7.0010e-03 -9.3824e-03 -1.6193e-02  6.7654e-03  1.2865e-02\n",
       " -1.7797e-02  2.5439e-02  9.0229e-03 -3.1898e-03 -2.0862e-02\n",
       "  7.8012e-03 -1.3555e-02 -1.8953e-02 -2.6799e-04  2.6673e-03\n",
       "  2.0622e-02 -4.0580e-03  2.1546e-03  7.7087e-04 -2.0151e-02\n",
       "  1.2140e-02  2.4143e-02 -1.6828e-02 -9.6826e-04  3.7081e-04\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1.8570e-02 -4.5321e-03  5.7150e-03 -1.2961e-02  1.3622e-02\n",
       "  2.4875e-02  6.0688e-03  2.0093e-02 -2.0456e-02 -7.5649e-03\n",
       " -1.7649e-02  1.8139e-02  6.0733e-03 -1.8994e-02  2.0107e-02\n",
       " -1.9482e-02 -5.0889e-03 -1.6822e-02 -5.0680e-03 -1.1071e-02\n",
       "  2.3988e-02 -1.2587e-02 -1.1856e-02  8.4337e-03 -2.1866e-02\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  3.5241e-03 -2.7112e-03  6.2973e-03  9.7319e-03  1.8664e-02\n",
       "  2.4439e-02  1.0968e-02 -8.6145e-03 -1.5376e-02  1.7948e-02\n",
       " -1.2935e-02  2.5098e-02  5.8532e-03  1.2842e-02  2.3337e-02\n",
       " -5.3009e-04  2.1346e-02  1.2471e-03  2.2014e-03 -9.8304e-03\n",
       " -5.8872e-03 -3.7409e-03 -1.7845e-02 -1.7671e-02  1.7177e-02\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  2.8432e-03 -1.7505e-02 -1.6710e-02  1.0037e-02 -1.3251e-03\n",
       " -2.3581e-02 -1.7777e-02  1.8641e-02  1.7586e-02  2.2702e-02\n",
       " -1.0578e-02 -2.0103e-02 -2.3598e-02 -2.1876e-02  1.6998e-02\n",
       " -9.6156e-03 -1.5975e-03 -1.3788e-02  1.1007e-02  1.8400e-02\n",
       " -5.2096e-03 -2.4455e-02 -1.1717e-02  1.7118e-02 -5.6230e-03\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  9.5110e-03  7.4729e-03 -5.3286e-03 -1.6515e-02 -9.7323e-03\n",
       " -1.8635e-02 -4.1297e-03  2.0368e-03  3.0251e-03  5.5285e-03\n",
       " -4.8662e-03  1.6057e-02  1.9415e-02 -1.0821e-02  9.1630e-03\n",
       " -1.5349e-02  1.6564e-02  7.8934e-03  6.5254e-03  2.3483e-03\n",
       "  3.0808e-03 -1.6148e-02  1.6661e-02 -2.4807e-02 -1.4977e-02\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  3.0798e-03  8.1739e-03  1.0655e-02  1.2138e-02 -3.9180e-03\n",
       " -1.7254e-02 -7.5638e-03 -7.3530e-03  6.7505e-03 -1.6334e-02\n",
       " -8.5179e-03  2.0351e-02  2.0164e-02  6.0206e-03 -2.4103e-02\n",
       "  7.5062e-03 -1.4729e-02  2.4534e-02  2.2018e-02  1.8050e-02\n",
       " -1.9543e-02  1.0441e-03 -4.0711e-03  8.2021e-03 -2.2020e-02\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1.3580e-02 -9.2796e-03  2.4614e-02  2.3861e-02 -1.5630e-02\n",
       "  1.5582e-02  2.2632e-02  1.1446e-02 -1.0233e-02  2.0752e-02\n",
       "  8.9332e-03 -2.0991e-02 -1.1641e-02 -6.2087e-03 -2.1310e-02\n",
       " -5.4981e-03 -2.1777e-02  5.7550e-03  1.4263e-02  1.4705e-02\n",
       "  8.4598e-03 -2.3380e-02  1.4696e-02  5.5077e-03 -2.1390e-02\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  3.1032e-03  1.6355e-02  2.4870e-02 -1.4955e-02  5.3029e-03\n",
       " -1.5089e-02 -2.9769e-03  4.3013e-03 -2.5432e-02  5.7325e-03\n",
       " -1.5201e-02 -2.1835e-02  2.0351e-02  1.0961e-02  4.5630e-03\n",
       "  1.4695e-02  2.8762e-03  1.7635e-02 -3.7575e-03 -9.2043e-05\n",
       " -1.6739e-02 -1.3173e-02 -7.2054e-03  2.2961e-02 -2.8976e-03\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1.3310e-02  1.1999e-02  1.2588e-02  2.3936e-02 -2.8494e-03\n",
       " -1.9326e-02 -6.9395e-03  1.4118e-02  2.2814e-02  1.0237e-02\n",
       "  1.1986e-02  8.3442e-03 -1.6309e-02  9.8632e-03  2.7161e-02\n",
       " -1.2876e-02  1.4518e-02 -7.6771e-03 -1.3956e-03  1.8585e-02\n",
       " -4.9774e-03  6.4292e-03  1.8809e-02 -1.9363e-02  3.4013e-03\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  1.2765e-04  8.6574e-03 -8.3671e-03 -2.5633e-02 -1.9983e-02\n",
       " -2.2185e-02 -2.0584e-02  1.5347e-02  6.2846e-04  2.0257e-02\n",
       " -5.5350e-03 -1.2754e-02 -1.3388e-02  1.8476e-02  2.0015e-02\n",
       " -4.9486e-03  1.3905e-02  1.5753e-02 -1.7002e-02  4.0364e-03\n",
       " -2.0980e-02  1.5303e-02  2.1979e-03  1.5153e-02  5.8710e-03\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1.6153e-02 -2.4644e-03  6.8616e-04  2.2691e-03 -1.0124e-02\n",
       " -1.9592e-02  5.9084e-03  8.0661e-03 -2.3547e-03  6.6889e-03\n",
       "  2.2338e-02 -2.5033e-02  6.2770e-03 -1.5753e-02  2.3221e-02\n",
       " -2.4655e-06  4.3782e-03 -2.0854e-02  5.5371e-03  2.3293e-03\n",
       "  4.5042e-03  2.1750e-02  1.4522e-02 -1.4266e-02  1.2989e-02\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1.9934e-02 -1.1313e-02  1.4127e-02 -2.0543e-02  6.3847e-03\n",
       "  1.9417e-02  2.6635e-02  3.5149e-04 -2.4509e-02 -2.0893e-02\n",
       " -1.4504e-02 -2.2117e-03  3.7037e-03  1.8773e-02 -1.6588e-02\n",
       " -7.6201e-03  8.2898e-03  4.6790e-03 -4.6200e-03 -8.1411e-03\n",
       "  7.9393e-03  1.4588e-02 -1.4134e-02 -5.0342e-03  6.5145e-03\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.binarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1  1  1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1  1  1 -1\n",
       " -1  1 -1 -1 -1\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -1 -1 -1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1 -1  1  1\n",
       "  1  1 -1  1 -1\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1  1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -1  1  1  1 -1\n",
       " -1 -1  1 -1 -1\n",
       "  1 -1 -1  1 -1\n",
       "  1 -1 -1  1  1\n",
       " -1 -1  1 -1 -1\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  1 -1 -1  1 -1\n",
       " -1 -1  1  1  1\n",
       " -1 -1 -1  1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1 -1  1 -1 -1\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       " -1  1 -1  1 -1\n",
       "  1 -1 -1 -1  1\n",
       " -1  1 -1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1 -1 -1  1  1\n",
       "  1  1  1  1  1\n",
       " -1  1  1  1 -1\n",
       " -1 -1  1  1  1\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       "  1 -1  1  1 -1\n",
       "  1 -1  1  1  1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1 -1 -1  1\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1  1  1  1  1\n",
       " -1  1  1 -1 -1\n",
       " -1 -1  1 -1 -1\n",
       "  1  1 -1  1 -1\n",
       "  1  1 -1  1 -1\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  1  1 -1  1  1\n",
       " -1  1 -1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1  1  1  1  1\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  1  1  1  1  1\n",
       "  1 -1 -1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1  1  1  1\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -1  1 -1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "  1  1  1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       " -1 -1  1 -1  1\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -1 -1 -1 -1  1\n",
       " -1  1 -1 -1  1\n",
       "  1  1 -1  1 -1\n",
       " -1 -1 -1 -1 -1\n",
       "  1  1  1 -1  1\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "  1  1  1  1  1\n",
       "  1  1 -1  1 -1\n",
       "  1 -1  1  1  1\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       " -1 -1 -1 -1 -1\n",
       "  1 -1  1  1 -1\n",
       " -1  1  1  1  1\n",
       "  1  1  1  1 -1\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1 -1 -1  1  1\n",
       "  1 -1  1  1 -1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       "  1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "  1  1 -1 -1  1\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1  1  1  1 -1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1 -1  1 -1\n",
       "  1  1 -1 -1 -1\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1 -1 -1 -1 -1\n",
       "  1  1 -1 -1  1\n",
       "  1 -1  1  1  1\n",
       " -1 -1  1 -1  1\n",
       "  1  1 -1 -1  1\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1  1  1  1  1\n",
       "  1  1 -1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       "  1 -1 -1 -1  1\n",
       " -1  1 -1  1  1\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1 -1 -1  1 -1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1  1  1  1\n",
       "  1  1  1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       " -1  1 -1  1 -1\n",
       " -1 -1  1  1 -1\n",
       "  1 -1  1 -1 -1\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       "  1  1  1 -1 -1\n",
       "  1 -1 -1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "  1  1 -1  1  1\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1 -1  1  1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1  1 -1  1 -1\n",
       " -1  1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -1 -1 -1  1  1\n",
       " -1  1  1 -1 -1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1  1  1 -1\n",
       "  1  1 -1 -1 -1\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1  1  1 -1 -1\n",
       " -1  1  1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "  1 -1 -1  1 -1\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  1 -1  1  1  1\n",
       "  1  1 -1 -1  1\n",
       " -1  1  1  1  1\n",
       " -1  1 -1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  1 -1 -1  1 -1\n",
       " -1 -1  1  1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1 -1  1 -1\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "  1 -1  1 -1 -1\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       " -1 -1 -1  1 -1\n",
       " -1  1  1  1 -1\n",
       "  1 -1  1  1  1\n",
       " -1 -1 -1  1 -1\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1 -1  1  1 -1\n",
       "  1  1  1 -1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       "  1 -1  1  1 -1\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  1  1  1 -1  1\n",
       " -1 -1  1 -1  1\n",
       " -1 -1  1  1  1\n",
       "  1  1  1 -1  1\n",
       " -1 -1 -1  1 -1\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       " -1 -1  1  1  1\n",
       "  1  1 -1  1  1\n",
       " -1  1 -1 -1  1\n",
       " -1  1  1 -1  1\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       " -1 -1 -1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1  1  1 -1  1\n",
       "  1 -1  1 -1  1\n",
       "  1  1 -1  1  1\n",
       "  1  1  1 -1  1\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       " -1  1  1 -1 -1\n",
       "  1  1 -1 -1  1\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1.8873e-02  7.5993e-04  2.5151e-03 -1.3646e-02  1.3772e-02\n",
       "  7.8605e-03  1.8820e-02 -1.1640e-02 -3.6067e-03 -1.2481e-02\n",
       "  2.2044e-02 -2.1994e-02 -9.5977e-03  4.8411e-03  2.5192e-02\n",
       "  2.3820e-02 -2.0491e-02  2.1387e-02  7.5171e-04 -1.1523e-02\n",
       " -1.1641e-02  2.2801e-03 -7.4655e-03 -2.1435e-02 -3.9254e-03\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -1.7088e-03 -3.4021e-03 -1.2472e-02 -1.4329e-02 -4.8686e-04\n",
       " -1.4436e-02 -2.2507e-02  1.0649e-02  2.4689e-02 -1.6210e-03\n",
       "  1.4775e-03 -1.4429e-02 -4.9556e-03  2.1881e-02  4.2458e-03\n",
       "  8.3148e-04 -4.5646e-03 -5.5588e-03  1.4497e-02  1.4675e-02\n",
       "  1.3019e-02  1.1046e-02 -9.1878e-05  1.0265e-02 -1.0822e-02\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -9.8510e-03 -2.4725e-04  7.3819e-03 -2.4469e-02 -8.3953e-04\n",
       " -1.4348e-02 -2.9021e-03  1.7654e-02  1.1927e-03 -1.9339e-02\n",
       " -2.3239e-02  2.8028e-03 -2.5049e-02 -9.4663e-03  1.1083e-02\n",
       "  2.2274e-02 -2.0638e-02  2.1082e-02  1.3878e-02 -1.6996e-02\n",
       " -1.4111e-02 -4.2093e-03 -1.9636e-02 -7.8934e-04  3.7102e-03\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -6.3411e-03  4.0243e-03  7.9108e-03  2.1537e-02 -1.1378e-03\n",
       " -1.0838e-02 -1.5243e-02  4.3175e-03 -2.3765e-02 -1.7476e-02\n",
       "  2.4465e-02 -2.5725e-02 -1.1770e-02  1.0681e-02 -2.1020e-02\n",
       "  9.9313e-03 -4.1354e-03 -1.0405e-02  2.0778e-02  2.3030e-02\n",
       " -2.1142e-02 -1.2516e-02  1.3223e-02 -1.1810e-02 -1.0359e-02\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  3.8911e-03 -2.1435e-02 -1.9913e-02  5.3589e-03 -1.0109e-02\n",
       " -2.0404e-02 -1.8715e-02  1.3386e-02  7.7525e-03  1.3887e-02\n",
       " -3.9169e-04 -1.0079e-02 -3.6061e-03  1.8469e-02  2.3778e-02\n",
       "  4.0349e-03 -1.6948e-02 -1.8247e-02 -1.0364e-02 -1.7491e-02\n",
       " -3.6643e-03 -1.5824e-02  1.9101e-02 -3.2313e-03 -8.2563e-03\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -7.7283e-03 -1.9673e-02  2.4791e-02 -7.1117e-03 -1.1897e-02\n",
       " -1.5557e-02  1.5443e-02 -1.2357e-02  9.2839e-04 -1.7495e-02\n",
       "  7.8128e-04 -2.3618e-02 -7.7252e-03 -6.8038e-04  2.0925e-02\n",
       " -6.7971e-03  1.5516e-02 -1.2196e-02  2.0723e-02 -4.2917e-03\n",
       " -1.9835e-03 -1.6046e-02 -2.0495e-02 -2.1791e-02  8.1017e-03\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  1.9844e-02 -1.8156e-02  2.1395e-02 -1.6033e-02  1.5197e-02\n",
       "  2.7841e-03 -1.5253e-02 -2.4095e-03  1.4480e-02  1.8496e-02\n",
       "  1.6425e-02  2.0631e-02  4.4349e-03  6.2910e-04  4.9091e-03\n",
       " -1.2448e-02  4.2352e-03  2.1354e-02  1.9868e-02 -5.8433e-03\n",
       " -8.6355e-03 -2.7518e-03  7.1805e-03  2.1675e-02  8.1735e-03\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  2.4088e-02  1.4989e-02 -1.7872e-02 -2.3818e-02 -9.2239e-03\n",
       "  2.3243e-02 -2.0313e-02  2.6963e-03  1.8287e-03 -1.2372e-02\n",
       "  2.2804e-03 -1.7609e-02  5.4868e-03  3.6703e-03  1.4207e-02\n",
       "  1.0992e-02 -1.7439e-02 -1.2247e-02  1.4933e-02  2.1778e-02\n",
       "  6.0057e-03 -6.8270e-03 -2.2818e-02 -1.2310e-02  1.0997e-02\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1.8421e-02  2.0306e-02  1.4248e-03  8.2208e-03  2.5037e-02\n",
       " -2.0123e-02  1.5769e-02  1.9638e-02 -2.0141e-03 -2.3663e-02\n",
       " -2.8459e-03 -1.6847e-03  5.2134e-03 -8.8668e-03 -1.8751e-02\n",
       "  1.0144e-04  1.3982e-02 -7.1225e-03  3.6198e-03 -2.0225e-02\n",
       "  3.2762e-03  6.2192e-03 -6.4182e-03  1.9305e-02 -1.3289e-02\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  2.0434e-02  9.9912e-03 -1.4121e-02  2.2485e-02  1.6849e-02\n",
       " -1.5295e-03  2.3024e-02 -8.5548e-04  2.5132e-02  1.8502e-02\n",
       " -1.6641e-02  8.9460e-03  2.2402e-02 -1.2497e-02  2.1799e-02\n",
       " -4.3779e-03 -1.3508e-02 -3.4915e-03 -1.6341e-02  3.7628e-03\n",
       " -3.0543e-03  2.8612e-03  1.6013e-02  1.9434e-02  4.1950e-03\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  2.1895e-02  1.3150e-02  1.6681e-02  3.3286e-03  1.2173e-02\n",
       "  1.0443e-02 -1.7757e-02 -1.8114e-02 -1.0106e-02  4.1655e-03\n",
       " -1.6453e-02 -4.7237e-03 -7.1833e-03 -6.5593e-03 -2.6685e-03\n",
       " -1.8904e-02  2.8176e-03 -2.0575e-02 -1.8630e-02  7.1756e-03\n",
       "  1.2020e-02 -2.0156e-02  4.9785e-03  2.1261e-02  1.0514e-02\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -8.6376e-04  3.0061e-03 -1.7313e-02  2.2512e-02 -1.5496e-02\n",
       " -9.8457e-03 -1.1124e-02 -1.8353e-02 -1.4898e-02  3.7646e-03\n",
       "  1.1305e-02  1.2791e-02  2.2407e-02 -2.3119e-03  1.8938e-02\n",
       " -2.2167e-02 -1.8997e-02 -2.2362e-02 -1.4289e-02 -8.3647e-03\n",
       " -1.1326e-02 -2.3428e-02  2.2928e-02 -1.6584e-03  9.6283e-03\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -1.0837e-02 -8.4032e-03 -8.4152e-03 -1.6011e-02  1.6192e-02\n",
       " -8.2568e-03  1.1776e-02 -1.3968e-02 -2.2808e-02  8.2840e-03\n",
       "  1.8076e-02  2.0721e-02 -7.8200e-03  1.0179e-02 -2.2619e-02\n",
       " -6.1042e-03 -1.9702e-02 -1.3823e-02 -2.3316e-02 -6.7515e-03\n",
       "  2.3047e-02  1.9652e-02  1.6191e-02 -3.8290e-03  5.9550e-03\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  8.9714e-03 -4.3575e-04  1.0732e-02  5.0587e-03 -1.5067e-02\n",
       " -1.6431e-02 -2.3202e-02 -1.4124e-02 -1.5050e-02  6.9236e-03\n",
       "  6.4380e-03  1.1278e-02  2.5105e-02  1.7465e-02  8.0542e-03\n",
       "  2.5215e-02  1.4459e-02 -1.0167e-02  1.1048e-02 -1.5960e-02\n",
       "  1.4056e-02 -7.5631e-03  4.7301e-03  7.1624e-03  9.8070e-03\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1.5609e-02 -5.0260e-03  5.1248e-03 -7.7080e-03 -1.4386e-02\n",
       " -1.9576e-03 -1.4872e-02 -1.7688e-02 -8.2844e-03 -2.0018e-02\n",
       "  2.3991e-02 -2.4003e-02  1.2204e-02  6.4748e-03 -1.3651e-02\n",
       " -1.7971e-02  3.1873e-04  7.9283e-03  1.7656e-02  1.8601e-02\n",
       "  1.6685e-02  9.7989e-03  1.3949e-02  1.1193e-02 -1.2905e-03\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1.9387e-02 -1.4122e-03 -1.0318e-02  2.8818e-03  2.1019e-02\n",
       "  9.4458e-03 -8.6057e-04  1.1739e-02  1.1866e-02 -1.2514e-02\n",
       " -3.7262e-03 -1.1232e-02 -1.4673e-02  2.3204e-02  1.8730e-02\n",
       " -1.5420e-02 -1.6523e-02 -2.3954e-02 -6.9532e-03  6.6762e-03\n",
       " -1.0545e-02 -1.5625e-02 -2.1178e-02 -2.0686e-02 -8.3262e-03\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1.8041e-02  1.0192e-02  2.0092e-02  1.4693e-02 -1.1757e-02\n",
       "  1.2645e-02 -1.9376e-02  2.1135e-02  2.5638e-02 -1.1559e-02\n",
       "  1.8662e-02  4.9249e-03  1.5992e-02 -1.0858e-02  5.2174e-03\n",
       " -3.8190e-03  8.5228e-03  4.3064e-03  1.4139e-02  1.6611e-02\n",
       "  4.6332e-03  1.9598e-02 -8.5756e-04 -1.7695e-02  1.0517e-02\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1.3799e-02 -2.4039e-02  5.5593e-03  1.3921e-02 -1.9278e-02\n",
       " -2.3463e-02  1.6960e-02  8.4596e-03  1.6871e-02 -2.8590e-04\n",
       " -5.2175e-03  2.1009e-02 -4.3725e-03 -2.0503e-02  1.7933e-02\n",
       "  2.3126e-02 -2.1178e-02 -1.6994e-02  2.3269e-02 -1.1542e-02\n",
       "  2.4701e-02  2.4752e-02 -1.0434e-02 -7.4703e-03 -8.2170e-03\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1.6613e-02 -2.2886e-02 -1.5885e-02 -6.2669e-03 -1.5019e-02\n",
       "  1.8385e-02  1.1809e-02 -7.8820e-03 -2.0193e-02  1.9612e-02\n",
       "  2.2217e-02 -8.1810e-03  1.6292e-02  2.1053e-02  7.5604e-03\n",
       " -1.3824e-03 -1.1841e-02  1.5223e-03 -9.0697e-03  2.0951e-02\n",
       "  5.3791e-03  1.5134e-02 -1.0324e-02 -1.5756e-02  1.0784e-02\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1.4812e-02  1.1365e-02  7.0708e-03  1.5169e-02  2.3029e-03\n",
       "  1.7384e-02  6.0333e-03 -1.2281e-02 -1.5213e-02  1.3971e-02\n",
       "  9.5597e-03  1.6260e-02 -1.2741e-02 -2.2209e-03 -2.6418e-04\n",
       "  7.8888e-03 -9.5033e-03 -1.4275e-02 -1.3385e-02  1.9459e-02\n",
       " -2.1158e-04  1.4506e-02 -1.9797e-02  5.5011e-03  9.0219e-04\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1.3096e-02 -5.9467e-03 -1.6514e-02  1.1632e-02 -1.1013e-02\n",
       " -1.9806e-02 -1.9726e-02 -5.9735e-03  1.9682e-03  2.2786e-02\n",
       " -2.6277e-03 -2.0994e-02  9.5077e-03  1.4894e-02  2.5259e-02\n",
       "  8.5419e-03  2.0143e-02  2.3317e-02 -1.9999e-02 -9.3312e-03\n",
       " -1.2521e-03 -1.2848e-02  5.0678e-03  1.9196e-02 -1.3103e-02\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  9.8438e-03 -7.4413e-03  2.3466e-02  1.0664e-02 -2.8734e-03\n",
       "  6.9859e-03 -9.9863e-03  7.4275e-03  1.8623e-02 -7.9618e-03\n",
       " -2.1956e-02  1.8136e-02 -8.1548e-03  1.1823e-03 -1.2700e-02\n",
       " -2.2834e-02 -8.8400e-03  1.8479e-02  1.9105e-02 -2.2790e-02\n",
       "  1.9659e-02 -4.2960e-03  2.3484e-02 -2.4766e-02 -2.5474e-02\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -4.9824e-03 -4.5596e-03  1.1411e-02 -1.2851e-02 -6.2358e-03\n",
       "  2.3359e-04  1.3211e-02  2.2690e-02 -4.5226e-03 -7.7814e-03\n",
       "  1.1322e-02 -2.0863e-02 -8.8313e-04 -4.9782e-03  2.4438e-02\n",
       " -8.4262e-03 -9.6739e-03 -9.4382e-03 -2.0687e-02 -2.1918e-02\n",
       "  8.7239e-03  1.2602e-02 -2.3247e-02  2.4590e-02  1.2570e-02\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1.3130e-02 -1.1730e-02  9.4904e-03  3.3437e-03  2.0266e-02\n",
       "  2.5777e-02 -1.6381e-02 -2.1684e-02 -2.9011e-03 -1.2213e-02\n",
       " -2.0652e-02  5.9133e-03 -1.0956e-02  2.0686e-02 -1.1243e-02\n",
       " -8.3474e-03  1.6375e-02  1.1138e-02  8.8956e-03 -1.9677e-02\n",
       "  5.9986e-03 -1.8478e-02  1.3713e-02  1.4692e-02 -1.3158e-02\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -8.0487e-03 -1.0383e-02 -1.7508e-02  5.1739e-03  1.1359e-02\n",
       " -1.9351e-02  2.4760e-02  1.0242e-02 -2.8771e-03 -2.0277e-02\n",
       "  6.2038e-03 -1.5092e-02 -1.8525e-02  6.3524e-04  2.8401e-03\n",
       "  1.9019e-02 -5.6514e-03  5.9684e-04  1.4458e-03 -2.1526e-02\n",
       "  1.0633e-02  2.2571e-02 -1.8400e-02 -2.5662e-03 -1.1753e-03\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1.8742e-02 -4.2124e-03  7.0572e-03 -1.2788e-02  1.5176e-02\n",
       "  2.5394e-02  6.3467e-03  2.0304e-02 -2.0105e-02 -6.6959e-03\n",
       " -1.7265e-02  1.8306e-02  4.6403e-03 -1.8812e-02  1.8595e-02\n",
       " -1.8350e-02 -4.8846e-03 -1.6297e-02 -4.4348e-03 -9.5267e-03\n",
       "  2.4573e-02 -1.1860e-02 -1.1273e-02  9.1719e-03 -2.0593e-02\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  2.6202e-03 -3.6839e-03  4.8600e-03  9.7666e-03  1.8810e-02\n",
       "  2.3102e-02  1.0707e-02 -9.9029e-03 -1.6966e-02  1.6515e-02\n",
       " -1.4534e-02  2.3663e-02  4.2590e-03  1.1319e-02  2.1933e-02\n",
       " -2.1198e-03  1.9748e-02 -2.9550e-04  6.2981e-04 -1.1024e-02\n",
       " -7.4889e-03 -5.3269e-03 -1.9380e-02 -1.6034e-02  1.5598e-02\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  3.0331e-03 -1.7037e-02 -1.5772e-02  1.0483e-02 -5.3627e-04\n",
       " -2.2567e-02 -1.6605e-02  1.9917e-02  1.8586e-02  2.3428e-02\n",
       " -1.0008e-02 -1.9490e-02 -2.2409e-02 -2.0533e-02  1.7565e-02\n",
       " -1.0984e-02 -1.1160e-03 -1.3157e-02  1.1969e-02  1.9088e-02\n",
       " -4.9069e-03 -2.3842e-02 -1.0942e-02  1.7978e-02 -5.1324e-03\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  8.2246e-03  6.9526e-03 -6.9166e-03 -1.8090e-02 -1.1295e-02\n",
       " -1.9810e-02 -5.5010e-03  1.1380e-03  1.5770e-03  4.1188e-03\n",
       " -6.0445e-03  1.4983e-02  1.8859e-02 -9.6841e-03  9.4751e-03\n",
       " -1.5141e-02  1.5054e-02  6.3377e-03  7.0526e-03  2.6551e-03\n",
       "  3.7987e-03 -1.7149e-02  1.6598e-02 -2.3244e-02 -1.4149e-02\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  1.5287e-03  7.9543e-03  1.1450e-02  1.3012e-02 -3.4372e-03\n",
       " -1.8834e-02 -9.1617e-03 -7.1692e-03  5.2180e-03 -1.7775e-02\n",
       " -9.8081e-03  1.8798e-02  2.0886e-02  6.2479e-03 -2.3974e-02\n",
       "  9.1216e-03 -1.6312e-02  2.3242e-02  2.3447e-02  1.9169e-02\n",
       " -2.0810e-02 -3.0732e-04 -4.9543e-03  7.5026e-03 -2.3655e-02\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1.5051e-02 -1.0335e-02  2.2998e-02  2.2260e-02 -1.7052e-02\n",
       "  1.5021e-02  2.2429e-02  9.9356e-03 -1.1826e-02  1.9894e-02\n",
       "  7.7142e-03 -2.2123e-02 -1.2642e-02 -7.6788e-03 -2.2815e-02\n",
       " -7.0753e-03 -2.2778e-02  5.9357e-03  1.2794e-02  1.3117e-02\n",
       "  7.0572e-03 -2.3037e-02  1.5274e-02  4.4500e-03 -2.1206e-02\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  4.4512e-03  1.6963e-02  2.5723e-02 -1.3546e-02  6.4457e-03\n",
       " -1.4223e-02 -2.3333e-03  5.1215e-03 -2.3841e-02  7.2729e-03\n",
       " -1.4190e-02 -2.0731e-02  2.1510e-02  1.2276e-02  5.9443e-03\n",
       "  1.5512e-02  4.3072e-03  1.8506e-02 -2.1582e-03  1.2703e-03\n",
       " -1.6196e-02 -1.2608e-02 -5.6047e-03  2.4539e-02 -1.4074e-03\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1.1795e-02  1.0422e-02  1.0993e-02  2.2456e-02 -4.2983e-03\n",
       " -2.0798e-02 -8.5427e-03  1.2548e-02  2.1261e-02  8.6331e-03\n",
       "  1.0449e-02  6.7424e-03 -1.7902e-02  8.3656e-03  2.5670e-02\n",
       " -1.4413e-02  1.3048e-02 -8.9081e-03 -2.9853e-03  1.6999e-02\n",
       " -6.5839e-03  4.9066e-03  1.7252e-02 -2.0893e-02  1.8474e-03\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  1.4847e-03  1.0025e-02 -7.1674e-03 -2.4185e-02 -1.8570e-02\n",
       " -2.0945e-02 -1.9179e-02  1.6823e-02  2.1900e-03  2.1675e-02\n",
       " -4.2502e-03 -1.1380e-02 -1.2043e-02  1.9862e-02  2.1516e-02\n",
       " -3.7076e-03  1.5002e-02  1.7142e-02 -1.5614e-02  5.4898e-03\n",
       " -1.9932e-02  1.6506e-02  3.5464e-03  1.6460e-02  7.2486e-03\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1.7563e-02 -1.2354e-03  1.7307e-03  3.2241e-03 -9.1307e-03\n",
       " -1.8913e-02  7.0451e-03  8.9872e-03 -1.2460e-03  7.9187e-03\n",
       "  2.3057e-02 -2.3899e-02  7.5769e-03 -1.4825e-02  2.3900e-02\n",
       "  1.0676e-03  5.4388e-03 -1.9349e-02  6.7580e-03  3.2281e-03\n",
       "  5.0478e-03  2.2738e-02  1.5657e-02 -1.3144e-02  1.4036e-02\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1.8393e-02 -1.2916e-02  1.4268e-02 -1.9248e-02  5.1454e-03\n",
       "  2.0311e-02  2.5032e-02 -1.2503e-03 -2.3538e-02 -1.9703e-02\n",
       " -1.5343e-02 -3.6563e-03  4.3335e-03  1.9407e-02 -1.6091e-02\n",
       " -6.0439e-03  9.1570e-03  6.2726e-03 -3.6989e-03 -7.3185e-03\n",
       "  6.3458e-03  1.5584e-02 -1.3631e-02 -4.4525e-03  6.8851e-03\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1.weight.grad.data\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(189,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> ('', VGG(\n",
      "  (first_feature): Sequential(\n",
      "    (0): Conv2d (3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  )\n",
      "  (last_layer): Sequential(\n",
      "    (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Linear(in_features=4096, out_features=1000)\n",
      "    (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (1): BinConv(\n",
      "      (ConvLayer): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): BinActive(\n",
      "        )\n",
      "        (2): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (3): BinConv(\n",
      "      (ConvLayer): Sequential(\n",
      "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): BinActive(\n",
      "        )\n",
      "        (2): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (4): BinConv(\n",
      "      (ConvLayer): Sequential(\n",
      "        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): BinActive(\n",
      "        )\n",
      "        (2): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (5): BinConv(\n",
      "      (ConvLayer): Sequential(\n",
      "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): BinActive(\n",
      "        )\n",
      "        (2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (8): BinActive(\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=9216, out_features=4096)\n",
      "    (1): BinConv(\n",
      "      (ConvLayer): Sequential(\n",
      "        (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): BinActive(\n",
      "        )\n",
      "        (2): Linear(in_features=4096, out_features=4096)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "))\n",
      "1 -> ('first_feature', Sequential(\n",
      "  (0): Conv2d (3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "))\n",
      "2 -> ('first_feature.0', Conv2d (3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)))\n",
      "3 -> ('last_layer', Sequential(\n",
      "  (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Linear(in_features=4096, out_features=1000)\n",
      "  (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
      "))\n",
      "4 -> ('last_layer.0', BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True))\n",
      "5 -> ('last_layer.1', BinActive(\n",
      "))\n",
      "6 -> ('last_layer.2', Linear(in_features=4096, out_features=1000))\n",
      "7 -> ('last_layer.3', BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True))\n",
      "8 -> ('features', Sequential(\n",
      "  (0): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "  (1): BinConv(\n",
      "    (ConvLayer): Sequential(\n",
      "      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): BinActive(\n",
      "      )\n",
      "      (2): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "  (3): BinConv(\n",
      "    (ConvLayer): Sequential(\n",
      "      (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): BinActive(\n",
      "      )\n",
      "      (2): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (4): BinConv(\n",
      "    (ConvLayer): Sequential(\n",
      "      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): BinActive(\n",
      "      )\n",
      "      (2): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (5): BinConv(\n",
      "    (ConvLayer): Sequential(\n",
      "      (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): BinActive(\n",
      "      )\n",
      "      (2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (6): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "  (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (8): BinActive(\n",
      "  )\n",
      "))\n",
      "9 -> ('features.0', MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1)))\n",
      "10 -> ('features.1', BinConv(\n",
      "  (ConvLayer): Sequential(\n",
      "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "))\n",
      "11 -> ('features.1.ConvLayer', Sequential(\n",
      "  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "))\n",
      "12 -> ('features.1.ConvLayer.0', BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True))\n",
      "13 -> ('features.1.ConvLayer.1', BinActive(\n",
      "))\n",
      "14 -> ('features.1.ConvLayer.2', Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)))\n",
      "15 -> ('features.2', MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1)))\n",
      "16 -> ('features.3', BinConv(\n",
      "  (ConvLayer): Sequential(\n",
      "    (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "))\n",
      "17 -> ('features.3.ConvLayer', Sequential(\n",
      "  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "))\n",
      "18 -> ('features.3.ConvLayer.0', BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True))\n",
      "19 -> ('features.3.ConvLayer.1', BinActive(\n",
      "))\n",
      "20 -> ('features.3.ConvLayer.2', Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
      "21 -> ('features.4', BinConv(\n",
      "  (ConvLayer): Sequential(\n",
      "    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "))\n",
      "22 -> ('features.4.ConvLayer', Sequential(\n",
      "  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "))\n",
      "23 -> ('features.4.ConvLayer.0', BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True))\n",
      "24 -> ('features.4.ConvLayer.1', BinActive(\n",
      "))\n",
      "25 -> ('features.4.ConvLayer.2', Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
      "26 -> ('features.5', BinConv(\n",
      "  (ConvLayer): Sequential(\n",
      "    (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "))\n",
      "27 -> ('features.5.ConvLayer', Sequential(\n",
      "  (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "))\n",
      "28 -> ('features.5.ConvLayer.0', BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True))\n",
      "29 -> ('features.5.ConvLayer.1', BinActive(\n",
      "))\n",
      "30 -> ('features.5.ConvLayer.2', Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
      "31 -> ('features.6', MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1)))\n",
      "32 -> ('features.7', BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True))\n",
      "33 -> ('features.8', BinActive(\n",
      "))\n",
      "34 -> ('classifier', Sequential(\n",
      "  (0): Linear(in_features=9216, out_features=4096)\n",
      "  (1): BinConv(\n",
      "    (ConvLayer): Sequential(\n",
      "      (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): BinActive(\n",
      "      )\n",
      "      (2): Linear(in_features=4096, out_features=4096)\n",
      "    )\n",
      "  )\n",
      "))\n",
      "35 -> ('classifier.0', Linear(in_features=9216, out_features=4096))\n",
      "36 -> ('classifier.1', BinConv(\n",
      "  (ConvLayer): Sequential(\n",
      "    (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Linear(in_features=4096, out_features=4096)\n",
      "  )\n",
      "))\n",
      "37 -> ('classifier.1.ConvLayer', Sequential(\n",
      "  (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Linear(in_features=4096, out_features=4096)\n",
      "))\n",
      "38 -> ('classifier.1.ConvLayer.0', BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True))\n",
      "39 -> ('classifier.1.ConvLayer.1', BinActive(\n",
      "))\n",
      "40 -> ('classifier.1.ConvLayer.2', Linear(in_features=4096, out_features=4096))\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(model.named_modules()):\n",
    "    print(i, '->', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (first_feature): Sequential(\n",
       "    (0): Conv2d (3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "  )\n",
       "  (last_layer): Sequential(\n",
       "    (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (1): BinActive(\n",
       "    )\n",
       "    (2): Linear(in_features=4096, out_features=1000)\n",
       "    (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (features): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (1): BinConv(\n",
       "      (ConvLayer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (1): BinActive(\n",
       "        )\n",
       "        (2): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (3): BinConv(\n",
       "      (ConvLayer): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (1): BinActive(\n",
       "        )\n",
       "        (2): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (4): BinConv(\n",
       "      (ConvLayer): Sequential(\n",
       "        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (1): BinActive(\n",
       "        )\n",
       "        (2): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): BinConv(\n",
       "      (ConvLayer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (1): BinActive(\n",
       "        )\n",
       "        (2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (8): BinActive(\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=9216, out_features=4096)\n",
       "    (1): BinConv(\n",
       "      (ConvLayer): Sequential(\n",
       "        (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (1): BinActive(\n",
       "        )\n",
       "        (2): Linear(in_features=4096, out_features=4096)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv0 = model.first_feature[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "binconv0 = model.features[1]\n",
    "conv1 = binconv0.ConvLayer[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "  3.2999e-02 -1.0853e-02  1.1804e-02  ...  -4.1577e-02 -4.5362e-02 -3.6687e-02\n",
       " -3.4907e-02  4.2962e-02  1.5821e-02  ...   1.2582e-02 -1.1026e-02  2.7164e-02\n",
       "  3.2744e-02  4.2307e-02 -4.9959e-02  ...   1.2236e-02  3.4432e-02 -2.2982e-02\n",
       "                 ...                                      ...                \n",
       " -2.3553e-02 -1.7643e-02  8.8369e-03  ...   1.0509e-02 -4.5603e-03 -1.2793e-02\n",
       "  3.6100e-02  1.9320e-02 -4.3570e-03  ...   3.2710e-02  4.9394e-02  3.6253e-02\n",
       " -1.5289e-02  1.6344e-02 -1.5270e-02  ...  -8.6423e-03 -4.2889e-02  4.5982e-02\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       " -2.7279e-02  5.1558e-02 -4.3274e-02  ...   4.1317e-02  1.3197e-02  3.0461e-02\n",
       " -3.9469e-02  1.6336e-02 -3.3295e-02  ...  -1.9971e-02  1.6847e-02  1.2991e-02\n",
       "  3.8012e-02  3.1919e-02  2.9151e-02  ...  -2.0594e-02 -1.5257e-02  1.7365e-02\n",
       "                 ...                                      ...                \n",
       "  2.5971e-02  4.3248e-02  4.6850e-02  ...   2.4054e-02  3.7525e-02 -3.7133e-02\n",
       "  4.7485e-02  1.9903e-02 -4.4855e-02  ...   4.3847e-03  1.7699e-03 -1.4564e-03\n",
       "  7.3245e-03  3.3046e-02 -4.0343e-02  ...   4.1058e-02  2.5732e-02 -3.3914e-02\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       "  1.8446e-02 -3.7720e-02  2.0100e-02  ...  -4.7902e-02 -3.6259e-02  4.9335e-02\n",
       "  3.6833e-02  2.3231e-02 -2.1703e-02  ...   3.1414e-02 -4.7204e-03 -3.3831e-02\n",
       " -3.9251e-02  1.3924e-02  4.2639e-02  ...  -5.0001e-03  3.1393e-02  1.0323e-05\n",
       "                 ...                                      ...                \n",
       " -4.7761e-02 -1.1185e-02 -2.6725e-03  ...  -2.0391e-02 -3.1019e-02 -9.8514e-03\n",
       " -2.5495e-02 -2.0870e-02  1.2991e-02  ...  -2.6045e-03  4.4398e-02  4.1875e-02\n",
       " -3.5004e-02 -3.8402e-02 -5.2194e-02  ...   1.0940e-02  4.7764e-02  3.2339e-02\n",
       "      \n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       " -4.8949e-02  1.0801e-02  1.1840e-02  ...  -4.3780e-02 -3.7543e-02 -3.3167e-03\n",
       "  4.9034e-02 -6.3099e-03  4.5036e-02  ...   2.5275e-02  1.6026e-02  1.9810e-02\n",
       " -3.3303e-02 -4.6995e-05 -2.7874e-02  ...   1.9950e-03  3.1941e-02  1.9932e-02\n",
       "                 ...                                      ...                \n",
       " -3.6337e-02  2.6111e-03  2.7535e-02  ...  -2.1565e-02 -1.1471e-02 -4.0516e-02\n",
       " -4.1085e-02 -2.5533e-02  1.4314e-02  ...  -2.3443e-02 -4.4454e-02  2.5374e-02\n",
       "  3.0477e-02  8.5221e-03  2.1774e-02  ...   3.4153e-02 -3.5590e-02  4.0577e-02\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       " -5.0247e-02 -1.7728e-02 -2.0536e-02  ...  -2.8544e-02 -3.2004e-02  2.7565e-02\n",
       " -2.5172e-02  3.5839e-02  1.0001e-02  ...  -7.5996e-03  3.2656e-02 -3.9260e-02\n",
       " -5.1810e-02  1.0543e-02  1.2813e-03  ...   4.0077e-02  3.5727e-02  3.4856e-03\n",
       "                 ...                                      ...                \n",
       " -3.2160e-03  1.7420e-02  2.6060e-02  ...   4.2485e-02  1.9580e-03 -3.2402e-03\n",
       "  4.5588e-02  2.1972e-02 -2.3518e-02  ...  -2.8124e-02 -1.0550e-02  2.6779e-02\n",
       " -3.3804e-02 -3.5657e-02  4.8387e-02  ...   1.5823e-02  4.7710e-02 -4.0243e-02\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       " -1.8897e-02 -5.0992e-02  1.5223e-02  ...   4.0199e-02  2.4667e-03  3.6036e-02\n",
       " -6.9892e-03  4.5105e-02  3.6083e-02  ...   3.0142e-02  3.7849e-02 -5.1707e-02\n",
       " -3.6184e-02  2.1879e-02 -1.9368e-02  ...   4.7268e-02  8.0504e-03 -4.4737e-02\n",
       "                 ...                                      ...                \n",
       "  1.3322e-03 -1.0451e-02 -3.2699e-02  ...   3.4329e-02  1.0897e-02 -4.8557e-02\n",
       " -4.0427e-02  3.4188e-02 -4.7399e-03  ...   2.6455e-02 -8.9261e-03 -2.9779e-02\n",
       " -4.9021e-02  8.5581e-03 -3.2265e-02  ...   1.6170e-02  1.3189e-02  5.2298e-03\n",
       "      \n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       " -4.9754e-02  4.5416e-02 -5.0693e-02  ...   3.2644e-02  2.3523e-02 -4.2697e-02\n",
       "  4.4257e-02  7.4371e-04  3.1994e-02  ...  -1.7712e-02 -2.6291e-02 -3.6240e-02\n",
       " -4.9615e-02  3.6499e-03  4.9852e-02  ...   1.9752e-02  7.0548e-03 -2.4841e-02\n",
       "                 ...                                      ...                \n",
       "  2.6958e-02  3.4276e-02  1.7205e-02  ...   1.6361e-02  3.8701e-02  2.1451e-02\n",
       " -2.0986e-02 -1.0001e-02 -1.6346e-02  ...  -4.1696e-02  9.9569e-03 -1.7953e-02\n",
       " -3.4293e-02  1.6747e-02 -4.7268e-02  ...  -2.8127e-02 -2.6314e-02  4.2379e-02\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       " -2.9391e-02  4.3907e-02  3.6541e-05  ...   1.4785e-02 -1.0000e-02 -2.1125e-02\n",
       "  4.8315e-02  3.7067e-02 -2.2366e-03  ...  -1.2091e-02 -2.5682e-02 -2.7795e-02\n",
       "  1.3105e-02  3.1762e-02 -4.2714e-03  ...  -1.9078e-02  1.4803e-02 -1.9222e-02\n",
       "                 ...                                      ...                \n",
       " -9.4321e-03 -3.1413e-02 -4.9547e-02  ...  -1.9316e-02  4.8413e-03  1.7179e-03\n",
       "  8.7469e-03 -2.5569e-03  2.9455e-02  ...   8.6682e-03  2.9681e-02 -5.6467e-03\n",
       " -4.0702e-02 -2.4344e-02 -4.4011e-02  ...   7.3988e-03 -8.9066e-03  2.8827e-02\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       " -4.9800e-02 -5.1054e-02 -3.8221e-02  ...   4.8082e-02  4.1030e-02  5.7901e-03\n",
       " -3.1044e-02  4.8783e-02  3.6585e-02  ...  -4.8259e-02  2.5614e-03  3.1870e-02\n",
       " -1.2801e-02 -4.6420e-02 -3.7321e-02  ...   1.2702e-02  4.3262e-02 -1.0455e-02\n",
       "                 ...                                      ...                \n",
       " -4.1411e-02 -1.4644e-03 -2.8396e-02  ...   3.7390e-04 -2.0314e-02 -9.1728e-03\n",
       "  2.9835e-02 -1.2354e-03  3.5785e-02  ...   2.8200e-02 -1.3448e-02  5.6612e-03\n",
       " -6.0041e-03 -4.3043e-02 -1.7418e-02  ...  -1.3919e-02  4.0268e-02 -5.0351e-02\n",
       "...   \n",
       "      \n",
       "\n",
       "(61,0 ,.,.) = \n",
       "  7.3115e-03  3.8591e-02  2.7361e-02  ...  -4.4841e-02  8.7547e-03 -3.3365e-02\n",
       "  3.4948e-03  4.5212e-02  1.1058e-02  ...   3.9052e-02 -3.0010e-02  3.8651e-02\n",
       "  6.2280e-03 -1.6482e-02 -2.9194e-02  ...   2.8355e-02 -3.7675e-02 -9.6229e-03\n",
       "                 ...                                      ...                \n",
       " -1.4234e-02 -2.6179e-02 -2.8449e-02  ...   9.5990e-03 -4.1067e-02  4.7750e-02\n",
       " -5.1657e-03 -2.1737e-02  4.0944e-02  ...  -4.0484e-02 -1.6657e-02  3.8688e-02\n",
       " -4.2760e-02  4.3737e-02  1.3252e-04  ...  -1.9911e-02  4.9514e-02  6.7544e-04\n",
       "\n",
       "(61,1 ,.,.) = \n",
       " -3.4590e-02 -6.6681e-03  7.8101e-03  ...   2.3470e-02 -3.0213e-02 -2.4038e-02\n",
       " -4.8962e-02 -3.0325e-02  1.2641e-02  ...   1.9539e-02 -2.5474e-02 -3.9432e-02\n",
       "  1.5776e-02  2.0596e-02 -2.6774e-02  ...   5.1481e-02 -4.3664e-02  2.1911e-02\n",
       "                 ...                                      ...                \n",
       " -3.5131e-02  3.0520e-02 -4.3845e-02  ...  -4.1844e-02 -3.4401e-02  3.0631e-02\n",
       " -5.2180e-02  1.5623e-02  3.6122e-02  ...  -6.4967e-03 -3.4376e-02  5.0647e-02\n",
       " -4.5267e-02  4.3073e-02 -3.8866e-02  ...  -3.6736e-02 -5.0881e-02  3.6510e-02\n",
       "\n",
       "(61,2 ,.,.) = \n",
       "  1.4893e-02  3.7595e-02  2.7698e-02  ...  -5.1658e-02  3.3902e-02  2.1046e-02\n",
       "  3.3472e-02  2.8681e-02  4.7969e-02  ...   4.6877e-02  3.5795e-02 -4.3196e-02\n",
       " -4.9246e-02  2.9552e-02  8.5751e-03  ...   1.4409e-02 -2.1694e-02 -4.7043e-02\n",
       "                 ...                                      ...                \n",
       " -2.2710e-02  4.7148e-02 -2.1859e-02  ...  -1.1290e-02 -2.6986e-02 -1.1519e-02\n",
       "  6.0625e-04 -9.0952e-03 -3.1353e-02  ...   8.9367e-03  4.1905e-02  1.3784e-02\n",
       " -3.5075e-02 -1.1479e-02 -1.0306e-02  ...  -1.5345e-02 -3.7110e-02  2.2129e-02\n",
       "      \n",
       "\n",
       "(62,0 ,.,.) = \n",
       "  1.3750e-02 -3.1203e-02  8.9080e-03  ...  -1.5969e-02 -1.6387e-02  5.1317e-02\n",
       "  4.1289e-02 -2.7868e-02 -4.3330e-02  ...   2.7857e-02 -2.5026e-04 -7.7631e-03\n",
       " -2.0977e-02 -2.1228e-02  4.4112e-02  ...   1.8558e-02 -2.5405e-02 -4.6899e-02\n",
       "                 ...                                      ...                \n",
       "  4.2343e-02  1.1087e-02 -4.7878e-02  ...  -2.8540e-02  6.0669e-04  2.7575e-02\n",
       " -4.2476e-03  1.3819e-02 -4.5166e-02  ...  -3.9234e-02 -1.6218e-02  5.0701e-02\n",
       " -4.0623e-02 -4.3302e-02 -3.7265e-03  ...  -3.0813e-02  4.2840e-03  4.4062e-02\n",
       "\n",
       "(62,1 ,.,.) = \n",
       " -3.2055e-02  3.3598e-02 -4.4914e-02  ...  -2.3554e-02  6.3798e-03  7.8456e-03\n",
       " -4.2648e-02 -1.7394e-03 -3.7993e-02  ...  -4.7689e-02 -5.0943e-02 -7.9190e-03\n",
       "  1.2606e-03  3.9080e-02 -3.1434e-03  ...  -4.6597e-02  4.5749e-02  2.4194e-02\n",
       "                 ...                                      ...                \n",
       "  3.8746e-02  5.0368e-02 -1.9522e-02  ...  -2.2404e-02  1.4490e-02  1.4451e-02\n",
       " -4.0561e-02  4.0578e-02  4.1698e-02  ...  -2.1993e-02  4.8109e-02 -1.7657e-02\n",
       "  2.9685e-02 -1.7017e-04 -6.0510e-03  ...   4.5362e-02 -1.7913e-02 -1.0549e-02\n",
       "\n",
       "(62,2 ,.,.) = \n",
       "  3.1349e-02  4.6469e-02 -1.9582e-02  ...  -2.7292e-02  6.9795e-03 -3.3763e-02\n",
       " -3.1817e-02 -2.1572e-03 -1.0463e-02  ...   2.5315e-02  8.2748e-03  2.4131e-02\n",
       "  5.0181e-04 -4.2734e-02 -4.3099e-02  ...  -5.0548e-02  2.1963e-02 -1.1622e-02\n",
       "                 ...                                      ...                \n",
       "  5.4666e-03 -2.1715e-02 -4.8719e-02  ...   4.2015e-02 -1.2923e-02 -3.0908e-02\n",
       "  2.7619e-02 -4.0617e-02  2.4933e-02  ...   2.7097e-02  2.6576e-02  1.0143e-02\n",
       " -4.4403e-02 -1.6402e-02  1.5109e-02  ...   1.5449e-02 -1.1133e-02  2.1465e-02\n",
       "      \n",
       "\n",
       "(63,0 ,.,.) = \n",
       " -2.2425e-02 -3.7557e-02  6.3248e-03  ...  -4.8733e-02  1.9949e-02  6.3706e-03\n",
       "  1.5029e-02 -1.7495e-02  2.2686e-02  ...   4.5170e-02  3.5441e-02  4.6835e-02\n",
       "  3.0772e-02 -4.9918e-03  4.8952e-02  ...   4.7277e-02  3.5557e-02  4.1353e-02\n",
       "                 ...                                      ...                \n",
       "  2.3704e-02  1.0380e-02  2.0053e-02  ...   1.6392e-02  3.0748e-02 -4.9073e-02\n",
       " -2.1951e-02  5.1511e-02 -4.5638e-02  ...   2.4018e-03  4.2857e-02 -1.8558e-02\n",
       " -1.8469e-02 -3.8206e-02  4.0208e-02  ...   4.0313e-02 -4.3609e-02 -8.7752e-03\n",
       "\n",
       "(63,1 ,.,.) = \n",
       "  2.9873e-02  4.2784e-02 -3.0551e-02  ...   2.3626e-02 -3.0017e-02 -7.6230e-03\n",
       "  3.1964e-02 -7.7508e-03  5.0717e-02  ...   4.9635e-03 -2.8670e-02  4.3045e-02\n",
       " -1.6577e-02 -3.9755e-02 -4.9705e-02  ...   1.5441e-02 -2.5371e-02  3.4165e-02\n",
       "                 ...                                      ...                \n",
       "  1.0620e-02  4.7821e-02  4.2127e-02  ...   3.3615e-02 -6.4136e-03  3.6790e-02\n",
       "  3.2490e-02  3.1920e-02 -2.9890e-02  ...   1.5705e-02  2.0314e-02  1.2580e-03\n",
       "  1.0209e-02  3.8687e-02 -3.2131e-02  ...  -2.9449e-02  2.2135e-02  4.2448e-02\n",
       "\n",
       "(63,2 ,.,.) = \n",
       "  5.0314e-02 -7.7857e-03 -4.3619e-02  ...  -6.9891e-03  1.8081e-03 -3.7500e-02\n",
       " -3.5853e-02  4.7128e-02 -2.1555e-02  ...  -2.8939e-02 -4.4286e-02 -2.9030e-02\n",
       " -1.2133e-02 -3.8383e-02 -4.7598e-02  ...   4.9313e-02  3.2528e-02 -3.6192e-02\n",
       "                 ...                                      ...                \n",
       " -2.0070e-02  3.9971e-02  4.3316e-02  ...  -5.7051e-03  1.9613e-03  8.4997e-03\n",
       "  1.1742e-02  2.9378e-02 -2.6888e-03  ...  -5.3498e-03  2.9373e-02  2.5790e-02\n",
       "  2.2103e-03 -2.3938e-02 -2.6683e-02  ...   5.0494e-02  6.9834e-03  2.6730e-02\n",
       "[torch.FloatTensor of size 64x3x11x11]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1  1  1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1  1 -1 -1\n",
       " -1  1 -1 -1 -1\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -1 -1 -1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       "  1 -1 -1  1  1\n",
       " -1 -1 -1  1  1\n",
       "  1  1 -1  1 -1\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -1 -1  1 -1  1\n",
       " -1 -1  1  1 -1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1  1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -1  1  1  1 -1\n",
       " -1 -1  1 -1 -1\n",
       "  1 -1 -1  1 -1\n",
       "  1 -1 -1  1  1\n",
       " -1 -1  1 -1 -1\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  1 -1 -1  1 -1\n",
       " -1 -1  1  1  1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1 -1  1 -1 -1\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       " -1  1 -1 -1 -1\n",
       " -1 -1 -1  1  1\n",
       " -1  1 -1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1 -1 -1  1  1\n",
       "  1  1  1 -1  1\n",
       " -1  1  1  1 -1\n",
       " -1 -1  1  1  1\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       "  1 -1  1  1 -1\n",
       "  1 -1  1  1  1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1 -1 -1  1\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1  1  1  1  1\n",
       " -1  1  1 -1 -1\n",
       " -1 -1  1 -1 -1\n",
       "  1  1 -1  1 -1\n",
       "  1  1 -1  1 -1\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  1  1 -1  1  1\n",
       " -1  1  1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1  1  1  1  1\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  1  1  1  1  1\n",
       "  1 -1 -1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1  1  1  1\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -1  1 -1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "  1  1  1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       " -1 -1  1 -1  1\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -1 -1 -1 -1  1\n",
       " -1  1 -1 -1  1\n",
       "  1  1 -1  1 -1\n",
       " -1 -1 -1 -1 -1\n",
       "  1  1  1 -1  1\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "  1  1  1  1  1\n",
       "  1  1 -1  1 -1\n",
       "  1 -1  1  1  1\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       " -1 -1 -1 -1 -1\n",
       "  1 -1  1  1 -1\n",
       " -1  1  1  1  1\n",
       "  1  1  1  1 -1\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1 -1 -1  1  1\n",
       "  1  1  1  1 -1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       "  1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "  1  1  1 -1  1\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1  1  1  1  1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1 -1  1 -1\n",
       "  1  1 -1 -1 -1\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1 -1 -1 -1 -1\n",
       "  1  1 -1 -1  1\n",
       "  1 -1  1  1  1\n",
       " -1 -1  1 -1  1\n",
       "  1  1 -1 -1  1\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1  1  1  1  1\n",
       "  1  1 -1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       "  1 -1 -1 -1  1\n",
       " -1  1 -1  1  1\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1 -1 -1  1 -1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1  1  1  1\n",
       "  1  1  1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       " -1  1 -1  1 -1\n",
       " -1 -1  1  1 -1\n",
       "  1 -1  1 -1 -1\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       "  1  1  1 -1 -1\n",
       "  1 -1  1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "  1  1 -1  1  1\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1 -1  1  1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1  1 -1  1 -1\n",
       " -1  1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -1 -1 -1  1  1\n",
       " -1  1  1 -1 -1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1  1  1 -1\n",
       "  1  1 -1 -1 -1\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1  1  1 -1 -1\n",
       " -1  1  1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "  1 -1 -1  1 -1\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  1 -1  1  1  1\n",
       "  1  1 -1 -1  1\n",
       " -1  1  1  1  1\n",
       " -1  1  1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  1 -1 -1  1  1\n",
       " -1 -1  1  1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1 -1  1 -1\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "  1 -1  1 -1 -1\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       " -1 -1 -1  1 -1\n",
       " -1  1  1  1 -1\n",
       "  1 -1  1  1  1\n",
       " -1  1 -1  1 -1\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1 -1  1  1 -1\n",
       "  1  1  1 -1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       "  1 -1  1  1 -1\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  1  1  1 -1  1\n",
       " -1 -1  1 -1  1\n",
       " -1 -1  1  1  1\n",
       "  1  1  1 -1  1\n",
       " -1 -1 -1  1 -1\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       " -1 -1  1  1  1\n",
       "  1  1 -1  1  1\n",
       " -1  1 -1 -1  1\n",
       " -1  1  1 -1  1\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       " -1 -1 -1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1  1  1 -1  1\n",
       "  1 -1  1 -1  1\n",
       "  1  1 -1  1  1\n",
       "  1  1  1 -1  1\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       " -1  1  1 -1 -1\n",
       "  1  1 -1 -1  1\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.binarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-3105480b3974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Work/quantized_neural_networks/quantization/binop.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateBinaryGradWeight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'BNN'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateBinaryGradWeight2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/quantized_neural_networks/quantization/binop.py\u001b[0m in \u001b[0;36mupdateBinaryGradWeight2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_conv2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;31m#            n = weight[0].nelement()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m#            s = weight.size()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "quantizer.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0 = model.first_feature(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpoo = model.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = maxpoo(out0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1 = binconv0.ConvLayer[0]\n",
    "binact1 = binconv0.ConvLayer[1]\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = batch1(out1)\n",
    "out3 = binact1(out2)\n",
    "conv1.bias = None\n",
    "out4 = conv1(out3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "   -8  -26   -4  ...   -20  -22  -18\n",
       "   -2  -18  -10  ...   -28  -22  -22\n",
       "   -6  -24   -2  ...   -18  -16  -12\n",
       "      ...                 ...      \n",
       "    2  -10    8  ...    -2  -12   -8\n",
       "   -2  -14    6  ...    -6  -14   -2\n",
       "   10   10   18  ...    22    8   10\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "   44   28   24  ...    36   22   16\n",
       "   64   50   40  ...    58   40   36\n",
       "   98   76   62  ...    62   56   42\n",
       "      ...                 ...      \n",
       "   90   66   64  ...    70   48   38\n",
       "   62   36   58  ...    66   60   44\n",
       "   54   46   48  ...    48   46   38\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "    2    4   12  ...     4    4    6\n",
       "   20   12   34  ...    20    8   20\n",
       "   10    2   24  ...    12    2   16\n",
       "      ...                 ...      \n",
       "   22    4   18  ...     8    6   12\n",
       "    4  -22  -16  ...   -24  -12    4\n",
       "  -20  -34  -28  ...   -20  -30    8\n",
       "    ... \n",
       "\n",
       "( 0 ,189,.,.) = \n",
       "   -6  -10   -2  ...    -6    2   -8\n",
       "  -26  -20  -44  ...   -46  -34  -22\n",
       "  -36  -26  -56  ...   -60  -50  -26\n",
       "      ...                 ...      \n",
       "  -40  -28  -42  ...   -60  -38  -10\n",
       "  -70  -62  -88  ...  -104  -70  -32\n",
       "  -28  -14  -26  ...   -42  -36  -14\n",
       "\n",
       "( 0 ,190,.,.) = \n",
       "   32   30    0  ...     0  -20  -10\n",
       "   38   50   30  ...    12  -20  -18\n",
       "   80   78   68  ...    56   -4   -8\n",
       "      ...                 ...      \n",
       "   68   72   62  ...    64   -4   -8\n",
       "   60   78   64  ...    68   22    8\n",
       "   52   72   60  ...    52   10    2\n",
       "\n",
       "( 0 ,191,.,.) = \n",
       "   22   50   40  ...    32   30   16\n",
       "   32   54   22  ...    32   32   22\n",
       "   48   74   40  ...    48   44   32\n",
       "      ...                 ...      \n",
       "   48   72   30  ...    40   32   12\n",
       "   34   62   40  ...    36   52   20\n",
       "   28   52   22  ...    26   24   26\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  -24   12   48  ...   -66  -46  -46\n",
       "   28   36   64  ...   -68  -34  -42\n",
       "   -8   42   22  ...   -90  -22  -10\n",
       "      ...                 ...      \n",
       "   38   22  -18  ...    -8    4  -10\n",
       "   34    2  -34  ...   -24    0   16\n",
       "  -10  -16  -20  ...   -32   10   10\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  -36  -46  -60  ...    -2   14   16\n",
       "   -2  -20  -66  ...     6   16    0\n",
       "  -12  -22  -58  ...    42   30   16\n",
       "      ...                 ...      \n",
       "  -70  -14   34  ...    32   72   64\n",
       "  -34  -24    6  ...   -24  -10   -6\n",
       "  -34  -24  -10  ...    -6    0   26\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       "   14    2    8  ...    -6  -28  -50\n",
       "   -2  -18    0  ...    36  -20  -24\n",
       "   -8  -28  -44  ...   -16  -24  -22\n",
       "      ...                 ...      \n",
       "  -10   12   40  ...   -26  -22   -6\n",
       "  -28   14   20  ...    10   58  -38\n",
       "   -8   36   22  ...   -50  -40   -4\n",
       "    ... \n",
       "\n",
       "( 1 ,189,.,.) = \n",
       "   22   16   -6  ...   -24   18    8\n",
       "   20   -2   30  ...   -66    6   -2\n",
       "   66   36   68  ...   -60    4    0\n",
       "      ...                 ...      \n",
       "   28    4    0  ...   -38   18   12\n",
       "   -2  -42   12  ...   -14  -12   10\n",
       "    0  -36    8  ...   -12    6  -22\n",
       "\n",
       "( 1 ,190,.,.) = \n",
       "   12   40   44  ...   -26  -32  -14\n",
       "  -24   -8   -8  ...   -36  -20  -14\n",
       "  -30  -56  -80  ...   -12  -34   -6\n",
       "      ...                 ...      \n",
       "  -36  -16  -52  ...   -10  -56   -2\n",
       "   -8  -42  -32  ...   -22    8   -2\n",
       "  -20  -26  -66  ...    62   24   -2\n",
       "\n",
       "( 1 ,191,.,.) = \n",
       "    2   12   12  ...    42   50    8\n",
       "    2  -20  -32  ...    40    4    6\n",
       "    2   16   -8  ...    60   26  -10\n",
       "      ...                 ...      \n",
       "   12  -24   -8  ...     2   12   26\n",
       "   -6  -90  -28  ...    -2   -6   10\n",
       "    4  -14    4  ...   -24   -2    2\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       "  -28  -24   -8  ...   -24   -8   -8\n",
       "  -24   -4   22  ...   -12  -14   -4\n",
       "   -6  -12   30  ...    40    4   32\n",
       "      ...                 ...      \n",
       "   50   76   54  ...     8   14    0\n",
       "   30   52   44  ...    14   16   10\n",
       "    4   18   16  ...    24   18   10\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  -16  -10   16  ...    52   44   26\n",
       "  -14   28   68  ...    98   80   66\n",
       "  -34   20   70  ...   124   88   74\n",
       "      ...                 ...      \n",
       "  -66  -76  -98  ...   -76  -70  -50\n",
       "  -18  -10  -36  ...   -62  -54  -52\n",
       "  -24  -38  -30  ...   -22    0  -22\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       "   10   26   20  ...   -20   -6   -4\n",
       "    2   10   18  ...   -12   16   42\n",
       "   22    2   44  ...   -26    6    4\n",
       "      ...                 ...      \n",
       "  -10   34   20  ...    34   48   24\n",
       "  -32   16    2  ...    36    6   -4\n",
       "  -30   14   26  ...    -6    0    8\n",
       "    ... \n",
       "\n",
       "( 2 ,189,.,.) = \n",
       "  -10  -12   26  ...    22   20    6\n",
       "  -12  -22  -12  ...   -42  -42  -16\n",
       "   -8  -22   12  ...   -38  -26  -10\n",
       "      ...                 ...      \n",
       "   40   -2   16  ...    26    4   14\n",
       "   78   56   58  ...    48   28   12\n",
       "   46   42   56  ...     8    6    6\n",
       "\n",
       "( 2 ,190,.,.) = \n",
       "   12   16   12  ...   -12  -34  -44\n",
       "   40   44   30  ...   -28  -76  -68\n",
       "   48   58   24  ...    34  -36  -44\n",
       "      ...                 ...      \n",
       "   -4  -10    4  ...    18  -26  -16\n",
       "  -44  -64  -58  ...   -20  -12   -8\n",
       "  -38  -56  -70  ...   -54  -32  -30\n",
       "\n",
       "( 2 ,191,.,.) = \n",
       "   -2   56   16  ...    40   36    2\n",
       "   18   64   58  ...    16    0    8\n",
       "   12   50   80  ...    18   40  -24\n",
       "      ...                 ...      \n",
       "  -12  -46  -28  ...   -26  -34    4\n",
       "   10  -24   22  ...   -12  -30    4\n",
       "   14  -20   28  ...     0  -22   10\n",
       "        \n",
       "\n",
       "( 3 , 0 ,.,.) = \n",
       "   24   52   26  ...    40   34   14\n",
       "   32   64   30  ...    54   40   20\n",
       "    8   32   38  ...    60   44   20\n",
       "      ...                 ...      \n",
       "    4   38   18  ...    46   32   22\n",
       "   18   24   -8  ...    26   16   12\n",
       "    6   20   -2  ...     0   -6  -14\n",
       "\n",
       "( 3 , 1 ,.,.) = \n",
       "  -32  -46  -30  ...   -64  -34  -12\n",
       "  -30  -40  -40  ...   -64  -22  -26\n",
       "  -32  -68  -78  ...   -68  -32  -18\n",
       "      ...                 ...      \n",
       "  -96  -98  -70  ...   -86  -52  -28\n",
       "  -30  -42  -36  ...   -38  -34  -18\n",
       "  -34  -28  -20  ...   -34  -24  -26\n",
       "\n",
       "( 3 , 2 ,.,.) = \n",
       "   -6   22   26  ...    24   36   26\n",
       "   -6   10   42  ...    -2   22    6\n",
       "    4   90   32  ...     6   22    4\n",
       "      ...                 ...      \n",
       "    4   32   16  ...    20   34   18\n",
       "  -16   24   38  ...    40   30   26\n",
       "  -16   32   24  ...    22   36   16\n",
       "    ... \n",
       "\n",
       "( 3 ,189,.,.) = \n",
       "    2  -28    4  ...    -6  -18  -24\n",
       "   20   14   28  ...     8    8    0\n",
       "    6   18  -12  ...    18   14    6\n",
       "      ...                 ...      \n",
       "   58   20   28  ...    24   14    0\n",
       "   70   56   66  ...    72   48   34\n",
       "   32   28   38  ...    20   18   14\n",
       "\n",
       "( 3 ,190,.,.) = \n",
       "    0  -12    6  ...    -4   12   10\n",
       "    8   -4  -22  ...   -10   14   12\n",
       "  -18  -42  -68  ...   -30    0   -8\n",
       "      ...                 ...      \n",
       "  -38  -28  -16  ...   -32    0    2\n",
       "  -32  -60  -34  ...   -60  -20  -10\n",
       "  -40  -46  -52  ...   -70  -36  -26\n",
       "\n",
       "( 3 ,191,.,.) = \n",
       "  -14  -44  -10  ...   -56  -42  -28\n",
       "  -18  -28   18  ...   -58  -46  -28\n",
       "  -22  -34   16  ...   -58  -48  -28\n",
       "      ...                 ...      \n",
       "  -74 -116  -80  ...   -52  -52  -34\n",
       "  -26  -64  -42  ...   -20  -30  -30\n",
       "   -4  -30   -2  ...    -4  -18  -26\n",
       "[torch.FloatTensor of size 4x192x27x27]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1.9854e-02  1.7430e-03  1.5917e-03 -1.2705e-02  1.4768e-02\n",
       "  8.8550e-03  1.9816e-02 -1.2634e-02 -4.5918e-03 -1.1488e-02\n",
       "  2.3038e-02 -2.1017e-02 -1.0588e-02  3.8625e-03  2.4222e-02\n",
       "  2.4811e-02 -1.9536e-02  2.0398e-02 -2.3971e-04 -1.2513e-02\n",
       " -1.0656e-02  1.3622e-03 -6.4876e-03 -2.2427e-02 -3.1160e-03\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -2.7056e-03 -4.3975e-03 -1.3461e-02 -1.5300e-02 -1.4789e-03\n",
       " -1.5433e-02 -2.3503e-02  9.6584e-03  2.3697e-02 -2.5940e-03\n",
       "  4.8158e-04 -1.5426e-02 -5.9504e-03  2.0913e-02  3.2624e-03\n",
       " -1.6488e-04 -5.5602e-03 -6.5532e-03  1.3503e-02  1.3679e-02\n",
       "  1.2023e-02  1.0051e-02 -1.0832e-03  9.2694e-03 -1.1804e-02\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -8.8570e-03 -1.2342e-03  6.4216e-03 -2.3477e-02  1.5734e-04\n",
       " -1.3358e-02 -3.8932e-03  1.6753e-02  2.1799e-03 -1.8357e-02\n",
       " -2.2244e-02  3.7937e-03 -2.4062e-02 -8.4986e-03  1.2075e-02\n",
       "  2.3265e-02 -1.9646e-02  2.0102e-02  1.2907e-02 -1.7970e-02\n",
       " -1.4858e-02 -3.2150e-03 -1.8726e-02 -1.7776e-03  2.7310e-03\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -7.3302e-03  3.0292e-03  6.9196e-03  2.0550e-02 -1.6001e-04\n",
       " -1.1818e-02 -1.6222e-02  3.3308e-03 -2.4760e-02 -1.8461e-02\n",
       "  2.3474e-02 -2.4786e-02 -1.2760e-02  9.6861e-03 -2.2014e-02\n",
       "  8.9485e-03 -5.1274e-03 -1.1390e-02  1.9786e-02  2.2038e-02\n",
       " -2.2137e-02 -1.3512e-02  1.2226e-02 -1.2807e-02 -1.1306e-02\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  4.8804e-03 -2.0451e-02 -2.0770e-02  6.3498e-03 -9.1271e-03\n",
       " -1.9409e-02 -1.7721e-02  1.4353e-02  8.7453e-03  1.4858e-02\n",
       "  6.0360e-04 -9.0843e-03 -2.6196e-03  1.9454e-02  2.4763e-02\n",
       "  5.0270e-03 -1.5967e-02 -1.9239e-02 -1.1342e-02 -1.8448e-02\n",
       " -4.6556e-03 -1.6809e-02  2.0078e-02 -4.1498e-03 -7.2842e-03\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -8.6723e-03 -2.0662e-02  2.3798e-02 -8.1032e-03 -1.2880e-02\n",
       " -1.6554e-02  1.4447e-02 -1.3346e-02 -6.4114e-05 -1.8483e-02\n",
       " -2.1542e-04 -2.4597e-02 -6.8266e-03  3.0878e-04  2.1622e-02\n",
       " -7.7929e-03  1.4521e-02 -1.3189e-02  1.9729e-02 -5.2617e-03\n",
       " -2.9792e-03 -1.7042e-02 -2.1484e-02 -2.2779e-02  9.0785e-03\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  2.0838e-02 -1.7166e-02  2.0419e-02 -1.7027e-02  1.4212e-02\n",
       "  3.7366e-03 -1.4275e-02 -3.4016e-03  1.3498e-02  1.7567e-02\n",
       "  1.7418e-02  1.9651e-02  3.4423e-03 -3.4519e-04  5.8621e-03\n",
       " -1.3422e-02  3.2643e-03  2.2332e-02  2.0855e-02 -4.8584e-03\n",
       " -7.6416e-03 -1.7579e-03  8.1759e-03  2.2670e-02  9.1680e-03\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  2.3112e-02  1.4516e-02 -1.8848e-02 -2.2834e-02 -8.2304e-03\n",
       "  2.4237e-02 -1.9323e-02  3.6882e-03  2.7886e-03 -1.3281e-02\n",
       "  3.2728e-03 -1.6618e-02  6.4820e-03  4.6459e-03  1.5193e-02\n",
       "  1.1980e-02 -1.6462e-02 -1.1259e-02  1.5864e-02  2.0908e-02\n",
       "  5.0138e-03 -7.8098e-03 -2.1828e-02 -1.3298e-02  1.1850e-02\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1.7439e-02  1.9328e-02  2.3524e-03  7.2331e-03  2.4050e-02\n",
       " -1.9137e-02  1.4781e-02  1.8644e-02 -3.0069e-03 -2.4654e-02\n",
       " -3.8269e-03 -2.6762e-03  4.2193e-03 -9.5031e-03 -1.9745e-02\n",
       "  1.0290e-03  1.4972e-02 -6.1306e-03  4.6130e-03 -1.9238e-02\n",
       "  4.2447e-03  7.1926e-03 -5.4337e-03  1.8431e-02 -1.2376e-02\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  1.9709e-02  9.2529e-03 -1.5114e-02  2.1490e-02  1.7834e-02\n",
       " -2.4458e-03  2.4002e-02  1.2939e-04  2.4232e-02  1.9488e-02\n",
       " -1.5682e-02  9.9362e-03  2.1416e-02 -1.3486e-02  2.2604e-02\n",
       " -5.3627e-03 -1.2552e-02 -2.5265e-03 -1.5452e-02  4.7519e-03\n",
       " -4.0226e-03  3.8359e-03  1.7007e-02  2.0425e-02  5.1884e-03\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  2.2875e-02  1.2164e-02  1.5693e-02  2.3740e-03  1.1231e-02\n",
       "  9.4551e-03 -1.8749e-02 -1.9110e-02 -1.1099e-02  3.1733e-03\n",
       " -1.7446e-02 -5.7096e-03 -8.1767e-03 -5.5684e-03 -1.7105e-03\n",
       " -1.9883e-02  3.8021e-03 -1.9653e-02 -1.7639e-02  6.4315e-03\n",
       "  1.2999e-02 -2.1152e-02  4.0205e-03  2.2254e-02  1.1492e-02\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -1.8502e-03  2.0180e-03 -1.8306e-02  2.1523e-02 -1.4540e-02\n",
       " -1.0754e-02 -1.2098e-02 -1.9344e-02 -1.5891e-02  2.7898e-03\n",
       "  1.0310e-02  1.1895e-02  2.1419e-02 -3.2787e-03  1.9926e-02\n",
       " -2.3149e-02 -1.9988e-02 -2.1385e-02 -1.3293e-02 -7.3689e-03\n",
       " -1.1967e-02 -2.4420e-02  2.3919e-02 -6.6533e-04  1.0620e-02\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -9.9342e-03 -7.5570e-03 -7.4831e-03 -1.5130e-02  1.5311e-02\n",
       " -7.3587e-03  1.0918e-02 -1.3033e-02 -2.1886e-02  7.3727e-03\n",
       "  1.8948e-02  1.9881e-02 -8.3328e-03  1.1044e-02 -2.1743e-02\n",
       " -5.1825e-03 -1.8960e-02 -1.2902e-02 -2.2781e-02 -6.1994e-03\n",
       "  2.2198e-02  1.8943e-02  1.6051e-02 -4.7882e-03  5.0082e-03\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  8.1466e-03 -1.3025e-03  9.8606e-03  4.3614e-03 -1.4155e-02\n",
       " -1.5571e-02 -2.4084e-02 -1.5086e-02 -1.5991e-02  6.3563e-03\n",
       "  5.5683e-03  1.0311e-02  2.4140e-02  1.6521e-02  8.2294e-03\n",
       "  2.4304e-02  1.3524e-02 -1.1113e-02  1.1686e-02 -1.6772e-02\n",
       "  1.3817e-02 -8.4687e-03  5.6774e-03  8.1050e-03  1.0674e-02\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1.6359e-02 -5.9574e-03  4.6379e-03 -8.6397e-03 -1.5324e-02\n",
       " -2.8514e-03 -1.5732e-02 -1.7158e-02 -9.2285e-03 -2.0912e-02\n",
       "  2.4143e-02 -2.4835e-02  1.2965e-02  5.7063e-03 -1.2693e-02\n",
       " -1.7027e-02  1.2335e-03  8.5668e-03  1.8335e-02  1.9499e-02\n",
       "  1.7550e-02  1.0694e-02  1.4704e-02  1.0481e-02 -3.9471e-04\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1.8472e-02 -4.7188e-04 -9.3636e-03  3.8030e-03  2.0256e-02\n",
       "  1.0402e-02  9.4909e-05  1.2701e-02  1.2810e-02 -1.1584e-02\n",
       " -3.1607e-03 -1.0397e-02 -1.3792e-02  2.4134e-02  1.9665e-02\n",
       " -1.4472e-02 -1.5569e-02 -2.3018e-02 -5.9907e-03  7.6282e-03\n",
       " -9.5890e-03 -1.4694e-02 -2.0230e-02 -1.9771e-02 -7.4646e-03\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1.7123e-02  9.3277e-03  1.9131e-02  1.3765e-02 -1.1188e-02\n",
       "  1.1731e-02 -2.0179e-02  2.0241e-02  2.4899e-02 -1.0635e-02\n",
       "  1.9443e-02  5.8916e-03  1.6959e-02 -9.9126e-03  6.1570e-03\n",
       " -3.0022e-03  9.4650e-03  5.2554e-03  1.5059e-02  1.7561e-02\n",
       "  5.5280e-03  1.8784e-02  1.4374e-05 -1.6806e-02  1.1462e-02\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1.2957e-02 -2.4798e-02  6.4062e-03  1.3216e-02 -1.8366e-02\n",
       " -2.3874e-02  1.6108e-02  9.3074e-03  1.7774e-02  6.6088e-04\n",
       " -4.3282e-03  2.1828e-02 -3.4549e-03 -1.9577e-02  1.8828e-02\n",
       "  2.2225e-02 -2.1485e-02 -1.6169e-02  2.3927e-02 -1.0626e-02\n",
       "  2.3764e-02  2.3794e-02 -1.1182e-02 -6.5404e-03 -7.2618e-03\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1.5635e-02 -2.1905e-02 -1.4917e-02 -5.3635e-03 -1.5996e-02\n",
       "  1.7399e-02  1.0834e-02 -6.9166e-03 -1.9229e-02  1.8639e-02\n",
       "  2.3202e-02 -7.1915e-03  1.7287e-02  2.2046e-02  8.5515e-03\n",
       " -3.9023e-04 -1.0858e-02  2.5152e-03 -8.0841e-03  2.0089e-02\n",
       "  6.3684e-03  1.6113e-02 -9.3315e-03 -1.6732e-02  9.7985e-03\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1.5764e-02  1.2319e-02  6.0850e-03  1.4213e-02  3.2527e-03\n",
       "  1.8271e-02  5.1857e-03 -1.3262e-02 -1.4277e-02  1.3021e-02\n",
       "  8.6343e-03  1.5280e-02 -1.3722e-02 -3.0856e-03 -1.2075e-03\n",
       "  6.9296e-03 -1.0479e-02 -1.5263e-02 -1.4376e-02  1.8475e-02\n",
       " -1.1565e-03  1.5493e-02 -2.0651e-02  6.4736e-03  1.8749e-03\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1.4078e-02 -6.9254e-03 -1.5528e-02  1.0844e-02 -1.1971e-02\n",
       " -2.0339e-02 -1.9003e-02 -5.3223e-03  9.8675e-04  2.3771e-02\n",
       " -1.6623e-03 -2.0006e-02  8.5511e-03  1.3913e-02  2.4279e-02\n",
       "  9.5306e-03  2.1134e-02  2.4300e-02 -2.0977e-02 -1.0278e-02\n",
       " -2.6330e-04 -1.1883e-02  6.0306e-03  1.8253e-02 -1.4015e-02\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  8.8635e-03 -8.4161e-03  2.2538e-02  9.7375e-03 -1.9545e-03\n",
       "  7.9347e-03 -1.0975e-02  8.3711e-03  1.9597e-02 -6.9698e-03\n",
       " -2.0963e-02  1.8808e-02 -7.1602e-03  2.1720e-03 -1.1723e-02\n",
       " -2.1846e-02 -7.8462e-03  1.9474e-02  2.0094e-02 -2.1806e-02\n",
       "  2.0398e-02 -3.3057e-03  2.4476e-02 -2.3777e-02 -2.4508e-02\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -4.0223e-03 -5.5134e-03  1.2366e-02 -1.1902e-02 -7.2144e-03\n",
       "  1.2204e-03  1.4203e-02  2.3681e-02 -3.5730e-03 -8.7688e-03\n",
       "  1.2305e-02 -1.9874e-02  6.9832e-05 -5.8470e-03  2.3449e-02\n",
       " -7.4324e-03 -8.7251e-03 -1.0410e-02 -1.9703e-02 -2.0944e-02\n",
       "  9.7120e-03  1.3593e-02 -2.2260e-02  2.3648e-02  1.3540e-02\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1.2142e-02 -1.2699e-02  1.0479e-02  2.3704e-03  2.1253e-02\n",
       "  2.4791e-02 -1.5393e-02 -2.0718e-02 -3.8529e-03 -1.1238e-02\n",
       " -1.9666e-02  6.9082e-03 -1.0071e-02  1.9719e-02 -1.0337e-02\n",
       " -7.3649e-03  1.7355e-02  1.0153e-02  9.8394e-03 -2.0604e-02\n",
       "  5.0624e-03 -1.7510e-02  1.4682e-02  1.3721e-02 -1.4146e-02\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -9.0408e-03 -1.1374e-02 -1.6532e-02  4.1837e-03  1.2326e-02\n",
       " -2.0340e-02  2.3781e-02  9.2538e-03 -3.8662e-03 -1.9292e-02\n",
       "  7.1958e-03 -1.4099e-02 -1.7540e-02  1.6272e-03  3.8305e-03\n",
       "  1.9320e-02 -4.6725e-03  1.5192e-03  2.4391e-03 -2.0538e-02\n",
       "  9.6530e-03  2.1617e-02 -1.7407e-02 -1.5714e-03 -1.8874e-04\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1.9720e-02 -3.2369e-03  8.0435e-03 -1.1808e-02  1.4272e-02\n",
       "  2.4401e-02  7.3105e-03  2.1292e-02 -2.1069e-02 -7.6669e-03\n",
       " -1.8252e-02  1.7331e-02  3.6883e-03 -1.9803e-02  1.7602e-02\n",
       " -1.7566e-02 -5.8743e-03 -1.7282e-02 -5.4293e-03 -1.0521e-02\n",
       "  2.3586e-02 -1.2848e-02 -1.2264e-02  8.1812e-03 -2.1584e-02\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  1.6321e-03 -4.6769e-03  3.8751e-03  8.7882e-03  1.7828e-02\n",
       "  2.2123e-02  9.7174e-03 -1.0895e-02 -1.7916e-02  1.7501e-02\n",
       " -1.3542e-02  2.4631e-02  5.2292e-03  1.2313e-02  2.2903e-02\n",
       " -1.1280e-03  2.0739e-02  6.9324e-04  1.6232e-03 -1.0036e-02\n",
       " -6.5036e-03 -4.3390e-03 -1.8387e-02 -1.5042e-02  1.6587e-02\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  4.0268e-03 -1.7984e-02 -1.4797e-02  1.1469e-02  4.4895e-04\n",
       " -2.1843e-02 -1.5622e-02  2.0893e-02  1.9581e-02  2.4423e-02\n",
       " -9.0238e-03 -1.8496e-02 -2.1421e-02 -1.9540e-02  1.8311e-02\n",
       " -9.9901e-03 -1.2800e-04 -1.2173e-02  1.2952e-02  2.0067e-02\n",
       " -3.9160e-03 -2.2863e-02 -9.9492e-03  1.8969e-02 -4.1425e-03\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  7.2305e-03  5.9649e-03 -7.9054e-03 -1.9049e-02 -1.0308e-02\n",
       " -2.0787e-02 -6.4898e-03  2.0110e-03  5.8777e-04  4.7147e-03\n",
       " -5.0571e-03  1.5965e-02  1.9840e-02 -8.7040e-03  1.0467e-02\n",
       " -1.4221e-02  1.6044e-02  7.3287e-03  8.0455e-03  3.6455e-03\n",
       "  4.7871e-03 -1.8121e-02  1.7589e-02 -2.2254e-02 -1.3177e-02\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  5.4768e-04  8.9232e-03  1.2413e-02  1.3991e-02 -2.4472e-03\n",
       " -1.7849e-02 -8.2152e-03 -6.1811e-03  4.2707e-03 -1.8766e-02\n",
       " -1.0777e-02  1.9783e-02  2.1870e-02  7.2394e-03 -2.4944e-02\n",
       "  1.0110e-02 -1.5324e-02  2.4233e-02  2.2607e-02  1.8181e-02\n",
       " -1.9821e-02  6.2794e-04 -3.9675e-03  6.5120e-03 -2.4649e-02\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1.4060e-02 -1.1325e-02  2.2004e-02  2.3182e-02 -1.8038e-02\n",
       "  1.4292e-02  2.3396e-02  1.0923e-02 -1.0830e-02  2.0888e-02\n",
       "  8.7100e-03 -2.3080e-02 -1.1648e-02 -8.6593e-03 -2.3800e-02\n",
       " -6.1057e-03 -2.3775e-02  4.9375e-03  1.3761e-02  1.4105e-02\n",
       "  6.0622e-03 -2.4035e-02  1.4276e-02  3.4533e-03 -2.2199e-02\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  3.4678e-03  1.5969e-02  2.4729e-02 -1.2554e-02  7.4369e-03\n",
       " -1.5219e-02 -3.3141e-03  4.1415e-03 -2.2851e-02  8.2671e-03\n",
       " -1.5156e-02 -1.9750e-02  2.0525e-02  1.3250e-02  4.9593e-03\n",
       "  1.4523e-02  5.2613e-03  1.7515e-02 -3.1524e-03  2.8825e-04\n",
       " -1.7185e-02 -1.3600e-02 -4.6367e-03  2.3545e-02 -2.4017e-03\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1.0800e-02  9.4258e-03  9.9980e-03  2.1472e-02 -5.2750e-03\n",
       " -2.1795e-02 -9.5383e-03  1.1552e-02  2.0296e-02  7.6475e-03\n",
       "  9.4512e-03  5.7468e-03 -1.8897e-02  7.3692e-03  2.4674e-02\n",
       " -1.5409e-02  1.2050e-02 -9.8997e-03 -3.9811e-03  1.6003e-02\n",
       " -7.5817e-03  3.9097e-03  1.6256e-02 -2.1891e-02  8.5148e-04\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  4.9299e-04  9.0351e-03 -6.1744e-03 -2.3208e-02 -1.9184e-02\n",
       " -1.9955e-02 -1.8184e-02  1.7821e-02  1.1954e-03  2.2655e-02\n",
       " -3.3274e-03 -1.0390e-02 -1.1048e-02  1.8866e-02  2.0701e-02\n",
       " -4.6972e-03  1.4192e-02  1.6157e-02 -1.6608e-02  4.4919e-03\n",
       " -2.0864e-02  1.5643e-02  2.5571e-03  1.5464e-02  6.2548e-03\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1.6565e-02 -2.2293e-03  7.4521e-04  2.3971e-03 -1.0121e-02\n",
       " -1.9910e-02  6.0482e-03  7.9966e-03 -2.2359e-03  6.9555e-03\n",
       "  2.2061e-02 -2.4897e-02  6.5946e-03 -1.5809e-02  2.2906e-02\n",
       "  7.0468e-05  4.4416e-03 -2.0347e-02  5.7629e-03  2.2310e-03\n",
       "  4.0507e-03  2.1743e-02  1.4660e-02 -1.4139e-02  1.3042e-02\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1.7396e-02 -1.3910e-02  1.5241e-02 -2.0242e-02  4.3292e-03\n",
       "  1.9315e-02  2.4034e-02 -2.2437e-03 -2.4523e-02 -2.0698e-02\n",
       " -1.6340e-02 -4.6525e-03  3.3425e-03  1.8519e-02 -1.5147e-02\n",
       " -7.0399e-03  8.1849e-03  5.2765e-03 -4.6968e-03 -8.3125e-03\n",
       "  5.3671e-03  1.6573e-02 -1.4628e-02 -5.4505e-03  5.8893e-03\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.saved_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_feature.0.weight\n",
      "first_feature.0.bias\n",
      "last_layer.0.weight\n",
      "last_layer.0.bias\n",
      "last_layer.0.running_mean\n",
      "last_layer.0.running_var\n",
      "last_layer.2.weight\n",
      "last_layer.2.bias\n",
      "last_layer.3.weight\n",
      "last_layer.3.bias\n",
      "last_layer.3.running_mean\n",
      "last_layer.3.running_var\n",
      "features.1.ConvLayer.0.weight\n",
      "features.1.ConvLayer.0.bias\n",
      "features.1.ConvLayer.0.running_mean\n",
      "features.1.ConvLayer.0.running_var\n",
      "features.1.ConvLayer.2.weight\n",
      "features.3.ConvLayer.0.weight\n",
      "features.3.ConvLayer.0.bias\n",
      "features.3.ConvLayer.0.running_mean\n",
      "features.3.ConvLayer.0.running_var\n",
      "features.3.ConvLayer.2.weight\n",
      "features.3.ConvLayer.2.bias\n",
      "features.4.ConvLayer.0.weight\n",
      "features.4.ConvLayer.0.bias\n",
      "features.4.ConvLayer.0.running_mean\n",
      "features.4.ConvLayer.0.running_var\n",
      "features.4.ConvLayer.2.weight\n",
      "features.4.ConvLayer.2.bias\n",
      "features.5.ConvLayer.0.weight\n",
      "features.5.ConvLayer.0.bias\n",
      "features.5.ConvLayer.0.running_mean\n",
      "features.5.ConvLayer.0.running_var\n",
      "features.5.ConvLayer.2.weight\n",
      "features.5.ConvLayer.2.bias\n",
      "features.7.weight\n",
      "features.7.bias\n",
      "features.7.running_mean\n",
      "features.7.running_var\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.1.ConvLayer.0.weight\n",
      "classifier.1.ConvLayer.0.bias\n",
      "classifier.1.ConvLayer.0.running_mean\n",
      "classifier.1.ConvLayer.0.running_var\n",
      "classifier.1.ConvLayer.2.weight\n",
      "classifier.1.ConvLayer.2.bias\n"
     ]
    }
   ],
   "source": [
    "for layer in model.state_dict():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2841  0.4114  0.9601  0.6169\n",
       " 0.2486  0.3997  0.9347  0.5635\n",
       " 0.0921  0.6981  0.7937  0.1600\n",
       " 0.7425  0.5263  0.4888  0.2621\n",
       " 0.8920  0.1198  0.2727  0.6142\n",
       "[torch.FloatTensor of size 5x4]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(5,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s =t.size()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.5681 -0.5681 -0.5681 -0.5681\n",
       "-0.5366 -0.5366 -0.5366 -0.5366\n",
       "-0.4360 -0.4360 -0.4360 -0.4360\n",
       "-0.5049 -0.5049 -0.5049 -0.5049\n",
       "-0.4747 -0.4747 -0.4747 -0.4747\n",
       "[torch.FloatTensor of size 5x4]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negMean = t.mean(1, keepdim=True).mul(-1).expand_as(t)\n",
    "negMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = t.nelement()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2.2725\n",
       " 2.1464\n",
       " 1.7439\n",
       " 2.0197\n",
       " 1.8987\n",
       "[torch.FloatTensor of size 5x1]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.norm(1,1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computing Lut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tobin = lambda x, count=8: \"\".join(map(lambda y:str((x>>y)&1), range(count-1, -1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1111111111111001'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def from_uint4_to_int16(x):\n",
    "    b = ''\n",
    "    a = tobin(x,4)\n",
    "    for x in range(12):\n",
    "        b = b+ a[0]\n",
    "    b += a\n",
    "    return b\n",
    "\n",
    "from_uint4_to_int16(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twos_comp(binary_string, bits):\n",
    "    val = int(binary_string,2)\n",
    "    \"\"\"compute the 2's complement of int value val\"\"\"\n",
    "    if (val & (1 << (bits - 1))) != 0: # if sign bit is set e.g., 8bit: 128-255\n",
    "        val = val - (1 << bits)        # compute negative value\n",
    "    return val      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,65528,65529,65530,65531,65532,65533,65534,65535,65536,65537,65538,65539,65540,65541,65542,65543,131064,131065,131066,131067,131068,131069,131070,131071,131072,131073,131074,131075,131076,131077,131078,131079,196600,196601,196602,196603,196604,196605,196606,196607,196608,196609,196610,196611,196612,196613,196614,196615,262136,262137,262138,262139,262140,262141,262142,262143,262144,262145,262146,262147,262148,262149,262150,262151,327672,327673,327674,327675,327676,327677,327678,327679,327680,327681,327682,327683,327684,327685,327686,327687,393208,393209,393210,393211,393212,393213,393214,393215,393216,393217,393218,393219,393220,393221,393222,393223,458744,458745,458746,458747,458748,458749,458750,458751,458752,458753,458754,458755,458756,458757,458758,458759,524280,524281,524282,524283,524284,524285,524286,524287,-524288,-524287,-524286,-524285,-524284,-524283,-524282,-524281,-458760,-458759,-458758,-458757,-458756,-458755,-458754,-458753,-458752,-458751,-458750,-458749,-458748,-458747,-458746,-458745,-393224,-393223,-393222,-393221,-393220,-393219,-393218,-393217,-393216,-393215,-393214,-393213,-393212,-393211,-393210,-393209,-327688,-327687,-327686,-327685,-327684,-327683,-327682,-327681,-327680,-327679,-327678,-327677,-327676,-327675,-327674,-327673,-262152,-262151,-262150,-262149,-262148,-262147,-262146,-262145,-262144,-262143,-262142,-262141,-262140,-262139,-262138,-262137,-196616,-196615,-196614,-196613,-196612,-196611,-196610,-196609,-196608,-196607,-196606,-196605,-196604,-196603,-196602,-196601,-131080,-131079,-131078,-131077,-131076,-131075,-131074,-131073,-131072,-131071,-131070,-131069,-131068,-131067,-131066,-131065,-65544,-65543,-65542,-65541,-65540,-65539,-65538,-65537,-65536,-65535,-65534,-65533,-65532,-65531,-65530,-65529,-8,-7,-6,-5,-4,-3,-2,-1,\n"
     ]
    }
   ],
   "source": [
    "c = ''\n",
    "for i in range(256):\n",
    "    n_low = i % 16\n",
    "    n_low = from_uint4_to_int16(n_low)\n",
    "    n_high = int(i / 16)\n",
    "    n_high = from_uint4_to_int16(n_high)\n",
    "    int16_val = twos_comp(n_high+n_low, 32)\n",
    "    c += str(int16_val) + ','\n",
    "    #print(i,n_low, n_high,int16_val)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111111111111000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1', '1', '1', '1', '1', '1', '1', '1']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ''\n",
    "a = tobin(8,4)\n",
    "for x in range(12):\n",
    "    b = b+ a[0]\n",
    "b += a\n",
    "print(b)\n",
    "[a[0] for x in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1111111111111000'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_string = '1001' # or whatever... no '0b' prefix\n",
    "out = twos_comp(int(binary_string,2), len(binary_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_quant_devel(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d_SAME(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d_SAME(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (1): BatchNorm2d(32, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d_SAME(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d_SAME(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "      (1): BatchNorm2d(64, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d_SAME(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d_SAME(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv2d_SAME(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv2d_SAME(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Conv2d_SAME(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (22): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (24): Sequential(\n",
       "      (0): Conv2d_SAME(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (25): Sequential(\n",
       "      (0): Conv2d_SAME(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (0): Conv2d_SAME(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmdnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdnn.conversion.pytorch.pytorch_graph import PytorchGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = PytorchGraph(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_jit_pass_onnx(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: torch::jit::Graph, arg1: torch._C._onnx.OperatorExportTypes) -> torch::jit::Graph\n\nInvoked with: graph(%0 : Float(1, 3, 224, 224),\n      %1 : Float(32, 3, 3, 3),\n      %2 : Float(32),\n      %3 : Float(32),\n      %4 : Float(32),\n      %5 : Float(32),\n      %6 : Long(),\n      %7 : Float(32, 1, 3, 3),\n      %8 : Float(32),\n      %9 : Float(32),\n      %10 : Float(32),\n      %11 : Float(32),\n      %12 : Long(),\n      %13 : Float(64, 32, 1, 1),\n      %14 : Float(64),\n      %15 : Float(64),\n      %16 : Float(64),\n      %17 : Float(64),\n      %18 : Long(),\n      %19 : Float(64, 1, 3, 3),\n      %20 : Float(64),\n      %21 : Float(64),\n      %22 : Float(64),\n      %23 : Float(64),\n      %24 : Long(),\n      %25 : Float(128, 64, 1, 1),\n      %26 : Float(128),\n      %27 : Float(128),\n      %28 : Float(128),\n      %29 : Float(128),\n      %30 : Long(),\n      %31 : Float(128, 1, 3, 3),\n      %32 : Float(128),\n      %33 : Float(128),\n      %34 : Float(128),\n      %35 : Float(128),\n      %36 : Long(),\n      %37 : Float(128, 128, 1, 1),\n      %38 : Float(128),\n      %39 : Float(128),\n      %40 : Float(128),\n      %41 : Float(128),\n      %42 : Long(),\n      %43 : Float(128, 1, 3, 3),\n      %44 : Float(128),\n      %45 : Float(128),\n      %46 : Float(128),\n      %47 : Float(128),\n      %48 : Long(),\n      %49 : Float(256, 128, 1, 1),\n      %50 : Float(256),\n      %51 : Float(256),\n      %52 : Float(256),\n      %53 : Float(256),\n      %54 : Long(),\n      %55 : Float(256, 1, 3, 3),\n      %56 : Float(256),\n      %57 : Float(256),\n      %58 : Float(256),\n      %59 : Float(256),\n      %60 : Long(),\n      %61 : Float(256, 256, 1, 1),\n      %62 : Float(256),\n      %63 : Float(256),\n      %64 : Float(256),\n      %65 : Float(256),\n      %66 : Long(),\n      %67 : Float(256, 1, 3, 3),\n      %68 : Float(256),\n      %69 : Float(256),\n      %70 : Float(256),\n      %71 : Float(256),\n      %72 : Long(),\n      %73 : Float(512, 256, 1, 1),\n      %74 : Float(512),\n      %75 : Float(512),\n      %76 : Float(512),\n      %77 : Float(512),\n      %78 : Long(),\n      %79 : Float(512, 1, 3, 3),\n      %80 : Float(512),\n      %81 : Float(512),\n      %82 : Float(512),\n      %83 : Float(512),\n      %84 : Long(),\n      %85 : Float(512, 512, 1, 1),\n      %86 : Float(512),\n      %87 : Float(512),\n      %88 : Float(512),\n      %89 : Float(512),\n      %90 : Long(),\n      %91 : Float(512, 1, 3, 3),\n      %92 : Float(512),\n      %93 : Float(512),\n      %94 : Float(512),\n      %95 : Float(512),\n      %96 : Long(),\n      %97 : Float(512, 512, 1, 1),\n      %98 : Float(512),\n      %99 : Float(512),\n      %100 : Float(512),\n      %101 : Float(512),\n      %102 : Long(),\n      %103 : Float(512, 1, 3, 3),\n      %104 : Float(512),\n      %105 : Float(512),\n      %106 : Float(512),\n      %107 : Float(512),\n      %108 : Long(),\n      %109 : Float(512, 512, 1, 1),\n      %110 : Float(512),\n      %111 : Float(512),\n      %112 : Float(512),\n      %113 : Float(512),\n      %114 : Long(),\n      %115 : Float(512, 1, 3, 3),\n      %116 : Float(512),\n      %117 : Float(512),\n      %118 : Float(512),\n      %119 : Float(512),\n      %120 : Long(),\n      %121 : Float(512, 512, 1, 1),\n      %122 : Float(512),\n      %123 : Float(512),\n      %124 : Float(512),\n      %125 : Float(512),\n      %126 : Long(),\n      %127 : Float(512, 1, 3, 3),\n      %128 : Float(512),\n      %129 : Float(512),\n      %130 : Float(512),\n      %131 : Float(512),\n      %132 : Long(),\n      %133 : Float(512, 512, 1, 1),\n      %134 : Float(512),\n      %135 : Float(512),\n      %136 : Float(512),\n      %137 : Float(512),\n      %138 : Long(),\n      %139 : Float(512, 1, 3, 3),\n      %140 : Float(512),\n      %141 : Float(512),\n      %142 : Float(512),\n      %143 : Float(512),\n      %144 : Long(),\n      %145 : Float(1024, 512, 1, 1),\n      %146 : Float(1024),\n      %147 : Float(1024),\n      %148 : Float(1024),\n      %149 : Float(1024),\n      %150 : Long(),\n      %151 : Float(1024, 1, 3, 3),\n      %152 : Float(1024),\n      %153 : Float(1024),\n      %154 : Float(1024),\n      %155 : Float(1024),\n      %156 : Long(),\n      %157 : Float(1024, 1024, 1, 1),\n      %158 : Float(1024),\n      %159 : Float(1024),\n      %160 : Float(1024),\n      %161 : Float(1024),\n      %162 : Long(),\n      %163 : Float(1000, 1024),\n      %164 : Float(1000)):\n  %330 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %331 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %332 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %333 : int[] = prim::ListConstruct(%331, %332), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %334 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %335 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %336 : int[] = prim::ListConstruct(%334, %335), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %337 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %338 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %339 : int[] = prim::ListConstruct(%337, %338), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %340 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %341 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %342 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %343 : int[] = prim::ListConstruct(%341, %342), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %344 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %345 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %346 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %347 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %input.1 : Float(1, 32, 112, 112) = aten::_convolution(%0, %1, %330, %333, %336, %339, %340, %343, %344, %345, %346, %347), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %349 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %350 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %351 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %352 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %input.2 : Float(1, 32, 112, 112) = aten::batch_norm(%input.1, %2, %3, %4, %5, %349, %350, %351, %352), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %354 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %355 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %356 : Float(1, 32, 112, 112) = aten::hardtanh(%input.2, %354, %355), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %357 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %358 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %359 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %360 : int[] = prim::ListConstruct(%358, %359), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %361 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %362 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %363 : int[] = prim::ListConstruct(%361, %362), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %364 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %365 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %366 : int[] = prim::ListConstruct(%364, %365), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %367 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %368 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %369 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %370 : int[] = prim::ListConstruct(%368, %369), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %371 : int = prim::Constant[value=32](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %372 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %373 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %374 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %input.3 : Float(1, 32, 112, 112) = aten::_convolution(%356, %7, %357, %360, %363, %366, %367, %370, %371, %372, %373, %374), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %376 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %377 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %378 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %379 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %input.4 : Float(1, 32, 112, 112) = aten::batch_norm(%input.3, %8, %9, %10, %11, %376, %377, %378, %379), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %381 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %382 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %383 : Float(1, 32, 112, 112) = aten::hardtanh(%input.4, %381, %382), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %384 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %385 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %386 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %387 : int[] = prim::ListConstruct(%385, %386), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %388 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %389 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %390 : int[] = prim::ListConstruct(%388, %389), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %391 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %392 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %393 : int[] = prim::ListConstruct(%391, %392), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %394 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %395 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %396 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %397 : int[] = prim::ListConstruct(%395, %396), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %398 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %399 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %400 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %401 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %input.5 : Float(1, 64, 112, 112) = aten::_convolution(%383, %13, %384, %387, %390, %393, %394, %397, %398, %399, %400, %401), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %403 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %404 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %405 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %406 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %input.6 : Float(1, 64, 112, 112) = aten::batch_norm(%input.5, %14, %15, %16, %17, %403, %404, %405, %406), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %408 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %409 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %410 : Float(1, 64, 112, 112) = aten::hardtanh(%input.6, %408, %409), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %411 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %412 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %413 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %414 : int[] = prim::ListConstruct(%412, %413), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %415 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %416 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %417 : int[] = prim::ListConstruct(%415, %416), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %418 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %419 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %420 : int[] = prim::ListConstruct(%418, %419), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %421 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %422 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %423 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %424 : int[] = prim::ListConstruct(%422, %423), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %425 : int = prim::Constant[value=64](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %426 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %427 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %428 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %input.7 : Float(1, 64, 56, 56) = aten::_convolution(%410, %19, %411, %414, %417, %420, %421, %424, %425, %426, %427, %428), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %430 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %431 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %432 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %433 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %input.8 : Float(1, 64, 56, 56) = aten::batch_norm(%input.7, %20, %21, %22, %23, %430, %431, %432, %433), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %435 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %436 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %437 : Float(1, 64, 56, 56) = aten::hardtanh(%input.8, %435, %436), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %438 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %439 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %440 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %441 : int[] = prim::ListConstruct(%439, %440), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %442 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %443 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %444 : int[] = prim::ListConstruct(%442, %443), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %445 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %446 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %447 : int[] = prim::ListConstruct(%445, %446), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %448 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %449 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %450 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %451 : int[] = prim::ListConstruct(%449, %450), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %452 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %453 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %454 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %455 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %input.9 : Float(1, 128, 56, 56) = aten::_convolution(%437, %25, %438, %441, %444, %447, %448, %451, %452, %453, %454, %455), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %457 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %458 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %459 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %460 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %input.10 : Float(1, 128, 56, 56) = aten::batch_norm(%input.9, %26, %27, %28, %29, %457, %458, %459, %460), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %462 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %463 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %464 : Float(1, 128, 56, 56) = aten::hardtanh(%input.10, %462, %463), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %465 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %466 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %467 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %468 : int[] = prim::ListConstruct(%466, %467), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %469 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %470 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %471 : int[] = prim::ListConstruct(%469, %470), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %472 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %473 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %474 : int[] = prim::ListConstruct(%472, %473), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %475 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %476 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %477 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %478 : int[] = prim::ListConstruct(%476, %477), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %479 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %480 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %481 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %482 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %input.11 : Float(1, 128, 56, 56) = aten::_convolution(%464, %31, %465, %468, %471, %474, %475, %478, %479, %480, %481, %482), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %484 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %485 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %486 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %487 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %input.12 : Float(1, 128, 56, 56) = aten::batch_norm(%input.11, %32, %33, %34, %35, %484, %485, %486, %487), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %489 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %490 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %491 : Float(1, 128, 56, 56) = aten::hardtanh(%input.12, %489, %490), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %492 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %493 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %494 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %495 : int[] = prim::ListConstruct(%493, %494), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %496 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %497 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %498 : int[] = prim::ListConstruct(%496, %497), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %499 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %500 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %501 : int[] = prim::ListConstruct(%499, %500), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %502 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %503 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %504 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %505 : int[] = prim::ListConstruct(%503, %504), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %506 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %507 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %508 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %509 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %input.13 : Float(1, 128, 56, 56) = aten::_convolution(%491, %37, %492, %495, %498, %501, %502, %505, %506, %507, %508, %509), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %511 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %512 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %513 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %514 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %input.14 : Float(1, 128, 56, 56) = aten::batch_norm(%input.13, %38, %39, %40, %41, %511, %512, %513, %514), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %516 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %517 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %518 : Float(1, 128, 56, 56) = aten::hardtanh(%input.14, %516, %517), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %519 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %520 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %521 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %522 : int[] = prim::ListConstruct(%520, %521), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %523 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %524 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %525 : int[] = prim::ListConstruct(%523, %524), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %526 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %527 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %528 : int[] = prim::ListConstruct(%526, %527), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %529 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %530 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %531 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %532 : int[] = prim::ListConstruct(%530, %531), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %533 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %534 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %535 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %536 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %input.15 : Float(1, 128, 28, 28) = aten::_convolution(%518, %43, %519, %522, %525, %528, %529, %532, %533, %534, %535, %536), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %538 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %539 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %540 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %541 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %input.16 : Float(1, 128, 28, 28) = aten::batch_norm(%input.15, %44, %45, %46, %47, %538, %539, %540, %541), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %543 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %544 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %545 : Float(1, 128, 28, 28) = aten::hardtanh(%input.16, %543, %544), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %546 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %547 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %548 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %549 : int[] = prim::ListConstruct(%547, %548), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %550 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %551 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %552 : int[] = prim::ListConstruct(%550, %551), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %553 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %554 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %555 : int[] = prim::ListConstruct(%553, %554), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %556 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %557 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %558 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %559 : int[] = prim::ListConstruct(%557, %558), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %560 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %561 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %562 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %563 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %input.17 : Float(1, 256, 28, 28) = aten::_convolution(%545, %49, %546, %549, %552, %555, %556, %559, %560, %561, %562, %563), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %565 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %566 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %567 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %568 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %input.18 : Float(1, 256, 28, 28) = aten::batch_norm(%input.17, %50, %51, %52, %53, %565, %566, %567, %568), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %570 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %571 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %572 : Float(1, 256, 28, 28) = aten::hardtanh(%input.18, %570, %571), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %573 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %574 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %575 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %576 : int[] = prim::ListConstruct(%574, %575), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %577 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %578 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %579 : int[] = prim::ListConstruct(%577, %578), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %580 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %581 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %582 : int[] = prim::ListConstruct(%580, %581), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %583 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %584 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %585 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %586 : int[] = prim::ListConstruct(%584, %585), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %587 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %588 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %589 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %590 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %input.19 : Float(1, 256, 28, 28) = aten::_convolution(%572, %55, %573, %576, %579, %582, %583, %586, %587, %588, %589, %590), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %592 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %593 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %594 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %595 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %input.20 : Float(1, 256, 28, 28) = aten::batch_norm(%input.19, %56, %57, %58, %59, %592, %593, %594, %595), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %597 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %598 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %599 : Float(1, 256, 28, 28) = aten::hardtanh(%input.20, %597, %598), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %600 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %601 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %602 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %603 : int[] = prim::ListConstruct(%601, %602), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %604 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %605 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %606 : int[] = prim::ListConstruct(%604, %605), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %607 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %608 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %609 : int[] = prim::ListConstruct(%607, %608), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %610 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %611 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %612 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %613 : int[] = prim::ListConstruct(%611, %612), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %614 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %615 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %616 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %617 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %input.21 : Float(1, 256, 28, 28) = aten::_convolution(%599, %61, %600, %603, %606, %609, %610, %613, %614, %615, %616, %617), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %619 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %620 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %621 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %622 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %input.22 : Float(1, 256, 28, 28) = aten::batch_norm(%input.21, %62, %63, %64, %65, %619, %620, %621, %622), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %624 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %625 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %626 : Float(1, 256, 28, 28) = aten::hardtanh(%input.22, %624, %625), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %627 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %628 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %629 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %630 : int[] = prim::ListConstruct(%628, %629), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %631 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %632 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %633 : int[] = prim::ListConstruct(%631, %632), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %634 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %635 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %636 : int[] = prim::ListConstruct(%634, %635), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %637 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %638 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %639 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %640 : int[] = prim::ListConstruct(%638, %639), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %641 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %642 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %643 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %644 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %input.23 : Float(1, 256, 14, 14) = aten::_convolution(%626, %67, %627, %630, %633, %636, %637, %640, %641, %642, %643, %644), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %646 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %647 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %648 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %649 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %input.24 : Float(1, 256, 14, 14) = aten::batch_norm(%input.23, %68, %69, %70, %71, %646, %647, %648, %649), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %651 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %652 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %653 : Float(1, 256, 14, 14) = aten::hardtanh(%input.24, %651, %652), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %654 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %655 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %656 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %657 : int[] = prim::ListConstruct(%655, %656), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %658 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %659 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %660 : int[] = prim::ListConstruct(%658, %659), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %661 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %662 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %663 : int[] = prim::ListConstruct(%661, %662), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %664 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %665 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %666 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %667 : int[] = prim::ListConstruct(%665, %666), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %668 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %669 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %670 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %671 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %input.25 : Float(1, 512, 14, 14) = aten::_convolution(%653, %73, %654, %657, %660, %663, %664, %667, %668, %669, %670, %671), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %673 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %674 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %675 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %676 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %input.26 : Float(1, 512, 14, 14) = aten::batch_norm(%input.25, %74, %75, %76, %77, %673, %674, %675, %676), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %678 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %679 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %680 : Float(1, 512, 14, 14) = aten::hardtanh(%input.26, %678, %679), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %681 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %682 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %683 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %684 : int[] = prim::ListConstruct(%682, %683), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %685 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %686 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %687 : int[] = prim::ListConstruct(%685, %686), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %688 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %689 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %690 : int[] = prim::ListConstruct(%688, %689), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %691 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %692 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %693 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %694 : int[] = prim::ListConstruct(%692, %693), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %695 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %696 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %697 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %698 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %input.27 : Float(1, 512, 14, 14) = aten::_convolution(%680, %79, %681, %684, %687, %690, %691, %694, %695, %696, %697, %698), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %700 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %701 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %702 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %703 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %input.28 : Float(1, 512, 14, 14) = aten::batch_norm(%input.27, %80, %81, %82, %83, %700, %701, %702, %703), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %705 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %706 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %707 : Float(1, 512, 14, 14) = aten::hardtanh(%input.28, %705, %706), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %708 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %709 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %710 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %711 : int[] = prim::ListConstruct(%709, %710), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %712 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %713 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %714 : int[] = prim::ListConstruct(%712, %713), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %715 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %716 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %717 : int[] = prim::ListConstruct(%715, %716), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %718 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %719 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %720 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %721 : int[] = prim::ListConstruct(%719, %720), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %722 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %723 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %724 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %725 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %input.29 : Float(1, 512, 14, 14) = aten::_convolution(%707, %85, %708, %711, %714, %717, %718, %721, %722, %723, %724, %725), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %727 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %728 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %729 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %730 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %input.30 : Float(1, 512, 14, 14) = aten::batch_norm(%input.29, %86, %87, %88, %89, %727, %728, %729, %730), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %732 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %733 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %734 : Float(1, 512, 14, 14) = aten::hardtanh(%input.30, %732, %733), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %735 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %736 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %737 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %738 : int[] = prim::ListConstruct(%736, %737), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %739 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %740 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %741 : int[] = prim::ListConstruct(%739, %740), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %742 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %743 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %744 : int[] = prim::ListConstruct(%742, %743), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %745 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %746 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %747 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %748 : int[] = prim::ListConstruct(%746, %747), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %749 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %750 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %751 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %752 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %input.31 : Float(1, 512, 14, 14) = aten::_convolution(%734, %91, %735, %738, %741, %744, %745, %748, %749, %750, %751, %752), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %754 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %755 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %756 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %757 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %input.32 : Float(1, 512, 14, 14) = aten::batch_norm(%input.31, %92, %93, %94, %95, %754, %755, %756, %757), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %759 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %760 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %761 : Float(1, 512, 14, 14) = aten::hardtanh(%input.32, %759, %760), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %762 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %763 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %764 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %765 : int[] = prim::ListConstruct(%763, %764), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %766 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %767 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %768 : int[] = prim::ListConstruct(%766, %767), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %769 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %770 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %771 : int[] = prim::ListConstruct(%769, %770), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %772 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %773 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %774 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %775 : int[] = prim::ListConstruct(%773, %774), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %776 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %777 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %778 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %779 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %input.33 : Float(1, 512, 14, 14) = aten::_convolution(%761, %97, %762, %765, %768, %771, %772, %775, %776, %777, %778, %779), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %781 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %782 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %783 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %784 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %input.34 : Float(1, 512, 14, 14) = aten::batch_norm(%input.33, %98, %99, %100, %101, %781, %782, %783, %784), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %786 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %787 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %788 : Float(1, 512, 14, 14) = aten::hardtanh(%input.34, %786, %787), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %789 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %790 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %791 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %792 : int[] = prim::ListConstruct(%790, %791), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %793 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %794 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %795 : int[] = prim::ListConstruct(%793, %794), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %796 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %797 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %798 : int[] = prim::ListConstruct(%796, %797), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %799 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %800 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %801 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %802 : int[] = prim::ListConstruct(%800, %801), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %803 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %804 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %805 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %806 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %input.35 : Float(1, 512, 14, 14) = aten::_convolution(%788, %103, %789, %792, %795, %798, %799, %802, %803, %804, %805, %806), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %808 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %809 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %810 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %811 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %input.36 : Float(1, 512, 14, 14) = aten::batch_norm(%input.35, %104, %105, %106, %107, %808, %809, %810, %811), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %813 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %814 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %815 : Float(1, 512, 14, 14) = aten::hardtanh(%input.36, %813, %814), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %816 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %817 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %818 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %819 : int[] = prim::ListConstruct(%817, %818), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %820 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %821 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %822 : int[] = prim::ListConstruct(%820, %821), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %823 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %824 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %825 : int[] = prim::ListConstruct(%823, %824), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %826 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %827 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %828 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %829 : int[] = prim::ListConstruct(%827, %828), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %830 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %831 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %832 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %833 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %input.37 : Float(1, 512, 14, 14) = aten::_convolution(%815, %109, %816, %819, %822, %825, %826, %829, %830, %831, %832, %833), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %835 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %836 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %837 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %838 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %input.38 : Float(1, 512, 14, 14) = aten::batch_norm(%input.37, %110, %111, %112, %113, %835, %836, %837, %838), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %840 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %841 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %842 : Float(1, 512, 14, 14) = aten::hardtanh(%input.38, %840, %841), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %843 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %844 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %845 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %846 : int[] = prim::ListConstruct(%844, %845), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %847 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %848 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %849 : int[] = prim::ListConstruct(%847, %848), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %850 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %851 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %852 : int[] = prim::ListConstruct(%850, %851), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %853 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %854 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %855 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %856 : int[] = prim::ListConstruct(%854, %855), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %857 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %858 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %859 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %860 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %input.39 : Float(1, 512, 14, 14) = aten::_convolution(%842, %115, %843, %846, %849, %852, %853, %856, %857, %858, %859, %860), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %862 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %863 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %864 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %865 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %input.40 : Float(1, 512, 14, 14) = aten::batch_norm(%input.39, %116, %117, %118, %119, %862, %863, %864, %865), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %867 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %868 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %869 : Float(1, 512, 14, 14) = aten::hardtanh(%input.40, %867, %868), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %870 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %871 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %872 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %873 : int[] = prim::ListConstruct(%871, %872), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %874 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %875 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %876 : int[] = prim::ListConstruct(%874, %875), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %877 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %878 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %879 : int[] = prim::ListConstruct(%877, %878), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %880 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %881 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %882 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %883 : int[] = prim::ListConstruct(%881, %882), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %884 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %885 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %886 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %887 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %input.41 : Float(1, 512, 14, 14) = aten::_convolution(%869, %121, %870, %873, %876, %879, %880, %883, %884, %885, %886, %887), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %889 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %890 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %891 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %892 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %input.42 : Float(1, 512, 14, 14) = aten::batch_norm(%input.41, %122, %123, %124, %125, %889, %890, %891, %892), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %894 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %895 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %896 : Float(1, 512, 14, 14) = aten::hardtanh(%input.42, %894, %895), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %897 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %898 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %899 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %900 : int[] = prim::ListConstruct(%898, %899), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %901 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %902 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %903 : int[] = prim::ListConstruct(%901, %902), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %904 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %905 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %906 : int[] = prim::ListConstruct(%904, %905), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %907 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %908 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %909 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %910 : int[] = prim::ListConstruct(%908, %909), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %911 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %912 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %913 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %914 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %input.43 : Float(1, 512, 14, 14) = aten::_convolution(%896, %127, %897, %900, %903, %906, %907, %910, %911, %912, %913, %914), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %916 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %917 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %918 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %919 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %input.44 : Float(1, 512, 14, 14) = aten::batch_norm(%input.43, %128, %129, %130, %131, %916, %917, %918, %919), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %921 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %922 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %923 : Float(1, 512, 14, 14) = aten::hardtanh(%input.44, %921, %922), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %924 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %925 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %926 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %927 : int[] = prim::ListConstruct(%925, %926), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %928 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %929 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %930 : int[] = prim::ListConstruct(%928, %929), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %931 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %932 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %933 : int[] = prim::ListConstruct(%931, %932), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %934 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %935 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %936 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %937 : int[] = prim::ListConstruct(%935, %936), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %938 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %939 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %940 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %941 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %input.45 : Float(1, 512, 14, 14) = aten::_convolution(%923, %133, %924, %927, %930, %933, %934, %937, %938, %939, %940, %941), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %943 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %944 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %945 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %946 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %input.46 : Float(1, 512, 14, 14) = aten::batch_norm(%input.45, %134, %135, %136, %137, %943, %944, %945, %946), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %948 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %949 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %950 : Float(1, 512, 14, 14) = aten::hardtanh(%input.46, %948, %949), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %951 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %952 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %953 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %954 : int[] = prim::ListConstruct(%952, %953), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %955 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %956 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %957 : int[] = prim::ListConstruct(%955, %956), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %958 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %959 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %960 : int[] = prim::ListConstruct(%958, %959), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %961 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %962 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %963 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %964 : int[] = prim::ListConstruct(%962, %963), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %965 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %966 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %967 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %968 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %input.47 : Float(1, 512, 7, 7) = aten::_convolution(%950, %139, %951, %954, %957, %960, %961, %964, %965, %966, %967, %968), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %970 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %971 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %972 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %973 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %input.48 : Float(1, 512, 7, 7) = aten::batch_norm(%input.47, %140, %141, %142, %143, %970, %971, %972, %973), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %975 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %976 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %977 : Float(1, 512, 7, 7) = aten::hardtanh(%input.48, %975, %976), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %978 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %979 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %980 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %981 : int[] = prim::ListConstruct(%979, %980), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %982 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %983 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %984 : int[] = prim::ListConstruct(%982, %983), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %985 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %986 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %987 : int[] = prim::ListConstruct(%985, %986), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %988 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %989 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %990 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %991 : int[] = prim::ListConstruct(%989, %990), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %992 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %993 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %994 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %995 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %input.49 : Float(1, 1024, 7, 7) = aten::_convolution(%977, %145, %978, %981, %984, %987, %988, %991, %992, %993, %994, %995), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %997 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %998 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %999 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %1000 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %input.50 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.49, %146, %147, %148, %149, %997, %998, %999, %1000), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %1002 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1003 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1004 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.50, %1002, %1003), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1005 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1006 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1007 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1008 : int[] = prim::ListConstruct(%1006, %1007), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1009 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1010 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1011 : int[] = prim::ListConstruct(%1009, %1010), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1012 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1013 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1014 : int[] = prim::ListConstruct(%1012, %1013), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1015 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1016 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1017 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1018 : int[] = prim::ListConstruct(%1016, %1017), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1019 : int = prim::Constant[value=1024](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1020 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1021 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1022 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %input.51 : Float(1, 1024, 7, 7) = aten::_convolution(%1004, %151, %1005, %1008, %1011, %1014, %1015, %1018, %1019, %1020, %1021, %1022), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1024 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1025 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1026 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1027 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %input.52 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.51, %152, %153, %154, %155, %1024, %1025, %1026, %1027), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1029 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1030 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1031 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.52, %1029, %1030), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1032 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1033 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1034 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1035 : int[] = prim::ListConstruct(%1033, %1034), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1036 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1037 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1038 : int[] = prim::ListConstruct(%1036, %1037), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1039 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1040 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1041 : int[] = prim::ListConstruct(%1039, %1040), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1042 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1043 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1044 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1045 : int[] = prim::ListConstruct(%1043, %1044), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1046 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1047 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1048 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1049 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %input.53 : Float(1, 1024, 7, 7) = aten::_convolution(%1031, %157, %1032, %1035, %1038, %1041, %1042, %1045, %1046, %1047, %1048, %1049), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1051 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1052 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1053 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1054 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %input.54 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.53, %158, %159, %160, %161, %1051, %1052, %1053, %1054), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1056 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1057 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1058 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.54, %1056, %1057), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1059 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1060 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1061 : int[] = prim::ListConstruct(%1059, %1060), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1062 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1063 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1064 : int[] = prim::ListConstruct(%1062, %1063), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1065 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1066 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1067 : int[] = prim::ListConstruct(%1065, %1066), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1068 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1069 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1070 : Float(1, 1024, 1, 1) = aten::avg_pool2d(%1058, %1061, %1064, %1067, %1068, %1069), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1071 : int = prim::Constant[value=-1](), scope: mobilenet_real\n  %1072 : int = prim::Constant[value=1024](), scope: mobilenet_real\n  %1073 : int[] = prim::ListConstruct(%1071, %1072), scope: mobilenet_real\n  %input : Float(1, 1024) = aten::view(%1070, %1073), scope: mobilenet_real\n  %1075 : Float(1024!, 1000!) = aten::t(%163), scope: mobilenet_real/Linear[fc]\n  %1076 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n  %1077 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n  %1078 : Float(1, 1000) = aten::addmm(%164, %input, %1075, %1076, %1077), scope: mobilenet_real/Linear[fc]\n  return (%1078)\n, False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ac90457ff46d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mmdnn/conversion/pytorch/pytorch_graph.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPytorchGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;31m# nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mmdnn/conversion/pytorch/pytorch_graph.py\u001b[0m in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, aten, export_raw_ir)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexport_raw_ir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_peephole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _jit_pass_onnx(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: torch::jit::Graph, arg1: torch._C._onnx.OperatorExportTypes) -> torch::jit::Graph\n\nInvoked with: graph(%0 : Float(1, 3, 224, 224),\n      %1 : Float(32, 3, 3, 3),\n      %2 : Float(32),\n      %3 : Float(32),\n      %4 : Float(32),\n      %5 : Float(32),\n      %6 : Long(),\n      %7 : Float(32, 1, 3, 3),\n      %8 : Float(32),\n      %9 : Float(32),\n      %10 : Float(32),\n      %11 : Float(32),\n      %12 : Long(),\n      %13 : Float(64, 32, 1, 1),\n      %14 : Float(64),\n      %15 : Float(64),\n      %16 : Float(64),\n      %17 : Float(64),\n      %18 : Long(),\n      %19 : Float(64, 1, 3, 3),\n      %20 : Float(64),\n      %21 : Float(64),\n      %22 : Float(64),\n      %23 : Float(64),\n      %24 : Long(),\n      %25 : Float(128, 64, 1, 1),\n      %26 : Float(128),\n      %27 : Float(128),\n      %28 : Float(128),\n      %29 : Float(128),\n      %30 : Long(),\n      %31 : Float(128, 1, 3, 3),\n      %32 : Float(128),\n      %33 : Float(128),\n      %34 : Float(128),\n      %35 : Float(128),\n      %36 : Long(),\n      %37 : Float(128, 128, 1, 1),\n      %38 : Float(128),\n      %39 : Float(128),\n      %40 : Float(128),\n      %41 : Float(128),\n      %42 : Long(),\n      %43 : Float(128, 1, 3, 3),\n      %44 : Float(128),\n      %45 : Float(128),\n      %46 : Float(128),\n      %47 : Float(128),\n      %48 : Long(),\n      %49 : Float(256, 128, 1, 1),\n      %50 : Float(256),\n      %51 : Float(256),\n      %52 : Float(256),\n      %53 : Float(256),\n      %54 : Long(),\n      %55 : Float(256, 1, 3, 3),\n      %56 : Float(256),\n      %57 : Float(256),\n      %58 : Float(256),\n      %59 : Float(256),\n      %60 : Long(),\n      %61 : Float(256, 256, 1, 1),\n      %62 : Float(256),\n      %63 : Float(256),\n      %64 : Float(256),\n      %65 : Float(256),\n      %66 : Long(),\n      %67 : Float(256, 1, 3, 3),\n      %68 : Float(256),\n      %69 : Float(256),\n      %70 : Float(256),\n      %71 : Float(256),\n      %72 : Long(),\n      %73 : Float(512, 256, 1, 1),\n      %74 : Float(512),\n      %75 : Float(512),\n      %76 : Float(512),\n      %77 : Float(512),\n      %78 : Long(),\n      %79 : Float(512, 1, 3, 3),\n      %80 : Float(512),\n      %81 : Float(512),\n      %82 : Float(512),\n      %83 : Float(512),\n      %84 : Long(),\n      %85 : Float(512, 512, 1, 1),\n      %86 : Float(512),\n      %87 : Float(512),\n      %88 : Float(512),\n      %89 : Float(512),\n      %90 : Long(),\n      %91 : Float(512, 1, 3, 3),\n      %92 : Float(512),\n      %93 : Float(512),\n      %94 : Float(512),\n      %95 : Float(512),\n      %96 : Long(),\n      %97 : Float(512, 512, 1, 1),\n      %98 : Float(512),\n      %99 : Float(512),\n      %100 : Float(512),\n      %101 : Float(512),\n      %102 : Long(),\n      %103 : Float(512, 1, 3, 3),\n      %104 : Float(512),\n      %105 : Float(512),\n      %106 : Float(512),\n      %107 : Float(512),\n      %108 : Long(),\n      %109 : Float(512, 512, 1, 1),\n      %110 : Float(512),\n      %111 : Float(512),\n      %112 : Float(512),\n      %113 : Float(512),\n      %114 : Long(),\n      %115 : Float(512, 1, 3, 3),\n      %116 : Float(512),\n      %117 : Float(512),\n      %118 : Float(512),\n      %119 : Float(512),\n      %120 : Long(),\n      %121 : Float(512, 512, 1, 1),\n      %122 : Float(512),\n      %123 : Float(512),\n      %124 : Float(512),\n      %125 : Float(512),\n      %126 : Long(),\n      %127 : Float(512, 1, 3, 3),\n      %128 : Float(512),\n      %129 : Float(512),\n      %130 : Float(512),\n      %131 : Float(512),\n      %132 : Long(),\n      %133 : Float(512, 512, 1, 1),\n      %134 : Float(512),\n      %135 : Float(512),\n      %136 : Float(512),\n      %137 : Float(512),\n      %138 : Long(),\n      %139 : Float(512, 1, 3, 3),\n      %140 : Float(512),\n      %141 : Float(512),\n      %142 : Float(512),\n      %143 : Float(512),\n      %144 : Long(),\n      %145 : Float(1024, 512, 1, 1),\n      %146 : Float(1024),\n      %147 : Float(1024),\n      %148 : Float(1024),\n      %149 : Float(1024),\n      %150 : Long(),\n      %151 : Float(1024, 1, 3, 3),\n      %152 : Float(1024),\n      %153 : Float(1024),\n      %154 : Float(1024),\n      %155 : Float(1024),\n      %156 : Long(),\n      %157 : Float(1024, 1024, 1, 1),\n      %158 : Float(1024),\n      %159 : Float(1024),\n      %160 : Float(1024),\n      %161 : Float(1024),\n      %162 : Long(),\n      %163 : Float(1000, 1024),\n      %164 : Float(1000)):\n  %330 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %331 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %332 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %333 : int[] = prim::ListConstruct(%331, %332), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %334 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %335 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %336 : int[] = prim::ListConstruct(%334, %335), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %337 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %338 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %339 : int[] = prim::ListConstruct(%337, %338), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %340 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %341 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %342 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %343 : int[] = prim::ListConstruct(%341, %342), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %344 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %345 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %346 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %347 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %input.1 : Float(1, 32, 112, 112) = aten::_convolution(%0, %1, %330, %333, %336, %339, %340, %343, %344, %345, %346, %347), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %349 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %350 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %351 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %352 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %input.2 : Float(1, 32, 112, 112) = aten::batch_norm(%input.1, %2, %3, %4, %5, %349, %350, %351, %352), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %354 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %355 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %356 : Float(1, 32, 112, 112) = aten::hardtanh(%input.2, %354, %355), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %357 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %358 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %359 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %360 : int[] = prim::ListConstruct(%358, %359), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %361 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %362 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %363 : int[] = prim::ListConstruct(%361, %362), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %364 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %365 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %366 : int[] = prim::ListConstruct(%364, %365), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %367 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %368 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %369 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %370 : int[] = prim::ListConstruct(%368, %369), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %371 : int = prim::Constant[value=32](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %372 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %373 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %374 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %input.3 : Float(1, 32, 112, 112) = aten::_convolution(%356, %7, %357, %360, %363, %366, %367, %370, %371, %372, %373, %374), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %376 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %377 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %378 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %379 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %input.4 : Float(1, 32, 112, 112) = aten::batch_norm(%input.3, %8, %9, %10, %11, %376, %377, %378, %379), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %381 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %382 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %383 : Float(1, 32, 112, 112) = aten::hardtanh(%input.4, %381, %382), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %384 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %385 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %386 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %387 : int[] = prim::ListConstruct(%385, %386), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %388 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %389 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %390 : int[] = prim::ListConstruct(%388, %389), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %391 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %392 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %393 : int[] = prim::ListConstruct(%391, %392), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %394 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %395 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %396 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %397 : int[] = prim::ListConstruct(%395, %396), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %398 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %399 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %400 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %401 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %input.5 : Float(1, 64, 112, 112) = aten::_convolution(%383, %13, %384, %387, %390, %393, %394, %397, %398, %399, %400, %401), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %403 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %404 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %405 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %406 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %input.6 : Float(1, 64, 112, 112) = aten::batch_norm(%input.5, %14, %15, %16, %17, %403, %404, %405, %406), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %408 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %409 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %410 : Float(1, 64, 112, 112) = aten::hardtanh(%input.6, %408, %409), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %411 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %412 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %413 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %414 : int[] = prim::ListConstruct(%412, %413), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %415 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %416 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %417 : int[] = prim::ListConstruct(%415, %416), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %418 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %419 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %420 : int[] = prim::ListConstruct(%418, %419), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %421 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %422 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %423 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %424 : int[] = prim::ListConstruct(%422, %423), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %425 : int = prim::Constant[value=64](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %426 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %427 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %428 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %input.7 : Float(1, 64, 56, 56) = aten::_convolution(%410, %19, %411, %414, %417, %420, %421, %424, %425, %426, %427, %428), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %430 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %431 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %432 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %433 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %input.8 : Float(1, 64, 56, 56) = aten::batch_norm(%input.7, %20, %21, %22, %23, %430, %431, %432, %433), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %435 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %436 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %437 : Float(1, 64, 56, 56) = aten::hardtanh(%input.8, %435, %436), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %438 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %439 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %440 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %441 : int[] = prim::ListConstruct(%439, %440), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %442 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %443 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %444 : int[] = prim::ListConstruct(%442, %443), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %445 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %446 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %447 : int[] = prim::ListConstruct(%445, %446), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %448 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %449 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %450 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %451 : int[] = prim::ListConstruct(%449, %450), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %452 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %453 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %454 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %455 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %input.9 : Float(1, 128, 56, 56) = aten::_convolution(%437, %25, %438, %441, %444, %447, %448, %451, %452, %453, %454, %455), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %457 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %458 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %459 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %460 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %input.10 : Float(1, 128, 56, 56) = aten::batch_norm(%input.9, %26, %27, %28, %29, %457, %458, %459, %460), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %462 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %463 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %464 : Float(1, 128, 56, 56) = aten::hardtanh(%input.10, %462, %463), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %465 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %466 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %467 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %468 : int[] = prim::ListConstruct(%466, %467), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %469 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %470 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %471 : int[] = prim::ListConstruct(%469, %470), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %472 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %473 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %474 : int[] = prim::ListConstruct(%472, %473), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %475 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %476 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %477 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %478 : int[] = prim::ListConstruct(%476, %477), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %479 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %480 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %481 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %482 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %input.11 : Float(1, 128, 56, 56) = aten::_convolution(%464, %31, %465, %468, %471, %474, %475, %478, %479, %480, %481, %482), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %484 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %485 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %486 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %487 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %input.12 : Float(1, 128, 56, 56) = aten::batch_norm(%input.11, %32, %33, %34, %35, %484, %485, %486, %487), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %489 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %490 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %491 : Float(1, 128, 56, 56) = aten::hardtanh(%input.12, %489, %490), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %492 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %493 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %494 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %495 : int[] = prim::ListConstruct(%493, %494), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %496 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %497 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %498 : int[] = prim::ListConstruct(%496, %497), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %499 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %500 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %501 : int[] = prim::ListConstruct(%499, %500), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %502 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %503 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %504 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %505 : int[] = prim::ListConstruct(%503, %504), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %506 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %507 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %508 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %509 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %input.13 : Float(1, 128, 56, 56) = aten::_convolution(%491, %37, %492, %495, %498, %501, %502, %505, %506, %507, %508, %509), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %511 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %512 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %513 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %514 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %input.14 : Float(1, 128, 56, 56) = aten::batch_norm(%input.13, %38, %39, %40, %41, %511, %512, %513, %514), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %516 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %517 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %518 : Float(1, 128, 56, 56) = aten::hardtanh(%input.14, %516, %517), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %519 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %520 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %521 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %522 : int[] = prim::ListConstruct(%520, %521), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %523 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %524 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %525 : int[] = prim::ListConstruct(%523, %524), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %526 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %527 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %528 : int[] = prim::ListConstruct(%526, %527), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %529 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %530 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %531 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %532 : int[] = prim::ListConstruct(%530, %531), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %533 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %534 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %535 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %536 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %input.15 : Float(1, 128, 28, 28) = aten::_convolution(%518, %43, %519, %522, %525, %528, %529, %532, %533, %534, %535, %536), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %538 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %539 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %540 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %541 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %input.16 : Float(1, 128, 28, 28) = aten::batch_norm(%input.15, %44, %45, %46, %47, %538, %539, %540, %541), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %543 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %544 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %545 : Float(1, 128, 28, 28) = aten::hardtanh(%input.16, %543, %544), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %546 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %547 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %548 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %549 : int[] = prim::ListConstruct(%547, %548), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %550 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %551 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %552 : int[] = prim::ListConstruct(%550, %551), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %553 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %554 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %555 : int[] = prim::ListConstruct(%553, %554), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %556 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %557 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %558 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %559 : int[] = prim::ListConstruct(%557, %558), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %560 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %561 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %562 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %563 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %input.17 : Float(1, 256, 28, 28) = aten::_convolution(%545, %49, %546, %549, %552, %555, %556, %559, %560, %561, %562, %563), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %565 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %566 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %567 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %568 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %input.18 : Float(1, 256, 28, 28) = aten::batch_norm(%input.17, %50, %51, %52, %53, %565, %566, %567, %568), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %570 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %571 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %572 : Float(1, 256, 28, 28) = aten::hardtanh(%input.18, %570, %571), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %573 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %574 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %575 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %576 : int[] = prim::ListConstruct(%574, %575), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %577 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %578 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %579 : int[] = prim::ListConstruct(%577, %578), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %580 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %581 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %582 : int[] = prim::ListConstruct(%580, %581), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %583 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %584 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %585 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %586 : int[] = prim::ListConstruct(%584, %585), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %587 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %588 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %589 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %590 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %input.19 : Float(1, 256, 28, 28) = aten::_convolution(%572, %55, %573, %576, %579, %582, %583, %586, %587, %588, %589, %590), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %592 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %593 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %594 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %595 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %input.20 : Float(1, 256, 28, 28) = aten::batch_norm(%input.19, %56, %57, %58, %59, %592, %593, %594, %595), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %597 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %598 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %599 : Float(1, 256, 28, 28) = aten::hardtanh(%input.20, %597, %598), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %600 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %601 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %602 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %603 : int[] = prim::ListConstruct(%601, %602), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %604 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %605 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %606 : int[] = prim::ListConstruct(%604, %605), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %607 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %608 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %609 : int[] = prim::ListConstruct(%607, %608), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %610 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %611 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %612 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %613 : int[] = prim::ListConstruct(%611, %612), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %614 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %615 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %616 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %617 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %input.21 : Float(1, 256, 28, 28) = aten::_convolution(%599, %61, %600, %603, %606, %609, %610, %613, %614, %615, %616, %617), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %619 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %620 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %621 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %622 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %input.22 : Float(1, 256, 28, 28) = aten::batch_norm(%input.21, %62, %63, %64, %65, %619, %620, %621, %622), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %624 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %625 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %626 : Float(1, 256, 28, 28) = aten::hardtanh(%input.22, %624, %625), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %627 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %628 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %629 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %630 : int[] = prim::ListConstruct(%628, %629), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %631 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %632 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %633 : int[] = prim::ListConstruct(%631, %632), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %634 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %635 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %636 : int[] = prim::ListConstruct(%634, %635), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %637 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %638 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %639 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %640 : int[] = prim::ListConstruct(%638, %639), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %641 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %642 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %643 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %644 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %input.23 : Float(1, 256, 14, 14) = aten::_convolution(%626, %67, %627, %630, %633, %636, %637, %640, %641, %642, %643, %644), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %646 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %647 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %648 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %649 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %input.24 : Float(1, 256, 14, 14) = aten::batch_norm(%input.23, %68, %69, %70, %71, %646, %647, %648, %649), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %651 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %652 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %653 : Float(1, 256, 14, 14) = aten::hardtanh(%input.24, %651, %652), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %654 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %655 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %656 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %657 : int[] = prim::ListConstruct(%655, %656), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %658 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %659 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %660 : int[] = prim::ListConstruct(%658, %659), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %661 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %662 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %663 : int[] = prim::ListConstruct(%661, %662), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %664 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %665 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %666 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %667 : int[] = prim::ListConstruct(%665, %666), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %668 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %669 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %670 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %671 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %input.25 : Float(1, 512, 14, 14) = aten::_convolution(%653, %73, %654, %657, %660, %663, %664, %667, %668, %669, %670, %671), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %673 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %674 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %675 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %676 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %input.26 : Float(1, 512, 14, 14) = aten::batch_norm(%input.25, %74, %75, %76, %77, %673, %674, %675, %676), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %678 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %679 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %680 : Float(1, 512, 14, 14) = aten::hardtanh(%input.26, %678, %679), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %681 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %682 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %683 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %684 : int[] = prim::ListConstruct(%682, %683), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %685 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %686 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %687 : int[] = prim::ListConstruct(%685, %686), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %688 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %689 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %690 : int[] = prim::ListConstruct(%688, %689), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %691 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %692 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %693 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %694 : int[] = prim::ListConstruct(%692, %693), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %695 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %696 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %697 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %698 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %input.27 : Float(1, 512, 14, 14) = aten::_convolution(%680, %79, %681, %684, %687, %690, %691, %694, %695, %696, %697, %698), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %700 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %701 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %702 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %703 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %input.28 : Float(1, 512, 14, 14) = aten::batch_norm(%input.27, %80, %81, %82, %83, %700, %701, %702, %703), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %705 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %706 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %707 : Float(1, 512, 14, 14) = aten::hardtanh(%input.28, %705, %706), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %708 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %709 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %710 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %711 : int[] = prim::ListConstruct(%709, %710), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %712 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %713 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %714 : int[] = prim::ListConstruct(%712, %713), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %715 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %716 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %717 : int[] = prim::ListConstruct(%715, %716), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %718 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %719 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %720 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %721 : int[] = prim::ListConstruct(%719, %720), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %722 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %723 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %724 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %725 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %input.29 : Float(1, 512, 14, 14) = aten::_convolution(%707, %85, %708, %711, %714, %717, %718, %721, %722, %723, %724, %725), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %727 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %728 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %729 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %730 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %input.30 : Float(1, 512, 14, 14) = aten::batch_norm(%input.29, %86, %87, %88, %89, %727, %728, %729, %730), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %732 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %733 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %734 : Float(1, 512, 14, 14) = aten::hardtanh(%input.30, %732, %733), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %735 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %736 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %737 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %738 : int[] = prim::ListConstruct(%736, %737), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %739 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %740 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %741 : int[] = prim::ListConstruct(%739, %740), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %742 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %743 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %744 : int[] = prim::ListConstruct(%742, %743), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %745 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %746 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %747 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %748 : int[] = prim::ListConstruct(%746, %747), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %749 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %750 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %751 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %752 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %input.31 : Float(1, 512, 14, 14) = aten::_convolution(%734, %91, %735, %738, %741, %744, %745, %748, %749, %750, %751, %752), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %754 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %755 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %756 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %757 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %input.32 : Float(1, 512, 14, 14) = aten::batch_norm(%input.31, %92, %93, %94, %95, %754, %755, %756, %757), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %759 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %760 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %761 : Float(1, 512, 14, 14) = aten::hardtanh(%input.32, %759, %760), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %762 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %763 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %764 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %765 : int[] = prim::ListConstruct(%763, %764), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %766 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %767 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %768 : int[] = prim::ListConstruct(%766, %767), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %769 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %770 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %771 : int[] = prim::ListConstruct(%769, %770), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %772 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %773 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %774 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %775 : int[] = prim::ListConstruct(%773, %774), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %776 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %777 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %778 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %779 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %input.33 : Float(1, 512, 14, 14) = aten::_convolution(%761, %97, %762, %765, %768, %771, %772, %775, %776, %777, %778, %779), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %781 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %782 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %783 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %784 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %input.34 : Float(1, 512, 14, 14) = aten::batch_norm(%input.33, %98, %99, %100, %101, %781, %782, %783, %784), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %786 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %787 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %788 : Float(1, 512, 14, 14) = aten::hardtanh(%input.34, %786, %787), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %789 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %790 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %791 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %792 : int[] = prim::ListConstruct(%790, %791), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %793 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %794 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %795 : int[] = prim::ListConstruct(%793, %794), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %796 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %797 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %798 : int[] = prim::ListConstruct(%796, %797), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %799 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %800 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %801 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %802 : int[] = prim::ListConstruct(%800, %801), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %803 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %804 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %805 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %806 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %input.35 : Float(1, 512, 14, 14) = aten::_convolution(%788, %103, %789, %792, %795, %798, %799, %802, %803, %804, %805, %806), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %808 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %809 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %810 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %811 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %input.36 : Float(1, 512, 14, 14) = aten::batch_norm(%input.35, %104, %105, %106, %107, %808, %809, %810, %811), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %813 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %814 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %815 : Float(1, 512, 14, 14) = aten::hardtanh(%input.36, %813, %814), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %816 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %817 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %818 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %819 : int[] = prim::ListConstruct(%817, %818), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %820 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %821 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %822 : int[] = prim::ListConstruct(%820, %821), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %823 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %824 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %825 : int[] = prim::ListConstruct(%823, %824), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %826 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %827 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %828 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %829 : int[] = prim::ListConstruct(%827, %828), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %830 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %831 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %832 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %833 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %input.37 : Float(1, 512, 14, 14) = aten::_convolution(%815, %109, %816, %819, %822, %825, %826, %829, %830, %831, %832, %833), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %835 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %836 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %837 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %838 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %input.38 : Float(1, 512, 14, 14) = aten::batch_norm(%input.37, %110, %111, %112, %113, %835, %836, %837, %838), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %840 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %841 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %842 : Float(1, 512, 14, 14) = aten::hardtanh(%input.38, %840, %841), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %843 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %844 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %845 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %846 : int[] = prim::ListConstruct(%844, %845), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %847 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %848 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %849 : int[] = prim::ListConstruct(%847, %848), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %850 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %851 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %852 : int[] = prim::ListConstruct(%850, %851), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %853 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %854 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %855 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %856 : int[] = prim::ListConstruct(%854, %855), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %857 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %858 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %859 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %860 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %input.39 : Float(1, 512, 14, 14) = aten::_convolution(%842, %115, %843, %846, %849, %852, %853, %856, %857, %858, %859, %860), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %862 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %863 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %864 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %865 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %input.40 : Float(1, 512, 14, 14) = aten::batch_norm(%input.39, %116, %117, %118, %119, %862, %863, %864, %865), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %867 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %868 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %869 : Float(1, 512, 14, 14) = aten::hardtanh(%input.40, %867, %868), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %870 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %871 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %872 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %873 : int[] = prim::ListConstruct(%871, %872), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %874 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %875 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %876 : int[] = prim::ListConstruct(%874, %875), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %877 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %878 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %879 : int[] = prim::ListConstruct(%877, %878), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %880 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %881 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %882 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %883 : int[] = prim::ListConstruct(%881, %882), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %884 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %885 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %886 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %887 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %input.41 : Float(1, 512, 14, 14) = aten::_convolution(%869, %121, %870, %873, %876, %879, %880, %883, %884, %885, %886, %887), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %889 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %890 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %891 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %892 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %input.42 : Float(1, 512, 14, 14) = aten::batch_norm(%input.41, %122, %123, %124, %125, %889, %890, %891, %892), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %894 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %895 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %896 : Float(1, 512, 14, 14) = aten::hardtanh(%input.42, %894, %895), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %897 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %898 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %899 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %900 : int[] = prim::ListConstruct(%898, %899), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %901 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %902 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %903 : int[] = prim::ListConstruct(%901, %902), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %904 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %905 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %906 : int[] = prim::ListConstruct(%904, %905), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %907 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %908 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %909 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %910 : int[] = prim::ListConstruct(%908, %909), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %911 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %912 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %913 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %914 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %input.43 : Float(1, 512, 14, 14) = aten::_convolution(%896, %127, %897, %900, %903, %906, %907, %910, %911, %912, %913, %914), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %916 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %917 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %918 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %919 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %input.44 : Float(1, 512, 14, 14) = aten::batch_norm(%input.43, %128, %129, %130, %131, %916, %917, %918, %919), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %921 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %922 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %923 : Float(1, 512, 14, 14) = aten::hardtanh(%input.44, %921, %922), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %924 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %925 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %926 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %927 : int[] = prim::ListConstruct(%925, %926), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %928 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %929 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %930 : int[] = prim::ListConstruct(%928, %929), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %931 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %932 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %933 : int[] = prim::ListConstruct(%931, %932), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %934 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %935 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %936 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %937 : int[] = prim::ListConstruct(%935, %936), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %938 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %939 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %940 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %941 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %input.45 : Float(1, 512, 14, 14) = aten::_convolution(%923, %133, %924, %927, %930, %933, %934, %937, %938, %939, %940, %941), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %943 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %944 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %945 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %946 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %input.46 : Float(1, 512, 14, 14) = aten::batch_norm(%input.45, %134, %135, %136, %137, %943, %944, %945, %946), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %948 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %949 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %950 : Float(1, 512, 14, 14) = aten::hardtanh(%input.46, %948, %949), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %951 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %952 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %953 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %954 : int[] = prim::ListConstruct(%952, %953), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %955 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %956 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %957 : int[] = prim::ListConstruct(%955, %956), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %958 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %959 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %960 : int[] = prim::ListConstruct(%958, %959), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %961 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %962 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %963 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %964 : int[] = prim::ListConstruct(%962, %963), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %965 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %966 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %967 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %968 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %input.47 : Float(1, 512, 7, 7) = aten::_convolution(%950, %139, %951, %954, %957, %960, %961, %964, %965, %966, %967, %968), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %970 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %971 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %972 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %973 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %input.48 : Float(1, 512, 7, 7) = aten::batch_norm(%input.47, %140, %141, %142, %143, %970, %971, %972, %973), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %975 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %976 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %977 : Float(1, 512, 7, 7) = aten::hardtanh(%input.48, %975, %976), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %978 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %979 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %980 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %981 : int[] = prim::ListConstruct(%979, %980), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %982 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %983 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %984 : int[] = prim::ListConstruct(%982, %983), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %985 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %986 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %987 : int[] = prim::ListConstruct(%985, %986), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %988 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %989 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %990 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %991 : int[] = prim::ListConstruct(%989, %990), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %992 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %993 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %994 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %995 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %input.49 : Float(1, 1024, 7, 7) = aten::_convolution(%977, %145, %978, %981, %984, %987, %988, %991, %992, %993, %994, %995), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %997 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %998 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %999 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %1000 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %input.50 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.49, %146, %147, %148, %149, %997, %998, %999, %1000), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %1002 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1003 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1004 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.50, %1002, %1003), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1005 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1006 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1007 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1008 : int[] = prim::ListConstruct(%1006, %1007), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1009 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1010 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1011 : int[] = prim::ListConstruct(%1009, %1010), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1012 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1013 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1014 : int[] = prim::ListConstruct(%1012, %1013), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1015 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1016 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1017 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1018 : int[] = prim::ListConstruct(%1016, %1017), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1019 : int = prim::Constant[value=1024](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1020 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1021 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1022 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %input.51 : Float(1, 1024, 7, 7) = aten::_convolution(%1004, %151, %1005, %1008, %1011, %1014, %1015, %1018, %1019, %1020, %1021, %1022), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1024 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1025 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1026 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1027 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %input.52 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.51, %152, %153, %154, %155, %1024, %1025, %1026, %1027), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1029 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1030 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1031 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.52, %1029, %1030), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1032 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1033 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1034 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1035 : int[] = prim::ListConstruct(%1033, %1034), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1036 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1037 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1038 : int[] = prim::ListConstruct(%1036, %1037), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1039 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1040 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1041 : int[] = prim::ListConstruct(%1039, %1040), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1042 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1043 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1044 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1045 : int[] = prim::ListConstruct(%1043, %1044), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1046 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1047 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1048 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1049 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %input.53 : Float(1, 1024, 7, 7) = aten::_convolution(%1031, %157, %1032, %1035, %1038, %1041, %1042, %1045, %1046, %1047, %1048, %1049), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1051 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1052 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1053 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1054 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %input.54 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.53, %158, %159, %160, %161, %1051, %1052, %1053, %1054), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1056 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1057 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1058 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.54, %1056, %1057), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1059 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1060 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1061 : int[] = prim::ListConstruct(%1059, %1060), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1062 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1063 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1064 : int[] = prim::ListConstruct(%1062, %1063), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1065 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1066 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1067 : int[] = prim::ListConstruct(%1065, %1066), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1068 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1069 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1070 : Float(1, 1024, 1, 1) = aten::avg_pool2d(%1058, %1061, %1064, %1067, %1068, %1069), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1071 : int = prim::Constant[value=-1](), scope: mobilenet_real\n  %1072 : int = prim::Constant[value=1024](), scope: mobilenet_real\n  %1073 : int[] = prim::ListConstruct(%1071, %1072), scope: mobilenet_real\n  %input : Float(1, 1024) = aten::view(%1070, %1073), scope: mobilenet_real\n  %1075 : Float(1024!, 1000!) = aten::t(%163), scope: mobilenet_real/Linear[fc]\n  %1076 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n  %1077 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n  %1078 : Float(1, 1000) = aten::addmm(%164, %input, %1075, %1076, %1077), scope: mobilenet_real/Linear[fc]\n  return (%1078)\n, False"
     ]
    }
   ],
   "source": [
    "g.build([1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.output_layers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "None None None\n",
      "1.0 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:209: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "model = models.__dict__[model_name]\n",
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_real(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (22): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (24): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (25): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in g.get_nodes():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet = models.squeezenet1_0(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace)\n",
       "    (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "/pytorch/torch/csrc/jit/tracer.h:143: getTracingState: Assertion `var_state == state` failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-15efc96e9f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPytorchGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mmdnn/conversion/pytorch/pytorch_graph.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPytorchGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't stop after throw()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mmdnn/conversion/pytorch/pytorch_graph.py\u001b[0m in \u001b[0;36mset_training\u001b[0;34m(self, model, mode)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mold_mode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mmdnn/conversion/pytorch/pytorch_graph.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPytorchGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mget_trace_graph\u001b[0;34m(f, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLegacyTracedModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0m_tracing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtrace_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_trace_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mout_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0m_tracing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traced_module_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/quantized_neural_networks/models/imagenet/mobilenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth_mult\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traced_module_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traced_module_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traced_module_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: /pytorch/torch/csrc/jit/tracer.h:143: getTracingState: Assertion `var_state == state` failed."
     ]
    }
   ],
   "source": [
    "g = PytorchGraph(model)\n",
    "g.build([1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%0 : Float(1, 3, 224, 224),\n",
      "      %model.0.0.weight : Float(32, 3, 3, 3),\n",
      "      %model.0.1.weight : Float(32),\n",
      "      %model.0.1.bias : Float(32),\n",
      "      %model.0.1.running_mean : Float(32),\n",
      "      %model.0.1.running_var : Float(32),\n",
      "      %model.0.1.num_batches_tracked : Long(),\n",
      "      %model.1.0.weight : Float(32, 1, 3, 3),\n",
      "      %model.1.1.weight : Float(32),\n",
      "      %model.1.1.bias : Float(32),\n",
      "      %model.1.1.running_mean : Float(32),\n",
      "      %model.1.1.running_var : Float(32),\n",
      "      %model.1.1.num_batches_tracked : Long(),\n",
      "      %model.2.0.weight : Float(64, 32, 1, 1),\n",
      "      %model.2.1.weight : Float(64),\n",
      "      %model.2.1.bias : Float(64),\n",
      "      %model.2.1.running_mean : Float(64),\n",
      "      %model.2.1.running_var : Float(64),\n",
      "      %model.2.1.num_batches_tracked : Long(),\n",
      "      %model.3.0.weight : Float(64, 1, 3, 3),\n",
      "      %model.3.1.weight : Float(64),\n",
      "      %model.3.1.bias : Float(64),\n",
      "      %model.3.1.running_mean : Float(64),\n",
      "      %model.3.1.running_var : Float(64),\n",
      "      %model.3.1.num_batches_tracked : Long(),\n",
      "      %model.4.0.weight : Float(128, 64, 1, 1),\n",
      "      %model.4.1.weight : Float(128),\n",
      "      %model.4.1.bias : Float(128),\n",
      "      %model.4.1.running_mean : Float(128),\n",
      "      %model.4.1.running_var : Float(128),\n",
      "      %model.4.1.num_batches_tracked : Long(),\n",
      "      %model.5.0.weight : Float(128, 1, 3, 3),\n",
      "      %model.5.1.weight : Float(128),\n",
      "      %model.5.1.bias : Float(128),\n",
      "      %model.5.1.running_mean : Float(128),\n",
      "      %model.5.1.running_var : Float(128),\n",
      "      %model.5.1.num_batches_tracked : Long(),\n",
      "      %model.6.0.weight : Float(128, 128, 1, 1),\n",
      "      %model.6.1.weight : Float(128),\n",
      "      %model.6.1.bias : Float(128),\n",
      "      %model.6.1.running_mean : Float(128),\n",
      "      %model.6.1.running_var : Float(128),\n",
      "      %model.6.1.num_batches_tracked : Long(),\n",
      "      %model.7.0.weight : Float(128, 1, 3, 3),\n",
      "      %model.7.1.weight : Float(128),\n",
      "      %model.7.1.bias : Float(128),\n",
      "      %model.7.1.running_mean : Float(128),\n",
      "      %model.7.1.running_var : Float(128),\n",
      "      %model.7.1.num_batches_tracked : Long(),\n",
      "      %model.8.0.weight : Float(256, 128, 1, 1),\n",
      "      %model.8.1.weight : Float(256),\n",
      "      %model.8.1.bias : Float(256),\n",
      "      %model.8.1.running_mean : Float(256),\n",
      "      %model.8.1.running_var : Float(256),\n",
      "      %model.8.1.num_batches_tracked : Long(),\n",
      "      %model.9.0.weight : Float(256, 1, 3, 3),\n",
      "      %model.9.1.weight : Float(256),\n",
      "      %model.9.1.bias : Float(256),\n",
      "      %model.9.1.running_mean : Float(256),\n",
      "      %model.9.1.running_var : Float(256),\n",
      "      %model.9.1.num_batches_tracked : Long(),\n",
      "      %model.10.0.weight : Float(256, 256, 1, 1),\n",
      "      %model.10.1.weight : Float(256),\n",
      "      %model.10.1.bias : Float(256),\n",
      "      %model.10.1.running_mean : Float(256),\n",
      "      %model.10.1.running_var : Float(256),\n",
      "      %model.10.1.num_batches_tracked : Long(),\n",
      "      %model.11.0.weight : Float(256, 1, 3, 3),\n",
      "      %model.11.1.weight : Float(256),\n",
      "      %model.11.1.bias : Float(256),\n",
      "      %model.11.1.running_mean : Float(256),\n",
      "      %model.11.1.running_var : Float(256),\n",
      "      %model.11.1.num_batches_tracked : Long(),\n",
      "      %model.12.0.weight : Float(512, 256, 1, 1),\n",
      "      %model.12.1.weight : Float(512),\n",
      "      %model.12.1.bias : Float(512),\n",
      "      %model.12.1.running_mean : Float(512),\n",
      "      %model.12.1.running_var : Float(512),\n",
      "      %model.12.1.num_batches_tracked : Long(),\n",
      "      %model.13.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.13.1.weight : Float(512),\n",
      "      %model.13.1.bias : Float(512),\n",
      "      %model.13.1.running_mean : Float(512),\n",
      "      %model.13.1.running_var : Float(512),\n",
      "      %model.13.1.num_batches_tracked : Long(),\n",
      "      %model.14.0.weight : Float(512, 512, 1, 1),\n",
      "      %model.14.1.weight : Float(512),\n",
      "      %model.14.1.bias : Float(512),\n",
      "      %model.14.1.running_mean : Float(512),\n",
      "      %model.14.1.running_var : Float(512),\n",
      "      %model.14.1.num_batches_tracked : Long(),\n",
      "      %model.15.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.15.1.weight : Float(512),\n",
      "      %model.15.1.bias : Float(512),\n",
      "      %model.15.1.running_mean : Float(512),\n",
      "      %model.15.1.running_var : Float(512),\n",
      "      %model.15.1.num_batches_tracked : Long(),\n",
      "      %model.16.0.weight : Float(512, 512, 1, 1),\n",
      "      %model.16.1.weight : Float(512),\n",
      "      %model.16.1.bias : Float(512),\n",
      "      %model.16.1.running_mean : Float(512),\n",
      "      %model.16.1.running_var : Float(512),\n",
      "      %model.16.1.num_batches_tracked : Long(),\n",
      "      %model.17.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.17.1.weight : Float(512),\n",
      "      %model.17.1.bias : Float(512),\n",
      "      %model.17.1.running_mean : Float(512),\n",
      "      %model.17.1.running_var : Float(512),\n",
      "      %model.17.1.num_batches_tracked : Long(),\n",
      "      %model.18.0.weight : Float(512, 512, 1, 1),\n",
      "      %model.18.1.weight : Float(512),\n",
      "      %model.18.1.bias : Float(512),\n",
      "      %model.18.1.running_mean : Float(512),\n",
      "      %model.18.1.running_var : Float(512),\n",
      "      %model.18.1.num_batches_tracked : Long(),\n",
      "      %model.19.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.19.1.weight : Float(512),\n",
      "      %model.19.1.bias : Float(512),\n",
      "      %model.19.1.running_mean : Float(512),\n",
      "      %model.19.1.running_var : Float(512),\n",
      "      %model.19.1.num_batches_tracked : Long(),\n",
      "      %model.20.0.weight : Float(512, 512, 1, 1),\n",
      "      %model.20.1.weight : Float(512),\n",
      "      %model.20.1.bias : Float(512),\n",
      "      %model.20.1.running_mean : Float(512),\n",
      "      %model.20.1.running_var : Float(512),\n",
      "      %model.20.1.num_batches_tracked : Long(),\n",
      "      %model.21.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.21.1.weight : Float(512),\n",
      "      %model.21.1.bias : Float(512),\n",
      "      %model.21.1.running_mean : Float(512),\n",
      "      %model.21.1.running_var : Float(512),\n",
      "      %model.21.1.num_batches_tracked : Long(),\n",
      "      %model.22.0.weight : Float(512, 512, 1, 1),\n",
      "      %model.22.1.weight : Float(512),\n",
      "      %model.22.1.bias : Float(512),\n",
      "      %model.22.1.running_mean : Float(512),\n",
      "      %model.22.1.running_var : Float(512),\n",
      "      %model.22.1.num_batches_tracked : Long(),\n",
      "      %model.23.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.23.1.weight : Float(512),\n",
      "      %model.23.1.bias : Float(512),\n",
      "      %model.23.1.running_mean : Float(512),\n",
      "      %model.23.1.running_var : Float(512),\n",
      "      %model.23.1.num_batches_tracked : Long(),\n",
      "      %model.24.0.weight : Float(1024, 512, 1, 1),\n",
      "      %model.24.1.weight : Float(1024),\n",
      "      %model.24.1.bias : Float(1024),\n",
      "      %model.24.1.running_mean : Float(1024),\n",
      "      %model.24.1.running_var : Float(1024),\n",
      "      %model.24.1.num_batches_tracked : Long(),\n",
      "      %model.25.0.weight : Float(1024, 1, 3, 3),\n",
      "      %model.25.1.weight : Float(1024),\n",
      "      %model.25.1.bias : Float(1024),\n",
      "      %model.25.1.running_mean : Float(1024),\n",
      "      %model.25.1.running_var : Float(1024),\n",
      "      %model.25.1.num_batches_tracked : Long(),\n",
      "      %model.26.0.weight : Float(1024, 1024, 1, 1),\n",
      "      %model.26.1.weight : Float(1024),\n",
      "      %model.26.1.bias : Float(1024),\n",
      "      %model.26.1.running_mean : Float(1024),\n",
      "      %model.26.1.running_var : Float(1024),\n",
      "      %model.26.1.num_batches_tracked : Long(),\n",
      "      %fc.weight : Float(1000, 1024),\n",
      "      %fc.bias : Float(1000)):\n",
      "  %165 : Float(1, 32, 112, 112) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%0, %model.0.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "  %166 : Float(1, 32, 112, 112) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%165, %model.0.1.weight, %model.0.1.bias, %model.0.1.running_mean, %model.0.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "  %167 : Float(1, 32, 112, 112) = onnx::Clip[max=6, min=0](%166), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "  %168 : Float(1, 32, 112, 112) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%167, %model.1.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "  %169 : Float(1, 32, 112, 112) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%168, %model.1.1.weight, %model.1.1.bias, %model.1.1.running_mean, %model.1.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "  %170 : Float(1, 32, 112, 112) = onnx::Clip[max=6, min=0](%169), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "  %171 : Float(1, 64, 112, 112) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%170, %model.2.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "  %172 : Float(1, 64, 112, 112) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%171, %model.2.1.weight, %model.2.1.bias, %model.2.1.running_mean, %model.2.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "  %173 : Float(1, 64, 112, 112) = onnx::Clip[max=6, min=0](%172), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "  %174 : Float(1, 64, 56, 56) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%173, %model.3.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "  %175 : Float(1, 64, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%174, %model.3.1.weight, %model.3.1.bias, %model.3.1.running_mean, %model.3.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "  %176 : Float(1, 64, 56, 56) = onnx::Clip[max=6, min=0](%175), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "  %177 : Float(1, 128, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%176, %model.4.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "  %178 : Float(1, 128, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%177, %model.4.1.weight, %model.4.1.bias, %model.4.1.running_mean, %model.4.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "  %179 : Float(1, 128, 56, 56) = onnx::Clip[max=6, min=0](%178), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "  %180 : Float(1, 128, 56, 56) = onnx::Conv[dilations=[1, 1], group=128, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%179, %model.5.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "  %181 : Float(1, 128, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%180, %model.5.1.weight, %model.5.1.bias, %model.5.1.running_mean, %model.5.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "  %182 : Float(1, 128, 56, 56) = onnx::Clip[max=6, min=0](%181), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "  %183 : Float(1, 128, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%182, %model.6.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "  %184 : Float(1, 128, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%183, %model.6.1.weight, %model.6.1.bias, %model.6.1.running_mean, %model.6.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "  %185 : Float(1, 128, 56, 56) = onnx::Clip[max=6, min=0](%184), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "  %186 : Float(1, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=128, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%185, %model.7.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "  %187 : Float(1, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%186, %model.7.1.weight, %model.7.1.bias, %model.7.1.running_mean, %model.7.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "  %188 : Float(1, 128, 28, 28) = onnx::Clip[max=6, min=0](%187), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "  %189 : Float(1, 256, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%188, %model.8.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "  %190 : Float(1, 256, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%189, %model.8.1.weight, %model.8.1.bias, %model.8.1.running_mean, %model.8.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "  %191 : Float(1, 256, 28, 28) = onnx::Clip[max=6, min=0](%190), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "  %192 : Float(1, 256, 28, 28) = onnx::Conv[dilations=[1, 1], group=256, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%191, %model.9.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "  %193 : Float(1, 256, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%192, %model.9.1.weight, %model.9.1.bias, %model.9.1.running_mean, %model.9.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "  %194 : Float(1, 256, 28, 28) = onnx::Clip[max=6, min=0](%193), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "  %195 : Float(1, 256, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%194, %model.10.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "  %196 : Float(1, 256, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%195, %model.10.1.weight, %model.10.1.bias, %model.10.1.running_mean, %model.10.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "  %197 : Float(1, 256, 28, 28) = onnx::Clip[max=6, min=0](%196), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "  %198 : Float(1, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=256, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%197, %model.11.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "  %199 : Float(1, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%198, %model.11.1.weight, %model.11.1.bias, %model.11.1.running_mean, %model.11.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "  %200 : Float(1, 256, 14, 14) = onnx::Clip[max=6, min=0](%199), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "  %201 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%200, %model.12.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "  %202 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%201, %model.12.1.weight, %model.12.1.bias, %model.12.1.running_mean, %model.12.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "  %203 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%202), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "  %204 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%203, %model.13.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "  %205 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%204, %model.13.1.weight, %model.13.1.bias, %model.13.1.running_mean, %model.13.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "  %206 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%205), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "  %207 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%206, %model.14.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "  %208 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%207, %model.14.1.weight, %model.14.1.bias, %model.14.1.running_mean, %model.14.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "  %209 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%208), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "  %210 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%209, %model.15.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "  %211 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%210, %model.15.1.weight, %model.15.1.bias, %model.15.1.running_mean, %model.15.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "  %212 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%211), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "  %213 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%212, %model.16.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "  %214 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%213, %model.16.1.weight, %model.16.1.bias, %model.16.1.running_mean, %model.16.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "  %215 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%214), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "  %216 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%215, %model.17.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "  %217 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%216, %model.17.1.weight, %model.17.1.bias, %model.17.1.running_mean, %model.17.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "  %218 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%217), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "  %219 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%218, %model.18.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "  %220 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%219, %model.18.1.weight, %model.18.1.bias, %model.18.1.running_mean, %model.18.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "  %221 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%220), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "  %222 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%221, %model.19.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "  %223 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%222, %model.19.1.weight, %model.19.1.bias, %model.19.1.running_mean, %model.19.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "  %224 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%223), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "  %225 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%224, %model.20.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "  %226 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%225, %model.20.1.weight, %model.20.1.bias, %model.20.1.running_mean, %model.20.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "  %227 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%226), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "  %228 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%227, %model.21.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "  %229 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%228, %model.21.1.weight, %model.21.1.bias, %model.21.1.running_mean, %model.21.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "  %230 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%229), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "  %231 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%230, %model.22.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "  %232 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%231, %model.22.1.weight, %model.22.1.bias, %model.22.1.running_mean, %model.22.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "  %233 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%232), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "  %234 : Float(1, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%233, %model.23.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "  %235 : Float(1, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%234, %model.23.1.weight, %model.23.1.bias, %model.23.1.running_mean, %model.23.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "  %236 : Float(1, 512, 7, 7) = onnx::Clip[max=6, min=0](%235), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "  %237 : Float(1, 1024, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%236, %model.24.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "  %238 : Float(1, 1024, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%237, %model.24.1.weight, %model.24.1.bias, %model.24.1.running_mean, %model.24.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "  %239 : Float(1, 1024, 7, 7) = onnx::Clip[max=6, min=0](%238), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "  %240 : Float(1, 1024, 7, 7) = onnx::Conv[dilations=[1, 1], group=1024, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%239, %model.25.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "  %241 : Float(1, 1024, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%240, %model.25.1.weight, %model.25.1.bias, %model.25.1.running_mean, %model.25.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "  %242 : Float(1, 1024, 7, 7) = onnx::Clip[max=6, min=0](%241), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "  %243 : Float(1, 1024, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%242, %model.26.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "  %244 : Float(1, 1024, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%243, %model.26.1.weight, %model.26.1.bias, %model.26.1.running_mean, %model.26.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "  %245 : Float(1, 1024, 7, 7) = onnx::Clip[max=6, min=0](%244), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
      "  %246 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0](%245), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
      "  %247 : Float(1, 1024, 1, 1) = onnx::AveragePool[kernel_shape=[7, 7], pads=[0, 0, 0, 0], strides=[7, 7]](%246), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
      "  %248 : Tensor = onnx::Constant[value=   -1  1024 [ Variable[CPUType]{2} ]](), scope: mobilenet_real\n",
      "  %249 : Float(1, 1024) = onnx::Reshape(%247, %248), scope: mobilenet_real\n",
      "  %output1 : Float(1, 1000) = onnx::Gemm[alpha=1, beta=1, transB=1](%249, %fc.weight, %fc.bias), scope: mobilenet_real/Linear[fc]\n",
      "  return (%output1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_input = Variable(torch.randn(1, 3, 224, 224))\n",
    "input_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\n",
    "output_names = [ \"output1\" ]\n",
    "torch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "modelonn = onnx.load(\"alexnet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.checker.check_model(modelonn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch-jit-export (\n",
      "  %0[FLOAT, 1x3x224x224]\n",
      ") initializers (\n",
      "  %model.0.0.weight[FLOAT, 32x3x3x3]\n",
      "  %model.0.1.weight[FLOAT, 32]\n",
      "  %model.0.1.bias[FLOAT, 32]\n",
      "  %model.0.1.running_mean[FLOAT, 32]\n",
      "  %model.0.1.running_var[FLOAT, 32]\n",
      "  %model.0.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.1.0.weight[FLOAT, 32x1x3x3]\n",
      "  %model.1.1.weight[FLOAT, 32]\n",
      "  %model.1.1.bias[FLOAT, 32]\n",
      "  %model.1.1.running_mean[FLOAT, 32]\n",
      "  %model.1.1.running_var[FLOAT, 32]\n",
      "  %model.1.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.2.0.weight[FLOAT, 64x32x1x1]\n",
      "  %model.2.1.weight[FLOAT, 64]\n",
      "  %model.2.1.bias[FLOAT, 64]\n",
      "  %model.2.1.running_mean[FLOAT, 64]\n",
      "  %model.2.1.running_var[FLOAT, 64]\n",
      "  %model.2.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.3.0.weight[FLOAT, 64x1x3x3]\n",
      "  %model.3.1.weight[FLOAT, 64]\n",
      "  %model.3.1.bias[FLOAT, 64]\n",
      "  %model.3.1.running_mean[FLOAT, 64]\n",
      "  %model.3.1.running_var[FLOAT, 64]\n",
      "  %model.3.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.4.0.weight[FLOAT, 128x64x1x1]\n",
      "  %model.4.1.weight[FLOAT, 128]\n",
      "  %model.4.1.bias[FLOAT, 128]\n",
      "  %model.4.1.running_mean[FLOAT, 128]\n",
      "  %model.4.1.running_var[FLOAT, 128]\n",
      "  %model.4.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.5.0.weight[FLOAT, 128x1x3x3]\n",
      "  %model.5.1.weight[FLOAT, 128]\n",
      "  %model.5.1.bias[FLOAT, 128]\n",
      "  %model.5.1.running_mean[FLOAT, 128]\n",
      "  %model.5.1.running_var[FLOAT, 128]\n",
      "  %model.5.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.6.0.weight[FLOAT, 128x128x1x1]\n",
      "  %model.6.1.weight[FLOAT, 128]\n",
      "  %model.6.1.bias[FLOAT, 128]\n",
      "  %model.6.1.running_mean[FLOAT, 128]\n",
      "  %model.6.1.running_var[FLOAT, 128]\n",
      "  %model.6.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.7.0.weight[FLOAT, 128x1x3x3]\n",
      "  %model.7.1.weight[FLOAT, 128]\n",
      "  %model.7.1.bias[FLOAT, 128]\n",
      "  %model.7.1.running_mean[FLOAT, 128]\n",
      "  %model.7.1.running_var[FLOAT, 128]\n",
      "  %model.7.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.8.0.weight[FLOAT, 256x128x1x1]\n",
      "  %model.8.1.weight[FLOAT, 256]\n",
      "  %model.8.1.bias[FLOAT, 256]\n",
      "  %model.8.1.running_mean[FLOAT, 256]\n",
      "  %model.8.1.running_var[FLOAT, 256]\n",
      "  %model.8.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.9.0.weight[FLOAT, 256x1x3x3]\n",
      "  %model.9.1.weight[FLOAT, 256]\n",
      "  %model.9.1.bias[FLOAT, 256]\n",
      "  %model.9.1.running_mean[FLOAT, 256]\n",
      "  %model.9.1.running_var[FLOAT, 256]\n",
      "  %model.9.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.10.0.weight[FLOAT, 256x256x1x1]\n",
      "  %model.10.1.weight[FLOAT, 256]\n",
      "  %model.10.1.bias[FLOAT, 256]\n",
      "  %model.10.1.running_mean[FLOAT, 256]\n",
      "  %model.10.1.running_var[FLOAT, 256]\n",
      "  %model.10.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.11.0.weight[FLOAT, 256x1x3x3]\n",
      "  %model.11.1.weight[FLOAT, 256]\n",
      "  %model.11.1.bias[FLOAT, 256]\n",
      "  %model.11.1.running_mean[FLOAT, 256]\n",
      "  %model.11.1.running_var[FLOAT, 256]\n",
      "  %model.11.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.12.0.weight[FLOAT, 512x256x1x1]\n",
      "  %model.12.1.weight[FLOAT, 512]\n",
      "  %model.12.1.bias[FLOAT, 512]\n",
      "  %model.12.1.running_mean[FLOAT, 512]\n",
      "  %model.12.1.running_var[FLOAT, 512]\n",
      "  %model.12.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.13.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.13.1.weight[FLOAT, 512]\n",
      "  %model.13.1.bias[FLOAT, 512]\n",
      "  %model.13.1.running_mean[FLOAT, 512]\n",
      "  %model.13.1.running_var[FLOAT, 512]\n",
      "  %model.13.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.14.0.weight[FLOAT, 512x512x1x1]\n",
      "  %model.14.1.weight[FLOAT, 512]\n",
      "  %model.14.1.bias[FLOAT, 512]\n",
      "  %model.14.1.running_mean[FLOAT, 512]\n",
      "  %model.14.1.running_var[FLOAT, 512]\n",
      "  %model.14.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.15.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.15.1.weight[FLOAT, 512]\n",
      "  %model.15.1.bias[FLOAT, 512]\n",
      "  %model.15.1.running_mean[FLOAT, 512]\n",
      "  %model.15.1.running_var[FLOAT, 512]\n",
      "  %model.15.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.16.0.weight[FLOAT, 512x512x1x1]\n",
      "  %model.16.1.weight[FLOAT, 512]\n",
      "  %model.16.1.bias[FLOAT, 512]\n",
      "  %model.16.1.running_mean[FLOAT, 512]\n",
      "  %model.16.1.running_var[FLOAT, 512]\n",
      "  %model.16.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.17.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.17.1.weight[FLOAT, 512]\n",
      "  %model.17.1.bias[FLOAT, 512]\n",
      "  %model.17.1.running_mean[FLOAT, 512]\n",
      "  %model.17.1.running_var[FLOAT, 512]\n",
      "  %model.17.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.18.0.weight[FLOAT, 512x512x1x1]\n",
      "  %model.18.1.weight[FLOAT, 512]\n",
      "  %model.18.1.bias[FLOAT, 512]\n",
      "  %model.18.1.running_mean[FLOAT, 512]\n",
      "  %model.18.1.running_var[FLOAT, 512]\n",
      "  %model.18.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.19.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.19.1.weight[FLOAT, 512]\n",
      "  %model.19.1.bias[FLOAT, 512]\n",
      "  %model.19.1.running_mean[FLOAT, 512]\n",
      "  %model.19.1.running_var[FLOAT, 512]\n",
      "  %model.19.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.20.0.weight[FLOAT, 512x512x1x1]\n",
      "  %model.20.1.weight[FLOAT, 512]\n",
      "  %model.20.1.bias[FLOAT, 512]\n",
      "  %model.20.1.running_mean[FLOAT, 512]\n",
      "  %model.20.1.running_var[FLOAT, 512]\n",
      "  %model.20.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.21.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.21.1.weight[FLOAT, 512]\n",
      "  %model.21.1.bias[FLOAT, 512]\n",
      "  %model.21.1.running_mean[FLOAT, 512]\n",
      "  %model.21.1.running_var[FLOAT, 512]\n",
      "  %model.21.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.22.0.weight[FLOAT, 512x512x1x1]\n",
      "  %model.22.1.weight[FLOAT, 512]\n",
      "  %model.22.1.bias[FLOAT, 512]\n",
      "  %model.22.1.running_mean[FLOAT, 512]\n",
      "  %model.22.1.running_var[FLOAT, 512]\n",
      "  %model.22.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.23.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.23.1.weight[FLOAT, 512]\n",
      "  %model.23.1.bias[FLOAT, 512]\n",
      "  %model.23.1.running_mean[FLOAT, 512]\n",
      "  %model.23.1.running_var[FLOAT, 512]\n",
      "  %model.23.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.24.0.weight[FLOAT, 1024x512x1x1]\n",
      "  %model.24.1.weight[FLOAT, 1024]\n",
      "  %model.24.1.bias[FLOAT, 1024]\n",
      "  %model.24.1.running_mean[FLOAT, 1024]\n",
      "  %model.24.1.running_var[FLOAT, 1024]\n",
      "  %model.24.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.25.0.weight[FLOAT, 1024x1x3x3]\n",
      "  %model.25.1.weight[FLOAT, 1024]\n",
      "  %model.25.1.bias[FLOAT, 1024]\n",
      "  %model.25.1.running_mean[FLOAT, 1024]\n",
      "  %model.25.1.running_var[FLOAT, 1024]\n",
      "  %model.25.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.26.0.weight[FLOAT, 1024x1024x1x1]\n",
      "  %model.26.1.weight[FLOAT, 1024]\n",
      "  %model.26.1.bias[FLOAT, 1024]\n",
      "  %model.26.1.running_mean[FLOAT, 1024]\n",
      "  %model.26.1.running_var[FLOAT, 1024]\n",
      "  %model.26.1.num_batches_tracked[INT64, scalar]\n",
      "  %fc.weight[FLOAT, 1000x1024]\n",
      "  %fc.bias[FLOAT, 1000]\n",
      ") {\n",
      "  %165 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%0, %model.0.0.weight)\n",
      "  %166 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%165, %model.0.1.weight, %model.0.1.bias, %model.0.1.running_mean, %model.0.1.running_var)\n",
      "  %167 = Clip[max = 6, min = 0](%166)\n",
      "  %168 = Conv[dilations = [1, 1], group = 32, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%167, %model.1.0.weight)\n",
      "  %169 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%168, %model.1.1.weight, %model.1.1.bias, %model.1.1.running_mean, %model.1.1.running_var)\n",
      "  %170 = Clip[max = 6, min = 0](%169)\n",
      "  %171 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%170, %model.2.0.weight)\n",
      "  %172 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%171, %model.2.1.weight, %model.2.1.bias, %model.2.1.running_mean, %model.2.1.running_var)\n",
      "  %173 = Clip[max = 6, min = 0](%172)\n",
      "  %174 = Conv[dilations = [1, 1], group = 64, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%173, %model.3.0.weight)\n",
      "  %175 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%174, %model.3.1.weight, %model.3.1.bias, %model.3.1.running_mean, %model.3.1.running_var)\n",
      "  %176 = Clip[max = 6, min = 0](%175)\n",
      "  %177 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%176, %model.4.0.weight)\n",
      "  %178 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%177, %model.4.1.weight, %model.4.1.bias, %model.4.1.running_mean, %model.4.1.running_var)\n",
      "  %179 = Clip[max = 6, min = 0](%178)\n",
      "  %180 = Conv[dilations = [1, 1], group = 128, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%179, %model.5.0.weight)\n",
      "  %181 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%180, %model.5.1.weight, %model.5.1.bias, %model.5.1.running_mean, %model.5.1.running_var)\n",
      "  %182 = Clip[max = 6, min = 0](%181)\n",
      "  %183 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%182, %model.6.0.weight)\n",
      "  %184 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%183, %model.6.1.weight, %model.6.1.bias, %model.6.1.running_mean, %model.6.1.running_var)\n",
      "  %185 = Clip[max = 6, min = 0](%184)\n",
      "  %186 = Conv[dilations = [1, 1], group = 128, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%185, %model.7.0.weight)\n",
      "  %187 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%186, %model.7.1.weight, %model.7.1.bias, %model.7.1.running_mean, %model.7.1.running_var)\n",
      "  %188 = Clip[max = 6, min = 0](%187)\n",
      "  %189 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%188, %model.8.0.weight)\n",
      "  %190 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%189, %model.8.1.weight, %model.8.1.bias, %model.8.1.running_mean, %model.8.1.running_var)\n",
      "  %191 = Clip[max = 6, min = 0](%190)\n",
      "  %192 = Conv[dilations = [1, 1], group = 256, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%191, %model.9.0.weight)\n",
      "  %193 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%192, %model.9.1.weight, %model.9.1.bias, %model.9.1.running_mean, %model.9.1.running_var)\n",
      "  %194 = Clip[max = 6, min = 0](%193)\n",
      "  %195 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%194, %model.10.0.weight)\n",
      "  %196 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%195, %model.10.1.weight, %model.10.1.bias, %model.10.1.running_mean, %model.10.1.running_var)\n",
      "  %197 = Clip[max = 6, min = 0](%196)\n",
      "  %198 = Conv[dilations = [1, 1], group = 256, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%197, %model.11.0.weight)\n",
      "  %199 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%198, %model.11.1.weight, %model.11.1.bias, %model.11.1.running_mean, %model.11.1.running_var)\n",
      "  %200 = Clip[max = 6, min = 0](%199)\n",
      "  %201 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%200, %model.12.0.weight)\n",
      "  %202 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%201, %model.12.1.weight, %model.12.1.bias, %model.12.1.running_mean, %model.12.1.running_var)\n",
      "  %203 = Clip[max = 6, min = 0](%202)\n",
      "  %204 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%203, %model.13.0.weight)\n",
      "  %205 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%204, %model.13.1.weight, %model.13.1.bias, %model.13.1.running_mean, %model.13.1.running_var)\n",
      "  %206 = Clip[max = 6, min = 0](%205)\n",
      "  %207 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%206, %model.14.0.weight)\n",
      "  %208 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%207, %model.14.1.weight, %model.14.1.bias, %model.14.1.running_mean, %model.14.1.running_var)\n",
      "  %209 = Clip[max = 6, min = 0](%208)\n",
      "  %210 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%209, %model.15.0.weight)\n",
      "  %211 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%210, %model.15.1.weight, %model.15.1.bias, %model.15.1.running_mean, %model.15.1.running_var)\n",
      "  %212 = Clip[max = 6, min = 0](%211)\n",
      "  %213 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%212, %model.16.0.weight)\n",
      "  %214 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%213, %model.16.1.weight, %model.16.1.bias, %model.16.1.running_mean, %model.16.1.running_var)\n",
      "  %215 = Clip[max = 6, min = 0](%214)\n",
      "  %216 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%215, %model.17.0.weight)\n",
      "  %217 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%216, %model.17.1.weight, %model.17.1.bias, %model.17.1.running_mean, %model.17.1.running_var)\n",
      "  %218 = Clip[max = 6, min = 0](%217)\n",
      "  %219 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%218, %model.18.0.weight)\n",
      "  %220 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%219, %model.18.1.weight, %model.18.1.bias, %model.18.1.running_mean, %model.18.1.running_var)\n",
      "  %221 = Clip[max = 6, min = 0](%220)\n",
      "  %222 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%221, %model.19.0.weight)\n",
      "  %223 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%222, %model.19.1.weight, %model.19.1.bias, %model.19.1.running_mean, %model.19.1.running_var)\n",
      "  %224 = Clip[max = 6, min = 0](%223)\n",
      "  %225 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%224, %model.20.0.weight)\n",
      "  %226 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%225, %model.20.1.weight, %model.20.1.bias, %model.20.1.running_mean, %model.20.1.running_var)\n",
      "  %227 = Clip[max = 6, min = 0](%226)\n",
      "  %228 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%227, %model.21.0.weight)\n",
      "  %229 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%228, %model.21.1.weight, %model.21.1.bias, %model.21.1.running_mean, %model.21.1.running_var)\n",
      "  %230 = Clip[max = 6, min = 0](%229)\n",
      "  %231 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%230, %model.22.0.weight)\n",
      "  %232 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%231, %model.22.1.weight, %model.22.1.bias, %model.22.1.running_mean, %model.22.1.running_var)\n",
      "  %233 = Clip[max = 6, min = 0](%232)\n",
      "  %234 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%233, %model.23.0.weight)\n",
      "  %235 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%234, %model.23.1.weight, %model.23.1.bias, %model.23.1.running_mean, %model.23.1.running_var)\n",
      "  %236 = Clip[max = 6, min = 0](%235)\n",
      "  %237 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%236, %model.24.0.weight)\n",
      "  %238 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%237, %model.24.1.weight, %model.24.1.bias, %model.24.1.running_mean, %model.24.1.running_var)\n",
      "  %239 = Clip[max = 6, min = 0](%238)\n",
      "  %240 = Conv[dilations = [1, 1], group = 1024, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%239, %model.25.0.weight)\n",
      "  %241 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%240, %model.25.1.weight, %model.25.1.bias, %model.25.1.running_mean, %model.25.1.running_var)\n",
      "  %242 = Clip[max = 6, min = 0](%241)\n",
      "  %243 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%242, %model.26.0.weight)\n",
      "  %244 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%243, %model.26.1.weight, %model.26.1.bias, %model.26.1.running_mean, %model.26.1.running_var)\n",
      "  %245 = Clip[max = 6, min = 0](%244)\n",
      "  %246 = Pad[mode = 'constant', pads = [0, 0, 0, 0, 0, 0, 0, 0], value = 0](%245)\n",
      "  %247 = AveragePool[kernel_shape = [7, 7], pads = [0, 0, 0, 0], strides = [7, 7]](%246)\n",
      "  %248 = Constant[value = <Tensor>]()\n",
      "  %249 = Reshape(%247, %248)\n",
      "  %output1 = Gemm[alpha = 1, beta = 1, transB = 1](%249, %fc.weight, %fc.bias)\n",
      "  return %output1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(onnx.helper.printable_graph(modelonn.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "trace, output = torch.jit.get_trace_graph(model, (dummy_input, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%0 : Float(1, 3, 224, 224),\n",
       "      %1 : Float(32, 3, 3, 3),\n",
       "      %2 : Float(32),\n",
       "      %3 : Float(32),\n",
       "      %4 : Float(32),\n",
       "      %5 : Float(32),\n",
       "      %6 : Long(),\n",
       "      %7 : Float(32, 1, 3, 3),\n",
       "      %8 : Float(32),\n",
       "      %9 : Float(32),\n",
       "      %10 : Float(32),\n",
       "      %11 : Float(32),\n",
       "      %12 : Long(),\n",
       "      %13 : Float(64, 32, 1, 1),\n",
       "      %14 : Float(64),\n",
       "      %15 : Float(64),\n",
       "      %16 : Float(64),\n",
       "      %17 : Float(64),\n",
       "      %18 : Long(),\n",
       "      %19 : Float(64, 1, 3, 3),\n",
       "      %20 : Float(64),\n",
       "      %21 : Float(64),\n",
       "      %22 : Float(64),\n",
       "      %23 : Float(64),\n",
       "      %24 : Long(),\n",
       "      %25 : Float(128, 64, 1, 1),\n",
       "      %26 : Float(128),\n",
       "      %27 : Float(128),\n",
       "      %28 : Float(128),\n",
       "      %29 : Float(128),\n",
       "      %30 : Long(),\n",
       "      %31 : Float(128, 1, 3, 3),\n",
       "      %32 : Float(128),\n",
       "      %33 : Float(128),\n",
       "      %34 : Float(128),\n",
       "      %35 : Float(128),\n",
       "      %36 : Long(),\n",
       "      %37 : Float(128, 128, 1, 1),\n",
       "      %38 : Float(128),\n",
       "      %39 : Float(128),\n",
       "      %40 : Float(128),\n",
       "      %41 : Float(128),\n",
       "      %42 : Long(),\n",
       "      %43 : Float(128, 1, 3, 3),\n",
       "      %44 : Float(128),\n",
       "      %45 : Float(128),\n",
       "      %46 : Float(128),\n",
       "      %47 : Float(128),\n",
       "      %48 : Long(),\n",
       "      %49 : Float(256, 128, 1, 1),\n",
       "      %50 : Float(256),\n",
       "      %51 : Float(256),\n",
       "      %52 : Float(256),\n",
       "      %53 : Float(256),\n",
       "      %54 : Long(),\n",
       "      %55 : Float(256, 1, 3, 3),\n",
       "      %56 : Float(256),\n",
       "      %57 : Float(256),\n",
       "      %58 : Float(256),\n",
       "      %59 : Float(256),\n",
       "      %60 : Long(),\n",
       "      %61 : Float(256, 256, 1, 1),\n",
       "      %62 : Float(256),\n",
       "      %63 : Float(256),\n",
       "      %64 : Float(256),\n",
       "      %65 : Float(256),\n",
       "      %66 : Long(),\n",
       "      %67 : Float(256, 1, 3, 3),\n",
       "      %68 : Float(256),\n",
       "      %69 : Float(256),\n",
       "      %70 : Float(256),\n",
       "      %71 : Float(256),\n",
       "      %72 : Long(),\n",
       "      %73 : Float(512, 256, 1, 1),\n",
       "      %74 : Float(512),\n",
       "      %75 : Float(512),\n",
       "      %76 : Float(512),\n",
       "      %77 : Float(512),\n",
       "      %78 : Long(),\n",
       "      %79 : Float(512, 1, 3, 3),\n",
       "      %80 : Float(512),\n",
       "      %81 : Float(512),\n",
       "      %82 : Float(512),\n",
       "      %83 : Float(512),\n",
       "      %84 : Long(),\n",
       "      %85 : Float(512, 512, 1, 1),\n",
       "      %86 : Float(512),\n",
       "      %87 : Float(512),\n",
       "      %88 : Float(512),\n",
       "      %89 : Float(512),\n",
       "      %90 : Long(),\n",
       "      %91 : Float(512, 1, 3, 3),\n",
       "      %92 : Float(512),\n",
       "      %93 : Float(512),\n",
       "      %94 : Float(512),\n",
       "      %95 : Float(512),\n",
       "      %96 : Long(),\n",
       "      %97 : Float(512, 512, 1, 1),\n",
       "      %98 : Float(512),\n",
       "      %99 : Float(512),\n",
       "      %100 : Float(512),\n",
       "      %101 : Float(512),\n",
       "      %102 : Long(),\n",
       "      %103 : Float(512, 1, 3, 3),\n",
       "      %104 : Float(512),\n",
       "      %105 : Float(512),\n",
       "      %106 : Float(512),\n",
       "      %107 : Float(512),\n",
       "      %108 : Long(),\n",
       "      %109 : Float(512, 512, 1, 1),\n",
       "      %110 : Float(512),\n",
       "      %111 : Float(512),\n",
       "      %112 : Float(512),\n",
       "      %113 : Float(512),\n",
       "      %114 : Long(),\n",
       "      %115 : Float(512, 1, 3, 3),\n",
       "      %116 : Float(512),\n",
       "      %117 : Float(512),\n",
       "      %118 : Float(512),\n",
       "      %119 : Float(512),\n",
       "      %120 : Long(),\n",
       "      %121 : Float(512, 512, 1, 1),\n",
       "      %122 : Float(512),\n",
       "      %123 : Float(512),\n",
       "      %124 : Float(512),\n",
       "      %125 : Float(512),\n",
       "      %126 : Long(),\n",
       "      %127 : Float(512, 1, 3, 3),\n",
       "      %128 : Float(512),\n",
       "      %129 : Float(512),\n",
       "      %130 : Float(512),\n",
       "      %131 : Float(512),\n",
       "      %132 : Long(),\n",
       "      %133 : Float(512, 512, 1, 1),\n",
       "      %134 : Float(512),\n",
       "      %135 : Float(512),\n",
       "      %136 : Float(512),\n",
       "      %137 : Float(512),\n",
       "      %138 : Long(),\n",
       "      %139 : Float(512, 1, 3, 3),\n",
       "      %140 : Float(512),\n",
       "      %141 : Float(512),\n",
       "      %142 : Float(512),\n",
       "      %143 : Float(512),\n",
       "      %144 : Long(),\n",
       "      %145 : Float(1024, 512, 1, 1),\n",
       "      %146 : Float(1024),\n",
       "      %147 : Float(1024),\n",
       "      %148 : Float(1024),\n",
       "      %149 : Float(1024),\n",
       "      %150 : Long(),\n",
       "      %151 : Float(1024, 1, 3, 3),\n",
       "      %152 : Float(1024),\n",
       "      %153 : Float(1024),\n",
       "      %154 : Float(1024),\n",
       "      %155 : Float(1024),\n",
       "      %156 : Long(),\n",
       "      %157 : Float(1024, 1024, 1, 1),\n",
       "      %158 : Float(1024),\n",
       "      %159 : Float(1024),\n",
       "      %160 : Float(1024),\n",
       "      %161 : Float(1024),\n",
       "      %162 : Long(),\n",
       "      %163 : Float(1000, 1024),\n",
       "      %164 : Float(1000)):\n",
       "  %165 : Float(1, 3, 224, 224) = aten::clone(%0)\n",
       "  %166 : Float(32, 3, 3, 3) = aten::clone(%1)\n",
       "  %167 : Float(32) = aten::clone(%2)\n",
       "  %168 : Float(32) = aten::clone(%3)\n",
       "  %169 : Float(32) = aten::clone(%4)\n",
       "  %170 : Float(32) = aten::clone(%5)\n",
       "  %171 : Long() = aten::clone(%6)\n",
       "  %172 : Float(32, 1, 3, 3) = aten::clone(%7)\n",
       "  %173 : Float(32) = aten::clone(%8)\n",
       "  %174 : Float(32) = aten::clone(%9)\n",
       "  %175 : Float(32) = aten::clone(%10)\n",
       "  %176 : Float(32) = aten::clone(%11)\n",
       "  %177 : Long() = aten::clone(%12)\n",
       "  %178 : Float(64, 32, 1, 1) = aten::clone(%13)\n",
       "  %179 : Float(64) = aten::clone(%14)\n",
       "  %180 : Float(64) = aten::clone(%15)\n",
       "  %181 : Float(64) = aten::clone(%16)\n",
       "  %182 : Float(64) = aten::clone(%17)\n",
       "  %183 : Long() = aten::clone(%18)\n",
       "  %184 : Float(64, 1, 3, 3) = aten::clone(%19)\n",
       "  %185 : Float(64) = aten::clone(%20)\n",
       "  %186 : Float(64) = aten::clone(%21)\n",
       "  %187 : Float(64) = aten::clone(%22)\n",
       "  %188 : Float(64) = aten::clone(%23)\n",
       "  %189 : Long() = aten::clone(%24)\n",
       "  %190 : Float(128, 64, 1, 1) = aten::clone(%25)\n",
       "  %191 : Float(128) = aten::clone(%26)\n",
       "  %192 : Float(128) = aten::clone(%27)\n",
       "  %193 : Float(128) = aten::clone(%28)\n",
       "  %194 : Float(128) = aten::clone(%29)\n",
       "  %195 : Long() = aten::clone(%30)\n",
       "  %196 : Float(128, 1, 3, 3) = aten::clone(%31)\n",
       "  %197 : Float(128) = aten::clone(%32)\n",
       "  %198 : Float(128) = aten::clone(%33)\n",
       "  %199 : Float(128) = aten::clone(%34)\n",
       "  %200 : Float(128) = aten::clone(%35)\n",
       "  %201 : Long() = aten::clone(%36)\n",
       "  %202 : Float(128, 128, 1, 1) = aten::clone(%37)\n",
       "  %203 : Float(128) = aten::clone(%38)\n",
       "  %204 : Float(128) = aten::clone(%39)\n",
       "  %205 : Float(128) = aten::clone(%40)\n",
       "  %206 : Float(128) = aten::clone(%41)\n",
       "  %207 : Long() = aten::clone(%42)\n",
       "  %208 : Float(128, 1, 3, 3) = aten::clone(%43)\n",
       "  %209 : Float(128) = aten::clone(%44)\n",
       "  %210 : Float(128) = aten::clone(%45)\n",
       "  %211 : Float(128) = aten::clone(%46)\n",
       "  %212 : Float(128) = aten::clone(%47)\n",
       "  %213 : Long() = aten::clone(%48)\n",
       "  %214 : Float(256, 128, 1, 1) = aten::clone(%49)\n",
       "  %215 : Float(256) = aten::clone(%50)\n",
       "  %216 : Float(256) = aten::clone(%51)\n",
       "  %217 : Float(256) = aten::clone(%52)\n",
       "  %218 : Float(256) = aten::clone(%53)\n",
       "  %219 : Long() = aten::clone(%54)\n",
       "  %220 : Float(256, 1, 3, 3) = aten::clone(%55)\n",
       "  %221 : Float(256) = aten::clone(%56)\n",
       "  %222 : Float(256) = aten::clone(%57)\n",
       "  %223 : Float(256) = aten::clone(%58)\n",
       "  %224 : Float(256) = aten::clone(%59)\n",
       "  %225 : Long() = aten::clone(%60)\n",
       "  %226 : Float(256, 256, 1, 1) = aten::clone(%61)\n",
       "  %227 : Float(256) = aten::clone(%62)\n",
       "  %228 : Float(256) = aten::clone(%63)\n",
       "  %229 : Float(256) = aten::clone(%64)\n",
       "  %230 : Float(256) = aten::clone(%65)\n",
       "  %231 : Long() = aten::clone(%66)\n",
       "  %232 : Float(256, 1, 3, 3) = aten::clone(%67)\n",
       "  %233 : Float(256) = aten::clone(%68)\n",
       "  %234 : Float(256) = aten::clone(%69)\n",
       "  %235 : Float(256) = aten::clone(%70)\n",
       "  %236 : Float(256) = aten::clone(%71)\n",
       "  %237 : Long() = aten::clone(%72)\n",
       "  %238 : Float(512, 256, 1, 1) = aten::clone(%73)\n",
       "  %239 : Float(512) = aten::clone(%74)\n",
       "  %240 : Float(512) = aten::clone(%75)\n",
       "  %241 : Float(512) = aten::clone(%76)\n",
       "  %242 : Float(512) = aten::clone(%77)\n",
       "  %243 : Long() = aten::clone(%78)\n",
       "  %244 : Float(512, 1, 3, 3) = aten::clone(%79)\n",
       "  %245 : Float(512) = aten::clone(%80)\n",
       "  %246 : Float(512) = aten::clone(%81)\n",
       "  %247 : Float(512) = aten::clone(%82)\n",
       "  %248 : Float(512) = aten::clone(%83)\n",
       "  %249 : Long() = aten::clone(%84)\n",
       "  %250 : Float(512, 512, 1, 1) = aten::clone(%85)\n",
       "  %251 : Float(512) = aten::clone(%86)\n",
       "  %252 : Float(512) = aten::clone(%87)\n",
       "  %253 : Float(512) = aten::clone(%88)\n",
       "  %254 : Float(512) = aten::clone(%89)\n",
       "  %255 : Long() = aten::clone(%90)\n",
       "  %256 : Float(512, 1, 3, 3) = aten::clone(%91)\n",
       "  %257 : Float(512) = aten::clone(%92)\n",
       "  %258 : Float(512) = aten::clone(%93)\n",
       "  %259 : Float(512) = aten::clone(%94)\n",
       "  %260 : Float(512) = aten::clone(%95)\n",
       "  %261 : Long() = aten::clone(%96)\n",
       "  %262 : Float(512, 512, 1, 1) = aten::clone(%97)\n",
       "  %263 : Float(512) = aten::clone(%98)\n",
       "  %264 : Float(512) = aten::clone(%99)\n",
       "  %265 : Float(512) = aten::clone(%100)\n",
       "  %266 : Float(512) = aten::clone(%101)\n",
       "  %267 : Long() = aten::clone(%102)\n",
       "  %268 : Float(512, 1, 3, 3) = aten::clone(%103)\n",
       "  %269 : Float(512) = aten::clone(%104)\n",
       "  %270 : Float(512) = aten::clone(%105)\n",
       "  %271 : Float(512) = aten::clone(%106)\n",
       "  %272 : Float(512) = aten::clone(%107)\n",
       "  %273 : Long() = aten::clone(%108)\n",
       "  %274 : Float(512, 512, 1, 1) = aten::clone(%109)\n",
       "  %275 : Float(512) = aten::clone(%110)\n",
       "  %276 : Float(512) = aten::clone(%111)\n",
       "  %277 : Float(512) = aten::clone(%112)\n",
       "  %278 : Float(512) = aten::clone(%113)\n",
       "  %279 : Long() = aten::clone(%114)\n",
       "  %280 : Float(512, 1, 3, 3) = aten::clone(%115)\n",
       "  %281 : Float(512) = aten::clone(%116)\n",
       "  %282 : Float(512) = aten::clone(%117)\n",
       "  %283 : Float(512) = aten::clone(%118)\n",
       "  %284 : Float(512) = aten::clone(%119)\n",
       "  %285 : Long() = aten::clone(%120)\n",
       "  %286 : Float(512, 512, 1, 1) = aten::clone(%121)\n",
       "  %287 : Float(512) = aten::clone(%122)\n",
       "  %288 : Float(512) = aten::clone(%123)\n",
       "  %289 : Float(512) = aten::clone(%124)\n",
       "  %290 : Float(512) = aten::clone(%125)\n",
       "  %291 : Long() = aten::clone(%126)\n",
       "  %292 : Float(512, 1, 3, 3) = aten::clone(%127)\n",
       "  %293 : Float(512) = aten::clone(%128)\n",
       "  %294 : Float(512) = aten::clone(%129)\n",
       "  %295 : Float(512) = aten::clone(%130)\n",
       "  %296 : Float(512) = aten::clone(%131)\n",
       "  %297 : Long() = aten::clone(%132)\n",
       "  %298 : Float(512, 512, 1, 1) = aten::clone(%133)\n",
       "  %299 : Float(512) = aten::clone(%134)\n",
       "  %300 : Float(512) = aten::clone(%135)\n",
       "  %301 : Float(512) = aten::clone(%136)\n",
       "  %302 : Float(512) = aten::clone(%137)\n",
       "  %303 : Long() = aten::clone(%138)\n",
       "  %304 : Float(512, 1, 3, 3) = aten::clone(%139)\n",
       "  %305 : Float(512) = aten::clone(%140)\n",
       "  %306 : Float(512) = aten::clone(%141)\n",
       "  %307 : Float(512) = aten::clone(%142)\n",
       "  %308 : Float(512) = aten::clone(%143)\n",
       "  %309 : Long() = aten::clone(%144)\n",
       "  %310 : Float(1024, 512, 1, 1) = aten::clone(%145)\n",
       "  %311 : Float(1024) = aten::clone(%146)\n",
       "  %312 : Float(1024) = aten::clone(%147)\n",
       "  %313 : Float(1024) = aten::clone(%148)\n",
       "  %314 : Float(1024) = aten::clone(%149)\n",
       "  %315 : Long() = aten::clone(%150)\n",
       "  %316 : Float(1024, 1, 3, 3) = aten::clone(%151)\n",
       "  %317 : Float(1024) = aten::clone(%152)\n",
       "  %318 : Float(1024) = aten::clone(%153)\n",
       "  %319 : Float(1024) = aten::clone(%154)\n",
       "  %320 : Float(1024) = aten::clone(%155)\n",
       "  %321 : Long() = aten::clone(%156)\n",
       "  %322 : Float(1024, 1024, 1, 1) = aten::clone(%157)\n",
       "  %323 : Float(1024) = aten::clone(%158)\n",
       "  %324 : Float(1024) = aten::clone(%159)\n",
       "  %325 : Float(1024) = aten::clone(%160)\n",
       "  %326 : Float(1024) = aten::clone(%161)\n",
       "  %327 : Long() = aten::clone(%162)\n",
       "  %328 : Float(1000, 1024) = aten::clone(%163)\n",
       "  %329 : Float(1000) = aten::clone(%164)\n",
       "  %330 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %331 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %332 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %333 : int[] = prim::ListConstruct(%331, %332), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %334 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %335 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %336 : int[] = prim::ListConstruct(%334, %335), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %337 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %338 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %339 : int[] = prim::ListConstruct(%337, %338), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %340 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %341 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %342 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %343 : int[] = prim::ListConstruct(%341, %342), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %344 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %345 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %346 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %347 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %input.1 : Float(1, 32, 112, 112) = aten::_convolution(%0, %1, %330, %333, %336, %339, %340, %343, %344, %345, %346, %347), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %349 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
       "  %350 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
       "  %351 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
       "  %352 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
       "  %input.2 : Float(1, 32, 112, 112) = aten::batch_norm(%input.1, %2, %3, %4, %5, %349, %350, %351, %352), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
       "  %354 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
       "  %355 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
       "  %356 : Float(1, 32, 112, 112) = aten::hardtanh(%input.2, %354, %355), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
       "  %357 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %358 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %359 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %360 : int[] = prim::ListConstruct(%358, %359), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %361 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %362 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %363 : int[] = prim::ListConstruct(%361, %362), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %364 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %365 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %366 : int[] = prim::ListConstruct(%364, %365), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %367 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %368 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %369 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %370 : int[] = prim::ListConstruct(%368, %369), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %371 : int = prim::Constant[value=32](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %372 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %373 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %374 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %input.3 : Float(1, 32, 112, 112) = aten::_convolution(%356, %7, %357, %360, %363, %366, %367, %370, %371, %372, %373, %374), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %376 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
       "  %377 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
       "  %378 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
       "  %379 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
       "  %input.4 : Float(1, 32, 112, 112) = aten::batch_norm(%input.3, %8, %9, %10, %11, %376, %377, %378, %379), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
       "  %381 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
       "  %382 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
       "  %383 : Float(1, 32, 112, 112) = aten::hardtanh(%input.4, %381, %382), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
       "  %384 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %385 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %386 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %387 : int[] = prim::ListConstruct(%385, %386), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %388 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %389 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %390 : int[] = prim::ListConstruct(%388, %389), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %391 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %392 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %393 : int[] = prim::ListConstruct(%391, %392), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %394 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %395 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %396 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %397 : int[] = prim::ListConstruct(%395, %396), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %398 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %399 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %400 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %401 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %input.5 : Float(1, 64, 112, 112) = aten::_convolution(%383, %13, %384, %387, %390, %393, %394, %397, %398, %399, %400, %401), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %403 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
       "  %404 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
       "  %405 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
       "  %406 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
       "  %input.6 : Float(1, 64, 112, 112) = aten::batch_norm(%input.5, %14, %15, %16, %17, %403, %404, %405, %406), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
       "  %408 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
       "  %409 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
       "  %410 : Float(1, 64, 112, 112) = aten::hardtanh(%input.6, %408, %409), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
       "  %411 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %412 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %413 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %414 : int[] = prim::ListConstruct(%412, %413), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %415 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %416 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %417 : int[] = prim::ListConstruct(%415, %416), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %418 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %419 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %420 : int[] = prim::ListConstruct(%418, %419), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %421 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %422 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %423 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %424 : int[] = prim::ListConstruct(%422, %423), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %425 : int = prim::Constant[value=64](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %426 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %427 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %428 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %input.7 : Float(1, 64, 56, 56) = aten::_convolution(%410, %19, %411, %414, %417, %420, %421, %424, %425, %426, %427, %428), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %430 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
       "  %431 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
       "  %432 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
       "  %433 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
       "  %input.8 : Float(1, 64, 56, 56) = aten::batch_norm(%input.7, %20, %21, %22, %23, %430, %431, %432, %433), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
       "  %435 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
       "  %436 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
       "  %437 : Float(1, 64, 56, 56) = aten::hardtanh(%input.8, %435, %436), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
       "  %438 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %439 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %440 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %441 : int[] = prim::ListConstruct(%439, %440), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %442 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %443 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %444 : int[] = prim::ListConstruct(%442, %443), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %445 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %446 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %447 : int[] = prim::ListConstruct(%445, %446), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %448 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %449 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %450 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %451 : int[] = prim::ListConstruct(%449, %450), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %452 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %453 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %454 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %455 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %input.9 : Float(1, 128, 56, 56) = aten::_convolution(%437, %25, %438, %441, %444, %447, %448, %451, %452, %453, %454, %455), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %457 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
       "  %458 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
       "  %459 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
       "  %460 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
       "  %input.10 : Float(1, 128, 56, 56) = aten::batch_norm(%input.9, %26, %27, %28, %29, %457, %458, %459, %460), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
       "  %462 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
       "  %463 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
       "  %464 : Float(1, 128, 56, 56) = aten::hardtanh(%input.10, %462, %463), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
       "  %465 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %466 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %467 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %468 : int[] = prim::ListConstruct(%466, %467), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %469 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %470 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %471 : int[] = prim::ListConstruct(%469, %470), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %472 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %473 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %474 : int[] = prim::ListConstruct(%472, %473), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %475 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %476 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %477 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %478 : int[] = prim::ListConstruct(%476, %477), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %479 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %480 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %481 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %482 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %input.11 : Float(1, 128, 56, 56) = aten::_convolution(%464, %31, %465, %468, %471, %474, %475, %478, %479, %480, %481, %482), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %484 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
       "  %485 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
       "  %486 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
       "  %487 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
       "  %input.12 : Float(1, 128, 56, 56) = aten::batch_norm(%input.11, %32, %33, %34, %35, %484, %485, %486, %487), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
       "  %489 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
       "  %490 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
       "  %491 : Float(1, 128, 56, 56) = aten::hardtanh(%input.12, %489, %490), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
       "  %492 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %493 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %494 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %495 : int[] = prim::ListConstruct(%493, %494), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %496 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %497 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %498 : int[] = prim::ListConstruct(%496, %497), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %499 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %500 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %501 : int[] = prim::ListConstruct(%499, %500), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %502 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %503 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %504 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %505 : int[] = prim::ListConstruct(%503, %504), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %506 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %507 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %508 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %509 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %input.13 : Float(1, 128, 56, 56) = aten::_convolution(%491, %37, %492, %495, %498, %501, %502, %505, %506, %507, %508, %509), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %511 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
       "  %512 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
       "  %513 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
       "  %514 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
       "  %input.14 : Float(1, 128, 56, 56) = aten::batch_norm(%input.13, %38, %39, %40, %41, %511, %512, %513, %514), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
       "  %516 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
       "  %517 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
       "  %518 : Float(1, 128, 56, 56) = aten::hardtanh(%input.14, %516, %517), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
       "  %519 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %520 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %521 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %522 : int[] = prim::ListConstruct(%520, %521), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %523 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %524 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %525 : int[] = prim::ListConstruct(%523, %524), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %526 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %527 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %528 : int[] = prim::ListConstruct(%526, %527), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %529 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %530 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %531 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %532 : int[] = prim::ListConstruct(%530, %531), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %533 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %534 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %535 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %536 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %input.15 : Float(1, 128, 28, 28) = aten::_convolution(%518, %43, %519, %522, %525, %528, %529, %532, %533, %534, %535, %536), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %538 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
       "  %539 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
       "  %540 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
       "  %541 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
       "  %input.16 : Float(1, 128, 28, 28) = aten::batch_norm(%input.15, %44, %45, %46, %47, %538, %539, %540, %541), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
       "  %543 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
       "  %544 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
       "  %545 : Float(1, 128, 28, 28) = aten::hardtanh(%input.16, %543, %544), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
       "  %546 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %547 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %548 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %549 : int[] = prim::ListConstruct(%547, %548), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %550 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %551 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %552 : int[] = prim::ListConstruct(%550, %551), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %553 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %554 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %555 : int[] = prim::ListConstruct(%553, %554), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %556 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %557 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %558 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %559 : int[] = prim::ListConstruct(%557, %558), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %560 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %561 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %562 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %563 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %input.17 : Float(1, 256, 28, 28) = aten::_convolution(%545, %49, %546, %549, %552, %555, %556, %559, %560, %561, %562, %563), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %565 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
       "  %566 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
       "  %567 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
       "  %568 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
       "  %input.18 : Float(1, 256, 28, 28) = aten::batch_norm(%input.17, %50, %51, %52, %53, %565, %566, %567, %568), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
       "  %570 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
       "  %571 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
       "  %572 : Float(1, 256, 28, 28) = aten::hardtanh(%input.18, %570, %571), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
       "  %573 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %574 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %575 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %576 : int[] = prim::ListConstruct(%574, %575), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %577 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %578 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %579 : int[] = prim::ListConstruct(%577, %578), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %580 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %581 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %582 : int[] = prim::ListConstruct(%580, %581), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %583 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %584 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %585 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %586 : int[] = prim::ListConstruct(%584, %585), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %587 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %588 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %589 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %590 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %input.19 : Float(1, 256, 28, 28) = aten::_convolution(%572, %55, %573, %576, %579, %582, %583, %586, %587, %588, %589, %590), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %592 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
       "  %593 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
       "  %594 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
       "  %595 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
       "  %input.20 : Float(1, 256, 28, 28) = aten::batch_norm(%input.19, %56, %57, %58, %59, %592, %593, %594, %595), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
       "  %597 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
       "  %598 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
       "  %599 : Float(1, 256, 28, 28) = aten::hardtanh(%input.20, %597, %598), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
       "  %600 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %601 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %602 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %603 : int[] = prim::ListConstruct(%601, %602), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %604 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %605 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %606 : int[] = prim::ListConstruct(%604, %605), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %607 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %608 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %609 : int[] = prim::ListConstruct(%607, %608), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %610 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %611 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %612 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %613 : int[] = prim::ListConstruct(%611, %612), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %614 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %615 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %616 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %617 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %input.21 : Float(1, 256, 28, 28) = aten::_convolution(%599, %61, %600, %603, %606, %609, %610, %613, %614, %615, %616, %617), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %619 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
       "  %620 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
       "  %621 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
       "  %622 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
       "  %input.22 : Float(1, 256, 28, 28) = aten::batch_norm(%input.21, %62, %63, %64, %65, %619, %620, %621, %622), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
       "  %624 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
       "  %625 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
       "  %626 : Float(1, 256, 28, 28) = aten::hardtanh(%input.22, %624, %625), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
       "  %627 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %628 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %629 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %630 : int[] = prim::ListConstruct(%628, %629), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %631 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %632 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %633 : int[] = prim::ListConstruct(%631, %632), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %634 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %635 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %636 : int[] = prim::ListConstruct(%634, %635), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %637 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %638 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %639 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %640 : int[] = prim::ListConstruct(%638, %639), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %641 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %642 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %643 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %644 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %input.23 : Float(1, 256, 14, 14) = aten::_convolution(%626, %67, %627, %630, %633, %636, %637, %640, %641, %642, %643, %644), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %646 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
       "  %647 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
       "  %648 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
       "  %649 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
       "  %input.24 : Float(1, 256, 14, 14) = aten::batch_norm(%input.23, %68, %69, %70, %71, %646, %647, %648, %649), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
       "  %651 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
       "  %652 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
       "  %653 : Float(1, 256, 14, 14) = aten::hardtanh(%input.24, %651, %652), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
       "  %654 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %655 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %656 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %657 : int[] = prim::ListConstruct(%655, %656), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %658 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %659 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %660 : int[] = prim::ListConstruct(%658, %659), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %661 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %662 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %663 : int[] = prim::ListConstruct(%661, %662), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %664 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %665 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %666 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %667 : int[] = prim::ListConstruct(%665, %666), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %668 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %669 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %670 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %671 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %input.25 : Float(1, 512, 14, 14) = aten::_convolution(%653, %73, %654, %657, %660, %663, %664, %667, %668, %669, %670, %671), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %673 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
       "  %674 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
       "  %675 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
       "  %676 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
       "  %input.26 : Float(1, 512, 14, 14) = aten::batch_norm(%input.25, %74, %75, %76, %77, %673, %674, %675, %676), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
       "  %678 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
       "  %679 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
       "  %680 : Float(1, 512, 14, 14) = aten::hardtanh(%input.26, %678, %679), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
       "  %681 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %682 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %683 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %684 : int[] = prim::ListConstruct(%682, %683), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %685 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %686 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %687 : int[] = prim::ListConstruct(%685, %686), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %688 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %689 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %690 : int[] = prim::ListConstruct(%688, %689), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %691 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %692 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %693 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %694 : int[] = prim::ListConstruct(%692, %693), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %695 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %696 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %697 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %698 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %input.27 : Float(1, 512, 14, 14) = aten::_convolution(%680, %79, %681, %684, %687, %690, %691, %694, %695, %696, %697, %698), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %700 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
       "  %701 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
       "  %702 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
       "  %703 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
       "  %input.28 : Float(1, 512, 14, 14) = aten::batch_norm(%input.27, %80, %81, %82, %83, %700, %701, %702, %703), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
       "  %705 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
       "  %706 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
       "  %707 : Float(1, 512, 14, 14) = aten::hardtanh(%input.28, %705, %706), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
       "  %708 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %709 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %710 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %711 : int[] = prim::ListConstruct(%709, %710), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %712 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %713 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %714 : int[] = prim::ListConstruct(%712, %713), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %715 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %716 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %717 : int[] = prim::ListConstruct(%715, %716), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %718 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %719 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %720 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %721 : int[] = prim::ListConstruct(%719, %720), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %722 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %723 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %724 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %725 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %input.29 : Float(1, 512, 14, 14) = aten::_convolution(%707, %85, %708, %711, %714, %717, %718, %721, %722, %723, %724, %725), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %727 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
       "  %728 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
       "  %729 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
       "  %730 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
       "  %input.30 : Float(1, 512, 14, 14) = aten::batch_norm(%input.29, %86, %87, %88, %89, %727, %728, %729, %730), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
       "  %732 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
       "  %733 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
       "  %734 : Float(1, 512, 14, 14) = aten::hardtanh(%input.30, %732, %733), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
       "  %735 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %736 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %737 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %738 : int[] = prim::ListConstruct(%736, %737), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %739 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %740 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %741 : int[] = prim::ListConstruct(%739, %740), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %742 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %743 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %744 : int[] = prim::ListConstruct(%742, %743), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %745 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %746 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %747 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %748 : int[] = prim::ListConstruct(%746, %747), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %749 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %750 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %751 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %752 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %input.31 : Float(1, 512, 14, 14) = aten::_convolution(%734, %91, %735, %738, %741, %744, %745, %748, %749, %750, %751, %752), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %754 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
       "  %755 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
       "  %756 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
       "  %757 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
       "  %input.32 : Float(1, 512, 14, 14) = aten::batch_norm(%input.31, %92, %93, %94, %95, %754, %755, %756, %757), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
       "  %759 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
       "  %760 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
       "  %761 : Float(1, 512, 14, 14) = aten::hardtanh(%input.32, %759, %760), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
       "  %762 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %763 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %764 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %765 : int[] = prim::ListConstruct(%763, %764), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %766 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %767 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %768 : int[] = prim::ListConstruct(%766, %767), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %769 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %770 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %771 : int[] = prim::ListConstruct(%769, %770), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %772 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %773 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %774 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %775 : int[] = prim::ListConstruct(%773, %774), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %776 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %777 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %778 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %779 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %input.33 : Float(1, 512, 14, 14) = aten::_convolution(%761, %97, %762, %765, %768, %771, %772, %775, %776, %777, %778, %779), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %781 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
       "  %782 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
       "  %783 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
       "  %784 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
       "  %input.34 : Float(1, 512, 14, 14) = aten::batch_norm(%input.33, %98, %99, %100, %101, %781, %782, %783, %784), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
       "  %786 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
       "  %787 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
       "  %788 : Float(1, 512, 14, 14) = aten::hardtanh(%input.34, %786, %787), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
       "  %789 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %790 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %791 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %792 : int[] = prim::ListConstruct(%790, %791), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %793 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %794 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %795 : int[] = prim::ListConstruct(%793, %794), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %796 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %797 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %798 : int[] = prim::ListConstruct(%796, %797), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %799 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %800 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %801 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %802 : int[] = prim::ListConstruct(%800, %801), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %803 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %804 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %805 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %806 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %input.35 : Float(1, 512, 14, 14) = aten::_convolution(%788, %103, %789, %792, %795, %798, %799, %802, %803, %804, %805, %806), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %808 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
       "  %809 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
       "  %810 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
       "  %811 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
       "  %input.36 : Float(1, 512, 14, 14) = aten::batch_norm(%input.35, %104, %105, %106, %107, %808, %809, %810, %811), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
       "  %813 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
       "  %814 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
       "  %815 : Float(1, 512, 14, 14) = aten::hardtanh(%input.36, %813, %814), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
       "  %816 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %817 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %818 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %819 : int[] = prim::ListConstruct(%817, %818), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %820 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %821 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %822 : int[] = prim::ListConstruct(%820, %821), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %823 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %824 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %825 : int[] = prim::ListConstruct(%823, %824), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %826 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %827 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %828 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %829 : int[] = prim::ListConstruct(%827, %828), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %830 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %831 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %832 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %833 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %input.37 : Float(1, 512, 14, 14) = aten::_convolution(%815, %109, %816, %819, %822, %825, %826, %829, %830, %831, %832, %833), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %835 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
       "  %836 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
       "  %837 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
       "  %838 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
       "  %input.38 : Float(1, 512, 14, 14) = aten::batch_norm(%input.37, %110, %111, %112, %113, %835, %836, %837, %838), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
       "  %840 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
       "  %841 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
       "  %842 : Float(1, 512, 14, 14) = aten::hardtanh(%input.38, %840, %841), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
       "  %843 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %844 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %845 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %846 : int[] = prim::ListConstruct(%844, %845), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %847 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %848 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %849 : int[] = prim::ListConstruct(%847, %848), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %850 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %851 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %852 : int[] = prim::ListConstruct(%850, %851), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %853 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %854 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %855 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %856 : int[] = prim::ListConstruct(%854, %855), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %857 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %858 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %859 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %860 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %input.39 : Float(1, 512, 14, 14) = aten::_convolution(%842, %115, %843, %846, %849, %852, %853, %856, %857, %858, %859, %860), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %862 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
       "  %863 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
       "  %864 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
       "  %865 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
       "  %input.40 : Float(1, 512, 14, 14) = aten::batch_norm(%input.39, %116, %117, %118, %119, %862, %863, %864, %865), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
       "  %867 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
       "  %868 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
       "  %869 : Float(1, 512, 14, 14) = aten::hardtanh(%input.40, %867, %868), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
       "  %870 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %871 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %872 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %873 : int[] = prim::ListConstruct(%871, %872), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %874 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %875 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %876 : int[] = prim::ListConstruct(%874, %875), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %877 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %878 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %879 : int[] = prim::ListConstruct(%877, %878), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %880 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %881 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %882 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %883 : int[] = prim::ListConstruct(%881, %882), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %884 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %885 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %886 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %887 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %input.41 : Float(1, 512, 14, 14) = aten::_convolution(%869, %121, %870, %873, %876, %879, %880, %883, %884, %885, %886, %887), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %889 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
       "  %890 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
       "  %891 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
       "  %892 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
       "  %input.42 : Float(1, 512, 14, 14) = aten::batch_norm(%input.41, %122, %123, %124, %125, %889, %890, %891, %892), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
       "  %894 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
       "  %895 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
       "  %896 : Float(1, 512, 14, 14) = aten::hardtanh(%input.42, %894, %895), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
       "  %897 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %898 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %899 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %900 : int[] = prim::ListConstruct(%898, %899), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %901 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %902 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %903 : int[] = prim::ListConstruct(%901, %902), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %904 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %905 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %906 : int[] = prim::ListConstruct(%904, %905), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %907 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %908 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %909 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %910 : int[] = prim::ListConstruct(%908, %909), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %911 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %912 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %913 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %914 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %input.43 : Float(1, 512, 14, 14) = aten::_convolution(%896, %127, %897, %900, %903, %906, %907, %910, %911, %912, %913, %914), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %916 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
       "  %917 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
       "  %918 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
       "  %919 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
       "  %input.44 : Float(1, 512, 14, 14) = aten::batch_norm(%input.43, %128, %129, %130, %131, %916, %917, %918, %919), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
       "  %921 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
       "  %922 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
       "  %923 : Float(1, 512, 14, 14) = aten::hardtanh(%input.44, %921, %922), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
       "  %924 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %925 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %926 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %927 : int[] = prim::ListConstruct(%925, %926), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %928 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %929 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %930 : int[] = prim::ListConstruct(%928, %929), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %931 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %932 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %933 : int[] = prim::ListConstruct(%931, %932), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %934 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %935 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %936 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %937 : int[] = prim::ListConstruct(%935, %936), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %938 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %939 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %940 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %941 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %input.45 : Float(1, 512, 14, 14) = aten::_convolution(%923, %133, %924, %927, %930, %933, %934, %937, %938, %939, %940, %941), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %943 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
       "  %944 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
       "  %945 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
       "  %946 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
       "  %input.46 : Float(1, 512, 14, 14) = aten::batch_norm(%input.45, %134, %135, %136, %137, %943, %944, %945, %946), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
       "  %948 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
       "  %949 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
       "  %950 : Float(1, 512, 14, 14) = aten::hardtanh(%input.46, %948, %949), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
       "  %951 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %952 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %953 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %954 : int[] = prim::ListConstruct(%952, %953), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %955 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %956 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %957 : int[] = prim::ListConstruct(%955, %956), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %958 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %959 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %960 : int[] = prim::ListConstruct(%958, %959), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %961 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %962 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %963 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %964 : int[] = prim::ListConstruct(%962, %963), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %965 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %966 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %967 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %968 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %input.47 : Float(1, 512, 7, 7) = aten::_convolution(%950, %139, %951, %954, %957, %960, %961, %964, %965, %966, %967, %968), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %970 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
       "  %971 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
       "  %972 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
       "  %973 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
       "  %input.48 : Float(1, 512, 7, 7) = aten::batch_norm(%input.47, %140, %141, %142, %143, %970, %971, %972, %973), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
       "  %975 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
       "  %976 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
       "  %977 : Float(1, 512, 7, 7) = aten::hardtanh(%input.48, %975, %976), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
       "  %978 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %979 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %980 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %981 : int[] = prim::ListConstruct(%979, %980), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %982 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %983 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %984 : int[] = prim::ListConstruct(%982, %983), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %985 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %986 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %987 : int[] = prim::ListConstruct(%985, %986), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %988 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %989 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %990 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %991 : int[] = prim::ListConstruct(%989, %990), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %992 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %993 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %994 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %995 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %input.49 : Float(1, 1024, 7, 7) = aten::_convolution(%977, %145, %978, %981, %984, %987, %988, %991, %992, %993, %994, %995), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %997 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
       "  %998 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
       "  %999 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
       "  %1000 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
       "  %input.50 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.49, %146, %147, %148, %149, %997, %998, %999, %1000), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
       "  %1002 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
       "  %1003 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
       "  %1004 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.50, %1002, %1003), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
       "  %1005 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1006 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1007 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1008 : int[] = prim::ListConstruct(%1006, %1007), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1009 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1010 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1011 : int[] = prim::ListConstruct(%1009, %1010), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1012 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1013 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1014 : int[] = prim::ListConstruct(%1012, %1013), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1015 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1016 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1017 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1018 : int[] = prim::ListConstruct(%1016, %1017), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1019 : int = prim::Constant[value=1024](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1020 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1021 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1022 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %input.51 : Float(1, 1024, 7, 7) = aten::_convolution(%1004, %151, %1005, %1008, %1011, %1014, %1015, %1018, %1019, %1020, %1021, %1022), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1024 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
       "  %1025 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
       "  %1026 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
       "  %1027 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
       "  %input.52 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.51, %152, %153, %154, %155, %1024, %1025, %1026, %1027), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
       "  %1029 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
       "  %1030 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
       "  %1031 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.52, %1029, %1030), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
       "  %1032 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1033 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1034 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1035 : int[] = prim::ListConstruct(%1033, %1034), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1036 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1037 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1038 : int[] = prim::ListConstruct(%1036, %1037), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1039 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1040 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1041 : int[] = prim::ListConstruct(%1039, %1040), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1042 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1043 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1044 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1045 : int[] = prim::ListConstruct(%1043, %1044), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1046 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1047 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1048 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1049 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %input.53 : Float(1, 1024, 7, 7) = aten::_convolution(%1031, %157, %1032, %1035, %1038, %1041, %1042, %1045, %1046, %1047, %1048, %1049), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1051 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
       "  %1052 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
       "  %1053 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
       "  %1054 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
       "  %input.54 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.53, %158, %159, %160, %161, %1051, %1052, %1053, %1054), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
       "  %1056 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
       "  %1057 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
       "  %1058 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.54, %1056, %1057), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
       "  %1059 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1060 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1061 : int[] = prim::ListConstruct(%1059, %1060), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1062 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1063 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1064 : int[] = prim::ListConstruct(%1062, %1063), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1065 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1066 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1067 : int[] = prim::ListConstruct(%1065, %1066), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1068 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1069 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1070 : Float(1, 1024, 1, 1) = aten::avg_pool2d(%1058, %1061, %1064, %1067, %1068, %1069), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1071 : int = prim::Constant[value=-1](), scope: mobilenet_real\n",
       "  %1072 : int = prim::Constant[value=1024](), scope: mobilenet_real\n",
       "  %1073 : int[] = prim::ListConstruct(%1071, %1072), scope: mobilenet_real\n",
       "  %input : Float(1, 1024) = aten::view(%1070, %1073), scope: mobilenet_real\n",
       "  %1075 : Float(1024!, 1000!) = aten::t(%163), scope: mobilenet_real/Linear[fc]\n",
       "  %1076 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n",
       "  %1077 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n",
       "  %1078 : Float(1, 1000) = aten::addmm(%164, %input, %1075, %1076, %1077), scope: mobilenet_real/Linear[fc]\n",
       "  return (%1078)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[%165 : Float(1, 3, 224, 224) = aten::clone(%0),\n",
       " %166 : Float(32, 3, 3, 3) = aten::clone(%1),\n",
       " %167 : Float(32) = aten::clone(%2),\n",
       " %168 : Float(32) = aten::clone(%3),\n",
       " %169 : Float(32) = aten::clone(%4),\n",
       " %170 : Float(32) = aten::clone(%5),\n",
       " %171 : Long() = aten::clone(%6),\n",
       " %172 : Float(32, 1, 3, 3) = aten::clone(%7),\n",
       " %173 : Float(32) = aten::clone(%8),\n",
       " %174 : Float(32) = aten::clone(%9),\n",
       " %175 : Float(32) = aten::clone(%10),\n",
       " %176 : Float(32) = aten::clone(%11),\n",
       " %177 : Long() = aten::clone(%12),\n",
       " %178 : Float(64, 32, 1, 1) = aten::clone(%13),\n",
       " %179 : Float(64) = aten::clone(%14),\n",
       " %180 : Float(64) = aten::clone(%15),\n",
       " %181 : Float(64) = aten::clone(%16),\n",
       " %182 : Float(64) = aten::clone(%17),\n",
       " %183 : Long() = aten::clone(%18),\n",
       " %184 : Float(64, 1, 3, 3) = aten::clone(%19),\n",
       " %185 : Float(64) = aten::clone(%20),\n",
       " %186 : Float(64) = aten::clone(%21),\n",
       " %187 : Float(64) = aten::clone(%22),\n",
       " %188 : Float(64) = aten::clone(%23),\n",
       " %189 : Long() = aten::clone(%24),\n",
       " %190 : Float(128, 64, 1, 1) = aten::clone(%25),\n",
       " %191 : Float(128) = aten::clone(%26),\n",
       " %192 : Float(128) = aten::clone(%27),\n",
       " %193 : Float(128) = aten::clone(%28),\n",
       " %194 : Float(128) = aten::clone(%29),\n",
       " %195 : Long() = aten::clone(%30),\n",
       " %196 : Float(128, 1, 3, 3) = aten::clone(%31),\n",
       " %197 : Float(128) = aten::clone(%32),\n",
       " %198 : Float(128) = aten::clone(%33),\n",
       " %199 : Float(128) = aten::clone(%34),\n",
       " %200 : Float(128) = aten::clone(%35),\n",
       " %201 : Long() = aten::clone(%36),\n",
       " %202 : Float(128, 128, 1, 1) = aten::clone(%37),\n",
       " %203 : Float(128) = aten::clone(%38),\n",
       " %204 : Float(128) = aten::clone(%39),\n",
       " %205 : Float(128) = aten::clone(%40),\n",
       " %206 : Float(128) = aten::clone(%41),\n",
       " %207 : Long() = aten::clone(%42),\n",
       " %208 : Float(128, 1, 3, 3) = aten::clone(%43),\n",
       " %209 : Float(128) = aten::clone(%44),\n",
       " %210 : Float(128) = aten::clone(%45),\n",
       " %211 : Float(128) = aten::clone(%46),\n",
       " %212 : Float(128) = aten::clone(%47),\n",
       " %213 : Long() = aten::clone(%48),\n",
       " %214 : Float(256, 128, 1, 1) = aten::clone(%49),\n",
       " %215 : Float(256) = aten::clone(%50),\n",
       " %216 : Float(256) = aten::clone(%51),\n",
       " %217 : Float(256) = aten::clone(%52),\n",
       " %218 : Float(256) = aten::clone(%53),\n",
       " %219 : Long() = aten::clone(%54),\n",
       " %220 : Float(256, 1, 3, 3) = aten::clone(%55),\n",
       " %221 : Float(256) = aten::clone(%56),\n",
       " %222 : Float(256) = aten::clone(%57),\n",
       " %223 : Float(256) = aten::clone(%58),\n",
       " %224 : Float(256) = aten::clone(%59),\n",
       " %225 : Long() = aten::clone(%60),\n",
       " %226 : Float(256, 256, 1, 1) = aten::clone(%61),\n",
       " %227 : Float(256) = aten::clone(%62),\n",
       " %228 : Float(256) = aten::clone(%63),\n",
       " %229 : Float(256) = aten::clone(%64),\n",
       " %230 : Float(256) = aten::clone(%65),\n",
       " %231 : Long() = aten::clone(%66),\n",
       " %232 : Float(256, 1, 3, 3) = aten::clone(%67),\n",
       " %233 : Float(256) = aten::clone(%68),\n",
       " %234 : Float(256) = aten::clone(%69),\n",
       " %235 : Float(256) = aten::clone(%70),\n",
       " %236 : Float(256) = aten::clone(%71),\n",
       " %237 : Long() = aten::clone(%72),\n",
       " %238 : Float(512, 256, 1, 1) = aten::clone(%73),\n",
       " %239 : Float(512) = aten::clone(%74),\n",
       " %240 : Float(512) = aten::clone(%75),\n",
       " %241 : Float(512) = aten::clone(%76),\n",
       " %242 : Float(512) = aten::clone(%77),\n",
       " %243 : Long() = aten::clone(%78),\n",
       " %244 : Float(512, 1, 3, 3) = aten::clone(%79),\n",
       " %245 : Float(512) = aten::clone(%80),\n",
       " %246 : Float(512) = aten::clone(%81),\n",
       " %247 : Float(512) = aten::clone(%82),\n",
       " %248 : Float(512) = aten::clone(%83),\n",
       " %249 : Long() = aten::clone(%84),\n",
       " %250 : Float(512, 512, 1, 1) = aten::clone(%85),\n",
       " %251 : Float(512) = aten::clone(%86),\n",
       " %252 : Float(512) = aten::clone(%87),\n",
       " %253 : Float(512) = aten::clone(%88),\n",
       " %254 : Float(512) = aten::clone(%89),\n",
       " %255 : Long() = aten::clone(%90),\n",
       " %256 : Float(512, 1, 3, 3) = aten::clone(%91),\n",
       " %257 : Float(512) = aten::clone(%92),\n",
       " %258 : Float(512) = aten::clone(%93),\n",
       " %259 : Float(512) = aten::clone(%94),\n",
       " %260 : Float(512) = aten::clone(%95),\n",
       " %261 : Long() = aten::clone(%96),\n",
       " %262 : Float(512, 512, 1, 1) = aten::clone(%97),\n",
       " %263 : Float(512) = aten::clone(%98),\n",
       " %264 : Float(512) = aten::clone(%99),\n",
       " %265 : Float(512) = aten::clone(%100),\n",
       " %266 : Float(512) = aten::clone(%101),\n",
       " %267 : Long() = aten::clone(%102),\n",
       " %268 : Float(512, 1, 3, 3) = aten::clone(%103),\n",
       " %269 : Float(512) = aten::clone(%104),\n",
       " %270 : Float(512) = aten::clone(%105),\n",
       " %271 : Float(512) = aten::clone(%106),\n",
       " %272 : Float(512) = aten::clone(%107),\n",
       " %273 : Long() = aten::clone(%108),\n",
       " %274 : Float(512, 512, 1, 1) = aten::clone(%109),\n",
       " %275 : Float(512) = aten::clone(%110),\n",
       " %276 : Float(512) = aten::clone(%111),\n",
       " %277 : Float(512) = aten::clone(%112),\n",
       " %278 : Float(512) = aten::clone(%113),\n",
       " %279 : Long() = aten::clone(%114),\n",
       " %280 : Float(512, 1, 3, 3) = aten::clone(%115),\n",
       " %281 : Float(512) = aten::clone(%116),\n",
       " %282 : Float(512) = aten::clone(%117),\n",
       " %283 : Float(512) = aten::clone(%118),\n",
       " %284 : Float(512) = aten::clone(%119),\n",
       " %285 : Long() = aten::clone(%120),\n",
       " %286 : Float(512, 512, 1, 1) = aten::clone(%121),\n",
       " %287 : Float(512) = aten::clone(%122),\n",
       " %288 : Float(512) = aten::clone(%123),\n",
       " %289 : Float(512) = aten::clone(%124),\n",
       " %290 : Float(512) = aten::clone(%125),\n",
       " %291 : Long() = aten::clone(%126),\n",
       " %292 : Float(512, 1, 3, 3) = aten::clone(%127),\n",
       " %293 : Float(512) = aten::clone(%128),\n",
       " %294 : Float(512) = aten::clone(%129),\n",
       " %295 : Float(512) = aten::clone(%130),\n",
       " %296 : Float(512) = aten::clone(%131),\n",
       " %297 : Long() = aten::clone(%132),\n",
       " %298 : Float(512, 512, 1, 1) = aten::clone(%133),\n",
       " %299 : Float(512) = aten::clone(%134),\n",
       " %300 : Float(512) = aten::clone(%135),\n",
       " %301 : Float(512) = aten::clone(%136),\n",
       " %302 : Float(512) = aten::clone(%137),\n",
       " %303 : Long() = aten::clone(%138),\n",
       " %304 : Float(512, 1, 3, 3) = aten::clone(%139),\n",
       " %305 : Float(512) = aten::clone(%140),\n",
       " %306 : Float(512) = aten::clone(%141),\n",
       " %307 : Float(512) = aten::clone(%142),\n",
       " %308 : Float(512) = aten::clone(%143),\n",
       " %309 : Long() = aten::clone(%144),\n",
       " %310 : Float(1024, 512, 1, 1) = aten::clone(%145),\n",
       " %311 : Float(1024) = aten::clone(%146),\n",
       " %312 : Float(1024) = aten::clone(%147),\n",
       " %313 : Float(1024) = aten::clone(%148),\n",
       " %314 : Float(1024) = aten::clone(%149),\n",
       " %315 : Long() = aten::clone(%150),\n",
       " %316 : Float(1024, 1, 3, 3) = aten::clone(%151),\n",
       " %317 : Float(1024) = aten::clone(%152),\n",
       " %318 : Float(1024) = aten::clone(%153),\n",
       " %319 : Float(1024) = aten::clone(%154),\n",
       " %320 : Float(1024) = aten::clone(%155),\n",
       " %321 : Long() = aten::clone(%156),\n",
       " %322 : Float(1024, 1024, 1, 1) = aten::clone(%157),\n",
       " %323 : Float(1024) = aten::clone(%158),\n",
       " %324 : Float(1024) = aten::clone(%159),\n",
       " %325 : Float(1024) = aten::clone(%160),\n",
       " %326 : Float(1024) = aten::clone(%161),\n",
       " %327 : Long() = aten::clone(%162),\n",
       " %328 : Float(1000, 1024) = aten::clone(%163),\n",
       " %329 : Float(1000) = aten::clone(%164),\n",
       " %330 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %331 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %332 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %333 : int[] = prim::ListConstruct(%331, %332), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %334 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %335 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %336 : int[] = prim::ListConstruct(%334, %335), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %337 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %338 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %339 : int[] = prim::ListConstruct(%337, %338), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %340 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %341 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %342 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %343 : int[] = prim::ListConstruct(%341, %342), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %344 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %345 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %346 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %347 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %input.1 : Float(1, 32, 112, 112) = aten::_convolution(%0, %1, %330, %333, %336, %339, %340, %343, %344, %345, %346, %347), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %349 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1],\n",
       " %350 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1],\n",
       " %351 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1],\n",
       " %352 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1],\n",
       " %input.2 : Float(1, 32, 112, 112) = aten::batch_norm(%input.1, %2, %3, %4, %5, %349, %350, %351, %352), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1],\n",
       " %354 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2],\n",
       " %355 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2],\n",
       " %356 : Float(1, 32, 112, 112) = aten::hardtanh(%input.2, %354, %355), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2],\n",
       " %357 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %358 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %359 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %360 : int[] = prim::ListConstruct(%358, %359), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %361 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %362 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %363 : int[] = prim::ListConstruct(%361, %362), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %364 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %365 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %366 : int[] = prim::ListConstruct(%364, %365), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %367 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %368 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %369 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %370 : int[] = prim::ListConstruct(%368, %369), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %371 : int = prim::Constant[value=32](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %372 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %373 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %374 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %input.3 : Float(1, 32, 112, 112) = aten::_convolution(%356, %7, %357, %360, %363, %366, %367, %370, %371, %372, %373, %374), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %376 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1],\n",
       " %377 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1],\n",
       " %378 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1],\n",
       " %379 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1],\n",
       " %input.4 : Float(1, 32, 112, 112) = aten::batch_norm(%input.3, %8, %9, %10, %11, %376, %377, %378, %379), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1],\n",
       " %381 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2],\n",
       " %382 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2],\n",
       " %383 : Float(1, 32, 112, 112) = aten::hardtanh(%input.4, %381, %382), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2],\n",
       " %384 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %385 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %386 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %387 : int[] = prim::ListConstruct(%385, %386), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %388 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %389 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %390 : int[] = prim::ListConstruct(%388, %389), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %391 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %392 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %393 : int[] = prim::ListConstruct(%391, %392), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %394 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %395 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %396 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %397 : int[] = prim::ListConstruct(%395, %396), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %398 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %399 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %400 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %401 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %input.5 : Float(1, 64, 112, 112) = aten::_convolution(%383, %13, %384, %387, %390, %393, %394, %397, %398, %399, %400, %401), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %403 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1],\n",
       " %404 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1],\n",
       " %405 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1],\n",
       " %406 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1],\n",
       " %input.6 : Float(1, 64, 112, 112) = aten::batch_norm(%input.5, %14, %15, %16, %17, %403, %404, %405, %406), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1],\n",
       " %408 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2],\n",
       " %409 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2],\n",
       " %410 : Float(1, 64, 112, 112) = aten::hardtanh(%input.6, %408, %409), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2],\n",
       " %411 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %412 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %413 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %414 : int[] = prim::ListConstruct(%412, %413), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %415 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %416 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %417 : int[] = prim::ListConstruct(%415, %416), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %418 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %419 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %420 : int[] = prim::ListConstruct(%418, %419), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %421 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %422 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %423 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %424 : int[] = prim::ListConstruct(%422, %423), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %425 : int = prim::Constant[value=64](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %426 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %427 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %428 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %input.7 : Float(1, 64, 56, 56) = aten::_convolution(%410, %19, %411, %414, %417, %420, %421, %424, %425, %426, %427, %428), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %430 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1],\n",
       " %431 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1],\n",
       " %432 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1],\n",
       " %433 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1],\n",
       " %input.8 : Float(1, 64, 56, 56) = aten::batch_norm(%input.7, %20, %21, %22, %23, %430, %431, %432, %433), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1],\n",
       " %435 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2],\n",
       " %436 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2],\n",
       " %437 : Float(1, 64, 56, 56) = aten::hardtanh(%input.8, %435, %436), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2],\n",
       " %438 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %439 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %440 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %441 : int[] = prim::ListConstruct(%439, %440), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %442 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %443 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %444 : int[] = prim::ListConstruct(%442, %443), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %445 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %446 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %447 : int[] = prim::ListConstruct(%445, %446), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %448 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %449 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %450 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %451 : int[] = prim::ListConstruct(%449, %450), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %452 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %453 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %454 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %455 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %input.9 : Float(1, 128, 56, 56) = aten::_convolution(%437, %25, %438, %441, %444, %447, %448, %451, %452, %453, %454, %455), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %457 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1],\n",
       " %458 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1],\n",
       " %459 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1],\n",
       " %460 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1],\n",
       " %input.10 : Float(1, 128, 56, 56) = aten::batch_norm(%input.9, %26, %27, %28, %29, %457, %458, %459, %460), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1],\n",
       " %462 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2],\n",
       " %463 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2],\n",
       " %464 : Float(1, 128, 56, 56) = aten::hardtanh(%input.10, %462, %463), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2],\n",
       " %465 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %466 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %467 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %468 : int[] = prim::ListConstruct(%466, %467), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %469 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %470 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %471 : int[] = prim::ListConstruct(%469, %470), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %472 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %473 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %474 : int[] = prim::ListConstruct(%472, %473), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %475 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %476 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %477 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %478 : int[] = prim::ListConstruct(%476, %477), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %479 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %480 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %481 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %482 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %input.11 : Float(1, 128, 56, 56) = aten::_convolution(%464, %31, %465, %468, %471, %474, %475, %478, %479, %480, %481, %482), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %484 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1],\n",
       " %485 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1],\n",
       " %486 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1],\n",
       " %487 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1],\n",
       " %input.12 : Float(1, 128, 56, 56) = aten::batch_norm(%input.11, %32, %33, %34, %35, %484, %485, %486, %487), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1],\n",
       " %489 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2],\n",
       " %490 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2],\n",
       " %491 : Float(1, 128, 56, 56) = aten::hardtanh(%input.12, %489, %490), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2],\n",
       " %492 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %493 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %494 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %495 : int[] = prim::ListConstruct(%493, %494), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %496 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %497 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %498 : int[] = prim::ListConstruct(%496, %497), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %499 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %500 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %501 : int[] = prim::ListConstruct(%499, %500), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %502 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %503 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %504 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %505 : int[] = prim::ListConstruct(%503, %504), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %506 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %507 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %508 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %509 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %input.13 : Float(1, 128, 56, 56) = aten::_convolution(%491, %37, %492, %495, %498, %501, %502, %505, %506, %507, %508, %509), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %511 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1],\n",
       " %512 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1],\n",
       " %513 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1],\n",
       " %514 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1],\n",
       " %input.14 : Float(1, 128, 56, 56) = aten::batch_norm(%input.13, %38, %39, %40, %41, %511, %512, %513, %514), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1],\n",
       " %516 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2],\n",
       " %517 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2],\n",
       " %518 : Float(1, 128, 56, 56) = aten::hardtanh(%input.14, %516, %517), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2],\n",
       " %519 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %520 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %521 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %522 : int[] = prim::ListConstruct(%520, %521), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %523 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %524 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %525 : int[] = prim::ListConstruct(%523, %524), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %526 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %527 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %528 : int[] = prim::ListConstruct(%526, %527), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %529 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %530 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %531 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %532 : int[] = prim::ListConstruct(%530, %531), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %533 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %534 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %535 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %536 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %input.15 : Float(1, 128, 28, 28) = aten::_convolution(%518, %43, %519, %522, %525, %528, %529, %532, %533, %534, %535, %536), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %538 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1],\n",
       " %539 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1],\n",
       " %540 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1],\n",
       " %541 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1],\n",
       " %input.16 : Float(1, 128, 28, 28) = aten::batch_norm(%input.15, %44, %45, %46, %47, %538, %539, %540, %541), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1],\n",
       " %543 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2],\n",
       " %544 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2],\n",
       " %545 : Float(1, 128, 28, 28) = aten::hardtanh(%input.16, %543, %544), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2],\n",
       " %546 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %547 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %548 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %549 : int[] = prim::ListConstruct(%547, %548), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %550 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %551 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %552 : int[] = prim::ListConstruct(%550, %551), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %553 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %554 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %555 : int[] = prim::ListConstruct(%553, %554), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %556 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %557 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %558 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %559 : int[] = prim::ListConstruct(%557, %558), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %560 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %561 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %562 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %563 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %input.17 : Float(1, 256, 28, 28) = aten::_convolution(%545, %49, %546, %549, %552, %555, %556, %559, %560, %561, %562, %563), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %565 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1],\n",
       " %566 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1],\n",
       " %567 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1],\n",
       " %568 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1],\n",
       " %input.18 : Float(1, 256, 28, 28) = aten::batch_norm(%input.17, %50, %51, %52, %53, %565, %566, %567, %568), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1],\n",
       " %570 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2],\n",
       " %571 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2],\n",
       " %572 : Float(1, 256, 28, 28) = aten::hardtanh(%input.18, %570, %571), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2],\n",
       " %573 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %574 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %575 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %576 : int[] = prim::ListConstruct(%574, %575), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %577 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %578 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %579 : int[] = prim::ListConstruct(%577, %578), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %580 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %581 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %582 : int[] = prim::ListConstruct(%580, %581), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %583 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %584 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %585 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %586 : int[] = prim::ListConstruct(%584, %585), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %587 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %588 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %589 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %590 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %input.19 : Float(1, 256, 28, 28) = aten::_convolution(%572, %55, %573, %576, %579, %582, %583, %586, %587, %588, %589, %590), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %592 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1],\n",
       " %593 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1],\n",
       " %594 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1],\n",
       " %595 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1],\n",
       " %input.20 : Float(1, 256, 28, 28) = aten::batch_norm(%input.19, %56, %57, %58, %59, %592, %593, %594, %595), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1],\n",
       " %597 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2],\n",
       " %598 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2],\n",
       " %599 : Float(1, 256, 28, 28) = aten::hardtanh(%input.20, %597, %598), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2],\n",
       " %600 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %601 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %602 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %603 : int[] = prim::ListConstruct(%601, %602), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %604 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %605 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %606 : int[] = prim::ListConstruct(%604, %605), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %607 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %608 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %609 : int[] = prim::ListConstruct(%607, %608), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %610 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %611 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %612 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %613 : int[] = prim::ListConstruct(%611, %612), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %614 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %615 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %616 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %617 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %input.21 : Float(1, 256, 28, 28) = aten::_convolution(%599, %61, %600, %603, %606, %609, %610, %613, %614, %615, %616, %617), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %619 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1],\n",
       " %620 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1],\n",
       " %621 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1],\n",
       " %622 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1],\n",
       " %input.22 : Float(1, 256, 28, 28) = aten::batch_norm(%input.21, %62, %63, %64, %65, %619, %620, %621, %622), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1],\n",
       " %624 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2],\n",
       " %625 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2],\n",
       " %626 : Float(1, 256, 28, 28) = aten::hardtanh(%input.22, %624, %625), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2],\n",
       " %627 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %628 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %629 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %630 : int[] = prim::ListConstruct(%628, %629), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %631 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %632 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %633 : int[] = prim::ListConstruct(%631, %632), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %634 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %635 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %636 : int[] = prim::ListConstruct(%634, %635), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %637 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %638 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %639 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %640 : int[] = prim::ListConstruct(%638, %639), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %641 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %642 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %643 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %644 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %input.23 : Float(1, 256, 14, 14) = aten::_convolution(%626, %67, %627, %630, %633, %636, %637, %640, %641, %642, %643, %644), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %646 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1],\n",
       " %647 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1],\n",
       " %648 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1],\n",
       " %649 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1],\n",
       " %input.24 : Float(1, 256, 14, 14) = aten::batch_norm(%input.23, %68, %69, %70, %71, %646, %647, %648, %649), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1],\n",
       " %651 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2],\n",
       " %652 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2],\n",
       " %653 : Float(1, 256, 14, 14) = aten::hardtanh(%input.24, %651, %652), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2],\n",
       " %654 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %655 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %656 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %657 : int[] = prim::ListConstruct(%655, %656), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %658 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %659 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %660 : int[] = prim::ListConstruct(%658, %659), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %661 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %662 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %663 : int[] = prim::ListConstruct(%661, %662), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %664 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %665 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %666 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %667 : int[] = prim::ListConstruct(%665, %666), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %668 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %669 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %670 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %671 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %input.25 : Float(1, 512, 14, 14) = aten::_convolution(%653, %73, %654, %657, %660, %663, %664, %667, %668, %669, %670, %671), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %673 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1],\n",
       " %674 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1],\n",
       " %675 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1],\n",
       " %676 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1],\n",
       " %input.26 : Float(1, 512, 14, 14) = aten::batch_norm(%input.25, %74, %75, %76, %77, %673, %674, %675, %676), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1],\n",
       " %678 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2],\n",
       " %679 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2],\n",
       " %680 : Float(1, 512, 14, 14) = aten::hardtanh(%input.26, %678, %679), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2],\n",
       " %681 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %682 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %683 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %684 : int[] = prim::ListConstruct(%682, %683), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %685 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %686 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %687 : int[] = prim::ListConstruct(%685, %686), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %688 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %689 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %690 : int[] = prim::ListConstruct(%688, %689), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %691 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %692 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %693 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %694 : int[] = prim::ListConstruct(%692, %693), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %695 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %696 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %697 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %698 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %input.27 : Float(1, 512, 14, 14) = aten::_convolution(%680, %79, %681, %684, %687, %690, %691, %694, %695, %696, %697, %698), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %700 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1],\n",
       " %701 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1],\n",
       " %702 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1],\n",
       " %703 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1],\n",
       " %input.28 : Float(1, 512, 14, 14) = aten::batch_norm(%input.27, %80, %81, %82, %83, %700, %701, %702, %703), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1],\n",
       " %705 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2],\n",
       " %706 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2],\n",
       " %707 : Float(1, 512, 14, 14) = aten::hardtanh(%input.28, %705, %706), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2],\n",
       " %708 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %709 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %710 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %711 : int[] = prim::ListConstruct(%709, %710), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %712 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %713 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %714 : int[] = prim::ListConstruct(%712, %713), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %715 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %716 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %717 : int[] = prim::ListConstruct(%715, %716), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %718 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %719 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %720 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %721 : int[] = prim::ListConstruct(%719, %720), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %722 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %723 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %724 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %725 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %input.29 : Float(1, 512, 14, 14) = aten::_convolution(%707, %85, %708, %711, %714, %717, %718, %721, %722, %723, %724, %725), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %727 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1],\n",
       " %728 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1],\n",
       " %729 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1],\n",
       " %730 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1],\n",
       " %input.30 : Float(1, 512, 14, 14) = aten::batch_norm(%input.29, %86, %87, %88, %89, %727, %728, %729, %730), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1],\n",
       " %732 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2],\n",
       " %733 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2],\n",
       " %734 : Float(1, 512, 14, 14) = aten::hardtanh(%input.30, %732, %733), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2],\n",
       " %735 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %736 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %737 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %738 : int[] = prim::ListConstruct(%736, %737), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %739 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %740 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %741 : int[] = prim::ListConstruct(%739, %740), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %742 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %743 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %744 : int[] = prim::ListConstruct(%742, %743), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %745 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %746 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %747 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %748 : int[] = prim::ListConstruct(%746, %747), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %749 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %750 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %751 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %752 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %input.31 : Float(1, 512, 14, 14) = aten::_convolution(%734, %91, %735, %738, %741, %744, %745, %748, %749, %750, %751, %752), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %754 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1],\n",
       " %755 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1],\n",
       " %756 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1],\n",
       " %757 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1],\n",
       " %input.32 : Float(1, 512, 14, 14) = aten::batch_norm(%input.31, %92, %93, %94, %95, %754, %755, %756, %757), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1],\n",
       " %759 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2],\n",
       " %760 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2],\n",
       " %761 : Float(1, 512, 14, 14) = aten::hardtanh(%input.32, %759, %760), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2],\n",
       " %762 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %763 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %764 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %765 : int[] = prim::ListConstruct(%763, %764), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %766 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %767 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %768 : int[] = prim::ListConstruct(%766, %767), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %769 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %770 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %771 : int[] = prim::ListConstruct(%769, %770), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %772 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %773 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %774 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %775 : int[] = prim::ListConstruct(%773, %774), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %776 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %777 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %778 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %779 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %input.33 : Float(1, 512, 14, 14) = aten::_convolution(%761, %97, %762, %765, %768, %771, %772, %775, %776, %777, %778, %779), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %781 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1],\n",
       " %782 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1],\n",
       " %783 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1],\n",
       " %784 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1],\n",
       " %input.34 : Float(1, 512, 14, 14) = aten::batch_norm(%input.33, %98, %99, %100, %101, %781, %782, %783, %784), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1],\n",
       " %786 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2],\n",
       " %787 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2],\n",
       " %788 : Float(1, 512, 14, 14) = aten::hardtanh(%input.34, %786, %787), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2],\n",
       " %789 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %790 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %791 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %792 : int[] = prim::ListConstruct(%790, %791), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %793 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %794 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %795 : int[] = prim::ListConstruct(%793, %794), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %796 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %797 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %798 : int[] = prim::ListConstruct(%796, %797), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %799 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %800 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %801 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %802 : int[] = prim::ListConstruct(%800, %801), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %803 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %804 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %805 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %806 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %input.35 : Float(1, 512, 14, 14) = aten::_convolution(%788, %103, %789, %792, %795, %798, %799, %802, %803, %804, %805, %806), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %808 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1],\n",
       " %809 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1],\n",
       " %810 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1],\n",
       " %811 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1],\n",
       " %input.36 : Float(1, 512, 14, 14) = aten::batch_norm(%input.35, %104, %105, %106, %107, %808, %809, %810, %811), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1],\n",
       " %813 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2],\n",
       " %814 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2],\n",
       " %815 : Float(1, 512, 14, 14) = aten::hardtanh(%input.36, %813, %814), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2],\n",
       " %816 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %817 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %818 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %819 : int[] = prim::ListConstruct(%817, %818), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %820 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %821 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %822 : int[] = prim::ListConstruct(%820, %821), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %823 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %824 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %825 : int[] = prim::ListConstruct(%823, %824), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %826 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %827 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %828 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %829 : int[] = prim::ListConstruct(%827, %828), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %830 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %831 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %832 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %833 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %input.37 : Float(1, 512, 14, 14) = aten::_convolution(%815, %109, %816, %819, %822, %825, %826, %829, %830, %831, %832, %833), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %835 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1],\n",
       " %836 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1],\n",
       " %837 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1],\n",
       " %838 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1],\n",
       " %input.38 : Float(1, 512, 14, 14) = aten::batch_norm(%input.37, %110, %111, %112, %113, %835, %836, %837, %838), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1],\n",
       " %840 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2],\n",
       " %841 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2],\n",
       " %842 : Float(1, 512, 14, 14) = aten::hardtanh(%input.38, %840, %841), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2],\n",
       " %843 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %844 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %845 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %846 : int[] = prim::ListConstruct(%844, %845), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %847 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %848 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %849 : int[] = prim::ListConstruct(%847, %848), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %850 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %851 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %852 : int[] = prim::ListConstruct(%850, %851), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %853 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %854 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %855 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %856 : int[] = prim::ListConstruct(%854, %855), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %857 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %858 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %859 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %860 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %input.39 : Float(1, 512, 14, 14) = aten::_convolution(%842, %115, %843, %846, %849, %852, %853, %856, %857, %858, %859, %860), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %862 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1],\n",
       " %863 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1],\n",
       " %864 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1],\n",
       " %865 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1],\n",
       " %input.40 : Float(1, 512, 14, 14) = aten::batch_norm(%input.39, %116, %117, %118, %119, %862, %863, %864, %865), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1],\n",
       " %867 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2],\n",
       " %868 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2],\n",
       " %869 : Float(1, 512, 14, 14) = aten::hardtanh(%input.40, %867, %868), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2],\n",
       " %870 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %871 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %872 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %873 : int[] = prim::ListConstruct(%871, %872), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %874 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %875 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %876 : int[] = prim::ListConstruct(%874, %875), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %877 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %878 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %879 : int[] = prim::ListConstruct(%877, %878), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %880 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %881 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %882 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %883 : int[] = prim::ListConstruct(%881, %882), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %884 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %885 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %886 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %887 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %input.41 : Float(1, 512, 14, 14) = aten::_convolution(%869, %121, %870, %873, %876, %879, %880, %883, %884, %885, %886, %887), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %889 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1],\n",
       " %890 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1],\n",
       " %891 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1],\n",
       " %892 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1],\n",
       " %input.42 : Float(1, 512, 14, 14) = aten::batch_norm(%input.41, %122, %123, %124, %125, %889, %890, %891, %892), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1],\n",
       " %894 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2],\n",
       " %895 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2],\n",
       " %896 : Float(1, 512, 14, 14) = aten::hardtanh(%input.42, %894, %895), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2],\n",
       " %897 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %898 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %899 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %900 : int[] = prim::ListConstruct(%898, %899), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %901 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %902 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %903 : int[] = prim::ListConstruct(%901, %902), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %904 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %905 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %906 : int[] = prim::ListConstruct(%904, %905), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %907 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %908 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %909 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %910 : int[] = prim::ListConstruct(%908, %909), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %911 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %912 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %913 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %914 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %input.43 : Float(1, 512, 14, 14) = aten::_convolution(%896, %127, %897, %900, %903, %906, %907, %910, %911, %912, %913, %914), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %916 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1],\n",
       " %917 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1],\n",
       " %918 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1],\n",
       " %919 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1],\n",
       " %input.44 : Float(1, 512, 14, 14) = aten::batch_norm(%input.43, %128, %129, %130, %131, %916, %917, %918, %919), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1],\n",
       " %921 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2],\n",
       " %922 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2],\n",
       " %923 : Float(1, 512, 14, 14) = aten::hardtanh(%input.44, %921, %922), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2],\n",
       " %924 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %925 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %926 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %927 : int[] = prim::ListConstruct(%925, %926), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %928 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %929 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %930 : int[] = prim::ListConstruct(%928, %929), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %931 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %932 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %933 : int[] = prim::ListConstruct(%931, %932), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %934 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %935 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %936 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %937 : int[] = prim::ListConstruct(%935, %936), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %938 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %939 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %940 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %941 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %input.45 : Float(1, 512, 14, 14) = aten::_convolution(%923, %133, %924, %927, %930, %933, %934, %937, %938, %939, %940, %941), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %943 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1],\n",
       " %944 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1],\n",
       " %945 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1],\n",
       " %946 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1],\n",
       " %input.46 : Float(1, 512, 14, 14) = aten::batch_norm(%input.45, %134, %135, %136, %137, %943, %944, %945, %946), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1],\n",
       " %948 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2],\n",
       " %949 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2],\n",
       " %950 : Float(1, 512, 14, 14) = aten::hardtanh(%input.46, %948, %949), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2],\n",
       " %951 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %952 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %953 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %954 : int[] = prim::ListConstruct(%952, %953), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %955 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %956 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %957 : int[] = prim::ListConstruct(%955, %956), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %958 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %959 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %960 : int[] = prim::ListConstruct(%958, %959), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %961 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %962 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %963 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %964 : int[] = prim::ListConstruct(%962, %963), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %965 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %966 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %967 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %968 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %input.47 : Float(1, 512, 7, 7) = aten::_convolution(%950, %139, %951, %954, %957, %960, %961, %964, %965, %966, %967, %968), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %970 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1],\n",
       " %971 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1],\n",
       " %972 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1],\n",
       " %973 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1],\n",
       " %input.48 : Float(1, 512, 7, 7) = aten::batch_norm(%input.47, %140, %141, %142, %143, %970, %971, %972, %973), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1],\n",
       " %975 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2],\n",
       " %976 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2],\n",
       " %977 : Float(1, 512, 7, 7) = aten::hardtanh(%input.48, %975, %976), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2],\n",
       " %978 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %979 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %980 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %981 : int[] = prim::ListConstruct(%979, %980), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %982 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %983 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %984 : int[] = prim::ListConstruct(%982, %983), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %985 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %986 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %987 : int[] = prim::ListConstruct(%985, %986), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %988 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %989 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %990 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %991 : int[] = prim::ListConstruct(%989, %990), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %992 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %993 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %994 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %995 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %input.49 : Float(1, 1024, 7, 7) = aten::_convolution(%977, %145, %978, %981, %984, %987, %988, %991, %992, %993, %994, %995), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %997 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1],\n",
       " %998 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1],\n",
       " %999 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1],\n",
       " %1000 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1],\n",
       " %input.50 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.49, %146, %147, %148, %149, %997, %998, %999, %1000), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1],\n",
       " %1002 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2],\n",
       " %1003 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2],\n",
       " %1004 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.50, %1002, %1003), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2],\n",
       " %1005 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1006 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1007 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1008 : int[] = prim::ListConstruct(%1006, %1007), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1009 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1010 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1011 : int[] = prim::ListConstruct(%1009, %1010), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1012 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1013 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1014 : int[] = prim::ListConstruct(%1012, %1013), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1015 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1016 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1017 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1018 : int[] = prim::ListConstruct(%1016, %1017), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1019 : int = prim::Constant[value=1024](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1020 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1021 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1022 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %input.51 : Float(1, 1024, 7, 7) = aten::_convolution(%1004, %151, %1005, %1008, %1011, %1014, %1015, %1018, %1019, %1020, %1021, %1022), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1024 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1],\n",
       " %1025 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1],\n",
       " %1026 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1],\n",
       " %1027 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1],\n",
       " %input.52 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.51, %152, %153, %154, %155, %1024, %1025, %1026, %1027), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1],\n",
       " %1029 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2],\n",
       " %1030 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2],\n",
       " %1031 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.52, %1029, %1030), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2],\n",
       " %1032 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1033 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1034 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1035 : int[] = prim::ListConstruct(%1033, %1034), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1036 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1037 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1038 : int[] = prim::ListConstruct(%1036, %1037), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1039 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1040 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1041 : int[] = prim::ListConstruct(%1039, %1040), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1042 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1043 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1044 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1045 : int[] = prim::ListConstruct(%1043, %1044), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1046 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1047 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1048 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1049 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %input.53 : Float(1, 1024, 7, 7) = aten::_convolution(%1031, %157, %1032, %1035, %1038, %1041, %1042, %1045, %1046, %1047, %1048, %1049), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1051 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1],\n",
       " %1052 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1],\n",
       " %1053 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1],\n",
       " %1054 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1],\n",
       " %input.54 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.53, %158, %159, %160, %161, %1051, %1052, %1053, %1054), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1],\n",
       " %1056 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2],\n",
       " %1057 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2],\n",
       " %1058 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.54, %1056, %1057), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2],\n",
       " %1059 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1060 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1061 : int[] = prim::ListConstruct(%1059, %1060), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1062 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1063 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1064 : int[] = prim::ListConstruct(%1062, %1063), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1065 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1066 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1067 : int[] = prim::ListConstruct(%1065, %1066), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1068 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1069 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1070 : Float(1, 1024, 1, 1) = aten::avg_pool2d(%1058, %1061, %1064, %1067, %1068, %1069), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1071 : int = prim::Constant[value=-1](), scope: mobilenet_real,\n",
       " %1072 : int = prim::Constant[value=1024](), scope: mobilenet_real,\n",
       " %1073 : int[] = prim::ListConstruct(%1071, %1072), scope: mobilenet_real,\n",
       " %input : Float(1, 1024) = aten::view(%1070, %1073), scope: mobilenet_real,\n",
       " %1075 : Float(1024!, 1000!) = aten::t(%163), scope: mobilenet_real/Linear[fc],\n",
       " %1076 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc],\n",
       " %1077 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc],\n",
       " %1078 : Float(1, 1000) = aten::addmm(%164, %input, %1075, %1076, %1077), scope: mobilenet_real/Linear[fc]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trace.graph().nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdnn.conversion.common.DataStructure.graph import GraphNode, Graph\n",
    "import torch\n",
    "import torch.jit\n",
    "import torch.autograd\n",
    "import torch.serialization\n",
    "import contextlib\n",
    "from torch.jit import _unique_state_dict\n",
    "\n",
    "\n",
    "\n",
    "class PytorchGraphNode(GraphNode):\n",
    "\n",
    "    def __init__(self, layer):\n",
    "        self._name = layer.scopeName()\n",
    "        self._kind = layer.kind()\n",
    "        import re\n",
    "        node_id = re.search(r\"[\\d]+\", layer.__str__())\n",
    "        self.id = node_id.group(0)\n",
    "\n",
    "        super(PytorchGraphNode, self).__init__(layer)\n",
    "        self.attrs = {k : layer[k] for k in layer.attributeNames()}\n",
    "\n",
    "        self.weights_name = '.'.join(\n",
    "            re.findall(r'\\[([\\w\\d.]+)\\]', self._name)\n",
    "        )\n",
    "\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        name = self._name + self.id\n",
    "        # Scopes created in a nested scope may have initial characters\n",
    "        # that are illegal as the initial character of an op name\n",
    "        # (viz. '-', '\\', '/', and '_').\n",
    "        name = name.replace('-','n').replace('\\\\','n').replace('/','n').replace('_','n').replace('[','n').replace(']','n')\n",
    "        return name\n",
    "\n",
    "    @property\n",
    "    def type(self):\n",
    "        return self._kind\n",
    "\n",
    "    @property\n",
    "    def pytorch_layer(self):\n",
    "        return self.layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PytorchGraph(Graph):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        # sanity check.\n",
    "        super(PytorchGraph, self).__init__(model)\n",
    "        self.model = model\n",
    "        self.state_dict = _unique_state_dict(self.model)\n",
    "        self.shape_dict = dict()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _optimize_graph(graph, aten, export_raw_ir=False):\n",
    "        # run dce first to eliminate dead parts of the graph that might have been\n",
    "        # left behind by things like symbolic_override\n",
    "\n",
    "        torch._C._jit_pass_dce(graph)\n",
    "        torch._C._jit_pass_lint(graph)\n",
    "\n",
    "        torch._C._jit_pass_peephole(graph)\n",
    "        torch._C._jit_pass_lint(graph)\n",
    "        if not export_raw_ir:\n",
    "            graph = torch._C._jit_pass_onnx(graph, aten)\n",
    " \n",
    "            torch._C._jit_pass_lint(graph)\n",
    "            torch._C._jit_pass_onnx_peephole(graph)\n",
    "            torch._C._jit_pass_lint(graph)\n",
    "        torch._C._jit_pass_dce(graph)\n",
    "        torch._C._jit_pass_lint(graph)\n",
    "        graph = torch._C._jit_pass_canonicalize(graph)\n",
    "        torch._C._jit_pass_lint(graph)\n",
    "        return graph\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_node_id(node):\n",
    "        import re\n",
    "        node_id = re.search(r\"[\\d]+\", node.__str__())\n",
    "        return node_id.group(0)\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def set_training(self, model, mode):\n",
    "        r\"\"\"\n",
    "        A context manager to temporarily set the training mode of 'model'\n",
    "        to 'mode', resetting it when we exit the with-block.  A no-op if\n",
    "        mode is None.\n",
    "        \"\"\"\n",
    "        if mode is None:\n",
    "            yield\n",
    "            return\n",
    "        old_mode = model.training\n",
    "        if old_mode != mode:\n",
    "            model.train(mode)\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            if old_mode != mode:\n",
    "                model.train(old_mode)\n",
    "\n",
    "\n",
    "    def build(self, shape):\n",
    "        \"\"\"\n",
    "        build graph for pytorch 0.4.0\n",
    "        \"\"\"\n",
    "\n",
    "        import re\n",
    "        # construct graph\n",
    "        dummy_input = torch.autograd.Variable(torch.randn(shape), requires_grad=False)\n",
    "\n",
    "\n",
    "        with self.set_training(self.model, False):\n",
    "            trace, output = torch.jit.get_trace_graph(self.model, (dummy_input, ))\n",
    "\n",
    "        #trace.set_graph(PytorchGraph._optimize_graph(trace.graph(), False))\n",
    "        # nodes\n",
    "        nodes = list(trace.graph().nodes())\n",
    "        for node in nodes:\n",
    "            if 'input' in str(node):\n",
    "\n",
    "                print('*************************')\n",
    "                print(node)\n",
    "                print(node.scopeName())\n",
    "                for i,node_input in enumerate(list(node.inputs())):\n",
    "                    print(i, '\\t',PytorchGraph.get_node_id(node_input.node()), node_input.node().scopeName())\n",
    "                \n",
    "                \n",
    "\n",
    "        # input layer\n",
    "        # TODO\n",
    "        return True\n",
    "        print('going to read stuff')\n",
    "\n",
    "        # build each layer\n",
    "        for node in nodes:\n",
    "            node_id = PytorchGraph.get_node_id(node)\n",
    "            node_scope = node.scopeName()\n",
    "            node_name = node_scope + node_id\n",
    "            print(node)\n",
    "            print(node_name)\n",
    "            node_name = node_name.replace('-','n').replace('\\\\','n').replace('/','n').replace('_','n').replace('[','n').replace(']','n')\n",
    "            output_shape_str = re.findall(r'[^()!]+', node.__str__())[1]\n",
    "            #output_shape = [int(x.replace('!', '')) for x in output_shape_str.split(',')]\n",
    "            output_shape = [x.replace('!', '') for x in output_shape_str.split(',')]\n",
    "            print(output_shape)\n",
    "\n",
    "\n",
    "            self.shape_dict[node_name] = output_shape\n",
    "            self.layer_map[node_name] = PytorchGraphNode(node)\n",
    "            self.layer_name_map[node_name] = node_name\n",
    "\n",
    "            # input\n",
    "            for node_input in list(node.inputs()):\n",
    "\n",
    "                if PytorchGraph.get_node_id(node_input.node()) and node_input.node().scopeName():\n",
    "                    node_input_name = node_input.node().scopeName() + PytorchGraph.get_node_id(node_input.node())\n",
    "                    node_input_name = node_input_name.replace('-','n').replace('\\\\','n').replace('/','n').replace('_','n').replace('[','n').replace(']','n')\n",
    "                    print('Connection',node_input_name, node_name)\n",
    "                    self._make_connection(node_input_name, node_name)\n",
    "                    # print(node_input_name ,'->', node_name)\n",
    "        print('Complete')\n",
    "\n",
    "        super(PytorchGraph, self).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "%input.1 : Float(1, 32, 112, 112) = aten::_convolution(%0, %1, %330, %333, %336, %339, %340, %343, %344, %345, %346, %347), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "0 \t 0 \n",
      "1 \t 0 \n",
      "2 \t 330 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "3 \t 333 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "4 \t 336 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "5 \t 339 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "6 \t 340 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "7 \t 343 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "8 \t 344 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "9 \t 345 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "10 \t 346 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "11 \t 347 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "*************************\n",
      "%input.2 : Float(1, 32, 112, 112) = aten::batch_norm(%input.1, %2, %3, %4, %5, %349, %350, %351, %352), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "0 \t 1 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 349 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "6 \t 350 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "7 \t 351 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "8 \t 352 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "*************************\n",
      "%356 : Float(1, 32, 112, 112) = aten::hardtanh(%input.2, %354, %355), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "0 \t 2 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "1 \t 354 mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "2 \t 355 mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "*************************\n",
      "%input.3 : Float(1, 32, 112, 112) = aten::_convolution(%356, %7, %357, %360, %363, %366, %367, %370, %371, %372, %373, %374), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "0 \t 356 mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 357 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "3 \t 360 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "4 \t 363 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "5 \t 366 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "6 \t 367 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "7 \t 370 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "8 \t 371 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "9 \t 372 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "10 \t 373 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "11 \t 374 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "*************************\n",
      "%input.4 : Float(1, 32, 112, 112) = aten::batch_norm(%input.3, %8, %9, %10, %11, %376, %377, %378, %379), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "0 \t 3 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 376 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "6 \t 377 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "7 \t 378 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "8 \t 379 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "*************************\n",
      "%383 : Float(1, 32, 112, 112) = aten::hardtanh(%input.4, %381, %382), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "0 \t 4 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "1 \t 381 mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "2 \t 382 mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "*************************\n",
      "%input.5 : Float(1, 64, 112, 112) = aten::_convolution(%383, %13, %384, %387, %390, %393, %394, %397, %398, %399, %400, %401), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "0 \t 383 mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 384 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "3 \t 387 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "4 \t 390 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "5 \t 393 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "6 \t 394 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "7 \t 397 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "8 \t 398 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "9 \t 399 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "10 \t 400 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "11 \t 401 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "*************************\n",
      "%input.6 : Float(1, 64, 112, 112) = aten::batch_norm(%input.5, %14, %15, %16, %17, %403, %404, %405, %406), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "0 \t 5 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 403 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "6 \t 404 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "7 \t 405 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "8 \t 406 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "*************************\n",
      "%410 : Float(1, 64, 112, 112) = aten::hardtanh(%input.6, %408, %409), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "0 \t 6 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "1 \t 408 mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "2 \t 409 mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "*************************\n",
      "%input.7 : Float(1, 64, 56, 56) = aten::_convolution(%410, %19, %411, %414, %417, %420, %421, %424, %425, %426, %427, %428), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "0 \t 410 mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 411 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "3 \t 414 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "4 \t 417 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "5 \t 420 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "6 \t 421 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "7 \t 424 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "8 \t 425 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "9 \t 426 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "10 \t 427 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "11 \t 428 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "*************************\n",
      "%input.8 : Float(1, 64, 56, 56) = aten::batch_norm(%input.7, %20, %21, %22, %23, %430, %431, %432, %433), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "0 \t 7 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 430 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "6 \t 431 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "7 \t 432 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "8 \t 433 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "*************************\n",
      "%437 : Float(1, 64, 56, 56) = aten::hardtanh(%input.8, %435, %436), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "0 \t 8 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "1 \t 435 mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "2 \t 436 mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "*************************\n",
      "%input.9 : Float(1, 128, 56, 56) = aten::_convolution(%437, %25, %438, %441, %444, %447, %448, %451, %452, %453, %454, %455), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "0 \t 437 mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 438 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "3 \t 441 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "4 \t 444 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "5 \t 447 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "6 \t 448 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "7 \t 451 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "8 \t 452 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "9 \t 453 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "10 \t 454 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "11 \t 455 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "*************************\n",
      "%input.10 : Float(1, 128, 56, 56) = aten::batch_norm(%input.9, %26, %27, %28, %29, %457, %458, %459, %460), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "0 \t 9 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 457 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "6 \t 458 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "7 \t 459 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "8 \t 460 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "*************************\n",
      "%464 : Float(1, 128, 56, 56) = aten::hardtanh(%input.10, %462, %463), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "0 \t 10 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "1 \t 462 mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "2 \t 463 mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "*************************\n",
      "%input.11 : Float(1, 128, 56, 56) = aten::_convolution(%464, %31, %465, %468, %471, %474, %475, %478, %479, %480, %481, %482), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "0 \t 464 mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 465 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "3 \t 468 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "4 \t 471 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "5 \t 474 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "6 \t 475 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "7 \t 478 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "8 \t 479 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "9 \t 480 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "10 \t 481 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "11 \t 482 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "*************************\n",
      "%input.12 : Float(1, 128, 56, 56) = aten::batch_norm(%input.11, %32, %33, %34, %35, %484, %485, %486, %487), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "0 \t 11 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 484 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "6 \t 485 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "7 \t 486 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "8 \t 487 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "*************************\n",
      "%491 : Float(1, 128, 56, 56) = aten::hardtanh(%input.12, %489, %490), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "0 \t 12 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "1 \t 489 mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "2 \t 490 mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "*************************\n",
      "%input.13 : Float(1, 128, 56, 56) = aten::_convolution(%491, %37, %492, %495, %498, %501, %502, %505, %506, %507, %508, %509), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "0 \t 491 mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 492 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "3 \t 495 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "4 \t 498 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "5 \t 501 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "6 \t 502 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "7 \t 505 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "8 \t 506 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "9 \t 507 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "10 \t 508 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "11 \t 509 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "*************************\n",
      "%input.14 : Float(1, 128, 56, 56) = aten::batch_norm(%input.13, %38, %39, %40, %41, %511, %512, %513, %514), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "0 \t 13 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 511 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "6 \t 512 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "7 \t 513 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "8 \t 514 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "*************************\n",
      "%518 : Float(1, 128, 56, 56) = aten::hardtanh(%input.14, %516, %517), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "0 \t 14 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "1 \t 516 mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "2 \t 517 mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "*************************\n",
      "%input.15 : Float(1, 128, 28, 28) = aten::_convolution(%518, %43, %519, %522, %525, %528, %529, %532, %533, %534, %535, %536), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "0 \t 518 mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 519 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "3 \t 522 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "4 \t 525 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "5 \t 528 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "6 \t 529 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "7 \t 532 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "8 \t 533 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "9 \t 534 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "10 \t 535 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "11 \t 536 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "*************************\n",
      "%input.16 : Float(1, 128, 28, 28) = aten::batch_norm(%input.15, %44, %45, %46, %47, %538, %539, %540, %541), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "0 \t 15 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 538 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "6 \t 539 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "7 \t 540 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "8 \t 541 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "*************************\n",
      "%545 : Float(1, 128, 28, 28) = aten::hardtanh(%input.16, %543, %544), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "0 \t 16 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "1 \t 543 mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "2 \t 544 mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "*************************\n",
      "%input.17 : Float(1, 256, 28, 28) = aten::_convolution(%545, %49, %546, %549, %552, %555, %556, %559, %560, %561, %562, %563), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "0 \t 545 mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 546 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "3 \t 549 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "4 \t 552 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "5 \t 555 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "6 \t 556 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "7 \t 559 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "8 \t 560 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "9 \t 561 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "10 \t 562 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "11 \t 563 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "*************************\n",
      "%input.18 : Float(1, 256, 28, 28) = aten::batch_norm(%input.17, %50, %51, %52, %53, %565, %566, %567, %568), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "0 \t 17 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 565 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "6 \t 566 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "7 \t 567 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "8 \t 568 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "*************************\n",
      "%572 : Float(1, 256, 28, 28) = aten::hardtanh(%input.18, %570, %571), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "0 \t 18 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "1 \t 570 mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "2 \t 571 mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "*************************\n",
      "%input.19 : Float(1, 256, 28, 28) = aten::_convolution(%572, %55, %573, %576, %579, %582, %583, %586, %587, %588, %589, %590), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "0 \t 572 mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 573 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "3 \t 576 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "4 \t 579 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "5 \t 582 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "6 \t 583 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "7 \t 586 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "8 \t 587 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "9 \t 588 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "10 \t 589 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "11 \t 590 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "*************************\n",
      "%input.20 : Float(1, 256, 28, 28) = aten::batch_norm(%input.19, %56, %57, %58, %59, %592, %593, %594, %595), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "0 \t 19 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 592 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "6 \t 593 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "7 \t 594 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "8 \t 595 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "*************************\n",
      "%599 : Float(1, 256, 28, 28) = aten::hardtanh(%input.20, %597, %598), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "0 \t 20 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "1 \t 597 mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "2 \t 598 mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "*************************\n",
      "%input.21 : Float(1, 256, 28, 28) = aten::_convolution(%599, %61, %600, %603, %606, %609, %610, %613, %614, %615, %616, %617), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "0 \t 599 mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 600 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "3 \t 603 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "4 \t 606 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "5 \t 609 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "6 \t 610 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "7 \t 613 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "8 \t 614 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "9 \t 615 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "10 \t 616 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "11 \t 617 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "*************************\n",
      "%input.22 : Float(1, 256, 28, 28) = aten::batch_norm(%input.21, %62, %63, %64, %65, %619, %620, %621, %622), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "0 \t 21 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 619 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "6 \t 620 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "7 \t 621 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "8 \t 622 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "*************************\n",
      "%626 : Float(1, 256, 28, 28) = aten::hardtanh(%input.22, %624, %625), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "0 \t 22 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "1 \t 624 mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "2 \t 625 mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "*************************\n",
      "%input.23 : Float(1, 256, 14, 14) = aten::_convolution(%626, %67, %627, %630, %633, %636, %637, %640, %641, %642, %643, %644), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "0 \t 626 mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 627 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "3 \t 630 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "4 \t 633 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "5 \t 636 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "6 \t 637 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "7 \t 640 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "8 \t 641 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "9 \t 642 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "10 \t 643 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "11 \t 644 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "*************************\n",
      "%input.24 : Float(1, 256, 14, 14) = aten::batch_norm(%input.23, %68, %69, %70, %71, %646, %647, %648, %649), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "0 \t 23 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 646 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "6 \t 647 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "7 \t 648 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "8 \t 649 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "*************************\n",
      "%653 : Float(1, 256, 14, 14) = aten::hardtanh(%input.24, %651, %652), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "0 \t 24 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "1 \t 651 mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "2 \t 652 mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "*************************\n",
      "%input.25 : Float(1, 512, 14, 14) = aten::_convolution(%653, %73, %654, %657, %660, %663, %664, %667, %668, %669, %670, %671), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "0 \t 653 mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 654 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "3 \t 657 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "4 \t 660 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "5 \t 663 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "6 \t 664 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "7 \t 667 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "8 \t 668 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "9 \t 669 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "10 \t 670 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "11 \t 671 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "*************************\n",
      "%input.26 : Float(1, 512, 14, 14) = aten::batch_norm(%input.25, %74, %75, %76, %77, %673, %674, %675, %676), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "0 \t 25 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 673 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "6 \t 674 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "7 \t 675 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "8 \t 676 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "*************************\n",
      "%680 : Float(1, 512, 14, 14) = aten::hardtanh(%input.26, %678, %679), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "0 \t 26 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "1 \t 678 mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "2 \t 679 mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "*************************\n",
      "%input.27 : Float(1, 512, 14, 14) = aten::_convolution(%680, %79, %681, %684, %687, %690, %691, %694, %695, %696, %697, %698), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "0 \t 680 mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 681 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "3 \t 684 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "4 \t 687 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "5 \t 690 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "6 \t 691 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "7 \t 694 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "8 \t 695 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "9 \t 696 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "10 \t 697 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "11 \t 698 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "*************************\n",
      "%input.28 : Float(1, 512, 14, 14) = aten::batch_norm(%input.27, %80, %81, %82, %83, %700, %701, %702, %703), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "0 \t 27 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 700 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "6 \t 701 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "7 \t 702 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "8 \t 703 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "*************************\n",
      "%707 : Float(1, 512, 14, 14) = aten::hardtanh(%input.28, %705, %706), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "0 \t 28 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "1 \t 705 mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "2 \t 706 mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "*************************\n",
      "%input.29 : Float(1, 512, 14, 14) = aten::_convolution(%707, %85, %708, %711, %714, %717, %718, %721, %722, %723, %724, %725), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "0 \t 707 mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 708 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "3 \t 711 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "4 \t 714 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "5 \t 717 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "6 \t 718 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "7 \t 721 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "8 \t 722 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "9 \t 723 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "10 \t 724 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "11 \t 725 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "*************************\n",
      "%input.30 : Float(1, 512, 14, 14) = aten::batch_norm(%input.29, %86, %87, %88, %89, %727, %728, %729, %730), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "0 \t 29 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 727 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "6 \t 728 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "7 \t 729 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "8 \t 730 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "*************************\n",
      "%734 : Float(1, 512, 14, 14) = aten::hardtanh(%input.30, %732, %733), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "0 \t 30 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "1 \t 732 mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "2 \t 733 mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "*************************\n",
      "%input.31 : Float(1, 512, 14, 14) = aten::_convolution(%734, %91, %735, %738, %741, %744, %745, %748, %749, %750, %751, %752), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "0 \t 734 mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 735 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "3 \t 738 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "4 \t 741 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "5 \t 744 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "6 \t 745 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "7 \t 748 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "8 \t 749 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "9 \t 750 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "10 \t 751 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "11 \t 752 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "*************************\n",
      "%input.32 : Float(1, 512, 14, 14) = aten::batch_norm(%input.31, %92, %93, %94, %95, %754, %755, %756, %757), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "0 \t 31 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 754 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "6 \t 755 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "7 \t 756 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "8 \t 757 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "*************************\n",
      "%761 : Float(1, 512, 14, 14) = aten::hardtanh(%input.32, %759, %760), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "0 \t 32 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "1 \t 759 mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "2 \t 760 mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "*************************\n",
      "%input.33 : Float(1, 512, 14, 14) = aten::_convolution(%761, %97, %762, %765, %768, %771, %772, %775, %776, %777, %778, %779), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "0 \t 761 mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 762 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "3 \t 765 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "4 \t 768 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "5 \t 771 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "6 \t 772 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "7 \t 775 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "8 \t 776 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "9 \t 777 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "10 \t 778 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "11 \t 779 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "*************************\n",
      "%input.34 : Float(1, 512, 14, 14) = aten::batch_norm(%input.33, %98, %99, %100, %101, %781, %782, %783, %784), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "0 \t 33 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 781 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "6 \t 782 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "7 \t 783 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "8 \t 784 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "*************************\n",
      "%788 : Float(1, 512, 14, 14) = aten::hardtanh(%input.34, %786, %787), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "0 \t 34 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "1 \t 786 mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "2 \t 787 mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "*************************\n",
      "%input.35 : Float(1, 512, 14, 14) = aten::_convolution(%788, %103, %789, %792, %795, %798, %799, %802, %803, %804, %805, %806), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "0 \t 788 mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 789 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "3 \t 792 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "4 \t 795 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "5 \t 798 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "6 \t 799 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "7 \t 802 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "8 \t 803 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "9 \t 804 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "10 \t 805 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "11 \t 806 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "*************************\n",
      "%input.36 : Float(1, 512, 14, 14) = aten::batch_norm(%input.35, %104, %105, %106, %107, %808, %809, %810, %811), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "0 \t 35 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 808 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "6 \t 809 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "7 \t 810 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "8 \t 811 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "*************************\n",
      "%815 : Float(1, 512, 14, 14) = aten::hardtanh(%input.36, %813, %814), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "0 \t 36 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "1 \t 813 mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "2 \t 814 mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "*************************\n",
      "%input.37 : Float(1, 512, 14, 14) = aten::_convolution(%815, %109, %816, %819, %822, %825, %826, %829, %830, %831, %832, %833), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "0 \t 815 mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 816 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "3 \t 819 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "4 \t 822 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "5 \t 825 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "6 \t 826 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "7 \t 829 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "8 \t 830 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "9 \t 831 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "10 \t 832 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "11 \t 833 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "*************************\n",
      "%input.38 : Float(1, 512, 14, 14) = aten::batch_norm(%input.37, %110, %111, %112, %113, %835, %836, %837, %838), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "0 \t 37 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 835 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "6 \t 836 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "7 \t 837 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "8 \t 838 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "*************************\n",
      "%842 : Float(1, 512, 14, 14) = aten::hardtanh(%input.38, %840, %841), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "0 \t 38 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "1 \t 840 mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "2 \t 841 mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "*************************\n",
      "%input.39 : Float(1, 512, 14, 14) = aten::_convolution(%842, %115, %843, %846, %849, %852, %853, %856, %857, %858, %859, %860), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "0 \t 842 mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 843 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "3 \t 846 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "4 \t 849 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "5 \t 852 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "6 \t 853 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "7 \t 856 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "8 \t 857 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "9 \t 858 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "10 \t 859 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "11 \t 860 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "*************************\n",
      "%input.40 : Float(1, 512, 14, 14) = aten::batch_norm(%input.39, %116, %117, %118, %119, %862, %863, %864, %865), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "0 \t 39 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 862 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "6 \t 863 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "7 \t 864 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "8 \t 865 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "*************************\n",
      "%869 : Float(1, 512, 14, 14) = aten::hardtanh(%input.40, %867, %868), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "0 \t 40 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "1 \t 867 mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "2 \t 868 mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "*************************\n",
      "%input.41 : Float(1, 512, 14, 14) = aten::_convolution(%869, %121, %870, %873, %876, %879, %880, %883, %884, %885, %886, %887), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "0 \t 869 mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 870 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "3 \t 873 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "4 \t 876 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "5 \t 879 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "6 \t 880 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "7 \t 883 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "8 \t 884 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "9 \t 885 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "10 \t 886 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "11 \t 887 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "*************************\n",
      "%input.42 : Float(1, 512, 14, 14) = aten::batch_norm(%input.41, %122, %123, %124, %125, %889, %890, %891, %892), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "0 \t 41 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 889 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "6 \t 890 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "7 \t 891 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "8 \t 892 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "*************************\n",
      "%896 : Float(1, 512, 14, 14) = aten::hardtanh(%input.42, %894, %895), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "0 \t 42 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "1 \t 894 mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "2 \t 895 mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "*************************\n",
      "%input.43 : Float(1, 512, 14, 14) = aten::_convolution(%896, %127, %897, %900, %903, %906, %907, %910, %911, %912, %913, %914), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "0 \t 896 mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 897 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "3 \t 900 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "4 \t 903 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "5 \t 906 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "6 \t 907 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "7 \t 910 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "8 \t 911 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "9 \t 912 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "10 \t 913 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "11 \t 914 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "*************************\n",
      "%input.44 : Float(1, 512, 14, 14) = aten::batch_norm(%input.43, %128, %129, %130, %131, %916, %917, %918, %919), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "0 \t 43 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 916 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "6 \t 917 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "7 \t 918 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "8 \t 919 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "*************************\n",
      "%923 : Float(1, 512, 14, 14) = aten::hardtanh(%input.44, %921, %922), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "0 \t 44 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "1 \t 921 mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "2 \t 922 mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "*************************\n",
      "%input.45 : Float(1, 512, 14, 14) = aten::_convolution(%923, %133, %924, %927, %930, %933, %934, %937, %938, %939, %940, %941), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "0 \t 923 mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 924 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "3 \t 927 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "4 \t 930 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "5 \t 933 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "6 \t 934 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "7 \t 937 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "8 \t 938 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "9 \t 939 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "10 \t 940 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "11 \t 941 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "*************************\n",
      "%input.46 : Float(1, 512, 14, 14) = aten::batch_norm(%input.45, %134, %135, %136, %137, %943, %944, %945, %946), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "0 \t 45 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 943 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "6 \t 944 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "7 \t 945 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "8 \t 946 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "*************************\n",
      "%950 : Float(1, 512, 14, 14) = aten::hardtanh(%input.46, %948, %949), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "0 \t 46 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "1 \t 948 mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "2 \t 949 mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "*************************\n",
      "%input.47 : Float(1, 512, 7, 7) = aten::_convolution(%950, %139, %951, %954, %957, %960, %961, %964, %965, %966, %967, %968), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "0 \t 950 mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 951 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "3 \t 954 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "4 \t 957 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "5 \t 960 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "6 \t 961 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "7 \t 964 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "8 \t 965 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "9 \t 966 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "10 \t 967 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "11 \t 968 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "*************************\n",
      "%input.48 : Float(1, 512, 7, 7) = aten::batch_norm(%input.47, %140, %141, %142, %143, %970, %971, %972, %973), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "0 \t 47 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 970 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "6 \t 971 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "7 \t 972 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "8 \t 973 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "*************************\n",
      "%977 : Float(1, 512, 7, 7) = aten::hardtanh(%input.48, %975, %976), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "0 \t 48 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "1 \t 975 mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "2 \t 976 mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "*************************\n",
      "%input.49 : Float(1, 1024, 7, 7) = aten::_convolution(%977, %145, %978, %981, %984, %987, %988, %991, %992, %993, %994, %995), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "0 \t 977 mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 978 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "3 \t 981 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "4 \t 984 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "5 \t 987 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "6 \t 988 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "7 \t 991 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "8 \t 992 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "9 \t 993 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "10 \t 994 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "11 \t 995 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "*************************\n",
      "%input.50 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.49, %146, %147, %148, %149, %997, %998, %999, %1000), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "0 \t 49 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 997 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "6 \t 998 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "7 \t 999 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "8 \t 1000 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "*************************\n",
      "%1004 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.50, %1002, %1003), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "0 \t 50 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "1 \t 1002 mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "2 \t 1003 mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "*************************\n",
      "%input.51 : Float(1, 1024, 7, 7) = aten::_convolution(%1004, %151, %1005, %1008, %1011, %1014, %1015, %1018, %1019, %1020, %1021, %1022), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "0 \t 1004 mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 1005 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "3 \t 1008 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "4 \t 1011 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "5 \t 1014 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "6 \t 1015 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "7 \t 1018 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "8 \t 1019 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "9 \t 1020 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "10 \t 1021 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "11 \t 1022 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "*************************\n",
      "%input.52 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.51, %152, %153, %154, %155, %1024, %1025, %1026, %1027), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "0 \t 51 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 1024 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "6 \t 1025 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "7 \t 1026 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "8 \t 1027 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "*************************\n",
      "%1031 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.52, %1029, %1030), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "0 \t 52 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "1 \t 1029 mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "2 \t 1030 mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "*************************\n",
      "%input.53 : Float(1, 1024, 7, 7) = aten::_convolution(%1031, %157, %1032, %1035, %1038, %1041, %1042, %1045, %1046, %1047, %1048, %1049), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "0 \t 1031 mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 1032 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "3 \t 1035 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "4 \t 1038 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "5 \t 1041 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "6 \t 1042 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "7 \t 1045 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "8 \t 1046 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "9 \t 1047 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "10 \t 1048 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "11 \t 1049 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "*************************\n",
      "%input.54 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.53, %158, %159, %160, %161, %1051, %1052, %1053, %1054), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "0 \t 53 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 1051 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "6 \t 1052 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "7 \t 1053 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "8 \t 1054 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "*************************\n",
      "%1058 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.54, %1056, %1057), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
      "0 \t 54 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "1 \t 1056 mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
      "2 \t 1057 mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
      "*************************\n",
      "%input : Float(1, 1024) = aten::view(%1070, %1073), scope: mobilenet_real\n",
      "\n",
      "mobilenet_real\n",
      "0 \t 1070 mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
      "1 \t 1073 mobilenet_real\n",
      "*************************\n",
      "%1078 : Float(1, 1000) = aten::addmm(%164, %input, %1075, %1076, %1077), scope: mobilenet_real/Linear[fc]\n",
      "\n",
      "mobilenet_real/Linear[fc]\n",
      "0 \t 0 \n",
      "1 \t 1 mobilenet_real\n",
      "2 \t 1075 mobilenet_real/Linear[fc]\n",
      "3 \t 1076 mobilenet_real/Linear[fc]\n",
      "4 \t 1077 mobilenet_real/Linear[fc]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = PytorchGraph(model)\n",
    "g.build([1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'mobilenet_real' object has no attribute 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-d5dac23de4ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 539\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'mobilenet_real' object has no attribute 'graph'"
     ]
    }
   ],
   "source": [
    "model.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = OrderedDict(model.named_modules())['model.26.0']\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.autograd.Variable(torch.randn([1,3,224,224]), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "g,v = torch.utils.tensorboard._pytorch_graph.graph(model,dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['', 'model', 'model.0', 'model.0.0', 'model.0.1', 'model.0.2', 'model.1', 'model.1.0', 'model.1.1', 'model.1.2', 'model.2', 'model.2.0', 'model.2.1', 'model.2.2', 'model.3', 'model.3.0', 'model.3.1', 'model.3.2', 'model.4', 'model.4.0', 'model.4.1', 'model.4.2', 'model.5', 'model.5.0', 'model.5.1', 'model.5.2', 'model.6', 'model.6.0', 'model.6.1', 'model.6.2', 'model.7', 'model.7.0', 'model.7.1', 'model.7.2', 'model.8', 'model.8.0', 'model.8.1', 'model.8.2', 'model.9', 'model.9.0', 'model.9.1', 'model.9.2', 'model.10', 'model.10.0', 'model.10.1', 'model.10.2', 'model.11', 'model.11.0', 'model.11.1', 'model.11.2', 'model.12', 'model.12.0', 'model.12.1', 'model.12.2', 'model.13', 'model.13.0', 'model.13.1', 'model.13.2', 'model.14', 'model.14.0', 'model.14.1', 'model.14.2', 'model.15', 'model.15.0', 'model.15.1', 'model.15.2', 'model.16', 'model.16.0', 'model.16.1', 'model.16.2', 'model.17', 'model.17.0', 'model.17.1', 'model.17.2', 'model.18', 'model.18.0', 'model.18.1', 'model.18.2', 'model.19', 'model.19.0', 'model.19.1', 'model.19.2', 'model.20', 'model.20.0', 'model.20.1', 'model.20.2', 'model.21', 'model.21.0', 'model.21.1', 'model.21.2', 'model.22', 'model.22.0', 'model.22.1', 'model.22.2', 'model.23', 'model.23.0', 'model.23.1', 'model.23.2', 'model.24', 'model.24.0', 'model.24.1', 'model.24.2', 'model.25', 'model.25.0', 'model.25.1', 'model.25.2', 'model.26', 'model.26.0', 'model.26.1', 'model.26.2', 'model.27', 'fc'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrderedDict(model.named_modules()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "def get_model_name(a):\n",
    "    s = ''\n",
    "    while a.find('[') > 0:\n",
    "        s += a[a.find('[')+1:a.find(']')]+'.'\n",
    "        a = a[a.find(']')+1:]\n",
    "    return s[:-1]\n",
    "\n",
    "def get_model_id(a, g):\n",
    "    for i, node in enumerate(g.node):\n",
    "        if node.name == a:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "ff = 'mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/165'\n",
    "v = get_model_name(ff)\n",
    "v = get_model_id(ff, g)\n",
    "print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************\n",
      "input/0\n",
      "IO Node\n",
      "************\n",
      "138 138 Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/165 onnx::Conv\n",
      "0\n",
      "Input 0 : 0 input/0 IO Node \n",
      "1\n",
      "Input 1 : 1 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/1 Parameter model.0.0\n",
      "************\n",
      "139 139 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/166 onnx::BatchNormalization\n",
      "138\n",
      "Input 0 : 138 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/165 onnx::Conv model.0.0\n",
      "2\n",
      "Input 1 : 2 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/2 Parameter model.0.1\n",
      "3\n",
      "Input 2 : 3 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/3 Parameter model.0.1\n",
      "4\n",
      "Input 3 : 4 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/4 Parameter model.0.1\n",
      "5\n",
      "Input 4 : 5 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/5 Parameter model.0.1\n",
      "************\n",
      "140 140 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]/167 onnx::Clip\n",
      "139\n",
      "Input 0 : 139 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/166 onnx::BatchNormalization model.0.1\n",
      "************\n",
      "141 141 Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]/168 onnx::Conv\n",
      "140\n",
      "Input 0 : 140 mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]/167 onnx::Clip model.0.2\n",
      "6\n",
      "Input 1 : 6 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]/7 Parameter model.1.0\n",
      "************\n",
      "142 142 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/169 onnx::BatchNormalization\n",
      "141\n",
      "Input 0 : 141 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]/168 onnx::Conv model.1.0\n",
      "7\n",
      "Input 1 : 7 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/8 Parameter model.1.1\n",
      "8\n",
      "Input 2 : 8 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/9 Parameter model.1.1\n",
      "9\n",
      "Input 3 : 9 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/10 Parameter model.1.1\n",
      "10\n",
      "Input 4 : 10 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/11 Parameter model.1.1\n",
      "************\n",
      "143 143 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]/170 onnx::Clip\n",
      "142\n",
      "Input 0 : 142 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/169 onnx::BatchNormalization model.1.1\n",
      "************\n",
      "144 144 Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]/171 onnx::Conv\n",
      "143\n",
      "Input 0 : 143 mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]/170 onnx::Clip model.1.2\n",
      "11\n",
      "Input 1 : 11 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]/13 Parameter model.2.0\n",
      "************\n",
      "145 145 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/172 onnx::BatchNormalization\n",
      "144\n",
      "Input 0 : 144 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]/171 onnx::Conv model.2.0\n",
      "12\n",
      "Input 1 : 12 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/14 Parameter model.2.1\n",
      "13\n",
      "Input 2 : 13 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/15 Parameter model.2.1\n",
      "14\n",
      "Input 3 : 14 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/16 Parameter model.2.1\n",
      "15\n",
      "Input 4 : 15 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/17 Parameter model.2.1\n",
      "************\n",
      "146 146 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]/173 onnx::Clip\n",
      "145\n",
      "Input 0 : 145 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/172 onnx::BatchNormalization model.2.1\n",
      "************\n",
      "147 147 Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]/174 onnx::Conv\n",
      "146\n",
      "Input 0 : 146 mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]/173 onnx::Clip model.2.2\n",
      "16\n",
      "Input 1 : 16 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]/19 Parameter model.3.0\n",
      "************\n",
      "148 148 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/175 onnx::BatchNormalization\n",
      "147\n",
      "Input 0 : 147 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]/174 onnx::Conv model.3.0\n",
      "17\n",
      "Input 1 : 17 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/20 Parameter model.3.1\n",
      "18\n",
      "Input 2 : 18 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/21 Parameter model.3.1\n",
      "19\n",
      "Input 3 : 19 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/22 Parameter model.3.1\n",
      "20\n",
      "Input 4 : 20 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/23 Parameter model.3.1\n",
      "************\n",
      "149 149 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]/176 onnx::Clip\n",
      "148\n",
      "Input 0 : 148 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/175 onnx::BatchNormalization model.3.1\n",
      "************\n",
      "150 150 Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]/177 onnx::Conv\n",
      "149\n",
      "Input 0 : 149 mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]/176 onnx::Clip model.3.2\n",
      "21\n",
      "Input 1 : 21 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]/25 Parameter model.4.0\n",
      "************\n",
      "151 151 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/178 onnx::BatchNormalization\n",
      "150\n",
      "Input 0 : 150 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]/177 onnx::Conv model.4.0\n",
      "22\n",
      "Input 1 : 22 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/26 Parameter model.4.1\n",
      "23\n",
      "Input 2 : 23 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/27 Parameter model.4.1\n",
      "24\n",
      "Input 3 : 24 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/28 Parameter model.4.1\n",
      "25\n",
      "Input 4 : 25 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/29 Parameter model.4.1\n",
      "************\n",
      "152 152 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]/179 onnx::Clip\n",
      "151\n",
      "Input 0 : 151 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/178 onnx::BatchNormalization model.4.1\n",
      "************\n",
      "153 153 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]/180 onnx::Conv\n",
      "152\n",
      "Input 0 : 152 mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]/179 onnx::Clip model.4.2\n",
      "26\n",
      "Input 1 : 26 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]/31 Parameter model.5.0\n",
      "************\n",
      "154 154 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/181 onnx::BatchNormalization\n",
      "153\n",
      "Input 0 : 153 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]/180 onnx::Conv model.5.0\n",
      "27\n",
      "Input 1 : 27 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/32 Parameter model.5.1\n",
      "28\n",
      "Input 2 : 28 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/33 Parameter model.5.1\n",
      "29\n",
      "Input 3 : 29 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/34 Parameter model.5.1\n",
      "30\n",
      "Input 4 : 30 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/35 Parameter model.5.1\n",
      "************\n",
      "155 155 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]/182 onnx::Clip\n",
      "154\n",
      "Input 0 : 154 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/181 onnx::BatchNormalization model.5.1\n",
      "************\n",
      "156 156 Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]/183 onnx::Conv\n",
      "155\n",
      "Input 0 : 155 mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]/182 onnx::Clip model.5.2\n",
      "31\n",
      "Input 1 : 31 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]/37 Parameter model.6.0\n",
      "************\n",
      "157 157 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/184 onnx::BatchNormalization\n",
      "156\n",
      "Input 0 : 156 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]/183 onnx::Conv model.6.0\n",
      "32\n",
      "Input 1 : 32 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/38 Parameter model.6.1\n",
      "33\n",
      "Input 2 : 33 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/39 Parameter model.6.1\n",
      "34\n",
      "Input 3 : 34 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/40 Parameter model.6.1\n",
      "35\n",
      "Input 4 : 35 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/41 Parameter model.6.1\n",
      "************\n",
      "158 158 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]/185 onnx::Clip\n",
      "157\n",
      "Input 0 : 157 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/184 onnx::BatchNormalization model.6.1\n",
      "************\n",
      "159 159 Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]/186 onnx::Conv\n",
      "158\n",
      "Input 0 : 158 mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]/185 onnx::Clip model.6.2\n",
      "36\n",
      "Input 1 : 36 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]/43 Parameter model.7.0\n",
      "************\n",
      "160 160 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/187 onnx::BatchNormalization\n",
      "159\n",
      "Input 0 : 159 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]/186 onnx::Conv model.7.0\n",
      "37\n",
      "Input 1 : 37 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/44 Parameter model.7.1\n",
      "38\n",
      "Input 2 : 38 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/45 Parameter model.7.1\n",
      "39\n",
      "Input 3 : 39 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/46 Parameter model.7.1\n",
      "40\n",
      "Input 4 : 40 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/47 Parameter model.7.1\n",
      "************\n",
      "161 161 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]/188 onnx::Clip\n",
      "160\n",
      "Input 0 : 160 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/187 onnx::BatchNormalization model.7.1\n",
      "************\n",
      "162 162 Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]/189 onnx::Conv\n",
      "161\n",
      "Input 0 : 161 mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]/188 onnx::Clip model.7.2\n",
      "41\n",
      "Input 1 : 41 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]/49 Parameter model.8.0\n",
      "************\n",
      "163 163 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/190 onnx::BatchNormalization\n",
      "162\n",
      "Input 0 : 162 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]/189 onnx::Conv model.8.0\n",
      "42\n",
      "Input 1 : 42 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/50 Parameter model.8.1\n",
      "43\n",
      "Input 2 : 43 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/51 Parameter model.8.1\n",
      "44\n",
      "Input 3 : 44 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/52 Parameter model.8.1\n",
      "45\n",
      "Input 4 : 45 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/53 Parameter model.8.1\n",
      "************\n",
      "164 164 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]/191 onnx::Clip\n",
      "163\n",
      "Input 0 : 163 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/190 onnx::BatchNormalization model.8.1\n",
      "************\n",
      "165 165 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]/192 onnx::Conv\n",
      "164\n",
      "Input 0 : 164 mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]/191 onnx::Clip model.8.2\n",
      "46\n",
      "Input 1 : 46 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]/55 Parameter model.9.0\n",
      "************\n",
      "166 166 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/193 onnx::BatchNormalization\n",
      "165\n",
      "Input 0 : 165 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]/192 onnx::Conv model.9.0\n",
      "47\n",
      "Input 1 : 47 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/56 Parameter model.9.1\n",
      "48\n",
      "Input 2 : 48 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/57 Parameter model.9.1\n",
      "49\n",
      "Input 3 : 49 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/58 Parameter model.9.1\n",
      "50\n",
      "Input 4 : 50 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/59 Parameter model.9.1\n",
      "************\n",
      "167 167 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]/194 onnx::Clip\n",
      "166\n",
      "Input 0 : 166 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/193 onnx::BatchNormalization model.9.1\n",
      "************\n",
      "168 168 Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]/195 onnx::Conv\n",
      "167\n",
      "Input 0 : 167 mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]/194 onnx::Clip model.9.2\n",
      "51\n",
      "Input 1 : 51 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]/61 Parameter model.10.0\n",
      "************\n",
      "169 169 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/196 onnx::BatchNormalization\n",
      "168\n",
      "Input 0 : 168 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]/195 onnx::Conv model.10.0\n",
      "52\n",
      "Input 1 : 52 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/62 Parameter model.10.1\n",
      "53\n",
      "Input 2 : 53 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/63 Parameter model.10.1\n",
      "54\n",
      "Input 3 : 54 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/64 Parameter model.10.1\n",
      "55\n",
      "Input 4 : 55 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/65 Parameter model.10.1\n",
      "************\n",
      "170 170 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]/197 onnx::Clip\n",
      "169\n",
      "Input 0 : 169 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/196 onnx::BatchNormalization model.10.1\n",
      "************\n",
      "171 171 Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]/198 onnx::Conv\n",
      "170\n",
      "Input 0 : 170 mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]/197 onnx::Clip model.10.2\n",
      "56\n",
      "Input 1 : 56 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]/67 Parameter model.11.0\n",
      "************\n",
      "172 172 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/199 onnx::BatchNormalization\n",
      "171\n",
      "Input 0 : 171 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]/198 onnx::Conv model.11.0\n",
      "57\n",
      "Input 1 : 57 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/68 Parameter model.11.1\n",
      "58\n",
      "Input 2 : 58 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/69 Parameter model.11.1\n",
      "59\n",
      "Input 3 : 59 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/70 Parameter model.11.1\n",
      "60\n",
      "Input 4 : 60 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/71 Parameter model.11.1\n",
      "************\n",
      "173 173 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]/200 onnx::Clip\n",
      "172\n",
      "Input 0 : 172 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/199 onnx::BatchNormalization model.11.1\n",
      "************\n",
      "174 174 Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]/201 onnx::Conv\n",
      "173\n",
      "Input 0 : 173 mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]/200 onnx::Clip model.11.2\n",
      "61\n",
      "Input 1 : 61 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]/73 Parameter model.12.0\n",
      "************\n",
      "175 175 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/202 onnx::BatchNormalization\n",
      "174\n",
      "Input 0 : 174 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]/201 onnx::Conv model.12.0\n",
      "62\n",
      "Input 1 : 62 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/74 Parameter model.12.1\n",
      "63\n",
      "Input 2 : 63 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/75 Parameter model.12.1\n",
      "64\n",
      "Input 3 : 64 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/76 Parameter model.12.1\n",
      "65\n",
      "Input 4 : 65 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/77 Parameter model.12.1\n",
      "************\n",
      "176 176 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]/203 onnx::Clip\n",
      "175\n",
      "Input 0 : 175 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/202 onnx::BatchNormalization model.12.1\n",
      "************\n",
      "177 177 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]/204 onnx::Conv\n",
      "176\n",
      "Input 0 : 176 mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]/203 onnx::Clip model.12.2\n",
      "66\n",
      "Input 1 : 66 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]/79 Parameter model.13.0\n",
      "************\n",
      "178 178 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/205 onnx::BatchNormalization\n",
      "177\n",
      "Input 0 : 177 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]/204 onnx::Conv model.13.0\n",
      "67\n",
      "Input 1 : 67 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/80 Parameter model.13.1\n",
      "68\n",
      "Input 2 : 68 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/81 Parameter model.13.1\n",
      "69\n",
      "Input 3 : 69 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/82 Parameter model.13.1\n",
      "70\n",
      "Input 4 : 70 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/83 Parameter model.13.1\n",
      "************\n",
      "179 179 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]/206 onnx::Clip\n",
      "178\n",
      "Input 0 : 178 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/205 onnx::BatchNormalization model.13.1\n",
      "************\n",
      "180 180 Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]/207 onnx::Conv\n",
      "179\n",
      "Input 0 : 179 mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]/206 onnx::Clip model.13.2\n",
      "71\n",
      "Input 1 : 71 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]/85 Parameter model.14.0\n",
      "************\n",
      "181 181 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/208 onnx::BatchNormalization\n",
      "180\n",
      "Input 0 : 180 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]/207 onnx::Conv model.14.0\n",
      "72\n",
      "Input 1 : 72 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/86 Parameter model.14.1\n",
      "73\n",
      "Input 2 : 73 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/87 Parameter model.14.1\n",
      "74\n",
      "Input 3 : 74 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/88 Parameter model.14.1\n",
      "75\n",
      "Input 4 : 75 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/89 Parameter model.14.1\n",
      "************\n",
      "182 182 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]/209 onnx::Clip\n",
      "181\n",
      "Input 0 : 181 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/208 onnx::BatchNormalization model.14.1\n",
      "************\n",
      "183 183 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]/210 onnx::Conv\n",
      "182\n",
      "Input 0 : 182 mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]/209 onnx::Clip model.14.2\n",
      "76\n",
      "Input 1 : 76 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]/91 Parameter model.15.0\n",
      "************\n",
      "184 184 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/211 onnx::BatchNormalization\n",
      "183\n",
      "Input 0 : 183 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]/210 onnx::Conv model.15.0\n",
      "77\n",
      "Input 1 : 77 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/92 Parameter model.15.1\n",
      "78\n",
      "Input 2 : 78 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/93 Parameter model.15.1\n",
      "79\n",
      "Input 3 : 79 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/94 Parameter model.15.1\n",
      "80\n",
      "Input 4 : 80 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/95 Parameter model.15.1\n",
      "************\n",
      "185 185 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]/212 onnx::Clip\n",
      "184\n",
      "Input 0 : 184 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/211 onnx::BatchNormalization model.15.1\n",
      "************\n",
      "186 186 Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]/213 onnx::Conv\n",
      "185\n",
      "Input 0 : 185 mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]/212 onnx::Clip model.15.2\n",
      "81\n",
      "Input 1 : 81 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]/97 Parameter model.16.0\n",
      "************\n",
      "187 187 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/214 onnx::BatchNormalization\n",
      "186\n",
      "Input 0 : 186 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]/213 onnx::Conv model.16.0\n",
      "82\n",
      "Input 1 : 82 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/98 Parameter model.16.1\n",
      "83\n",
      "Input 2 : 83 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/99 Parameter model.16.1\n",
      "84\n",
      "Input 3 : 84 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/100 Parameter model.16.1\n",
      "85\n",
      "Input 4 : 85 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/101 Parameter model.16.1\n",
      "************\n",
      "188 188 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]/215 onnx::Clip\n",
      "187\n",
      "Input 0 : 187 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/214 onnx::BatchNormalization model.16.1\n",
      "************\n",
      "189 189 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]/216 onnx::Conv\n",
      "188\n",
      "Input 0 : 188 mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]/215 onnx::Clip model.16.2\n",
      "86\n",
      "Input 1 : 86 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]/103 Parameter model.17.0\n",
      "************\n",
      "190 190 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/217 onnx::BatchNormalization\n",
      "189\n",
      "Input 0 : 189 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]/216 onnx::Conv model.17.0\n",
      "87\n",
      "Input 1 : 87 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/104 Parameter model.17.1\n",
      "88\n",
      "Input 2 : 88 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/105 Parameter model.17.1\n",
      "89\n",
      "Input 3 : 89 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/106 Parameter model.17.1\n",
      "90\n",
      "Input 4 : 90 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/107 Parameter model.17.1\n",
      "************\n",
      "191 191 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]/218 onnx::Clip\n",
      "190\n",
      "Input 0 : 190 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/217 onnx::BatchNormalization model.17.1\n",
      "************\n",
      "192 192 Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]/219 onnx::Conv\n",
      "191\n",
      "Input 0 : 191 mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]/218 onnx::Clip model.17.2\n",
      "91\n",
      "Input 1 : 91 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]/109 Parameter model.18.0\n",
      "************\n",
      "193 193 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/220 onnx::BatchNormalization\n",
      "192\n",
      "Input 0 : 192 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]/219 onnx::Conv model.18.0\n",
      "92\n",
      "Input 1 : 92 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/110 Parameter model.18.1\n",
      "93\n",
      "Input 2 : 93 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/111 Parameter model.18.1\n",
      "94\n",
      "Input 3 : 94 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/112 Parameter model.18.1\n",
      "95\n",
      "Input 4 : 95 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/113 Parameter model.18.1\n",
      "************\n",
      "194 194 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]/221 onnx::Clip\n",
      "193\n",
      "Input 0 : 193 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/220 onnx::BatchNormalization model.18.1\n",
      "************\n",
      "195 195 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]/222 onnx::Conv\n",
      "194\n",
      "Input 0 : 194 mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]/221 onnx::Clip model.18.2\n",
      "96\n",
      "Input 1 : 96 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]/115 Parameter model.19.0\n",
      "************\n",
      "196 196 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/223 onnx::BatchNormalization\n",
      "195\n",
      "Input 0 : 195 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]/222 onnx::Conv model.19.0\n",
      "97\n",
      "Input 1 : 97 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/116 Parameter model.19.1\n",
      "98\n",
      "Input 2 : 98 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/117 Parameter model.19.1\n",
      "99\n",
      "Input 3 : 99 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/118 Parameter model.19.1\n",
      "100\n",
      "Input 4 : 100 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/119 Parameter model.19.1\n",
      "************\n",
      "197 197 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]/224 onnx::Clip\n",
      "196\n",
      "Input 0 : 196 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/223 onnx::BatchNormalization model.19.1\n",
      "************\n",
      "198 198 Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]/225 onnx::Conv\n",
      "197\n",
      "Input 0 : 197 mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]/224 onnx::Clip model.19.2\n",
      "101\n",
      "Input 1 : 101 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]/121 Parameter model.20.0\n",
      "************\n",
      "199 199 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/226 onnx::BatchNormalization\n",
      "198\n",
      "Input 0 : 198 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]/225 onnx::Conv model.20.0\n",
      "102\n",
      "Input 1 : 102 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/122 Parameter model.20.1\n",
      "103\n",
      "Input 2 : 103 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/123 Parameter model.20.1\n",
      "104\n",
      "Input 3 : 104 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/124 Parameter model.20.1\n",
      "105\n",
      "Input 4 : 105 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/125 Parameter model.20.1\n",
      "************\n",
      "200 200 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]/227 onnx::Clip\n",
      "199\n",
      "Input 0 : 199 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/226 onnx::BatchNormalization model.20.1\n",
      "************\n",
      "201 201 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]/228 onnx::Conv\n",
      "200\n",
      "Input 0 : 200 mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]/227 onnx::Clip model.20.2\n",
      "106\n",
      "Input 1 : 106 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]/127 Parameter model.21.0\n",
      "************\n",
      "202 202 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/229 onnx::BatchNormalization\n",
      "201\n",
      "Input 0 : 201 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]/228 onnx::Conv model.21.0\n",
      "107\n",
      "Input 1 : 107 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/128 Parameter model.21.1\n",
      "108\n",
      "Input 2 : 108 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/129 Parameter model.21.1\n",
      "109\n",
      "Input 3 : 109 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/130 Parameter model.21.1\n",
      "110\n",
      "Input 4 : 110 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/131 Parameter model.21.1\n",
      "************\n",
      "203 203 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]/230 onnx::Clip\n",
      "202\n",
      "Input 0 : 202 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/229 onnx::BatchNormalization model.21.1\n",
      "************\n",
      "204 204 Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]/231 onnx::Conv\n",
      "203\n",
      "Input 0 : 203 mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]/230 onnx::Clip model.21.2\n",
      "111\n",
      "Input 1 : 111 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]/133 Parameter model.22.0\n",
      "************\n",
      "205 205 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/232 onnx::BatchNormalization\n",
      "204\n",
      "Input 0 : 204 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]/231 onnx::Conv model.22.0\n",
      "112\n",
      "Input 1 : 112 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/134 Parameter model.22.1\n",
      "113\n",
      "Input 2 : 113 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/135 Parameter model.22.1\n",
      "114\n",
      "Input 3 : 114 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/136 Parameter model.22.1\n",
      "115\n",
      "Input 4 : 115 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/137 Parameter model.22.1\n",
      "************\n",
      "206 206 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]/233 onnx::Clip\n",
      "205\n",
      "Input 0 : 205 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/232 onnx::BatchNormalization model.22.1\n",
      "************\n",
      "207 207 Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]/234 onnx::Conv\n",
      "206\n",
      "Input 0 : 206 mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]/233 onnx::Clip model.22.2\n",
      "116\n",
      "Input 1 : 116 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]/139 Parameter model.23.0\n",
      "************\n",
      "208 208 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/235 onnx::BatchNormalization\n",
      "207\n",
      "Input 0 : 207 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]/234 onnx::Conv model.23.0\n",
      "117\n",
      "Input 1 : 117 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/140 Parameter model.23.1\n",
      "118\n",
      "Input 2 : 118 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/141 Parameter model.23.1\n",
      "119\n",
      "Input 3 : 119 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/142 Parameter model.23.1\n",
      "120\n",
      "Input 4 : 120 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/143 Parameter model.23.1\n",
      "************\n",
      "209 209 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]/236 onnx::Clip\n",
      "208\n",
      "Input 0 : 208 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/235 onnx::BatchNormalization model.23.1\n",
      "************\n",
      "210 210 Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]/237 onnx::Conv\n",
      "209\n",
      "Input 0 : 209 mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]/236 onnx::Clip model.23.2\n",
      "121\n",
      "Input 1 : 121 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]/145 Parameter model.24.0\n",
      "************\n",
      "211 211 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/238 onnx::BatchNormalization\n",
      "210\n",
      "Input 0 : 210 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]/237 onnx::Conv model.24.0\n",
      "122\n",
      "Input 1 : 122 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/146 Parameter model.24.1\n",
      "123\n",
      "Input 2 : 123 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/147 Parameter model.24.1\n",
      "124\n",
      "Input 3 : 124 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/148 Parameter model.24.1\n",
      "125\n",
      "Input 4 : 125 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/149 Parameter model.24.1\n",
      "************\n",
      "212 212 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]/239 onnx::Clip\n",
      "211\n",
      "Input 0 : 211 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/238 onnx::BatchNormalization model.24.1\n",
      "************\n",
      "213 213 Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]/240 onnx::Conv\n",
      "212\n",
      "Input 0 : 212 mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]/239 onnx::Clip model.24.2\n",
      "126\n",
      "Input 1 : 126 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]/151 Parameter model.25.0\n",
      "************\n",
      "214 214 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/241 onnx::BatchNormalization\n",
      "213\n",
      "Input 0 : 213 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]/240 onnx::Conv model.25.0\n",
      "127\n",
      "Input 1 : 127 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/152 Parameter model.25.1\n",
      "128\n",
      "Input 2 : 128 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/153 Parameter model.25.1\n",
      "129\n",
      "Input 3 : 129 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/154 Parameter model.25.1\n",
      "130\n",
      "Input 4 : 130 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/155 Parameter model.25.1\n",
      "************\n",
      "215 215 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]/242 onnx::Clip\n",
      "214\n",
      "Input 0 : 214 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/241 onnx::BatchNormalization model.25.1\n",
      "************\n",
      "216 216 Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]/243 onnx::Conv\n",
      "215\n",
      "Input 0 : 215 mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]/242 onnx::Clip model.25.2\n",
      "131\n",
      "Input 1 : 131 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]/157 Parameter model.26.0\n",
      "************\n",
      "217 217 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/244 onnx::BatchNormalization\n",
      "216\n",
      "Input 0 : 216 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]/243 onnx::Conv model.26.0\n",
      "132\n",
      "Input 1 : 132 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/158 Parameter model.26.1\n",
      "133\n",
      "Input 2 : 133 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/159 Parameter model.26.1\n",
      "134\n",
      "Input 3 : 134 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/160 Parameter model.26.1\n",
      "135\n",
      "Input 4 : 135 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/161 Parameter model.26.1\n",
      "************\n",
      "218 218 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]/245 onnx::Clip\n",
      "217\n",
      "Input 0 : 217 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/244 onnx::BatchNormalization model.26.1\n",
      "************\n",
      "220 220 AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "mobilenet_real/Sequential[model]/AvgPool2d[27]/247 onnx::AveragePool\n",
      "219\n",
      "Input 0 : 219 mobilenet_real/Sequential[model]/AvgPool2d[27]/246 onnx::Pad model.27\n",
      "************\n",
      "222 222 mobilenet_real(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n",
      "mobilenet_real/249 onnx::Reshape\n",
      "220\n",
      "Input 0 : 220 mobilenet_real/Sequential[model]/AvgPool2d[27]/247 onnx::AveragePool model.27\n",
      "221\n",
      "Input 1 : 221 mobilenet_real/248 onnx::Constant \n",
      "************\n",
      "224 224 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Linear[fc]/251 onnx::Gemm\n",
      "222\n",
      "Input 0 : 222 mobilenet_real/249 onnx::Reshape \n",
      "223\n",
      "Input 1 : 223 mobilenet_real/Linear[fc]/250 onnx::Transpose fc\n",
      "137\n",
      "Input 2 : 137 mobilenet_real/Linear[fc]/164 Parameter fc\n"
     ]
    }
   ],
   "source": [
    "m = OrderedDict(model.named_modules())\n",
    "\n",
    "for i,node in enumerate(g.node):\n",
    "    if node.op in _IO_LIST:\n",
    "        print('************')\n",
    "        print(node.name)\n",
    "        print(node.op)\n",
    "    if node.op in _OP_LIST:\n",
    "        print('************')\n",
    "        v = get_model_name(node.name)\n",
    "        print(i,get_model_id(node.name,g), m[v])\n",
    "        print(node.name,node.op)\n",
    "        for j, x in enumerate(node.input):\n",
    "            id = get_model_id(x,g)\n",
    "            print('Input',j,':',id, x, g.node[id].op, get_model_name(x))\n",
    "            if g.node[id].op in _IO_LIST:\n",
    "            elif g.node[id].op in _OP_LIST:\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new Quantop Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************\n",
      "input/0\n",
      "IO Node\n",
      "************\n",
      "138 138 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/165 onnx::Conv\n",
      "************\n",
      "139 139 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/166 onnx::BatchNormalization\n",
      "************\n",
      "140 140 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]/167 onnx::Clip\n",
      "************\n",
      "141 141 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]/168 onnx::Conv\n",
      "************\n",
      "142 142 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/169 onnx::BatchNormalization\n",
      "************\n",
      "143 143 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]/170 onnx::Clip\n",
      "************\n",
      "144 144 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]/171 onnx::Conv\n",
      "************\n",
      "145 145 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/172 onnx::BatchNormalization\n",
      "************\n",
      "146 146 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]/173 onnx::Clip\n",
      "************\n",
      "147 147 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]/174 onnx::Conv\n",
      "************\n",
      "148 148 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/175 onnx::BatchNormalization\n",
      "************\n",
      "149 149 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]/176 onnx::Clip\n",
      "************\n",
      "150 150 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]/177 onnx::Conv\n",
      "************\n",
      "151 151 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/178 onnx::BatchNormalization\n",
      "************\n",
      "152 152 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]/179 onnx::Clip\n",
      "************\n",
      "153 153 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]/180 onnx::Conv\n",
      "************\n",
      "154 154 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/181 onnx::BatchNormalization\n",
      "************\n",
      "155 155 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]/182 onnx::Clip\n",
      "************\n",
      "156 156 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]/183 onnx::Conv\n",
      "************\n",
      "157 157 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/184 onnx::BatchNormalization\n",
      "************\n",
      "158 158 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]/185 onnx::Clip\n",
      "************\n",
      "159 159 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]/186 onnx::Conv\n",
      "************\n",
      "160 160 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/187 onnx::BatchNormalization\n",
      "************\n",
      "161 161 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]/188 onnx::Clip\n",
      "************\n",
      "162 162 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]/189 onnx::Conv\n",
      "************\n",
      "163 163 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/190 onnx::BatchNormalization\n",
      "************\n",
      "164 164 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]/191 onnx::Clip\n",
      "************\n",
      "165 165 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]/192 onnx::Conv\n",
      "************\n",
      "166 166 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/193 onnx::BatchNormalization\n",
      "************\n",
      "167 167 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]/194 onnx::Clip\n",
      "************\n",
      "168 168 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]/195 onnx::Conv\n",
      "************\n",
      "169 169 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/196 onnx::BatchNormalization\n",
      "************\n",
      "170 170 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]/197 onnx::Clip\n",
      "************\n",
      "171 171 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]/198 onnx::Conv\n",
      "************\n",
      "172 172 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/199 onnx::BatchNormalization\n",
      "************\n",
      "173 173 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]/200 onnx::Clip\n",
      "************\n",
      "174 174 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]/201 onnx::Conv\n",
      "************\n",
      "175 175 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/202 onnx::BatchNormalization\n",
      "************\n",
      "176 176 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]/203 onnx::Clip\n",
      "************\n",
      "177 177 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]/204 onnx::Conv\n",
      "************\n",
      "178 178 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/205 onnx::BatchNormalization\n",
      "************\n",
      "179 179 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]/206 onnx::Clip\n",
      "************\n",
      "180 180 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]/207 onnx::Conv\n",
      "************\n",
      "181 181 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/208 onnx::BatchNormalization\n",
      "************\n",
      "182 182 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]/209 onnx::Clip\n",
      "************\n",
      "183 183 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]/210 onnx::Conv\n",
      "************\n",
      "184 184 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/211 onnx::BatchNormalization\n",
      "************\n",
      "185 185 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]/212 onnx::Clip\n",
      "************\n",
      "186 186 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]/213 onnx::Conv\n",
      "************\n",
      "187 187 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/214 onnx::BatchNormalization\n",
      "************\n",
      "188 188 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]/215 onnx::Clip\n",
      "************\n",
      "189 189 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]/216 onnx::Conv\n",
      "************\n",
      "190 190 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/217 onnx::BatchNormalization\n",
      "************\n",
      "191 191 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]/218 onnx::Clip\n",
      "************\n",
      "192 192 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]/219 onnx::Conv\n",
      "************\n",
      "193 193 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/220 onnx::BatchNormalization\n",
      "************\n",
      "194 194 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]/221 onnx::Clip\n",
      "************\n",
      "195 195 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]/222 onnx::Conv\n",
      "************\n",
      "196 196 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/223 onnx::BatchNormalization\n",
      "************\n",
      "197 197 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]/224 onnx::Clip\n",
      "************\n",
      "198 198 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]/225 onnx::Conv\n",
      "************\n",
      "199 199 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/226 onnx::BatchNormalization\n",
      "************\n",
      "200 200 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]/227 onnx::Clip\n",
      "************\n",
      "201 201 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]/228 onnx::Conv\n",
      "************\n",
      "202 202 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/229 onnx::BatchNormalization\n",
      "************\n",
      "203 203 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]/230 onnx::Clip\n",
      "************\n",
      "204 204 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]/231 onnx::Conv\n",
      "************\n",
      "205 205 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/232 onnx::BatchNormalization\n",
      "************\n",
      "206 206 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]/233 onnx::Clip\n",
      "************\n",
      "207 207 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]/234 onnx::Conv\n",
      "************\n",
      "208 208 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/235 onnx::BatchNormalization\n",
      "************\n",
      "209 209 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]/236 onnx::Clip\n",
      "************\n",
      "210 210 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]/237 onnx::Conv\n",
      "************\n",
      "211 211 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/238 onnx::BatchNormalization\n",
      "************\n",
      "212 212 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]/239 onnx::Clip\n",
      "************\n",
      "213 213 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]/240 onnx::Conv\n",
      "************\n",
      "214 214 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/241 onnx::BatchNormalization\n",
      "************\n",
      "215 215 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]/242 onnx::Clip\n",
      "************\n",
      "216 216 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]/243 onnx::Conv\n",
      "************\n",
      "217 217 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/244 onnx::BatchNormalization\n",
      "************\n",
      "218 218 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]/245 onnx::Clip\n",
      "************\n",
      "220 220 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/AvgPool2d[27]/247 onnx::AveragePool\n",
      "************\n",
      "222 222 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/249 onnx::Reshape\n",
      "Else name =  \n",
      "************\n",
      "224 224 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Linear[fc]/251 onnx::Gemm\n",
      "This is the quantized network: \n",
      "mobilenet_real(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n",
      "***** Layer 0 **********\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "138\n",
      "Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 1 **********\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "141\n",
      "Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 2 **********\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "144\n",
      "Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 3 **********\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "147\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 4 **********\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "150\n",
      "Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 5 **********\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "153\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 6 **********\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "156\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 7 **********\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "159\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 8 **********\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "162\n",
      "Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 9 **********\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "165\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 10 **********\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "168\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 11 **********\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "171\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 12 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "174\n",
      "Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 13 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "177\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 14 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "180\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 15 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "183\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 16 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "186\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 17 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "189\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 18 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "192\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 19 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "195\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 20 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "198\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 21 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "201\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 22 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "204\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 23 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "207\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 24 **********\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "210\n",
      "Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 25 **********\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "213\n",
      "Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 26 **********\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "216\n",
      "Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 27 **********\n",
      "None\n",
      "224\n",
      "Linear(in_features=1024, out_features=1000, bias=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "m = OrderedDict(model.named_modules())\n",
    "_OP_LIST = ['onnx::Gemm','onnx::BatchNormalization','onnx::Conv','onnx::Clip','onnx::AveragePool','onnx::Reshape']\n",
    "_IO_LIST = ['IO Node']\n",
    "\n",
    "quant_type =  'PerLayerAsymPACT'\n",
    "weight_bits= 8\n",
    "bias_bits = 8\n",
    "batch_fold_type = 'folding_weights', \n",
    "batch_fold_delay = 0 \n",
    "act_bits = 8\n",
    "add_config = [] \n",
    "\n",
    "\n",
    "# generate quantized model\n",
    "deployment_model = copy.deepcopy(model)\n",
    "param_to_quantize = []\n",
    "batch_fold = False\n",
    "\n",
    "\n",
    "def has_children(module):\n",
    "    try:\n",
    "        next(module.children())\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "\n",
    "\n",
    "        \n",
    "def find_conv_node(node):\n",
    "    ret = 0\n",
    "    for j, x in enumerate(node.input):\n",
    "        id = get_model_id(x,g)\n",
    "        n = g.node[id]\n",
    "        if n.op in _IO_LIST:\n",
    "            pass\n",
    "        elif n.op in _OP_LIST:\n",
    "            if n.op in ['onnx::Gemm','onnx::Conv']:\n",
    "                ret = id\n",
    "            else:\n",
    "                ret = find_conv_node(n)\n",
    "            \n",
    "    return ret\n",
    "\n",
    "last_layer = None\n",
    "modules_quant = [ ]           \n",
    "\n",
    "replace_graph = {}\n",
    "###################################################\n",
    "##### Mapping the graph ###########################\n",
    "###################################################\n",
    "for i,node in enumerate(g.node):\n",
    "    \n",
    "    if node.op in _IO_LIST:\n",
    "        print('************')\n",
    "        print(node.name)\n",
    "        print(node.op)\n",
    "        \n",
    "    if node.op in _OP_LIST:\n",
    "        print('************')\n",
    "        name = get_model_name(node.name)\n",
    "        module = m[name]\n",
    "        idx = get_model_id(node.name,g)\n",
    "        print(i, idx, m[v])\n",
    "        print(node.name,node.op)\n",
    "        \n",
    "        if node.op in ['onnx::Gemm','onnx::Conv']:\n",
    "            layer_quant_descr = {}\n",
    "            # saving per-node characteristics\n",
    "            layer_quant_descr['w_bits'] = weight_bits\n",
    "            layer_quant_descr['fold_type'] = batch_fold_type\n",
    "\n",
    "            temp = getattr(submodel, 'quant_type', quant_type)\n",
    "            if temp in _Availble_Quant_Type :\n",
    "                layer_quant_descr['quant_type'] = temp\n",
    "            else:\n",
    "                print('Type of quantization not recognized')\n",
    "\n",
    "            layer_quant_descr['conv'] = module\n",
    "            layer_quant_descr['weight'] = module.weight.data.clone()\n",
    "            layer_quant_descr['w_clip'] = None\n",
    "            layer_quant_descr['id'] = idx\n",
    "            layer_quant_descr['leaf'] = 0\n",
    "\n",
    "            # import per-layer config from external config file\n",
    "            idx_layer = len(param_to_quantize)\n",
    "            for item_dict in add_config:\n",
    "                if item_dict['layer'] == idx_layer:\n",
    "                    if 'w_bits' in item_dict.keys():\n",
    "                        layer_quant_descr['w_bits'] = item_dict['w_bits'] \n",
    "\n",
    "                    if 'fold_type' in item_dict.keys():\n",
    "                        layer_quant_descr['fold_type'] = item_dict['fold_type'] \n",
    "\n",
    "                    if 'quant_type' in item_dict.keys():\n",
    "                        layer_quant_descr['quant_type'] = item_dict['quant_type'] \n",
    "\n",
    "            if module.bias is None:\n",
    "                layer_quant_descr['bias'] = False\n",
    "            else:\n",
    "                layer_quant_descr['bias'] = module.bias.data.clone()\n",
    "            layer_quant_descr['bias_bits'] = bias_bits\n",
    "            layer_quant_descr['batch_norm'] = None\n",
    "            layer_quant_descr['act'] = None\n",
    "            layer_quant_descr['quant_act'] = None\n",
    "\n",
    "            # append into deployment graph\n",
    "            quant_layer = copy.deepcopy(module)\n",
    "            modules_quant.append(quant_layer)\n",
    "            layer_quant_descr['quant_conv'] = quant_layer\n",
    "\n",
    "            #PACT needs extra parameters for learning quantization range\n",
    "            if layer_quant_descr['quant_type'] == 'PerLayerAsymPACT':\n",
    "                layer_quant_descr['w_max_thr'] = nn.Parameter( module.weight.data.max().cuda(),requires_grad=True )\n",
    "                layer_quant_descr['w_min_thr'] = nn.Parameter( module.weight.data.min().cuda(),requires_grad=True )\n",
    "                layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "                layer_quant_descr['w_min_thr'].add(1).sum().backward() #fake to create grad\n",
    "\n",
    "            elif layer_quant_descr['quant_type'] == 'PerLayerSymPACT':\n",
    "                layer_quant_descr['w_max_thr'] = nn.Parameter( module.weight.data.max().cuda(),requires_grad=True )\n",
    "                layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "\n",
    "            elif layer_quant_descr['quant_type'] == 'PerChannelsAsymMinMax':\n",
    "                weight = module.weight.data\n",
    "                max_weight,_ = weight.reshape(weight.size(0), -1).max(1)\n",
    "                min_weight,_ = weight.reshape(weight.size(0), -1).min(1)\n",
    "                layer_quant_descr['w_max_thr'] = nn.Parameter( max_weight.cuda(),requires_grad=True )\n",
    "                layer_quant_descr['w_min_thr'] = nn.Parameter( min_weight.cuda(),requires_grad=True )\n",
    "                layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "                layer_quant_descr['w_min_thr'].add(1).sum().backward() #fake to create grad\n",
    "                layer_quant_descr['w_min_mat'] = None\n",
    "                layer_quant_descr['w_max_mat'] = None\n",
    "                if type(module) == nn.Linear:\n",
    "                    print('PerLayerAsymPACT chennel with fixed batch')\n",
    "                    out_ch = weight.size(0)\n",
    "                    quant_act = ScaledClippedLinearQuantizationChannelBias(out_ch, clip_val=False, bias=False)\n",
    "                    layer_quant_descr['quant_act'] = quant_act\n",
    "                    modules_quant.append(quant_act)\n",
    "\n",
    "            # append the latest node\n",
    "            param_to_quantize.append(layer_quant_descr)\n",
    "            replace_graph[name] = [module, module]\n",
    "\n",
    "        elif node.op in ['onnx::BatchNormalization']:\n",
    "            id_layer = find_conv_node(node)\n",
    "            for item in param_to_quantize:\n",
    "                if item['id'] == id_layer:\n",
    "                    item['batch_norm'] = module\n",
    "                    break\n",
    "                    \n",
    "            replace_graph[name] = [module, None]\n",
    "\n",
    "        elif node.op in ['onnx::AveragePool']:\n",
    "            modules_quant.append(module) # temporary - this should me merged into or previous layer\n",
    "            replace_graph[name] = [module, module]\n",
    "\n",
    "        elif node.op in ['onnx::Clip']:\n",
    "            \n",
    "            id_layer = find_conv_node(node)\n",
    "            for item in param_to_quantize:\n",
    "                if item['id'] == id_layer:\n",
    "                    last_layer = item\n",
    "                    break\n",
    "\n",
    "            last_layer['act'] = module\n",
    "\n",
    "            # quantized activations\n",
    "            if type(module) in [ClippedLinearQuantization, LearnedClippedLinearQuantization, QuantActive, nn.ReLU6]:\n",
    "\n",
    "                act_bits = 8\n",
    "                #check if number of bits need to be changed\n",
    "                for item_dict in add_config:\n",
    "                    if item_dict['layer'] == idx_layer:\n",
    "                        if 'a_bits' in item_dict.keys():\n",
    "                            act_bits = item_dict['a_bits'] \n",
    "                            module.num_bits = act_bits                                 \n",
    "\n",
    "                if last_layer['fold_type'] == 'folding_thresh':\n",
    "                    quant_act = ScaledThresholdsQuantization4d()\n",
    "                    quant_act.num_bits = act_bits\n",
    "                    quant_act.n_thresholds = 2**act_bits -1\n",
    "\n",
    "                elif last_layer['fold_type'] == 'ICN':\n",
    "                    out_ch = last_layer['conv'].out_channels\n",
    "                    quant_act = ScaledClippedLinearQuantizationChannel(out_ch,clip_val=2**act_bits -1)\n",
    "\n",
    "                else:\n",
    "                    quant_act = ScaledClippedLinearQuantization(clip_val=2**act_bits -1)\n",
    "\n",
    "                last_layer['act_o_bits'] = act_bits\n",
    "\n",
    "            else:\n",
    "                print('Supported activation layer but no method is here yet!')\n",
    "\n",
    "\n",
    "            replace_graph[name] = [LearnedClippedLinearQuantization(init_act_clip_val=6,num_bits=act_bits), quant_act]\n",
    "\n",
    "            last_layer['quant_act'] = quant_act\n",
    "\n",
    "            modules_quant.append(quant_act)\n",
    "            \n",
    "        else:\n",
    "            print('Else name = ', name)\n",
    "            replace_graph[name] = [module, module]\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "n_quantize_layers = len(param_to_quantize)\n",
    "\n",
    "print('This is the quantized network: ')\n",
    "print(deployment_model)\n",
    "for i,item in enumerate(param_to_quantize):\n",
    "    print('***** Layer', i,'**********')\n",
    "    print(item['batch_norm'])\n",
    "    print(item['id'])\n",
    "    print(item['quant_conv'])\n",
    "    print(item['quant_act'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': [mobilenet_real(\n",
       "    (model): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (11): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (12): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (13): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (14): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (15): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (16): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (17): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (18): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (19): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (20): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (21): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (22): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (23): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (24): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (25): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (26): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  ), mobilenet_real(\n",
       "    (model): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (11): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (12): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (13): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (14): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (15): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (16): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (17): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (18): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (19): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (20): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (21): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (22): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (23): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (24): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (25): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (26): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )],\n",
       " 'fc': [Linear(in_features=1024, out_features=1000, bias=True),\n",
       "  Linear(in_features=1024, out_features=1000, bias=True)],\n",
       " 'model.0.0': [Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
       "  Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)],\n",
       " 'model.0.1': [BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.0.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.1.0': [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False),\n",
       "  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)],\n",
       " 'model.1.1': [BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.1.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.10.0': [Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.10.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.10.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.11.0': [Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False),\n",
       "  Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)],\n",
       " 'model.11.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.11.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.12.0': [Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.12.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.12.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.13.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.13.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.13.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.14.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.14.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.14.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.15.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.15.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.15.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.16.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.16.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.16.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.17.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.17.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.17.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.18.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.18.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.18.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.19.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.19.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.19.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.2.0': [Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.2.1': [BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.2.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.20.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.20.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.20.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.21.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.21.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.21.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.22.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.22.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.22.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.23.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.23.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.23.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.24.0': [Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.24.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.24.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.25.0': [Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False),\n",
       "  Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)],\n",
       " 'model.25.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.25.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.26.0': [Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.26.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.26.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.27': [AvgPool2d(kernel_size=7, stride=7, padding=0),\n",
       "  AvgPool2d(kernel_size=7, stride=7, padding=0)],\n",
       " 'model.3.0': [Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False),\n",
       "  Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)],\n",
       " 'model.3.1': [BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.3.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.4.0': [Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.4.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.4.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.5.0': [Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False),\n",
       "  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)],\n",
       " 'model.5.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.5.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.6.0': [Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.6.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.6.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.7.0': [Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False),\n",
       "  Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)],\n",
       " 'model.7.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.7.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.8.0': [Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.8.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.8.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.9.0': [Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False),\n",
       "  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)],\n",
       " 'model.9.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.9.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_children(module):\n",
    "    try:\n",
    "        next(module.children())\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-582fb400180b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_sub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model.0'"
     ]
    }
   ],
   "source": [
    "fake_model = model\n",
    "deployment_model = copy.deepcopy(model)\n",
    "\n",
    "for name_sub,submodel in model.named_children():\n",
    "    # map the sub-graph \n",
    "    modules_quant = [] \n",
    "    modules_fake = []\n",
    "    for name, module in submodel.named_modules():\n",
    "        if has_children(module) is False:\n",
    "            if name is not '':\n",
    "                n = name_sub+'.'+name\n",
    "            else:\n",
    "                n = name_sub\n",
    "            v = replace_graph[n]\n",
    "            fake = v[0]\n",
    "            other = v[1]\n",
    "            if fake is not None:\n",
    "                modules_fake.append(fake)\n",
    "            \n",
    "            if other is not None:\n",
    "                modules_quant.append(other)\n",
    "            \n",
    "    \n",
    "    fake_model._modules[name_sub] = nn.Sequential(*modules_fake)\n",
    "    deployment_model._modules[name_sub] = nn.Sequential(*modules_quant)\n",
    "\n",
    "\n",
    "print(deployment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model.0.0': [Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
       "   Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)]},\n",
       " {'model.0.1': [BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.0.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.1.0': [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False),\n",
       "   Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)]},\n",
       " {'model.1.1': [BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.1.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.2.0': [Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.2.1': [BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.2.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.3.0': [Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False),\n",
       "   Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)]},\n",
       " {'model.3.1': [BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.3.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.4.0': [Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.4.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.4.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.5.0': [Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False),\n",
       "   Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)]},\n",
       " {'model.5.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.5.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.6.0': [Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.6.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.6.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.7.0': [Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False),\n",
       "   Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)]},\n",
       " {'model.7.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.7.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.8.0': [Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.8.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.8.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.9.0': [Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False),\n",
       "   Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)]},\n",
       " {'model.9.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.9.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.10.0': [Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.10.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.10.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.11.0': [Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False),\n",
       "   Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)]},\n",
       " {'model.11.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.11.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.12.0': [Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.12.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.12.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.13.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.13.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.13.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.14.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.14.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.14.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.15.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.15.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.15.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.16.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.16.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.16.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.17.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.17.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.17.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.18.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.18.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.18.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.19.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.19.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.19.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.20.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.20.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.20.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.21.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.21.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.21.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.22.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.22.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.22.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.23.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.23.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.23.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.24.0': [Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.24.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.24.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.25.0': [Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False),\n",
       "   Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)]},\n",
       " {'model.25.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.25.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.26.0': [Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.26.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.26.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.27': [AvgPool2d(kernel_size=7, stride=7, padding=0),\n",
       "   AvgPool2d(kernel_size=7, stride=7, padding=0)]},\n",
       " {'fc': [Linear(in_features=1024, out_features=1000, bias=True),\n",
       "   Linear(in_features=1024, out_features=1000, bias=True)]}]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "Linear(in_features=1024, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for name_sub,submodel in model.named_children():\n",
    "    # map the sub-graph \n",
    "    for name, module in submodel.named_modules():\n",
    "        if has_children(module) is False:\n",
    "            print(module)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type object argument after * must be an iterable, not mobilenet_real",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-ab779122673a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: type object argument after * must be an iterable, not mobilenet_real"
     ]
    }
   ],
   "source": [
    "list(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Folding:  False Batch Folding Delay:  0 Type ('folding_weights',)\n",
      "[]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'has_children' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-369a4149dbc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# map the sub-graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhas_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mlayer_quant_descr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'has_children' is not defined"
     ]
    }
   ],
   "source": [
    "from models.quantized_modules import ABC_MxNBinConv, ABC_binarize, Quantize, QuantActive, LinQuantActive, BinActive\n",
    "from models.linear_quantized_modules import ClippedLinearQuantization, LearnedClippedLinearQuantization, ScaledClippedLinearQuantization, ScaledThresholdsQuantization4d, \\\n",
    "                                            Conv2d_SAME, ScaledClippedLinearQuantizationChannel, ScaledClippedLinearQuantizationChannelBias\n",
    "\n",
    "_Availble_Quant_Type = ['BNN', 'XNORNET', 'PerLayerAsymMinMax', 'PerLayerSymPACT', 'PerLayerAsymPACT', 'PerChannelsAsymMinMax', None]\n",
    "_Supported_Activ_Funct = [nn.ReLU6, nn.ReLU, ClippedLinearQuantization, LearnedClippedLinearQuantization, BinActive]\n",
    "\n",
    "\n",
    "\n",
    "quant_type =  'PerLayerAsymPACT'\n",
    "weight_bits= 8\n",
    "bias_bits = 8\n",
    "batch_fold_type = 'folding_weights', \n",
    "batch_fold_delay = 0 \n",
    "act_bits = 8\n",
    "add_config = [] \n",
    "\n",
    "\n",
    "# generate quantized model\n",
    "deployment_model = copy.deepcopy(model)\n",
    "param_to_quantize = []\n",
    "batch_fold = False\n",
    "\n",
    "print('Batch Folding: ', batch_fold, 'Batch Folding Delay: ', batch_fold_delay, 'Type', batch_fold_type )\n",
    "\n",
    "#start\n",
    "last_layer = None\n",
    "modules_quant = [ ]\n",
    "\n",
    "print(add_config)\n",
    "\n",
    "\n",
    "\n",
    "for name_sub,submodel in model.named_children():\n",
    "\n",
    "    # map the sub-graph \n",
    "    for name, module in submodel.named_modules():\n",
    "        if has_children(module) is False:\n",
    "\n",
    "            layer_quant_descr = {}\n",
    "\n",
    "            if type(module) in [nn.Conv2d, nn.Linear, Conv2d_SAME]:\n",
    "\n",
    "                # saving per-node characteristics\n",
    "                layer_quant_descr['name'] = name\n",
    "                layer_quant_descr['w_bits'] = weight_bits\n",
    "                layer_quant_descr['fold_type'] = batch_fold_type\n",
    "\n",
    "                temp = getattr(submodel, 'quant_type', quant_type)\n",
    "                if temp in _Availble_Quant_Type :\n",
    "                    layer_quant_descr['quant_type'] = temp\n",
    "                else:\n",
    "                    print('Type of quantization not recognized')\n",
    "\n",
    "                layer_quant_descr['conv'] = module\n",
    "                layer_quant_descr['weight'] = module.weight.data.clone()\n",
    "                layer_quant_descr['w_clip'] = None\n",
    "\n",
    "                # import per-layer config from external config file\n",
    "                idx_layer = len(param_to_quantize)\n",
    "                for item_dict in add_config:\n",
    "                    if item_dict['layer'] == idx_layer:\n",
    "                        if 'w_bits' in item_dict.keys():\n",
    "                            layer_quant_descr['w_bits'] = item_dict['w_bits'] \n",
    "\n",
    "                        if 'fold_type' in item_dict.keys():\n",
    "                            layer_quant_descr['fold_type'] = item_dict['fold_type'] \n",
    "\n",
    "                        if 'quant_type' in item_dict.keys():\n",
    "                            layer_quant_descr['quant_type'] = item_dict['quant_type'] \n",
    "\n",
    "                if module.bias is None:\n",
    "                    layer_quant_descr['bias'] = False\n",
    "                else:\n",
    "                    layer_quant_descr['bias'] = module.bias.data.clone()\n",
    "                layer_quant_descr['bias_bits'] = bias_bits\n",
    "                layer_quant_descr['batch_norm'] = None\n",
    "                layer_quant_descr['act'] = None\n",
    "                layer_quant_descr['quant_act'] = None\n",
    "\n",
    "                # append into deployment graph\n",
    "                quant_layer = copy.deepcopy(module)\n",
    "                modules_quant.append(quant_layer)\n",
    "                layer_quant_descr['quant_conv'] = quant_layer\n",
    "\n",
    "                #PACT needs extra parameters for learning quantization range\n",
    "                if layer_quant_descr['quant_type'] == 'PerLayerAsymPACT':\n",
    "                    layer_quant_descr['w_max_thr'] = nn.Parameter( module.weight.data.max().cuda(),requires_grad=True )\n",
    "                    layer_quant_descr['w_min_thr'] = nn.Parameter( module.weight.data.min().cuda(),requires_grad=True )\n",
    "                    layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "                    layer_quant_descr['w_min_thr'].add(1).sum().backward() #fake to create grad\n",
    "\n",
    "                elif layer_quant_descr['quant_type'] == 'PerLayerSymPACT':\n",
    "                    layer_quant_descr['w_max_thr'] = nn.Parameter( module.weight.data.max().cuda(),requires_grad=True )\n",
    "                    layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "\n",
    "                elif layer_quant_descr['quant_type'] == 'PerChannelsAsymMinMax':\n",
    "                    weight = module.weight.data\n",
    "                    max_weight,_ = weight.reshape(weight.size(0), -1).max(1)\n",
    "                    min_weight,_ = weight.reshape(weight.size(0), -1).min(1)\n",
    "                    layer_quant_descr['w_max_thr'] = nn.Parameter( max_weight.cuda(),requires_grad=True )\n",
    "                    layer_quant_descr['w_min_thr'] = nn.Parameter( min_weight.cuda(),requires_grad=True )\n",
    "                    layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "                    layer_quant_descr['w_min_thr'].add(1).sum().backward() #fake to create grad\n",
    "                    layer_quant_descr['w_min_mat'] = None\n",
    "                    layer_quant_descr['w_max_mat'] = None\n",
    "                    if type(module) == nn.Linear:\n",
    "                        print('PerLayerAsymPACT chennel with fixed batch')\n",
    "                        out_ch = weight.size(0)\n",
    "                        quant_act = ScaledClippedLinearQuantizationChannelBias(out_ch, clip_val=False, bias=False)\n",
    "                        layer_quant_descr['quant_act'] = quant_act\n",
    "                        modules_quant.append(quant_act)\n",
    "\n",
    "                # append the latest node\n",
    "                param_to_quantize.append(layer_quant_descr)\n",
    "                last_layer = param_to_quantize[-1]\n",
    "\n",
    "            elif (type(module) in [nn.BatchNorm2d]):\n",
    "                if last_layer is not None:\n",
    "                    last_layer['batch_norm'] = module\n",
    "\n",
    "            elif type(module) in [nn.AvgPool2d]:\n",
    "                modules_quant.append(module) # temporary - this should me merged into or previous layer\n",
    "\n",
    "            elif type(module) in _Supported_Activ_Funct:\n",
    "\n",
    "                # check if a quantized features an activation function\n",
    "                if last_layer is not None:\n",
    "\n",
    "                    last_layer['act'] = module\n",
    "\n",
    "                    # quantized activations\n",
    "                    if type(module) in [ClippedLinearQuantization, LearnedClippedLinearQuantization, QuantActive]:\n",
    "\n",
    "                        act_bits = module.num_bits\n",
    "                        #check if number of bits need to be changed\n",
    "                        for item_dict in add_config:\n",
    "                            if item_dict['layer'] == idx_layer:\n",
    "                                if 'a_bits' in item_dict.keys():\n",
    "                                    act_bits = item_dict['a_bits'] \n",
    "                                    module.num_bits = act_bits                                 \n",
    "\n",
    "                        if last_layer['fold_type'] == 'folding_thresh':\n",
    "                            quant_act = ScaledThresholdsQuantization4d()\n",
    "                            quant_act.num_bits = act_bits\n",
    "                            quant_act.n_thresholds = 2**act_bits -1\n",
    "\n",
    "                        elif last_layer['fold_type'] == 'ICN':\n",
    "                            out_ch = last_layer['conv'].out_channels\n",
    "                            quant_act = ScaledClippedLinearQuantizationChannel(out_ch,clip_val=2**act_bits -1)\n",
    "\n",
    "                        else:\n",
    "                            quant_act = ScaledClippedLinearQuantization(clip_val=2**act_bits -1)\n",
    "\n",
    "                        last_layer['act_o_bits'] = act_bits\n",
    "\n",
    "                    # full-precision activations\n",
    "                    elif  type(module) is nn.ReLU6:\n",
    "                        quant_act = nn.ReLU6(inplace=True)\n",
    "\n",
    "                    # binary acivation\n",
    "                    elif  type(module) is BinActive :\n",
    "                        act_bits = 1\n",
    "                        quant_act = BinActive()\n",
    "\n",
    "                    else:\n",
    "                        print('Supported activation layer but no method is here yet!')\n",
    "\n",
    "                        \n",
    "\n",
    "                    last_layer['quant_act'] = quant_act\n",
    "\n",
    "                    modules_quant.append(quant_act)\n",
    "\n",
    "            else:\n",
    "                print(type(module), 'not supported yet!')\n",
    "\n",
    "        else:\n",
    "            #if type(module) is not nn.Sequential:\n",
    "            last_layer = None\n",
    "\n",
    "    deployment_model._modules[name_sub] = nn.Sequential(*modules_quant)\n",
    "    modules_quant = [  ] \n",
    "\n",
    "\n",
    "n_quantize_layers = len(param_to_quantize)\n",
    "\n",
    "print('This is the quantized network: ')\n",
    "print(deployment_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.0\n",
      "model.0.0\n",
      "model.0.1\n",
      "model.0.2\n",
      "model.1\n",
      "model.1.0\n",
      "model.1.1\n",
      "model.1.2\n",
      "model.2\n",
      "model.2.0\n",
      "model.2.1\n",
      "model.2.2\n",
      "model.3\n",
      "model.3.0\n",
      "model.3.1\n",
      "model.3.2\n",
      "model.4\n",
      "model.4.0\n",
      "model.4.1\n",
      "model.4.2\n",
      "model.5\n",
      "model.5.0\n",
      "model.5.1\n",
      "model.5.2\n",
      "model.6\n",
      "model.6.0\n",
      "model.6.1\n",
      "model.6.2\n",
      "model.7\n",
      "model.7.0\n",
      "model.7.1\n",
      "model.7.2\n",
      "model.8\n",
      "model.8.0\n",
      "model.8.1\n",
      "model.8.2\n",
      "model.9\n",
      "model.9.0\n",
      "model.9.1\n",
      "model.9.2\n",
      "model.10\n",
      "model.10.0\n",
      "model.10.1\n",
      "model.10.2\n",
      "model.11\n",
      "model.11.0\n",
      "model.11.1\n",
      "model.11.2\n",
      "model.12\n",
      "model.12.0\n",
      "model.12.1\n",
      "model.12.2\n",
      "model.13\n",
      "model.13.0\n",
      "model.13.1\n",
      "model.13.2\n",
      "model.14\n",
      "model.14.0\n",
      "model.14.1\n",
      "model.14.2\n",
      "model.15\n",
      "model.15.0\n",
      "model.15.1\n",
      "model.15.2\n",
      "model.16\n",
      "model.16.0\n",
      "model.16.1\n",
      "model.16.2\n",
      "model.17\n",
      "model.17.0\n",
      "model.17.1\n",
      "model.17.2\n",
      "model.18\n",
      "model.18.0\n",
      "model.18.1\n",
      "model.18.2\n",
      "model.19\n",
      "model.19.0\n",
      "model.19.1\n",
      "model.19.2\n",
      "model.20\n",
      "model.20.0\n",
      "model.20.1\n",
      "model.20.2\n",
      "model.21\n",
      "model.21.0\n",
      "model.21.1\n",
      "model.21.2\n",
      "model.22\n",
      "model.22.0\n",
      "model.22.1\n",
      "model.22.2\n",
      "model.23\n",
      "model.23.0\n",
      "model.23.1\n",
      "model.23.2\n",
      "model.24\n",
      "model.24.0\n",
      "model.24.1\n",
      "model.24.2\n",
      "model.25\n",
      "model.25.0\n",
      "model.25.1\n",
      "model.25.2\n",
      "model.26\n",
      "model.26.0\n",
      "model.26.1\n",
      "model.26.2\n",
      "model.27\n",
      "fc\n"
     ]
    }
   ],
   "source": [
    "for item,v in model.named_modules():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type object argument after * must be an iterable, not mobilenet_real",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ab779122673a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: type object argument after * must be an iterable, not mobilenet_real"
     ]
    }
   ],
   "source": [
    "list(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = 'mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.find('[')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_children of mobilenet_real(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (8): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (9): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (10): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (11): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (12): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (13): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (14): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (15): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (16): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (17): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (18): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (19): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (20): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (21): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (22): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (23): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (24): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (25): Sequential(\n",
      "    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (26): Sequential(\n",
      "    (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      ")>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of AvgPool2d(kernel_size=7, stride=7, padding=0)>\n",
      "<bound method Module.named_children of Linear(in_features=1024, out_features=1000, bias=True)>\n"
     ]
    }
   ],
   "source": [
    "for item in model.modules():\n",
    "    print(item.named_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-134-0615a994443e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-134-0615a994443e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    model.model.1.1\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]/237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
